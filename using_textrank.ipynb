{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download('punkt') # one time execution\n",
    "#nltk.download('stopwords')\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTDIR = \"./prepro/text/\"\n",
    "ABSTRACTDIR = \"./prepro/abstract/\"\n",
    "CLEANEDDIR= \"./prepro/cleaned/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files=os.listdir(TEXTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_file = codecs.open(CLEANEDDIR+'1908.07062v3.Recurrent_Neural_Networks_An_Embedded_Computing_Perspective.txt',\"r\",\"utf-8\")\n",
    "curr_file_text = curr_file.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(curr_file_text)\n",
    "clean_sentences = []\n",
    "for s in sentences:\n",
    "    if len(s) >=10:\n",
    "        clean_sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    sen_new = re.sub(\"[^a-zA-Z]\", \" \",sen_new)\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from the sentences\n",
    "clean_sentences2 = clean_sentences.copy()\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction recurrent neural networks rnns class neural networks nns dealing applications sequential data inputs outputs ',\n",
       " 'rnns capture temporal relationship input output sequences introducing feedback feedforward ff neural networks ',\n",
       " 'thus many applications sequential data speech recognition language translation human activity recognition benefit rnns ',\n",
       " 'contrast cloud computing edge computing guarantee better response time enhance security running application ',\n",
       " 'augmenting edge devices rnns grant intelligence process respond sequential problems ',\n",
       " 'realization embedded platforms edge volume devices imposes optimizations rnn applications ',\n",
       " 'embedded platforms time constrained systems suffer limited memory power resources ',\n",
       " 'run rnn applications efficiently embedded platforms rnn applications need overcome restrictions ',\n",
       " 'a  scope article article study rnn models specifically focus rnn optimizations implementations embedded platforms ',\n",
       " 'article compares recent implementations rnn models embedded systems found literature ',\n",
       " 'research paper included comparison satisfy following conditions discusses implementation rnn model rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective figure structure survey article ',\n",
       " 'rnn models run embedded platform edge device ',\n",
       " 'section ii discusses objectives implementation challenges facing it ',\n",
       " 'section iii describes rnn models detail ',\n",
       " 'follows discussion algorithmic optimizations section iv may applied rnn models platform specific optimizations section iv b applied embedded platforms ',\n",
       " 'resulting implementations discussed section v compared objectives section vi ',\n",
       " 'recurrent layer rnn model ',\n",
       " 'target platform embedded platform fpga asic etc ',\n",
       " 'provide complete study survey also addresses methods used optimizing rnn models realizing embedded systems ',\n",
       " 'survey distinguishes related works existing article includes components rnn models optimizations implementations single analysis may seen table  ',\n",
       " 'surveys focus one two aspects compared covered article ',\n",
       " 'articles study rnns algorithmic point view  ',\n",
       " 'another group survey articles looks hardware implementations ',\n",
       " 'example one survey neural networks efficient processing studied cnns cnn optimizations cnn implementations another cnn survey studied cnn mappings fpgas ',\n",
       " 'articles specialized algorithmic optimizations compression  ',\n",
       " 'algorithmic optimizations cnns rnns surveyed one article also discussed implementations  ',\n",
       " 'however main scope article optimizations rnn models components studied ',\n",
       " 'furthermore rnn implementations included limited speech recognition applications tidigits dataset ',\n",
       " 'b  contributions survey article provides following detailed comparison rnn models components computer architecture perspective addresses computational memory requirements ',\n",
       " 'study optimizations applied rnns execute embedded platforms ',\n",
       " 'application independent comparison recent implementations rnns embedded platforms ',\n",
       " 'identification possible opportunities future research ',\n",
       " 'c  survey structure survey article organized shown figure  ',\n",
       " 'section ii defines objectives realizing rnn models embedded platforms challenges faced achieving them ',\n",
       " 'define general model rnn applications discuss different variations recurrent layers rnn models section iii ',\n",
       " 'however difficult run rnn models original form efficiently embedded platforms ',\n",
       " 'therefore researchers applied optimizations rnn model target platform ',\n",
       " 'optimizations applied rnn model called algorithmic optimizations discussed section iv optimizations applied hardware platform called platform specific optimizations discussed section iv b  section v present analysis hardware implementations rnns suggested literature ',\n",
       " 'implementations compared applied optimizations achieved performance ',\n",
       " 'section vi compare implementations analyzed section v objectives defined section ii define gap propose research opportunities fill gap ',\n",
       " 'finally section vii summarize survey ',\n",
       " 'volume rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective table comparison related survey articles ',\n",
       " 'article analysis rnn models components analysis cnn models components algorithmic optimizations platform specific optimizations rnn implementations cnn implementations speech recognition ii ',\n",
       " 'objectives challenges implementation efficiency primary objective implementing rnn applications embedded systems ',\n",
       " 'implementation efficiency requires implementation high throughput low energy consumption meet realtime requirements ',\n",
       " 'secondary objective implementation would flexibility ',\n",
       " 'flexibility requires implementation support variations rnn model allow online training meet different applications requirements ',\n",
       " 'meeting objectives exist challenges mapping applications onto embedded systems large number computations performed within limited available memory ',\n",
       " 'objectives challenges discussed detail below ',\n",
       " 'a  objectives realizing rnns embedded platforms realize rnn models embedded platforms define objectives influence solution ',\n",
       " 'objectives divided implementation efficiency objectives flexibility objectives ',\n",
       " 'implementation efficiency since target embedded platforms consider online execution application ',\n",
       " 'satisfy implementation efficiency objective implementation high throughput low energy consumption meet realtime requirements application ',\n",
       " 'real time requirements application pose additional demands throughput energy consumption accuracy implementation ',\n",
       " 'accuracy indicates correct model performing recognition classification translation etc ',\n",
       " 'high throughput throughput measure performance ',\n",
       " 'measures number processed input output samples per second ',\n",
       " 'application level inputs outputs diverse ',\n",
       " 'image processing applications input frames throughput number consumed frames per second may also depend frame size ',\n",
       " 'speech text applications number predicted words per second ',\n",
       " 'thus different sizes types input outputs throughput different units throughput value may interpreted various ways ',\n",
       " 'compare different applications use number operations per second measure throughput ',\n",
       " 'low energy consumption implementation considered efficient energy consumption volume fpgas article implementation meet embedded platforms energy constraints ',\n",
       " 'compare energy consumption different implementations use number operations per second per watt measure energy efficiency ',\n",
       " 'real time requirements real time implementations response cannot delayed beyond predefined deadline energy consumption cannot exceed predefined limit ',\n",
       " 'deadline defined application affected frequency sensor inputs system response time ',\n",
       " 'normally rnn execution meet predefined deadline ',\n",
       " 'flexibility flexibility solution context ability solution run different models different constraints without restricted one model one configuration ',\n",
       " 'implementation flexible define following requirements satisfied supporting variations rnn layer recurrent layers rnn models vary type layer different types recurrent layer discussed section iii b number hidden cells number recurrent layers ',\n",
       " 'supporting nn layers rnn models types nn layers well ',\n",
       " 'solution supports nn layers considered complete solution rnn models flexible solution ',\n",
       " 'convolution layers fully connected layers pooling layers might required rnn model ',\n",
       " 'supporting algorithmic optimization variations different algorithmic optimizations applied rnn models implement efficiently embedded systems section iv  ',\n",
       " 'supporting least one algorithmic optimization hardware solution many cases mandatory feasible execution rnn models embedded system ',\n",
       " 'combinations optimizations lead higher efficiency flexibility gives algorithmic designer choices optimizing model embedded execution ',\n",
       " 'online training training process sets parameter values within neural network ',\n",
       " 'embedded platforms training performed offline inference run platform run time ',\n",
       " 'real life problems often enough run inference embedded platforms level training required run time well ',\n",
       " 'online training allows rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective neural network adapt new data encountered within training data adapt changes environment ',\n",
       " 'example online training required object recognition autonomous cars achieve lifelong learning continuously receiving new training data fleets robots updating model parameters  ',\n",
       " 'another example automated visual monitoring systems continuously receive new labeled data  ',\n",
       " 'meeting requirements different application domains one aspect flexibility support requirements different application domains ',\n",
       " 'makes implementation attractive solution support wider range applications ',\n",
       " 'however different application domains different performance criteria ',\n",
       " 'application domains autonomous vehicles might require high throughput moderate power consumption others mobile applications require extremely low power consumption less stringent constraints throughput ',\n",
       " 'b  challenges mapping rnns embedded platforms shall take look challenges faced hardware solutions meet objectives discussed above ',\n",
       " 'computation challenges main computation bottleneck rnns matrix vector multiplications ',\n",
       " 'lstm layer explained detail section iii b four computation blocks one matrix vector multiplication ',\n",
       " 'example size vector size matrices matrix vector multiplication requires mac multiply accumulate operations ',\n",
       " 'total number mac operations lstm would  ',\n",
       " 'mega mac approximately equivalent  ',\n",
       " 'high number computations negatively affects throughput implementation energy consumption ',\n",
       " 'one problem rnns recurrent structure rnn ',\n",
       " 'rnns output fed back input way time step computation needs wait previous time step computation complete ',\n",
       " 'temporal dependency makes difficult parallelize implementation time steps ',\n",
       " 'memory challenges memory required matrix vector multiplications large ',\n",
       " 'size access time matrices become memory bottleneck ',\n",
       " 'previous example lstm layer requires four matrices size  ',\n",
       " 'consider bit floating point operations size required memory weights would b  also high number memory accesses affects throughput energy consumption implementation  ',\n",
       " 'accuracy challenges overcome previous two issues computation memory challenges optimizations applied rnn models discussed section iv ',\n",
       " 'optimizations may affect accuracy ',\n",
       " 'acceptable decrease accuracy varies application domain ',\n",
       " 'instance aircraft anomaly detection accepted range data fluctuation  ',\n",
       " 'recurrent neural networks intelligence humans well animals depends memory past ',\n",
       " 'shortterm combining sounds make words longterm example word refer back anne mentioned hundreds words earlier ',\n",
       " 'exactly rnn provides neural networks ',\n",
       " 'adds feedback enables using outputs previous time step processing current time step input ',\n",
       " 'aims add memory cells function similarly human long term shortterm memories ',\n",
       " 'rnns add recurrent layers nn neural network model ',\n",
       " 'figure presents generic model rnns consists three sets layers input recurrent output  ',\n",
       " 'input layers take sensor output convert vector conveys features input ',\n",
       " 'followed recurrent layers provide feedback ',\n",
       " 'recent recurrent layer models memory cells exist well ',\n",
       " 'subsequently model completes similarly nn models fully connected fc layers output layer softmax layer ',\n",
       " 'fc layers output layer grouped set output layers figure  ',\n",
       " 'section discuss input layers different types recurrent layer output layers rnn modes operation deep rnn rnn applications corresponding datasets ',\n",
       " 'figure generic model rnns diverse recurrent layers ',\n",
       " 'a  input layers features extractor corresponding applications datasets input layers needed many implementations prepare sensor output processing may also called feature extraction layers  ',\n",
       " 'often raw sensor data e g ',\n",
       " 'volume rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective audio samples video frames form unsuitable direct processing recurrent layer ',\n",
       " 'also rnn performance learning rate accuracy significantly improved suitable features extracted input layer ',\n",
       " 'sensor types numbers change application rnn models show large variation application types well ',\n",
       " 'thus important study applications rnn model used corresponding datasets ',\n",
       " 'datasets used researchers demonstrate success applying methods modifications them ',\n",
       " 'datasets differ size data samples values data samples total size dataset ',\n",
       " 'success nn models measured accuracy ',\n",
       " 'accuracy indicates correct model carrying recognition classification translation etc ',\n",
       " 'section discuss examples three application domains input layer pre processing used audio video text ',\n",
       " 'table summarize application domains corresponding datasets ',\n",
       " 'different datasets different metrics used assess accuracy ',\n",
       " 'audio inputs audio feature extractors translate sound signals feature vectors ',\n",
       " 'speech processing often want extract frequency content audio signal similar way human ear  ',\n",
       " 'many ways example using short time fourier transform stft mel frequency cepstral coefficients mfcc linear predictive coding lpc coefficients  ',\n",
       " 'applications speech recognition speech recognition applications receive audio input understand translate words ',\n",
       " 'speech recognition used phonetic recognition voice search conversational speech recognition speech text processing  ',\n",
       " 'video inputs input video signal sequence images frames natural use convolutional neural network cnn input layer ',\n",
       " 'cnn layers extract image features video frame feed resulting feature vector recurrent layer ',\n",
       " 'use cnn input layer recurrent layer employed many applications video inputs activity recognition image description video description  ',\n",
       " 'use cnn input layer also found audio signals  ',\n",
       " 'case short segment audio samples transformed frequency domain vector using example stft mfcc ',\n",
       " 'combining number segments spectrogram show information source frequency amplitude time ',\n",
       " 'visual representation fed cnn image ',\n",
       " 'cnn extracts speech audio features suitable recurrent layer ',\n",
       " 'applications image video applications volume image video applications cover application takes images input example image captioning activity recognition video description ',\n",
       " 'text inputs input form text often want represent words vectors word embedding one common way  ',\n",
       " 'word embedding layer extracts features word relation rest vocabulary ',\n",
       " 'output word embedding vector ',\n",
       " 'two words similar contexts distance two vectors short large two words different contexts ',\n",
       " 'following word embedding input layer deeper text analysis natural language processing performed recurrent layers ',\n",
       " 'applications text generation rnn models used language related applications text generation ',\n",
       " 'rnn models predict next words phrase using previous words inputs ',\n",
       " 'sentiment analysis sentiment analysis task understanding underlying opinion expressed words  ',\n",
       " 'since input words comprise sequence rnn methods well suited performing sentiment analysis ',\n",
       " 'b  recurrent layers section cover various types recurrent layers ',\n",
       " 'layer discuss structure layer gate equations ',\n",
       " 'popular recurrent layer long short term memory lstm  ',\n",
       " 'changes proposed lstm enhance algorithmic efficiency improve computational complexity ',\n",
       " 'enhancing algorithmic efficiency means improving accuracy achieved rnn model includes lstm peepholes convlstm discussed sections iii b iii b  ',\n",
       " 'improving computational complexity means reducing number computations amount memory required lstm run efficiently hardware platform ',\n",
       " 'techniques include lstm projection gru qrnn sru discussed sections iii b iii b iii b respectively ',\n",
       " 'changes applied gate equations interconnections even number gates ',\n",
       " 'finally compare different layers number operations number parameters table  ',\n",
       " 'lstm first explain lstm long short term memory layer ',\n",
       " 'looking lstm black box input lstm vector combination input vector xt previous time step output vector ht output vector time denoted ht  ',\n",
       " 'looking structure lstm rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective table rnn input layer types corresponding application domains datasets ',\n",
       " 'input type applications audio input speech recognition video input image video applications text input text generation sentiment analysis dataset tidigits timit wall street journal wsj librispeech asr corpus coco moving mnist comma ai driving dataset penn treebank ptb wikitext text wmt imdb memory cell state ct three gates ',\n",
       " 'gates control forgotten updated memory state forget input gates  ',\n",
       " 'also control part memory state used output output gate  ',\n",
       " 'description lstm unit based relationship hardware implementations ',\n",
       " 'thus figure show lstm four blocks instead three gates lstm composed four similar computation blocks ',\n",
       " 'computation block matrix vector multiplication combination xt ht one weight matrices wf wi wc wo  ',\n",
       " 'considered dominant computational task lstms ',\n",
       " 'block composed matrix vector multiplication followed addition bias vector bf bi bc bo application nonlinear function ',\n",
       " 'block might element wise multiplication operations well ',\n",
       " 'nonlinear functions used lstm tanh sigmoid functions ',\n",
       " 'four computation blocks follow forget gate role forget gate decide information forgotten ',\n",
       " 'forget gate output ft calculated ft wf ht xt bf xt input vector ht hidden state output vector wf weight matrix bf bias vector sigmoid function ',\n",
       " 'input gate role input gate decide information renewed ',\n",
       " 'input gate output computed similarly forget gate output wi ht xt bi using weight matrix wi bias vector bi  ',\n",
       " 'state computation role computation compute new memory state ct lstm cell ',\n",
       " 'first computes possible values new state et tanh wc ht xt bc c xt input vector ht hidden state output vector wc weight matrix bc bias vector ',\n",
       " 'new state vector ct calculated accuracy measure metric word error rate wer lower better phone error rate per lower better bleu higher better cross entropy loss lower better rms prediction error lower better perplexity per word ppw lower better bilingual evaluation understudy bleu higher better testing accuracy higher better addition previous state vector ct elementwise multiplied forget gate output vector ft et element wise new state candidate vector c multiplied input gate output vector ct ft ct et c used denote element wise multiplication ',\n",
       " 'output gate role output gate compute lstm output ',\n",
       " 'first output gate vector ot computed ot wo ht xt bo xt input vector ht hidden state output vector wo weight matrix bo bias vector sigmoid function ',\n",
       " 'hidden state output ht computed applying elementwise multiplication output gate vector ot holds decision part state output tanh state vector ct ht ot tanh ct  ',\n",
       " 'number computations parameters lstm shown table  ',\n",
       " 'matrix vector multiplications dominate number computations parameters ',\n",
       " 'matrix vector multiplication input vector xt size hidden state output vector ht size n multiplied weight matrices size n n  requires n n mac operations equivalent nm n multiplications nm n additions ',\n",
       " 'number parameters weight matrices nm n well ',\n",
       " 'since computation repeated four times within lstm computation numbers multiplied four total number operations parameters lstm ',\n",
       " 'models studied papers n larger m  thus n dominating effect computational complexity lstm ',\n",
       " 'lstm peepholes peephole connections added lstms make able count measure time events  ',\n",
       " 'seen figure b output state computation volume rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective long short term memory lstm  ',\n",
       " 'b lstm peepholes ',\n",
       " 'c lstm projection layer ',\n",
       " 'gated recurrent unit gru  ',\n",
       " 'e quasi rnn qrnn  ',\n",
       " 'f simple recurrent unit sru  ',\n",
       " 'figure different variations rnn layer ',\n",
       " 'volume rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective used input three gates ',\n",
       " 'lstm gate equations changed ft wf ht xt ct bf wi ht xt ct bi ot wo ht xt ct bo  ',\n",
       " 'xt input vector ht hidden state output vector ct state vector time wf wi wo weight matrices bf bi bo bias vectors ',\n",
       " 'number operations computations lstm peepholes shown table  ',\n",
       " 'exist two rows lstm peepholes ',\n",
       " 'first one considers multiplication cell state three gates matrix vector multiplication ',\n",
       " 'number multiplications additions weights increases n  ',\n",
       " 'however weight matrices multiplied cell state diagonal matrices  ',\n",
       " 'thus matrix vector multiplication considered element wise vector multiplication become widely used lstm peepholes ',\n",
       " 'case number multiplications additions weights increase n only ',\n",
       " 'convlstm convlstm lstm matrix vector multiplications replaced convolutions  ',\n",
       " 'idea input lstm data holds spatial relations visual frames better apply convolutions matrix vector multiplications ',\n",
       " 'convolution capable extracting spatial information data ',\n",
       " 'vectors xt ht ct replaced tensors ',\n",
       " 'one think element lstm vectors frame convlstm vectors ',\n",
       " 'convolution weights need less memory vector matrices weights ',\n",
       " 'however using involves computation ',\n",
       " 'number operations parameters required convlstm shown table  ',\n",
       " 'calculated numbers convlstm without peepholes ',\n",
       " 'peepholes added number multiplications additions weights increase n  since main change lstm replacement matrix vector multiplications convolutions change number operations parameters would via nm n factor appears multiplications additions number weight equations ',\n",
       " 'number multiplications additions macs convolutions input vector xt hidden state output vector ht rcnmki rcn ks r number rows c number columns frames n number frames input xt number frames output ht number hidden cells ki size filter used xt ks size filter used ht  ',\n",
       " 'number weights size filters used convolutions ',\n",
       " 'lstm projection layer lstm changed adding one extra step last gate  ',\n",
       " 'step called projection layer ',\n",
       " 'output projection layer output lstm feedback input lstm next time step shown figure c  simply projection layer like fc layer ',\n",
       " 'purpose layer allow increase number hidden cells controlling total number parameters ',\n",
       " 'performed using projection layer number units p less number hidden cells ',\n",
       " 'dominating factor number computations number weights pn instead n n number hidden cells p size projection layer ',\n",
       " 'since p n n increase smaller effect size model number computations ',\n",
       " 'table show number operations parameters required lstm projection layer ',\n",
       " 'original paper proposing projection layer authors considered output layer rnn part lstm  ',\n",
       " 'output layer fc layer changes size output vector output size ',\n",
       " 'thus extra po term number multiplications additions weights ',\n",
       " 'put extra terms curly brackets show optional terms ',\n",
       " 'projection layer applied lstm peepholes well ',\n",
       " 'table show number operations parameters lstm peepholes projection layer ',\n",
       " 'gru gated recurrent unit gru proposed  ',\n",
       " 'main purpose make recurrent layer able capture dependencies different time scales adaptive manner  ',\n",
       " 'however fact gru two gates three computational blocks instead three four computational blocks lstm makes computationally efficient promising highperformance hardware implementations ',\n",
       " 'three computational blocks follows reset gate reset gate used decide whether use previously computed output treat input first symbol sequence ',\n",
       " 'reset gate output vector rt computed rt wr ht xt xt input vector ht hidden state output vector wr weight matrix sigmoid function ',\n",
       " 'update gate update gate decides much output updated ',\n",
       " 'output update gate zt computed reset gate output rt using weight matrix wz zt wz ht xt  ',\n",
       " 'volume rnn layer lstm lstm peepholes lstm peepholes diagonalized lstm projection lstm peepholes diagonalized projection multiplications n nm n lst mmul n nm n lst mmul n n nm n lst mmul n np nm n np po lst p rojmul np nm n np po lst p rojmul n number operations additions n nm n lst madd n nm n lst madd n n nm n lst madd n np nm n np po lst p rojadd np nm n np po lst p rojadd n convlstm rcnmki rcn ks n gru n nm n  ',\n",
       " 'lst mmul rcnmki rcn ks n n nm n  ',\n",
       " 'lst madd qrnn sru knm n nm n knm n nm n nonlinear n lst mnonlinear n lst mnonlinear n lst mnonlinear n lst mnonlinear n number parameters weights biases n nm n lst mweights lst mbiases n nm n lst mweights n lst mbiases n nm n n lst mweights n lst mbiases np nm np po n lst p rojweights np nm n np po lst p rojweights n lst mbiases n n nmki n ks n n  ',\n",
       " 'lst mnonlinear n n n nm  ',\n",
       " 'lst mweights knm nm n n lst mnonlinear lst mbiases table use following symbols size input vector xt n number hidden cells ht p size projection layer size output layer r number rows frame c number columns frame ki size filter applied xt ks size filter applied ht k size convolution filter ',\n",
       " 'term po optional term discussed section iii b  ',\n",
       " 'rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective volume table comparing lstm variations ',\n",
       " 'rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective output computation role block compute hidden state vector ht  ',\n",
       " 'first computes possible values hidden state vector e ht e ht tanh w rt ht xt xt input vector ht hidden state output vector w weight matrix ',\n",
       " 'hidden state vector ht computed old output ht new possible output e ht ht zt ht zt e ht  ',\n",
       " 'lstm visualize gru figure three blocks two gates three blocks matrix vector multiplications ',\n",
       " 'table show number operations parameters required gru ',\n",
       " 'number operations parameters approximately  ',\n",
       " 'number operations parameters lstm ',\n",
       " 'qrnn sru purpose quasi rnn qrnn simple recurrent unit sru make recurrent unit friendlier computation parallelization ',\n",
       " 'bottleneck lstm gru matrix vector multiplications ',\n",
       " 'difficult parallelize part depends previous time step output ht previous time step state ct  ',\n",
       " 'qrnn sru ht ct removed matrix vector multiplications appear elementwise operations ',\n",
       " 'qrnn two gates memory state ',\n",
       " 'three heavy computational blocks ',\n",
       " 'blocks input vector xt used input ',\n",
       " 'replaces matrix vector multiplications convolutions inputs along time step dimension ',\n",
       " 'instance filter dimension two convolution applied xt xt  ',\n",
       " 'three computation blocks compute forget gate vector et output gate ft candidate new state vector c vector ot ft wf xt et tanh wc xt c ot wo xt ft wf xt vf ct bf rt wr xt vr ct br respectively ',\n",
       " 'gate calculations ct used element wise multiplications ',\n",
       " 'parameter vectors vf vr learned weight matrices biases training ',\n",
       " 'third computational block state computation ct ct ft ct ft w xt ct old state vector xt input vector ',\n",
       " 'computation controlled forget gate output vector ft decides forgotten treated new ',\n",
       " 'finally sru output ht computed new state ct input vector xt checked update gate decides parts output taken new state parts taken input using equation ht rt ct rt xt  ',\n",
       " 'figure f visualizes sru ',\n",
       " 'output computation performed block update gate ',\n",
       " 'worth observing neither qrnn sru ht used equations old state ct used ',\n",
       " 'number operations parameters sru shown table  ',\n",
       " 'table compare lstm variations memory requirements weights number computations per single time step ',\n",
       " 'comparison helps understand required hardware platform them ',\n",
       " 'make easier reader understand difference lstm variants show equations operations parameters terms lstm operations parameters comparable ',\n",
       " 'c  output layers output layers rnn model fc layers output function ',\n",
       " 'fc fully connected layers wf wc wo convolution filter banks denote convolution operation ',\n",
       " 'state vector ct computed ct ft ct ft et c hidden state vector ht computed using equation  ',\n",
       " 'figure e used visualize qrnn layer ',\n",
       " 'number operations parameters required qrnn shown table k size convolution filter ',\n",
       " 'sru two gates memory state well ',\n",
       " 'heavy computational blocks three blocks matrix vector multiplications convolutions ',\n",
       " 'two gates forget update gates computed using equations rnn model might one fc layers recurrent layers ',\n",
       " 'non linear functions may applied fc layers well ',\n",
       " 'called fully connected neuron input connected neuron output ',\n",
       " 'computationally done matrix vector multiplication using weight matrix size inputsize outputsize inputsize size input vector outputsize size output vector ',\n",
       " 'one purpose fc layer rnn models change dimension hidden state output vector ht dimension rnn model output prepare output function ',\n",
       " 'case fc layer might replaced adding projection layer recurrent layer ',\n",
       " 'volume rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective output function output function final step neural networks inference ',\n",
       " 'generates output neural network model ',\n",
       " 'output prediction classification recognition on ',\n",
       " 'example text prediction problem softmax function used output function ',\n",
       " 'output vector probabilities sum one ',\n",
       " 'probability corresponds one word ',\n",
       " 'word highest probability becomes prediction neural network  ',\n",
       " 'd  processing data rnn models many ways processing data may vary rnn models ',\n",
       " 'first vary way time steps treated ',\n",
       " 'influenced nature application may inputs temporal relations outputs temporal relations both ',\n",
       " 'second form variation related bidirectional rnns ',\n",
       " 'discuss bidirectional rnn process inputs forwards backwards time ',\n",
       " 'also discuss meant deep rnn model ',\n",
       " 'analysis two examples ',\n",
       " 'activity recognition model takes sequence images input determines activity taking place images ',\n",
       " 'sentiment analysis model takes sequence words sentence input generates single emotion end ',\n",
       " 'case temporal sequence sequence input ',\n",
       " 'many many many many model sequence input sequence output shown figure c  language translation video description two examples ',\n",
       " 'language translation model sequence words sentence input sequence words sentence output ',\n",
       " 'video description applications model sequence image frames input sequence words sentence output ',\n",
       " 'one one rnn model one one unrolling ',\n",
       " 'one one simply means temporal relation contained inputs outputs feedforward neural network  ',\n",
       " 'rnn unfolding variations time steps bidirectional rnn rnn unfolding unrolling performed reveal repetition recurrent layer show number time steps required complete task ',\n",
       " 'unfolding rnn illustrates different types rnn models one meet ',\n",
       " 'one many one many model generates sequence outputs single input shown figure a  image captioning one example  ',\n",
       " 'model takes one image input generates sentence output ',\n",
       " 'words sentence compose sequence temporally related data ',\n",
       " 'case temporal sequence output ',\n",
       " 'bidirectional rnn input fed recurrent layer two directions past future future past ',\n",
       " 'requires duplication recurrent layer two recurrent layers work simultaneously processing input different temporal direction ',\n",
       " 'help network better understand context obtaining data past future time ',\n",
       " 'concept applied different variations recurrent layers bilstm bigru  ',\n",
       " 'one many rnn ',\n",
       " 'b many one rnn ',\n",
       " 'c many many rnn ',\n",
       " 'figure unfolding rnn models multiple time steps ',\n",
       " 'many one many one model combines sequence inputs generate single output shown figure b  activity recognition sentiment volume e  deep recurrent neural networks drnn making neural network deep neural network achieved adding non linear layers input layer output layer  ',\n",
       " 'straightforward feedforward nns ',\n",
       " 'however rnns different approaches adopted ',\n",
       " 'similarly feedforward nns stack recurrent layers stacked rnn shown figure stack two recurrent layers ',\n",
       " 'output first layer considered input second layer ',\n",
       " 'alternately extra non linear layers within recurrent layer computations  ',\n",
       " 'extra non linear layers embedded within hidden layer vector ht calculation xt ht vectors used calculate ht pass additional non linear layers ',\n",
       " 'model called deep transition rnn model ',\n",
       " 'extra non linear layers also added computing output hidden state vector model called deep output rnn model ',\n",
       " 'possible rnn model deep transition deep output rnn model  ',\n",
       " 'one way extra non linear functions within recurrent layer within gate calculations method called h lstm hidden lstm  ',\n",
       " 'rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective optimization method figure stacked rnn ',\n",
       " 'first layer output h second layer output h  ',\n",
       " 'optimizations rnns neural network applications rnn applications based upon intensive operations performed high precision values ',\n",
       " 'therefore require high computation power large memory bandwidth high energy consumption ',\n",
       " 'resource constraints embedded platforms need decrease computation memory requirements rnn applications ',\n",
       " 'section present optimizations applied rnns realize embedded systems ',\n",
       " 'section v follows discuss hardware implementations rnns embedded platforms relate optimizations presented here ',\n",
       " 'researchers working two types optimizations ',\n",
       " 'first type related rnn algorithms rnn algorithms modified decrease computation memory requirements ',\n",
       " 'modification effect limited effect accuracy ',\n",
       " 'second type optimization related embedded platform hardware improvements applied increase parallelization application decrease overhead memory accesses ',\n",
       " 'figure illustrates two types optimizations ',\n",
       " 'a  algorithmic optimizations section discuss different algorithmic optimizations may performed recurrent layer rnn application decrease computation memory requirements ',\n",
       " 'discuss optimizations carried affect accuracy ',\n",
       " 'applying optimizations directly inference unacceptable effects accuracy ',\n",
       " 'thus training network would required enhance accuracy ',\n",
       " 'optimizations may applied model main training model trained model retrained epochs training cycles  ',\n",
       " 'different datasets measure accuracy using different units ',\n",
       " 'units higher values better others lower values better ',\n",
       " 'provide unified measure change accuracy calculate percentage change accuracy original value value applying va vb vb effect optimization method accuracy percentage original accuracy value vb value accuracy optimization va value accuracy optimization indicator value higher accuracy values better lower accuracy values better ',\n",
       " 'thus baseline accuracy achieved original model without optimizations accuracy optimization effect optimization accuracy  ',\n",
       " 'accuracy optimization effect optimization accuracy  ',\n",
       " 'optimization effect accuracy effect accuracy  ',\n",
       " 'shown figure algorithmic optimizations quantization compression deltarnn nonlinear ',\n",
       " 'first three optimizations applied matrix vector multiplications operations last applied computation non linear functions ',\n",
       " 'table figure compares quantization compression deltarnn effect memory requirements number memory accesses number computations mac operation cost ',\n",
       " 'mac operation cost decreased decreasing precision operands ',\n",
       " 'quantization quantization reduction precision operands ',\n",
       " 'quantization applied network parameters activations inputs well ',\n",
       " 'discussing quantization three important factors consider ',\n",
       " 'first number bits used weights biases activations inputs ',\n",
       " 'second quantization method ',\n",
       " 'quantization method defines store full precision values lower number bits ',\n",
       " 'third discussing whether quantization applied training outset model trained applying quantization ',\n",
       " 'three factors affect accuracy ',\n",
       " 'however factors affecting accuracy may also affected model architecture dataset factors ',\n",
       " 'yet three factors relevance applying quantization rnn model ',\n",
       " 'discussing quantization methods cover fixed point quantization multiple binary codes quantizations exponential quantization ',\n",
       " 'also study whether selection quantized value deterministic stochastic ',\n",
       " 'deterministic methods selection based static thresholds ',\n",
       " 'contrast selection stochastic methods relies probabilities random numbers ',\n",
       " 'relying random numbers difficult hardware ',\n",
       " 'quantized values representation different methods representing quantized values ',\n",
       " 'following explain three commonly used methods ',\n",
       " 'volume rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective figure optimizations applied rnn applications section numbers indicated comparing effect different algorithmic optimizations memory computation requirements ',\n",
       " 'fixed point quantization quantization method bit floating point values quantized fixed point representation notated qm f number integer bits f number fractional bits ',\n",
       " 'total number bits required k  sign bit may included number integer bits added extra bit added f  ',\n",
       " 'example first case q  ',\n",
       " 'used represent bits fixed point three values  ',\n",
       " 'quantization method also called pow ternarization  ',\n",
       " 'usually fixed point quantization deterministic floatingpoint value one quantized fixed point value defined equation i e ',\n",
       " 'rule based  ',\n",
       " 'fixed point quantization performed clipping floating point value minimum maximum boundaries rounding it ',\n",
       " 'exponential quantization exponential quantization quantizes value integer power two ',\n",
       " 'exponential quantization beneficial hardware multiplying exponentially quantized value equivalent shift operations second operand fixed point value addition exponent second operand floating point value  ',\n",
       " 'exponential quantization deterministic stochastic ',\n",
       " 'binary multi bit codes quantization lowest precision rnns binary precision  ',\n",
       " 'full precision value quantized one two values ',\n",
       " 'common two values also  ',\n",
       " 'combination two values  ',\n",
       " 'binarization deterministic stochastic ',\n",
       " 'deterministic binarization sign function used binarization ',\n",
       " 'stochastic binarization selection thresholds depend volume probabilities compute quantized value probability p h x xb probability p h hard sigmoid function defined x x h x clip max min  ',\n",
       " 'binarization great value hardware computation turns multiplication addition subtraction ',\n",
       " 'greatest value comes full binarization weights activations binary precision ',\n",
       " 'case possible concatenate weights activations bit operands multiple mac operations using xnor bit count operations ',\n",
       " 'full binarization reduce memory requirements factor decrease computation time considerably  ',\n",
       " 'adding one value binary precision called ternarization ',\n",
       " 'weights ternarized nn restricted three values ',\n",
       " 'three values  ',\n",
       " 'power two ternarization discussed form fixed point quantization example ternarization three different values  ',\n",
       " 'deterministic stochastic ternarization applied rnns  ',\n",
       " 'four possible quantization values called quaternarization ',\n",
       " 'quaternarization possible values  ',\n",
       " 'order benefit high computational benefit binary weights activations using higher number bits multiple binary codes used quantization  ',\n",
       " 'example two bit quantization four possible values  ',\n",
       " 'common method deterministic quantization uniform quantization ',\n",
       " 'uniform quantization may rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective best quantization method change distribution original data especially nonuniform data affect accuracy ',\n",
       " 'one solution balanced quantization  ',\n",
       " 'balanced quantization data divided groups amount data quantization ensure balanced distribution data following quantization ',\n",
       " 'suggested solutions treat quantization optimization problem include greedy quantization refined greedy quantization alternating multi bit quantization  ',\n",
       " 'b training retraining mentioned earlier three options minimize accuracy loss due quantization ',\n",
       " 'first apply quantization training quantized weights used forward backward propagation only ',\n",
       " 'full precision weights used parameters update step stochastic gradient descent sgd ',\n",
       " 'copies quantized full precision weights kept decide inference time one use  ',\n",
       " 'second approach quantization applied pretrained parameters rnn model retrained decrease accuracy loss ',\n",
       " 'also binarization lstm gate outputs training applied using gumbelsoftmax estimator  ',\n",
       " 'authors one rnn implementation adopted mix training retraining approaches activations quantized beginning ',\n",
       " 'activations quantized training model retrained epochs ',\n",
       " 'third approach use quantized parameters without training retraining ',\n",
       " 'commonly used bit fixed point quantization ',\n",
       " 'usually training happens training servers quantization applied inference platform without opportunity retrain model ',\n",
       " 'common well use bit fixed point quantization optimization techniques circulant matrices compression pruning deltarnn discussed later section iv  ',\n",
       " 'c effect accuracy table list studies included experiments quantization rnn models ',\n",
       " 'studies hardware implementation purpose show quantization performed keeping accuracy high ',\n",
       " 'table put three factors affecting accuracy discussed earlier number bits quantization method training addition type recurrent layer lstm gru    dataset ',\n",
       " 'show effect quantization accuracy computed respect accuracy achieved full precision parameters activation using eq ',\n",
       " 'number bits use w w number bits used weights number bits used activations ',\n",
       " 'rnn type put recurrent layers used experiments ',\n",
       " 'recurrent layers explained section iii ',\n",
       " 'use x z x number layers type layers z number hidden cells layer ',\n",
       " 'training quantization applied training beginning write training  ',\n",
       " 'quantization applied training model later retrained write retraining  ',\n",
       " 'positive values accuracy means quantization enhanced accuracy negative values accuracy means quantization caused model less accurate ',\n",
       " 'experiment table applied different model different dataset may also used different training methods ',\n",
       " 'thus conclusions accuracy table cannot generalized ',\n",
       " 'still make observations fixed point quantization exponential quantization mixed quantization negative effect accuracy ',\n",
       " 'accuracy increased applying quantization methods ',\n",
       " 'quantized models surpass baseline models accuracy weight quantization regularization effect overcomes fitting  ',\n",
       " 'regarding binary quantization negative effect accuracy varied within small ranges experiments  ',\n",
       " 'experiments showed using bits activations may enhance accuracy  ',\n",
       " 'using binary weights convlstm solely responsible poor accuracy obtained ternary quaternary quantization resulted poor accuracy convlstm well  ',\n",
       " 'however quantization methods successful applied lstm gru work  ',\n",
       " 'compression compression decreases model size decreasing number parameters connections ',\n",
       " 'number parameters reduced memory requirements number computations decrease ',\n",
       " 'table compares different compression methods ',\n",
       " 'compression ratio shows ratio number parameters models applying compression methods ',\n",
       " 'accuracy degradation computed using eq ',\n",
       " 'pruning pruning process eliminating redundancy ',\n",
       " 'computations rnns mainly dense matrix operations ',\n",
       " 'improve computation time dense matrices transformed sparse matrices affects accuracy ',\n",
       " 'however careful choice method used transform dense matrix sparse matrix may result limited impact accuracy providing significant gains computation time ',\n",
       " 'reduction memory footprint along computation optimization essential making rnns viable ',\n",
       " 'however pruning results two undesirable effects ',\n",
       " 'first loss regularity memory organization due sparsification dense matrix second loss accuracy account removal weights nodes model consideration ',\n",
       " 'transformation regular matrix computation irregular application often results use additional hardware computation time manage data ',\n",
       " 'compensate volume rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective table effect quantization methods accuracy ',\n",
       " 'method fixed point exponential mixed binary ternary quaternary multi binary w p real eq real eq fixed b real b real b b b real real real real q real q real rnn type bilstm bilstm gru bilstm gru convlstm bilstm bilstm gru gru convlstm gru gru convlstm lstm lstm lstm training training training training retraining training training training training training training training training training training retraining retraining training accuracy  ',\n",
       " 'paper accuracy also affected compression scheme nonlinear functions approximation used work ',\n",
       " 'calculate error tenth frame third predicted frame  ',\n",
       " 'table used symbols w number bits weights number bits activations p power two ternarization eq exponential quantization b binary quantization ternary quantization q quaternary quantization ',\n",
       " 'loss accuracy caused pruning various methods including retraining applied ',\n",
       " 'following sections describe methods pruning compensation techniques found literature ',\n",
       " 'table summarizes methods pruning impact sparsity accuracy ',\n",
       " 'sparsity context refers number empty entries matrices ',\n",
       " 'table sparsity indicates impact number entries eliminated method pruning used ',\n",
       " 'within rnns pruning classified either magnitude pruning weight matrix sparsification structurebased pruning ',\n",
       " 'magnitude pruning magnitude pruning relies eliminating weight values certain threshold ',\n",
       " 'method choice threshold crucial minimize negative impact accuracy ',\n",
       " 'magnitude pruning primarily based identifying correct threshold pruning weights ',\n",
       " 'weight sub groups weight matrix sparsification rnn model trained eliminate redundant weights retain weights necessary ',\n",
       " 'three categories create weight subgroups select pruning threshold  ',\n",
       " 'three categories class blind class uniform classdistribution ',\n",
       " 'class blind x weights lowest magnitude pruned regardless blind class ',\n",
       " 'class uniform lower pruning x weights uniformly performed classes ',\n",
       " 'class distribution weights within standard deviation class pruned ',\n",
       " 'hard thresholding identifies correct threshold value preserves accuracy ',\n",
       " 'ese uses hard thresholding training learn volume dataset ocr dataset wsj tidigits imdb moving mnist ocr dataset ocr dataset tdigits imdb moving mnist tdigits imdb moving mnist wikitext wikitext ptb weights contribute prediction accuracy ',\n",
       " 'gradual thresholding method uses set weight masks monotonically increasing threshold ',\n",
       " 'weight multiplied corresponding mask ',\n",
       " 'process iterative masks updated setting parameters lower threshold zero ',\n",
       " 'result technique gradually prunes weights introduced within training process contrast hard thresholding ',\n",
       " 'block pruning block pruning magnitude thresholding applied blocks matrix instead individual weights training ',\n",
       " 'weight maximum magnitude used representative entire block ',\n",
       " 'representative weight current threshold elements blocks set zero ',\n",
       " 'result block sparsification mitigates indexing overhead irregular memory accesses incompatibility array data paths characterises unstructured random pruning ',\n",
       " 'grow prune grow prune combines gradient based growth magnitude based pruning connections ',\n",
       " 'training starts randomly initialized seed architecture ',\n",
       " 'next growth phase new connections neurons feature maps added based average gradient entire training set ',\n",
       " 'required accuracy reached redundant connections neurons eliminated based magnitude pruning ',\n",
       " 'structure pruning modifying structure network eliminating nodes connections termed structure pruning ',\n",
       " 'connections may important learned training phase pruned using probability based techniques ',\n",
       " 'rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective network sparsification pruning network sparsification introduces sparsity connections every neuron output output number inputs ',\n",
       " 'furthermore optimization strategy formulated replaces nonzero elements row highest absolute value ',\n",
       " 'step avoids retraining may compute intensive difficult privacy critical applications ',\n",
       " 'however impact method pruning accuracy directly measured ',\n",
       " 'design space exploration different levels sparsity measures quality output gives indication relationship level approximation application level accuracy ',\n",
       " 'drop deepiot compresses neural network structures smaller dense matrices finding minimum number non redundant hidden elements without affecting performance network ',\n",
       " 'lstm networks bernoulli random probabilities used dropping hidden dimensions used within lstm blocks ',\n",
       " 'retaining accuracy levels pruning alongside training retraining employed retain accuracy levels pruned models ',\n",
       " 'retraining works pruned weights pruned model convergence specified level accuracy achieved ',\n",
       " 'pruning shown regularization effect retraining phase  ',\n",
       " 'regularization effect might reason outperforming baseline model accuracy ',\n",
       " 'another benefit pruning might reason outperforming baseline accuracy pruning allows finding better local minimum ',\n",
       " 'pruning increases loss function immediately results gradient descent ',\n",
       " 'handling irregularity pruned matrices pruning maximize sparsity results loss regularity structure memory organization due sparsification original dense matrix ',\n",
       " 'pruning techniques architecture agnostic mainly result unstructured irregular sparse matrices ',\n",
       " 'methods load balancingaware pruning block pruning explained earlier within magnitude pruning applied minimize effects ',\n",
       " 'load balancing aware pruning works towards ensuring sparsity ratio among pruned sub matrices thereby achieving even distribution non zero weights ',\n",
       " 'techniques introduce regularity sparse matrix improve performance avoid index tracking ',\n",
       " 'ii structured matrices circulant matrices circulant matrix matrix column row cyclic shift preceeding column row  ',\n",
       " 'considered special case toeplitz like matrices ',\n",
       " 'weight matrices reorganized circular matrices ',\n",
       " 'redundancy values matrices reduces space complexity weights matrices ',\n",
       " 'large matrices circulant matrices use nearly less memory space ',\n",
       " 'back propagation algorithm modified allow training weights form circulant matrices ',\n",
       " 'block circulant matrices instead transforming weight matrix circulant matrix transformed set circulant sub matrices  ',\n",
       " 'figure shows weight matrix parameters ',\n",
       " 'block size circular sub matrices  ',\n",
       " 'weight matrix transformed two circulant sub matrices parameters parameters  ',\n",
       " 'compression ratio block size ',\n",
       " 'thus larger block sizes result higher reduction model size ',\n",
       " 'however high compression ratio may degrade prediction accuracy ',\n",
       " 'addition fast fourier transform fft algorithm used speed computations ',\n",
       " 'consequently computational complexity decreases factor logk k  ',\n",
       " 'figure regular weight matrix transformed blockcirculant sub matrices block size  ',\n",
       " 'iii tensor decomposition tensors multidimensional arrays ',\n",
       " 'vector tensor rank one matrix tensor rank two on ',\n",
       " 'tensors decomposed lower ranks tensors tensor operations approximated using decompositions order decrease number parameters nn model ',\n",
       " 'canonical polyadic cp decomposition tucker decomposition tensor train decomposition techniques used apply tensor decomposition  ',\n",
       " 'tensor decomposition techniques applied fc layers convolution layers recurrent layers  ',\n",
       " 'table show example applying tensor decomposition gru layer using cp technique ',\n",
       " 'another example adam algorithm used optimizer training process  ',\n",
       " 'tensor decomposition techniques achieve high compression ratio compared compression methods ',\n",
       " 'volume method magnitude pruning structured pruning structured matrices tensor decomp ',\n",
       " 'knowledge distillation technique rnn type dataset compression ratio sparsity pruning training accuracy paper weight subgroups wmt hard thresholding gradual pruning block pruning grow prune network sparsification drop lstm lstm lstm lstm bilstm h lstm lstm bilstm retraining  ',\n",
       " 'none training training training none none  ',\n",
       " 'bilstm timit ptb speech data coco coco librispeech asr corpus circulant nearly training  ',\n",
       " 'block circulant cp lstm gru timit nottingham  ',\n",
       " 'training training  ',\n",
       " 'plain pruning lstm lstm wmt wmt training training retraining  ',\n",
       " 'h lstm hidden lstm ',\n",
       " 'non linear layers added gate computations explained section iii  ',\n",
       " 'dataset name mentioned paper ',\n",
       " 'accuracy also affected quantization table nonlinear functions approximation used work ',\n",
       " 'table effect deltarnn method accuracy rnn model gru cnn gru dataset tidigits open driving training training training accuracy  ',\n",
       " 'speedup  ',\n",
       " 'paper rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective volume table effect compression methods accuracy ',\n",
       " 'rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective iv knowledge distillation knowledge distillation method replaces large model smaller model behave like large model ',\n",
       " 'starting large model teacher trained parameters dataset small model student trained behave like large model  ',\n",
       " 'addition knowledge distillation pruning applied resulted model increase compression ratio shown table  ',\n",
       " 'deltarnn delta recurrent neural networks deltarnn makes use temporal relation input sequences ',\n",
       " 'two consecutive input vectors xt xt difference corresponding values two vectors may zero close zero ',\n",
       " 'holds hidden state output vector ',\n",
       " 'idea skip computations input hidden state values compared input hidden state values previous time step difference less predefined threshold called delta  ',\n",
       " 'improvement comes decreasing number computations number memory accesses required recurrent unit ',\n",
       " 'however memory requirements decrease still need store weights cannot predict computations skipped ',\n",
       " 'value delta threshold affects accuracy speedup ',\n",
       " 'table summarize effect deltarnn accuracy two different datasets ',\n",
       " 'occasions required train rnn using delta algorithm inference obtain better accuracy inference time ',\n",
       " 'furthermore speedup gained delta algorithm one delta value static ',\n",
       " 'depends relation input sequences ',\n",
       " 'highest speedup could reached using video frames open driving dataset input data seen table  ',\n",
       " 'however time consuming cnn recurrent layer negated speedup gained deltarnn ',\n",
       " 'thus x speedup gru execution dropped non significant speedup model whole ',\n",
       " 'hand cnn delta applied similar delta algorithm cnns ',\n",
       " 'applying delta algorithms recurrent layers cnn layers might prove beneficial ',\n",
       " 'non linear function approximation non linear functions second used operations rnn matrix vector multiplications may seen table  ',\n",
       " 'non linear functions used recurrent layers tanh sigmoid respectively ',\n",
       " 'functions require floating point division exponential operations expensive terms hardware resources ',\n",
       " 'order efficient implementation rnn nonlinear function approximations implemented hardware ',\n",
       " 'approximation satisfy balance high accuracy low hardware cost ',\n",
       " 'follows present approximations used implementations study ',\n",
       " 'look tables luts replacement non linear function computation look tables fastest method  ',\n",
       " 'input range divided segments constant output values ',\n",
       " 'however achieve high accuracy large luts required consumes large area silicon practical ',\n",
       " 'several methods proposed decrease luts size preserving high accuracy ',\n",
       " 'piecewise linear approximation approximation method done dividing non linear function curve number line segments ',\n",
       " 'line segment represented two values slope bias ',\n",
       " 'thus segment two values stored luts ',\n",
       " 'choice number segments affects accuracy size luts ',\n",
       " 'thus choice number segments must made wisely keep accuracy high keeping luts small possible ',\n",
       " 'computational complexity non linear function changes single comparison multiplication addition may implemented using shifts additions ',\n",
       " 'compared using look tables piecewise linear approximation requires fewer luts computations ',\n",
       " 'hard tanh hard sigmoid hard tanh hard sigmoid two examples piecewise linear approximation three segments ',\n",
       " 'first segment saturation zero zero case sigmoid case tanh last segment saturation one middle segment line segment joins two horizontal lines ',\n",
       " 'variant piecewise linear approximation called piecewise non linear approximation ',\n",
       " 'line segments replaced non linear segments use multipliers cannot avoided linear version ',\n",
       " 'made linear approximation preferable hardware design ',\n",
       " 'ralut one method reduce size luts use ralut range addressable look tables  ',\n",
       " 'raluts group inputs mapped single output ',\n",
       " 'b  platform specific optimizations section discuss optimizations performed hardware level run rnn model efficiently ',\n",
       " 'optimizations may related computation memory ',\n",
       " 'computation related optimizations techniques applied speedup computations obtain higher throughput ',\n",
       " 'memory related optimizations techniques applied carry memory usage accesses reduced memory overhead ',\n",
       " 'compute specific bottleneck rnn computations matrix vector multiplications ',\n",
       " 'difficult fully parallelize matrix vector multiplications time steps rnn model includes feedback part ',\n",
       " 'time step computation waits preceding time step computations complete use hidden state output input new time step computation ',\n",
       " 'loop unrolling loop unrolling parallelization technique creates multiple instances looped operations gain speedup expense resources ',\n",
       " 'volume rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective two kinds loop unrolling used rnn implementations ',\n",
       " 'first inner loop unrolling inner loop matrix vector multiplication unrolled  ',\n",
       " 'second kind unrolling time steps ',\n",
       " 'rnn needs run multiple timesteps task completed ',\n",
       " 'computation recurrent unit unrolled time steps  ',\n",
       " 'however cannot fully parallelized discussed earlier ',\n",
       " 'computations rely inputs parallelized computations relying hidden state outputs performed sequence ',\n",
       " 'one solution use qrnn sru discussed section iii b  qrnn sru matrix vector multiplications operate hidden state output thus fully parallelized unrolled time steps  ',\n",
       " 'systolic arrays systolic arrays good candidate matrix vector multiplication convolution units  ',\n",
       " 'systolic arrays efficient multiplications operands move locally neighbor pes processing elements  ',\n",
       " 'thus systolic arrays require less area less energy less control logic ',\n",
       " 'well designed systolic arrays guarantee pes remain busy maximize throughput ',\n",
       " 'pipelining pipelining implementation technique increase throughput ',\n",
       " 'pipelining used rnn implementations various ways ',\n",
       " 'coarse grained pipelining cgpipe used tailor lstm variants data dependency  ',\n",
       " 'lstm computation performed three stages double buffers between ',\n",
       " 'first stage weight matrices multiplications inputs hidden cells vectors second stage non matrix vector operations third stage projection layer computations ',\n",
       " 'fine grained pipelining fgpipe used schedule operations within cgpipe stages ',\n",
       " 'design pipelining scheduler critical task due data dependency lstm variants  ',\n",
       " 'operations need performed sequentially operations done concurrently ',\n",
       " 'sparse weight matrices due applying pruning increases complexity scheduler design ',\n",
       " 'tiling tiling consists dividing one matrix vector multiplication multiple matrix vector multiplications ',\n",
       " 'usually tiling used hardware solution built support matrix vector multiplication specific size one clock cycle ',\n",
       " 'input vector weight matrix size larger size vector matrix supported hardware tiling used divide matrix vector multiplication performed hardware multiple cycles  ',\n",
       " 'thus tiling combined inner loop unrolling systolic arrays ',\n",
       " 'figure shows vector broken three vectors matrix broken nine matrices ',\n",
       " 'thus one matrix vector multiplication broken nine matrix vector multiplications ',\n",
       " 'vector multiplied matrices volume similar color ',\n",
       " 'output vector built three vectors three output vectors accumulated together form one vector output ',\n",
       " 'computation requires nine cycles completed assuming new weights loaded hardware multiplication unit within cycle time ',\n",
       " 'figure tiling converting one matrix vector multiplication nine matrix vector multiplications ',\n",
       " 'hardware sharing gru recurrent layer execution rt e ht sequence e ht computation depends rt shown eq ',\n",
       " 'thus computation rt e ht critical path gru computation ',\n",
       " 'zt computed parallel independent e ht rt  ',\n",
       " 'hardware shared computing rt zt save hardware resources  ',\n",
       " 'load balancing case sparse weight matrices resulting pruning load balancing techniques might needed parallelization matrix vector multiplication processing elements  ',\n",
       " 'analog computing analog computing good candidate neural network accelerators  ',\n",
       " 'analog neural networks analog cnns studied recently ',\n",
       " 'interestingly rnn implementations using analog computing started attract attention researchers  ',\n",
       " 'analog computing brings significant benefits especially critical matrixvector computation making faster energy efficient ',\n",
       " 'true non linear functions normally calculated nn layers well ',\n",
       " 'analog computing also allows efficient communication wire represent many values instead binary value ',\n",
       " 'performance analog computer however critically depend digital analog analog digital converters speed energy consumption ',\n",
       " 'memory considerations processing rnn algorithm memory needed store weight matrices biases inputs activations weight matrices highest memory requirement ',\n",
       " 'first decision related memory location weights storage ',\n",
       " 'weights stored chip memory accessing weights comprises largest cost respect latency energy  ',\n",
       " 'rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective chip memory applying algorithmic optimizations introduced section iv memory requirements rnn layer reduced increases possibility storing weights chip memory ',\n",
       " 'however results restriction largest model size run embedded platform ',\n",
       " 'chip memory used storing weights many implementations  ',\n",
       " 'hybrid memory storing weights chip memory restricts size model executed embedded solution ',\n",
       " 'storing weights chip memory remainder chip memory might provide solution  ',\n",
       " 'addition maximizing use chip memory researchers use techniques reduce number cost memory accesses ',\n",
       " 'multi time step parallelization fact qrnn sru remove hidden state output matrix vector multiplications leveraged allow multi time step parallelization  ',\n",
       " 'multi time step parallelization performed converting multiple matrix vector multiplication fewer matrix matrix multiplications ',\n",
       " 'method decreases number memory accesses reusing weights computations involving multiple time steps ',\n",
       " 'reordering weights reordering weights occupy memory order computation helps decrease memory access time  ',\n",
       " 'reordering parameters memory carried way ensures memory accesses sequential ',\n",
       " 'compute load overlap order compute matrix vector multiplications weights need accessed loaded memory used computations ',\n",
       " 'total time sum access time computation time ',\n",
       " 'decrease time memory access computations overlapped ',\n",
       " 'overlap achieved fetching weights next timestep performing computation current time step ',\n",
       " 'overlap would require existence extra buffers storing weights next time step using weights current time step  ',\n",
       " 'doubling memory fetching method twice number required weights computation fetched  ',\n",
       " 'half weights consumed current time step computations rest buffered following time step  ',\n",
       " 'doubling memory fetching reduce memory bandwidth half ',\n",
       " 'domain wall memory dwm dwm new technology non volatile memories proposed parkin et al ',\n",
       " 'ibm  ',\n",
       " 'dwm technology based magnetic spin  ',\n",
       " 'information stored setting spin orientation magnetic domains nanoscopic permalloy wire ',\n",
       " 'multiple magnetic domains occupy one wire called race tracking ',\n",
       " 'race tracking allows representation bits ',\n",
       " 'dwm density hoped improve sram x dram x  ',\n",
       " 'using dwm rnn accelerator achieve better performance lower energy consumption  ',\n",
       " 'processing memory pim pim gets rid data fetching problem allowing computation take place memory eliminating memory access overhead ',\n",
       " 'architecture memory bank divided three subarray segments memory sub arrays buffer sub arrays processing sub arrays used conventional memory data buffer processing sub arrays respectively ',\n",
       " 'reram based pim arrays one approach used accelerate cnns rnns  ',\n",
       " 'reram supports xnor bit counting operations sufficient rnn implementation binary multi bit code section iv quantization applied  ',\n",
       " 'memristors crossbar arrays successfully used analog dot product engine accelerate cnns rnns  ',\n",
       " 'v  rnn implementations hardware previous section discussed optimizations applied decrease computation memory requirements rnn models ',\n",
       " 'section study recent implementations rnn applications embedded platforms ',\n",
       " 'implementations divided fpga asic implementations ',\n",
       " 'analyze implementations study effects applied optimizations ',\n",
       " 'however effect optimization shown separately ',\n",
       " 'instead outcomes applying mix optimizations discussed respect objectives presented section ii ',\n",
       " 'first regard efficiency implementations compared terms throughput energy consumption meeting real time requirements ',\n",
       " 'flexibility discuss implementations support variations models online training different application domains ',\n",
       " 'table shows details implementations studied here ',\n",
       " 'authors names shown along name architecture named affiliation year publication ',\n",
       " 'table table present implementations study ',\n",
       " 'table shows implementations performed fpgas table shows implementations performed platforms ',\n",
       " 'implementation index ',\n",
       " 'index starts f fpga implementations asic implementations c implementations ',\n",
       " 'implementation tables show platform rnn model applied optimizations runtime performance ',\n",
       " 'cases recurrent layers rnn model shown papers provided implementation layers only ',\n",
       " 'recurrent layers written format x z x number recurrent layers type recurrent layers e g ',\n",
       " 'lstm gru    z number hidden cells layer ',\n",
       " 'model different modules e g ',\n",
       " 'two different lstm models lstm cnn give number executed time steps volume rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective table detailed information papers study index f authors li et al ',\n",
       " 'name e rnn year affiliation syracuse university northeastern university florida international university mellon university carnegie university southern california suny university peking university syracuse university city university new york university kaiserslautern xilinix research lab stanford university deephi tech tsinghua university nvidia university zurich eth zurich university kaiserslautern german research center artificial intelligence university illinois inspirit iot inc tsinghua university beihang university seoul national university shanghai jiao tong university chinese academy sciences university cambridge imperial college peking university university california pku ucla joint research institute science engineering imperial college london purdue university purdue university hewlett packard enterprise university illinois urbana champaign nanjing university louisiana state university georgia institute technology atlanta fudan university zhejiang university university washington pohang university science technology ku leuven arizona state university goergia institute technology goergia institute technology arm inc  university california san diego seoul national university f wang et al ',\n",
       " 'c lstm f rybalkin et al ',\n",
       " 'finn l f han et al ',\n",
       " 'ese f f gao et al ',\n",
       " 'rybalkin et al ',\n",
       " 'deltarnn f zhang et al ',\n",
       " 'f f lee et al ',\n",
       " 'sun et al ',\n",
       " 'f guan et al ',\n",
       " 'f f rizakis et al ',\n",
       " 'chang et al ',\n",
       " 'ankit et al ',\n",
       " 'deeprnn puma wang et al ',\n",
       " 'zhao et al ',\n",
       " 'long et al ',\n",
       " 'chen et al ',\n",
       " 'ocean park et al ',\n",
       " 'giraldo et al ',\n",
       " 'yin et al ',\n",
       " 'kwon et al ',\n",
       " 'sharma et al ',\n",
       " 'laika maeri bit fusion c c c sung et al ',\n",
       " 'cao et al ',\n",
       " 'mobirnn stony brook university rnn model ',\n",
       " 'algorithmic platform optimizations shown tables ',\n",
       " 'optimizations found tables explained section iv using keywords tables ',\n",
       " 'quantized models quantization x written optimizations column x number bits used store weights ',\n",
       " 'effective throughput energy efficiency given tables discussed detail sub section below ',\n",
       " 'a  implementation efficiency study efficiency implementations understudy focus three aspects throughput energy consumption meeting real time requirements ',\n",
       " 'effective throughput compare throughput different implementations use number operations per second op measure ',\n",
       " 'papers surveyed directly state throughput ',\n",
       " 'papers tried deduce throughput information given ',\n",
       " 'one aspect volume consider compression optimization results decreasing number operations model running it ',\n",
       " 'consequently number operations per second fair indicator implementation efficiency ',\n",
       " 'case throughput calculated using number operations dense rnn model compressed model ',\n",
       " 'call effective throughput ',\n",
       " 'list methods used deduce throughput values different papers ',\n",
       " 'case q effective throughput given paper ',\n",
       " 'case q number operations dense model computation time given ',\n",
       " 'dividing number operations nop time get effective throughput qef f shown eq ',\n",
       " 'papers number operations computation time timecomp given multiple time steps multiple inputs would require running lstm nsteps times ',\n",
       " 'qef f nop nsteps tcomp case q implemented rnn model information rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective provided paper ',\n",
       " 'thus calculate number operations model information divide computation time get throughput case q  ',\n",
       " 'compute number operations number operations matrix vector multiplications counted dominant effect performance ',\n",
       " 'paper give enough information calculate number operations number operations approximately calculated multiplying number parameters two ',\n",
       " 'case q energy efficiency given terms op watt power consumption given watt ',\n",
       " 'multiplying two values throughput calculated ',\n",
       " 'case q effective throughput could computed ',\n",
       " 'fair comparison asic implementations applied scaling nm technology  ',\n",
       " 'v using general scaling equations rabaey scaling estimate equations nm smaller technologies  ',\n",
       " 'voltage value mentioned paper assume standard voltage implementation technology ',\n",
       " 'example since implemented nm assume voltage value  ',\n",
       " 'v  analyze table table understand effect different optimizations throughput tables entries ordered descending order starting highest throughput implementation ',\n",
       " 'exist two optimization groups appear frequently among high throughput implementations ',\n",
       " 'first group related decreasing memory access time ',\n",
       " 'memory access time decreased either using chip memory weights overlapping computation time weights loading time ',\n",
       " 'second group related algorithmic optimizations ',\n",
       " 'algorithmic optimizations present high throughput implementations compression pruning block circulant matrices etc ',\n",
       " 'deltarnn low precision quantization ',\n",
       " 'non linear function approximations bit quantization within groups high effect optimizations ',\n",
       " 'quantization bit present many implementations aim lower precision great effect computation cost ',\n",
       " 'thus differentiating factor ',\n",
       " 'non linear function approximations contribute used operations matrix vector multiplications  ',\n",
       " 'finally throughput values plotted implementations figure  ',\n",
       " 'scaled effective throughput values asic implementations used ',\n",
       " 'implementations memory access optimizations algorithmic optimizations highlighted putting inside square circle ',\n",
       " 'observed figure implementations high throughput algorithmic optimization applied memory access optimization ',\n",
       " 'example f applied low precision quantization placed weights chip memory ',\n",
       " 'f f f applied chip memory optimization algorithmic optimizations ',\n",
       " 'f architecture scheduler overlapped computation memory accesses ',\n",
       " 'weights required computation fetched computation starts ',\n",
       " 'thus managed eliminate chip memory access overhead efficient compute load overlap ',\n",
       " 'f f applied block circulant matrices optimization ',\n",
       " 'addition applied circulant matrices optimization ',\n",
       " 'indicates restructuring weight matrices circulant matrices sub matrices one fruitful optimizations ',\n",
       " 'reason could circulant matrices optimization cause irregularity weight matrices seen pruning  ',\n",
       " 'additionally circulant blockcirculant matrices accompanied low precision quantization without harsh effect accuracy bit f bit  ',\n",
       " 'observed table f f optimizations almost identical performance different ',\n",
       " 'f f differences hardware architecture f applied lower precision f important reason f used better approach training compressed rnn model ',\n",
       " 'f able reach accuracy level reached f block size using block size  ',\n",
       " 'thus rnn model size f approximately x less f  ',\n",
       " 'nevertheless noticed computeoptimization f f pipelining ',\n",
       " 'two implementations pipelining served two roles ',\n",
       " 'first coarsegrained pipelining lstm stages second fine grained pipelining within stage ',\n",
       " 'worth knowing f based architecture f  ',\n",
       " 'f achieved higher throughput f applying higher frequency using lower precision ',\n",
       " 'assuming linear frequency scaling ratio two implementations throughput close ratio precisions used storing weights two implementations ',\n",
       " 'lack algorithmic optimizations compensated use analog crossbar based matrix vector multiplication units ',\n",
       " 'analog crossbar units allowed low latency matrix vector multiplications ',\n",
       " 'implementations used analog computing marked sign figures  ',\n",
       " 'comparing using analog crossbars ',\n",
       " 'surpassed applying pim processing memory removes memory access overhead ',\n",
       " 'therefore figures consider pim memory access optimization ',\n",
       " 'one implementation stands low throughput asic implementation applying chip algorithmic optimizations ',\n",
       " 'particular implementation meant meet latency deadline ms consuming low power micro watt level ',\n",
       " 'thus high throughput objective beginning ',\n",
       " 'implementations defined real time requirements marked rt sign figures  ',\n",
       " 'another implementation rewards close inspection f  ',\n",
       " 'despite applying two mentioned optimizations could reach high performance expected ',\n",
       " 'conclusion applying memory access optimization algorithmic optimization necessary sufficient high performance ',\n",
       " 'volume platform optimizations chip pipelining inner loop unrolling unrolling timesteps tiling chip pipelining qef f gop q eef f gop watt e q e  ',\n",
       " 'chip pipelining chip pipelining q  ',\n",
       " 'e q  ',\n",
       " 'e q e q  ',\n",
       " 'e q e  ',\n",
       " 'e q  ',\n",
       " 'e pruning chip pipelining inner loop unrolling compute load overlap pipelining load balancing inner loop unrolling inner loop unrolling reordering weights chip inner loop unrolling tiling inner loop unrolling pipelining tiling inner loop unrolling pipelining reordering weights compute load overlap tiling inner loop unrolling q  ',\n",
       " 'e quantization piecewise approx ',\n",
       " 'hybrid memory compute load overlap q  ',\n",
       " 'index platform model f zync xczu ev mhz bilstm f alpha data adm v mhz lstm f zync mhz gru f alpha data adm v mhz lstm f bilstm f zynq xc z mhz xcku mhz block circulant piecewise approx ',\n",
       " 'quantization deltarnn ralut quantization block circulant piecewise approx ',\n",
       " 'quantization quantization lstm pruning quantization f virtex vc mhz quantization f f xc z mhz vc mhz alexnet steps lstm steps lstm steps lstm lstm fc f vc mhz lstm piecewise approx ',\n",
       " 'f f zynq zc mhz lstm zynq xc z mhz lstm algorithmic optimizations quantization quantization hard sigmoid cases q q explained section v  ',\n",
       " 'cases e e explained section v  ',\n",
       " 'effective throughput paper computed bit basis ',\n",
       " 'fair comparison recalculated number operations operand basis ',\n",
       " 'throughput running cnn lstm combined together ',\n",
       " 'number time steps model run per second reach real time behavior given ',\n",
       " 'computed number operations model multiplied number time steps one second multiplied speedup gained real time threshold get implementation throughput ',\n",
       " 'rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective volume table comparison rnns implementations fpgas ',\n",
       " 'table comparison rnns implementations asic platforms ',\n",
       " 'category e q  ',\n",
       " 'e q e  ',\n",
       " 'q e q  ',\n",
       " 'e unrolling timesteps q  ',\n",
       " 'e renderscript q  ',\n",
       " 'e cmos nm ghz lstm quantization tsmc nm mhz v lstm cmos nm lstm quantization circulant matrices piecewise approx ',\n",
       " 'quantization cmos nm mhz cmos nm mhz cmos nm khz cmos nm gru lstm quantization piecewise approx ',\n",
       " 'quantization armv ghz intel core  ',\n",
       " 'ghz adreno gpu mhz sru sru memristor pim analog computing pipelining chip tiling inner loop unrolling analog computing chip chip hardware sharing pipelining load balancing inner loop unrolling chip doubling memory fetching reram pim analog computing unrolling timesteps sru sru c c c lstm lstm lstm quantization piecewise approx ',\n",
       " 'pruning cases q q explained section v  ',\n",
       " 'cases e e explained section v  ',\n",
       " 'scaled nm  ',\n",
       " 'volt using general scaling scaling estimates nm smaller technologies  ',\n",
       " 'throughput high purpose reach low power consumption performing inference within ms  renderscript mobile specific parallelization framework  ',\n",
       " 'quantization used bit weights bits activations ',\n",
       " 'proposed gru core without providing specific model details ',\n",
       " 'specify model achieved provided throughput energy efficiency ',\n",
       " 'volume rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective q  ',\n",
       " 'platform optimizations others e algorithmic optimizations q model eef f gop watt original scaled e platform asic qef f gop original scaled q index rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective addition figure shows asic implementations exceeding fpga implementations terms throughput ',\n",
       " 'think reason asic implementations study use latest asic technologies shown table  ',\n",
       " 'low throughput implementations figure shows implementations apply either two optimizations memory access algorithmic f strict accuracy constraint bounding use algorithmic optimizations c  ',\n",
       " 'addition implementations applied one two optimizations including f f  ',\n",
       " 'energy efficiency compare implementations energy consumption perspective use number operations per second per watt measure ',\n",
       " 'last columns table table show energy efficiency ',\n",
       " 'energy efficiency calculated based dense model sparse model effective throughput ',\n",
       " 'however possible obtain values energy efficiency implementations ',\n",
       " 'cases power consumption mentioned paper others consumed power provided precise manner ',\n",
       " 'instance power whole fpga board may provided indicate much power used implementation respect peripherals  ',\n",
       " 'list cases used computing energy efficiency table table  ',\n",
       " 'case number appears triangular brackets numeric value case e eef f energy efficiency given paper ',\n",
       " 'case e power consumption given paper ',\n",
       " 'compute energy efficiency eef f effective throughput qef f op divided power p watt eef f qef f  ',\n",
       " 'p case e energy computation time provided ',\n",
       " 'first divide energy time get power ',\n",
       " 'divide effective throughput qef f power get energy efficiency case e  ',\n",
       " 'case e energy efficiency could computed ',\n",
       " 'figure plot energy efficiency found deduced implementations study implementation index ',\n",
       " 'implementations sorted plot according energy efficiency scaled values asic implementations used ',\n",
       " 'show effect optimizations chose two effective optimizations table table include figure ',\n",
       " 'figure memory access optimization algorithmic optimizations ',\n",
       " 'comparing effective throughput energy efficiency fpga asic implementations observed fpga asic close values volume effective throughput asic implementations energy efficient ',\n",
       " 'credit go asic technology ',\n",
       " 'observed highest energy efficiency achieved  ',\n",
       " 'implementations used analog crossbar based matrix vector multiplications ',\n",
       " 'managed save memory access time computing memory ',\n",
       " 'quantization method used multi bit code quantization bit weights bit activations  ',\n",
       " 'multi bit code quantization enables replacing mac operation xnor bit counting operations discussed section iv  ',\n",
       " 'sufficient use xnor rram based architecture implement rnn ',\n",
       " 'applying pim analog computing applying memory algorithmic optimizations less energy efficient expected ',\n",
       " 'less energy efficient applying memory optimization  ',\n",
       " 'quite high clock frequency ghz ',\n",
       " 'high frequency helped implementation achieve high throughput ',\n",
       " 'however suspect high frequency main reason energy efficiency degradation compared implementations ',\n",
       " 'least power consumption among implementations w  ',\n",
       " 'low throughput affected overall energy efficiency value ',\n",
       " 'meeting real time requirements implementations real time requirements throughput power determined ',\n",
       " 'example f speech recognition system two rnn models ',\n",
       " 'one model acoustic modeling character level language modeling ',\n",
       " 'real time requirement run first model times per second second model times per second ',\n",
       " 'lstm accelerator always keyword spotting system kws real time response demanded new input vector consumed every ms power consumption exceed w  ',\n",
       " 'b  flexibility flexibility defined section ii ability solution support different models configurations ',\n",
       " 'flexibility solution met supporting variations model ',\n",
       " 'models vary number layers number hidden units per layer optimizations applied model more ',\n",
       " 'flexibility met supporting online training meeting different application domain requirements ',\n",
       " 'flexibility quantitative like throughput ',\n",
       " 'thus use subjective measure flexibility reach flexibility score implementation ',\n",
       " 'table shows flexibility aspects supported implementation discussed papers flexibility score implementation ',\n",
       " 'papers discuss flexibility aspects omitted table  ',\n",
       " 'architecture able support various models number cells layers architecture support mentioned paper ',\n",
       " 'hence cannot deduce implementation could rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective figure effective throughput different implementations key optimizations affecting them ',\n",
       " 'figure energy efficiency different implementations key optimizations used ',\n",
       " 'support variations rnn model ',\n",
       " 'also variations supported hardware platform method fabrication ',\n",
       " 'design method support two different rnn layers ',\n",
       " 'however fabricated chip supports one them ',\n",
       " 'thus consider meet flexibility objective ',\n",
       " 'understand far flexibility met implementations figure shows percentage implementations supporting flexibility aspect ',\n",
       " 'flexibility visualized levels ',\n",
       " 'level used indicate flexibility ',\n",
       " 'level requires implementation support one recurrent layer configuration ',\n",
       " 'papers meet level requirement thereafter vary meeting flexibility aspects ',\n",
       " 'flexibility aspects met discussed below ',\n",
       " 'supporting variations rnn layers level recurrent layers vary type layers number cells layer number layers depth rnn model  ',\n",
       " 'one optimization might side effect flexibility solution choice using onchip chip memory store weights ',\n",
       " 'able store weights chip memory beneficial ',\n",
       " 'leads better performance less energy consumption decreasing cost memory accesses ',\n",
       " 'however solution may unfeasible larger problems ',\n",
       " 'example f number weights model precision restricted chip memory size ',\n",
       " 'possible run model increased number hidden cells increased precision ',\n",
       " 'possible solution use adaptable approach location chosen store weights dependent model size thus wide range models supported ',\n",
       " 'another solution adopted f weights stored internal memory rest stored chip memory hybrid memory  ',\n",
       " 'supporting nn layers level supporting nn layers allows solution run broader range nn applications ',\n",
       " 'also nn layers may exist rnn model convolutions used feature extractor ',\n",
       " 'supporting convolution implementation increases flexibility solution run rnn models visual inputs run cnn independent applications ',\n",
       " 'volume rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective figure percentage implementations meeting flexibility aspects different flexibility levels definition flexibility levels ',\n",
       " 'table flexibility score implementations study ',\n",
       " 'index f f f f f f f c c volume flexibility aspects papers varying layer lstm gru varying number cells varying block size block circulant matrices varying layer lstm bilstm varying number layers varying number cells varying layer lstm bilstm varying precision fc supported varying layer lstm bilstm fc supported convolution supported fc supported varying number layers varying number cells input layer varying number layers varying number cells online training varying number cells fc supported varying number layers varying number cells linear nonlinear quantization fc supported varying type layer lstm gru convolution supported fc supported varying number cells varying number layers dense sparse convolution supported varying number cells varying number layers convolution supported varying precision varying number cells varying number layers varying type layers convolution supported fc supported varying layer lstm sru qrnn varying number cells varying number layers varying number cells score xxx xxx xxx xx xx xxx xx x xx xxxx xxx xxxx xxxx xxxxx xx xx supporting algorithmic optimization variations level variations optimizations applied also considered variations model ',\n",
       " 'example variation due applying applying pruning related presence sparse dense matrices matrix vector multiplications computations ',\n",
       " 'design employed configurable interconnection network topology increase flexibility accelerator ',\n",
       " 'accelerator supported lstm cnn layers ',\n",
       " 'accelerators supported sparse dense matrices ',\n",
       " 'one variation precision weights activations ',\n",
       " 'design supported varying precision models allowing dynamic precision per layer cnn rnn models ',\n",
       " 'similarly microsoft npu brainwave architecture supported varying precision using narrow precision block floating point format  ',\n",
       " 'maximize benefit varying precision f applied parameterizable parallelization scheme ',\n",
       " 'lower precision required lstm units duplicated exploit unused resources gain speedup ',\n",
       " 'higher precision used simd folding applied save resources needed high precision ',\n",
       " 'online training level incremented online training included support retraining pre trained networks enhance accuracy ',\n",
       " 'changes hardware design applied support training inference without affecting quality inference ',\n",
       " 'example three modes data transfer applied ',\n",
       " 'first load new weights second load input sequences third update certain weights ',\n",
       " 'extra precision used training ',\n",
       " 'meeting different applications domains constraints level none implementations target variations application domain constraints ',\n",
       " 'netadapt good example implementation adapt different metric budgets  ',\n",
       " 'however targets cnns ',\n",
       " 'rezk et al ',\n",
       " 'recurrent neural networks embedded computing perspective vi ',\n",
       " 'discussions opportunities previous section studied implementations rnn embedded platforms ',\n",
       " 'section ii defined objectives realizing rnn models embedded platforms ',\n",
       " 'section investigate objectives met implementations ',\n",
       " 'throughput clear throughput main objective implementations ',\n",
       " 'seen figure high throughput achieved many them ',\n",
       " 'algorithmic memory optimizations present high throughput implementations ',\n",
       " 'algorithmic optimizations applied effective decrease computation memory requirements rnn models ',\n",
       " 'example bit precision used instead bit weights storage memory requirement decreased  ',\n",
       " 'multiple bit weights concatenated weights fetching ',\n",
       " 'thus number memory accesses decrease well ',\n",
       " 'furthermore hardware required bit operations simpler hardware required bit floating point operations ',\n",
       " 'memory specific optimizations effective decrease hide overhead accessing large number weights used rnn computations ',\n",
       " 'memory access time decreased storing weights chip memory ',\n",
       " 'however bound validity solution larger models chip memory may sufficient store weights ',\n",
       " 'overlapping memory access time computation computation memory also considered memory optimizations ',\n",
       " 'energy efficiency applying algorithmic memory access optimizations positive effect energy efficiency well ',\n",
       " 'algorithmic optimizations lead decrease number computations complexity computations number memory accesses decrease energy consumed implementation ',\n",
       " 'also minimizing chip memory use storing weights chip memory effective way enhance energy efficiency ',\n",
       " 'analog computing processing memory implementations showed superior energy efficiency asic implementations ',\n",
       " 'meeting real time requirements objective many implementations ',\n",
       " 'real time deadlines mentioned followed design solution ',\n",
       " 'flexibilty section ii flexibility defined secondary objective ',\n",
       " 'thus expect flexibility fully met implementations ',\n",
       " 'variations rnn model partially fulfilled many implementations ',\n",
       " 'however number variations covered implementation quite low ',\n",
       " 'implementations included nn layers variations algorithmic optimizations ',\n",
       " 'onlinetraining targeted one implementation ',\n",
       " 'embedded implementations usually support online training ',\n",
       " 'however algorithmic side researchers carrying interesting work based online continuous training  ',\n",
       " 'none rnn implementations support different applications done cnn solution  ',\n",
       " 'following similar method rnns addition also supporting model variations could lead interesting solutions ',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "  if len(i) != 0:\n",
    "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "  else:\n",
    "    v = np.zeros((100,))\n",
    "  sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048, 1048)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity matrix\n",
    "sim_mat = np.zeros([len(clean_sentences), len(clean_sentences)])\n",
    "sim_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(clean_sentences)):\n",
    "  for j in range(len(clean_sentences)):\n",
    "    if i != j:\n",
    "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(clean_sentences2)), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the computational complexity of the non linear function changes to  be a single comparison  multiplication and addition  which  may be implemented using shifts and additions.\n",
      "the transformation from a  regular matrix computation to an irregular application  often results in the use of additional hardware and  computation time to manage data.\n",
      "one other aspect to  volume                                                                                                                                                     consider is that compression optimization results in decreasing the number of operations in the model before running it.\n",
      "the extra  non linear layers can also be added in computing the output  from the hidden state vector  this model is called the deep  output rnn model.\n",
      "however  careful choice of the method used to  transform a dense matrix to a sparse matrix may result  in only a limited impact on accuracy while providing  significant gains in computation time.\n",
      "thus  we calculate the number of  operations from the model information and then divide  it by computation time to get the throughput as in case  q .\n",
      "in the table  we put the three factors affecting the accuracy  discussed earlier  number of bits  quantization method  and  training  with an addition of the type of recurrent layer   lstm  gru...  and the dataset.\n",
      "tuning the three  parameters delta  number of bits  quantization method  and  block size  the designer can achieve the highest performance  while keeping the accuracy within an acceptable range  the  range is dependent on the application .\n",
      "one other way to have extra non linear functions within the  recurrent layer is to have them within the gate calculations    a method called h lstm  hidden lstm .\n",
      "many to one a many to one model combines a sequence of inputs to generate a single output  as shown  in figure  b. activity recognition     and sentiment    volume           e. deep recurrent neural networks  drnn     making a neural network a deep neural network is achieved  by adding non linear layers between the input layer and  the output layer     .\n"
     ]
    }
   ],
   "source": [
    "# Extract top 10 sentences as the summary\n",
    "generated_abstract= \" \"\n",
    "for i in range(10):\n",
    "  print(ranked_sentences[i][1])\n",
    "  generated_abstract=generated_abstract+ranked_sentences[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the computational complexity of the non linear function changes to  be a single comparison  multiplication and addition  which  may be implemented using shifts and additions.the transformation from a  regular matrix computation to an irregular application  often results in the use of additional hardware and  computation time to manage data.one other aspect to  volume                                                                                                                                                     consider is that compression optimization results in decreasing the number of operations in the model before running it.the extra  non linear layers can also be added in computing the output  from the hidden state vector  this model is called the deep  output rnn model.however  careful choice of the method used to  transform a dense matrix to a sparse matrix may result  in only a limited impact on accuracy while providing  significant gains in computation time.thus  we calculate the number of  operations from the model information and then divide  it by computation time to get the throughput as in case  q .in the table  we put the three factors affecting the accuracy  discussed earlier  number of bits  quantization method  and  training  with an addition of the type of recurrent layer   lstm  gru...  and the dataset.tuning the three  parameters delta  number of bits  quantization method  and  block size  the designer can achieve the highest performance  while keeping the accuracy within an acceptable range  the  range is dependent on the application .one other way to have extra non linear functions within the  recurrent layer is to have them within the gate calculations    a method called h lstm  hidden lstm .many to one a many to one model combines a sequence of inputs to generate a single output  as shown  in figure  b. activity recognition     and sentiment    volume           e. deep recurrent neural networks  drnn     making a neural network a deep neural network is achieved  by adding non linear layers between the input layer and  the output layer     .\n"
     ]
    }
   ],
   "source": [
    "print(generated_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_file = codecs.open(ABSTRACTDIR+text_files[0],\"r\",\"utf-8\")\n",
    "original_abstract = curr_file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores( original_abstract,new_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.22505307408387096,\n",
       "   'p': 0.3333333333333333,\n",
       "   'r': 0.16987179487179488},\n",
       "  'rouge-2': {'f': 0.029850741800774447,\n",
       "   'p': 0.04430379746835443,\n",
       "   'r': 0.022508038585209004},\n",
       "  'rouge-l': {'f': 0.21804510795861845,\n",
       "   'p': 0.26851851851851855,\n",
       "   'r': 0.18354430379746836}}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
