arXiv:1711.05734v2 [cs.DC] 20 Feb 2018

C HIPMUNK: A Systolically Scalable 0.9 mm2,
3.08 Gop/s/mW @ 1.2 mW Accelerator for
Near-Sensor Recurrent Neural Network Inference
Francesco Conti∗†
fconti@iis.ee.ethz.ch

Lukas Cavigelli∗
cavigelli@iis.ee.ethz.ch

Igor Susmelj∗
isusmelj@student.ethz.ch

Gianna Paulin∗
pauling@student.ethz.ch

Luca Benini∗†
lbenini@iis.ee.ethz.ch

Abstract
Recurrent neural networks (RNNs) are state-of-the-art in voice awareness/understanding and speech recognition. On-device computation of RNNs
on low-power mobile and wearable devices would be key to applications such
as zero-latency voice-based human-machine interfaces. Here we present C HIP MUNK , a small (<1 mm2 ) hardware accelerator for Long-Short Term Memory
RNNs in UMC 65 nm technology capable to operate at a measured peak efficiency
up to 3.08 Gop/s/mW at 1.24 mW peak power. To implement big RNN models
without incurring in huge memory transfer overhead, multiple C HIPMUNK engines can cooperate to form a single systolic array. In this way, the C HIPMUNK
architecture in a 75 tiles configuration can achieve real-time phoneme extraction
on a demanding RNN topology proposed in [1], consuming less than 13 mW of
average power.

1

Introduction

In the last few years, we have witnessed an “artificial intelligence” revolution that has been fueled
by the concurrent availability of huge amounts of training data, computing power to learn upon it,
and evolution of “smart” algorithms, in particular those based on deep learning. Within this field,
recurrent neural networks (RNNs), particularly Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), are receiving increasing attention: They have shown state-of-the-art accuracy
in tasks such as speech recognition [1, 2] and language translation [3], making them the forefront of
the “intelligent” user interfaces of products such as Amazon Alexa, Google Assistant, Apple Siri,
Microsoft Cortana and others.
One of the key limitations of the current generation of commercial products based on RNNs is that
these embedded, edge devices depend on remote servers taking care of the computational workload
necessary for the deployment of these algorithms. Moreover, when RNNs are used as a component
of human-machine interfaces, the intrinsic latency of network communication can also be problematic, as people expect the “smart” devices to reply not only accurately, but also timely.
For these reasons, it is very attractive to integrate RNN capabilities locally in embedded mobile
and wearable platforms, making them capable of state-of-the-art voice and speech recognition autonomously and independent from external servers. Nonetheless, while much attention has recently
been dedicated to the deployment of embedded low-power inference accelerators for forward-only
deep networks deployment [4–8], making RNNs energy-efficient is a fundamentally harder problem:
the necessity to keep and update an internal state and the widespread usage of densely connected
layers translate to very large memory footprint and high bandwidth requirements.
1
2

Integrated Systems Laboratory, ETH Zurich.
Energy-Efficient Embedded Systems Laboratory, University of Bologna.

In this work, we present a twofold contribution towards the deployment of RNN-based algorithms in
devices such as smartphones, smartwatches and wearables. First, we designed C HIPMUNK, a small
and low-energy hardware accelerator engine targeted at real-time speech recognition and capable to
operate autonomously on moderate size LSTM networks. We present silicon results from a prototype
chip containing a C HIPMUNK engine, which has been fabricated in UMC 65 nm technology; the
chip can achieve up to 3.8 Gop/s at maximum efficiency operating point (@0.75 V), consuming
only 1.24 mW.
Second, we conceived a scalable computing architecture, apt to operate on bigger LSTM models
as well. As the main limitation to the deployment of big RNNs in embedded scenarios stems from
their memory boundedness, we designed the C HIPMUNK engines so that they can be replicated
in a systolic array, cooperating on a single bigger LSTM network. This methodology allows the
acceleration of large-scale RNNs, which can be made fast enough to operate in real-time under
realistically tight time, memory and battery constraints without requiring complex, power hungry
and expensive high-bandwidth main memory interfaces.

2

Related Work

A recent thorough survey of efforts on hardware acceleration and design of efficient shows that few
efforts have been focused on RNN inference [9]. We thus focus on this application, surveying stateof-the-art implementations from data-center to ultra-low power accelerators in the remainder of this
section.
Data center workloads for RNNs are often offloaded to GPUs or specialized semi-independent coprocessors such as Google’s Tensor Processing Unit (TPU) [10] consuming in the order of 50-300 W.
The TPU is a unified architecture to target DNNs with convolutional and densely connected layers
as well as LSTMs. However, TPUs suffer from low utilization when running RNNs. Yet 29% of the
workload running on Google’s TPUs is devoted to RNN inference [10], showing their relevance in
commercial applications.
In a lower power range (tens of Watts), several FPGA implementations can be found. The Efficient Speech-recognition Engine (ESE) [11] targets the deployment of RNNs on a Xilinx UltraScale
FPGA. To maximize efficiency and address the memory boundedness of RNNs, it heavily focuses
on network quantization and pruning of the recurrent topologies and thus this accelerator engine
is mainly targeted at sparse matrix-vector operations. Rybalkin et al. [12] also target bidirectional
LSTMs in their FPGA accelerator. Bidirectional LSTMs have been shown to obtain better accuracy
in some cases [1] but are less attractive for an online, real-time scenario as they inherently increase
the network latency. Finally, DeepStream [13] is a small hardware accelerator deployed on a Xilinx
Zynq 7020 targeted at text recognition with RNNs. It requires to continuously stream in weights,
which makes it impractical for big RNN topologies with millions of weights.
The only published ultra-low power (few mW) implementation, the DNPU [14], uses two separate
special-purpose engines for convolutional layers (called CP), on one side, and fully-connected and
recurrent ones on the other (called FRP). The FRP does not include any particular facilities to address
the stateful nature of RNNs, and it includes only a small amount of memory (10 kB) making external
memory accesses necessary for even small RNNs, thus limiting peak performance by introducing a
serious bandwidth bottleneck.

3
3.1

Architecture
Operating principle

Long Short-Term Memory (LSTM) network layers [15] are often described with the following set of
canonical equations:
it
ft
ct
ot
ht

= σ(Wxi xt + Whi ht−1 + wci ct−1 + bi )
= σ(Wxf xt + Whf ht−1 + wcf ct−1 + bf )
= ft ct−1 + it tanh(Wxc xt + Whc ht−1 + bc )
= σ(Wxo xt + Who ht−1 + wco ct + bo )
= ot tanh(ct )

(1)
(2)
(3)
(4)
(5)

where x is the input state vector; i, f , o are called input, forget and output gates respectively; c
and h are the cell and hidden states. The subscript indicates either the current state t or the previous
2

xt

ht−1
•

Wxf

•

Whf

ct−1
•

σ
Wxi

•

Whi

x(0)

ft

•

σ
Wxc

•

Whc

bc

•

•

chip (0,1) y(0)
W(0,1)

chip (1,0)
W(1,0)

chip (1,1) y(1)
W(1,1)

ct

wco

Who

bo
computed across all chips

chip (0,0)
W(0,0)

it
tanh

Wxo

x(1)

wci

bi
•

Systolic Matrix Multiplication

wcf

bf
•

•

tanh

σ

ot

•

ht
•

Why

σ

yt

comp. only in right-most chips

Figure 1: Data dependency graph of a LSTM. The majority of computations are the vector-matrix
mult. (green) and can be distributed across multiple chips. Top-right: distribution of a vector matrix
mult. to a systolic array of chips.
t − 1, and denotes element-wise multiplication1 . The characteristic dimensions of all vectors and
matrices depend on the size of the input state (Nx ) and on that of the hidden state (Nh ). Multiple
LSTM layers can be connected by using the hidden state of one layer as input of the next. Finally,
LSTM networks often include a final densely connected layer without recurrence: yt = σ(Why ht ).
In C HIPMUNK, we exploit two distinct observations regarding LSTMs. First, all compute steps
are based on the same set of basic operations: i) matrix-vector products, ii) element-wise vector
products, and iii) element-wise non-linear activations. The internal datapath of C HIPMUNK can be
configured to execute these three basic operations (Section 3.2) and the LSTM state parameters are
stored on-chip. Second, the vast amount of data required to compute one time step of a RNN are
the weights. Storing them on-chip is thus essential to achieve high energy efficiency. To this end,
we a large share of the overall chip area is dedicated to SRAM to keep the weights local. For larger
LSTMs not fitting on a single chip, we allow operation in a systolic mode where the weights are
split across multiple connected chips and only the much smaller intermediate results are exchanged
as further discussed in Section 3.3.
3.2

Tile architecture

A product between a matrix of size A × B and a vector of size B is composed of two nested loops,
i.e. in pseudo-code:
for a in range(0, A):
# row loop
for b in range(0, B): # column loop
z += W[a,b] * x[b]
In C HIPMUNK, the row loop is executed on multiple parallel units, while the inner loop is executed
sequentially.
Fig. 2a shows a high-level diagram of the C HIPMUNK LSTM datapath that implements this functionality. Nlstm parallel LSTM units are used to execute all the iterations of the row loop at the same
time. Each LSTM unit is composed of an embedded memory bank to store weights (W ), registers
for storing the ot , ft , it and ct values locally, a multiply-accumulate unit and two lookup tables to
implement the non-linear activation functions. xt and ht are kept outside of the LSTM units, in a
bank of Nlstm registers. At each cycle of a column loop, one element of the input state and one
of the hidden state are selected depending on the iteration index and broadcast to all LSTM units.
Fig. 2b shows the basic operation loops composing a LSTM network deployed on C HIPMUNK.
1
In most literature Eqs. (1), (2) and (4) use matrix notation for Wci , Wcf and Wco ; however as these
matrices are diagonal by construction, we use the element-wise product notation here for consistence with what
is actually implemented in the C HIPMUNK hardware.

3

(a) LSTM datapath.

(b) Sequence of datapath basic operation loops.

Figure 2: LSTM datapath used in C HIPMUNK and typical sequence of operations. The datapath can
be used to implement the operations in Eqs. (1) to (5) by appropriately controlling the muxes and
clearing the register states.
All state variables use 8 bit fixed point precision, while 16 bits are used within the multiplyaccumulate block to minimize overflows. I/O is performed via an input stream port and an output
stream port, each consisting of 8 bits of data and 2 bits to enable a simple ready/valid handshake.
Weights are loaded at the beginning of the computation of a LSTM layer, and inputs are streamed in
sequentially. The internal state of the LSTM cell in terms of cell state and hidden state is retained
between consecutive LSTM input “frames” to implement the recurrent nature of the network. A
C HIPMUNK engine can be used to implement a full LSTM network with Nx , Nh ≤ Nlstm storing
the weights on chip. Larger networks require to stream them in from an external source.
3.3

Systolic scaling

As the main target of the C HIPMUNK accelerator is to enable ultra-low latency applications such
as on-device real-time speech recognition, the computing power of a single engine might not be
sufficient. A single engine cannot be arbitrarily scaled up: LSTM units are all coupled to the same
set of registers via simple multiplexers, making it impractical to increase Nlstm above a few hundred
units. Instead, to provide a more scalable and elegant solution, we designed C HIPMUNK so that
multiple engines can be connected as tiles and share the burden of the RNN computation in a spatial
fashion.
Fig. 3 shows how the computation is split between multiple tiles in the case of a 3×3 array. The input
state is split into vectors of size Nlstm and each vector is broadcast vertically along a column. The
4

Figure 3: C HIPMUNK tile I/O and operation of a 3×3 systolic array during the load of the input state
xt , computation of the new it , ot , ft , ct , ht state values, and redistribution of the updated hidden
state ht .
CHIPMUNK (1 tile)
WEIGHT
SRAM #0

WEIGHT
SRAM #3

WEIGHT
SRAM #6

WEIGHT
SRAM #4

WEIGHT
SRAM #2

WEIGHT
SRAM #11

WEIGHT
SRAM #10

WEIGHT
SRAM #7

WEIGHT
SRAM #5

core pitch: 0.96 mm

LSTM CELLS

WEIGHT
SRAM #1

WEIGHT
SRAM #8

WEIGHT
SRAM #9

TECHNOLOGY

UMC 65nm
HVT std cells

AREA (core)

0.926 mm2

AREA (die)

1.568 mm2

I/O

32 logic pins
8 power pins
81.7 kB

MEMORY

PERFORMANCE 32 Gop/s @1.24 V
POWER (core)
OP. RANGE

3.8 Gop/s @0.75V
29 mW @1.24 V
1.2 mW @0.75V
0.75V - 1.24V

die pitch: 1.25 mm

Figure 4: Microphotograph of a C HIPMUNK die.
new value for the internal gates/states is computed by accumulating the results computed by each
row. Finally, the last column can compute the output hidden state, which is broadcasted vertically
to the columns for the next iteration (cf. Fig. 3c). For a given network size/systolic configuration,
these connections can be hard-wired such that no external multiplexing is required.

4

Results & Discussion

4.1

Silicon prototype & Comparison with State-of-the-Art

We designed and built a silicon prototype based on a single C HIPMUNK tile as described in Section
3.2. The prototype chip was fabricated in UMC 65 nm technology, using high voltage threshold
cells to minimize leakage. It features Nlstm = 96 LSTM units, which hold their weight and bias
parameters in 12 separate SRAM banks (81.7 kB in total). The full chip, shown in Fig. 4, occupies
1.57 mm2 including the pads. The chip exposes the interface described in Section 3.3 for tile-to-tile
communication, so that it would be possible to prototype a systolic array using many discrete chips.
Fig. 5 shows the experimental results obtained by testing the C HIPMUNK prototype at room temperature (25 ◦C). The prototype is fully functional in an operating range between 0.75 V (limited
by SRAM failure) and 1.24 V, corresponding to a range of 20 to 168 MHz of maximum clock
frequency and from 1.24 to 29 mW of power consumption. The peak performance in terms of operations per second2 of one C HIPMUNK chip is 32.2 Gop/s (at 1.24 V) and the peak energy efficiency
(3.08 Gop/s/mW) is reached at 0.75 V.
Table 1 compares architectural parameters and synthetic results between C HIPMUNK and the existing VLSI and FPGA-based implementations for which performance and energy numbers have been
published. Our work reaches comparable performance with the DNPU proposed by Shin et al. [14].
Performance is obviously below that claimed by Google TPU [10], but this is mostly due to the
different size. In fact, despite the TPU uses 28 nm integration, C HIPMUNK has 2.8× better area
2

As customary for neural network accelerators, we count 1 multiply-accumulate as 2 operations.

5

6

‡

†

*

UMC 65 nm CMOS
core: 0.93 mm2
die: 1.57 mm2
82 kB
8-16 bit
96
1.24 V / 0.75 V
168 / 20 MHz
29.03 / 1.24 mW
32.3 / 3.8 Gop/s
1.11 / 3.08 Gop/s/mW
34.4 Gop/s/mm2

65 nm CMOS
core*: ∼2.0 mm2
die: 16.0 mm2
10 kB
4-7 bit
64
1.1 V / 0.77 V
200 / 50 MHz
21 / 2.6 mW
25 / 6.25 Gop/s
1.10 / 2.22 Gop/s/mW
12.5 Gop/s/mm2

DNPU*[14]
28 nm CMOS
–
die: ∼300 mm2
28 MB
8-16 bit
66k
–
700 MHz
40-28 W
3.7-2.8 Top/s
<0.13 Gop/s/mW
12.3-9.3 Gop/s/mm2

Google TPU†[10]
Xilinx XCKU060
294k LUT
453k FF
4.2 MB
12 bit, pruned
–
–
200 MHz
41 W
2.5 Top/s (equiv.)‡
0.061 Gop/s/mW‡
–

Han et al.‡[11]

Xilinx Z7045
33k LUT
15k FF, 33 DSP
332 kB
5-16 bit
–
–
166 MHz
∼10 W
152 Gop/s
0.0152 Gop/s/mW
–

Rybalkin et al. [12]

Xilinx Z7020
7.6k LUT
13k FF, 50 DSP
–
16 bit
4
–
142 MHz
2.3 W
0.389 Gop/s
0.000146 Gop/s/mW
–

Chang et al. [13]

The DNPU is a mixed CNN-RNN processor. We report here only the figures related to the RNN subunit.
We present here the values from [10] based on the two LSTMs for which they measured the performance. For both, the TPU is severely memory bandwidth
limited.
They assume a well-structured sparsity of 11.2% in the weight matrices. Reported numbers are dense-equivalent throughput. Underlying compute throughput:
282 Gop/s.

On-chip memory
Arithmetic
Number of MACs
Core voltage
Frequency
Power
Peak performance
Energy efficiency
Area efficiency

Technology
Area

THIS WORK

Table 1: Comparison to Existing VLSI and FPGA Implementations

3.5

1.2

highest performance
32 Gop/s @ 1.24 V , 168 MHz

1.0
0.9

2.5

core power [mW]

core voltage [V]

1.1

2.0
1.5

0.8
0.7

1.0

highest efficiency
3.08 Gop/s/mW @ 0.75 V , 20 MHz
20

40

60

80
100
120
frequency [MHz]

140

160

180

0.7

0.8

0.9

1.0

1.1

1.2

energy efficiency [Gop/s/mW]

3.0

0.5
1.3

core voltage [V]

Figure 5: Frequency, power and performance of the C HIPMUNK prototype versus operating voltage
at room temperature (25 ◦C). The left shmoo plot shows core voltage versus operating frequency;
the color shade corresponds to the core power consumption (darker=less power). The right plot
shows energy efficiency versus core voltage; the color shade of the scattered dots corresponds to the
core power, while their size is proportional to the maximum frequency.
Table 2: CTC-3L-421H-UNI Speech Recognition LSTM Executed on C HIPMUNK With a 10 ms
Constraint
Configuration

PERF @1.24 V

EFF @0.75 V

Execution time

systolic 3×5×5
systolic 5×5
single

0.09 ms
1.59 ms
38.23 ms

0.76 ms
13.31 ms
321.14 ms

Peak power

systolic 3×5×5
systolic 5×5
single

1833.75 mW
611.25 mW
24.45 mW

165.75 mW
55.25 mW
2.21 mW

systolic 3×5×5
systolic 5×5

16.53 mW
96.89 mW

12.55 mW
-

Average power

efficiency - and a performance-wise “TPU-equivalent” array with ∼115 C HIPMUNK engines would
consume only 3.33 W, an order of magnitude less than the TPU. C HIPMUNK advances the state-ofthe-art energy efficiency with respect to the DNPU, showing a 39% improvement. Moreover, the
DNPU does not include any provision to address the fundamental memory boundedness of RNNs,
which C HIPMUNK addresses via systolic scaling. All FPGA implementations [11–13] are at least
two orders of magnitude less energy-efficient.
In terms of arithmetic precision we have chosen to use 8 bit fixed-point representations for storage
and perform the MAC operations with 16 bit precision. This is in line with Google’s TPU and higher
than the 4-7 bit of the DNPU.
4.2

Real-world speech recognition

To evaluate C HIPMUNK on a real-world problem, we targeted CTC-3L-421H-UNI, a 3-layer, 421hidden units per layer LSTM topology introduced by Graves et al. [1], which takes as input a stream
of 123 Mel-Frequency Cepstral Coefficients (MFCCs) extracted from an audio stream and identifies
phonemes with an error rate of 19.6%, evaluated on the TIMIT database. The MFCC input “frames”
are produced with a 10 ms rate, which means that any embedded low-latency real-time RNN implementation should be able to elaborate the full network in less than this time. We evaluate three
different C HIPMUNK configurations: a systolic array of 75 units, divided in 3 sub-arrays of 5 × 5
engines; a single array of 5 × 5 engines; and a single C HIPMUNK engine. The largest configuration
can host the full topology in a spatial fashion; each of the sub-arrays hosts one layer of the RNN.
After the initial programming phase, it does not need any reprogramming. The smaller arrays need
7

to be reconfigured at each new layer (in the 5 × 5 array case) or multiple times per layer (in the
single unit case).
Table 2 reports execution time and power for these three configurations. Execution times include
both computation and reconfiguration, excluding only the initial configuration which doesn’t need
to be repeated for each new frame/layer. Bold time/power values indicate configurations that can
meet the 10 ms deadline. As the CTC-3L-421H-UNI topology has ∼ 3.8 × 106 weights, a 3 × 5 × 5
systolic configuration is best used (all weights stored locally). Smaller configurations imply a >
80% overhead for reloading weights.
Average power, also shown in Table 2, is computed under the assumption that the array is perfectly
duty cycled when not in use over the 10 ms window. Even in the assumption that the C HIPMUNK
array is always-on, the 12.55 mW required to process this network would only add ∼4% to idle
power on a typical smartphone (300 to 400 mW [16]). Adding a filter to drop clearly uninteresting
input (e.g. silence) would likely decrease this overhead by an order of magnitude.

5

Conclusion

We have presented an architecture and silicon measurement results for a small (0.9 mm2 ) RNN hardware accelerator providing 3.8 Gop/s at 1.2 mW in 65 nm digital CMOS technology, resulting in new
state-of-the-art energy and area efficiencies of 3.08 Gop/s/mW and 34.4 Gop/s/mm2 . The systolic
design is scalable to accommodate also large RNNs efficiently by connecting multiple identical
chips on the circuit board.

Acknowledgements
This work was supported in part by the EU project ExaNoDe under grant H2020-671578, and in
part by the Swiss National Science Foundation project Micropower Deep Learning.

References
[1] A. Graves, A.-R. Mohamed, and G. Hinton, “Speech Recognition With Deep Recurrent Neural
Networks,” in Proc. IEEE ICASSP, 2013.
[2] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig, “The
Microsoft 2016 Conversational Speech Recognition System,” in Proc. IEEE ICASSP, 2017,
pp. 5255–5259.
[3] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, “Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine
Translation,” in Proc. ACL EMNLP, 2014, pp. 1724–1734.
[4] L. Cavigelli and L. Benini, “A 803 GOp/s/W Convolutional Network Accelerator,” IEEE
TCSVT, 2016.
[5] F. Conti, R. Schilling, P. D. Schiavone, A. Pullini, D. Rossi, F. K. Gurkaynak, M. Muehlberghuber, M. Gautschi, I. Loi, G. Haugou, S. Mangard, and L. Benini, “An IoT Endpoint Systemon-Chip for Secure and Energy-Efficient Near-Sensor Analytics,” IEEE TCAS, vol. 64, no. 9,
pp. 2481–2494, 9 2017.
[6] R. Andri, L. Cavigelli, D. Rossi, and L. Benini, “YodaNN: An Architecture for Ultra-Low
Power Binary-Weight CNN Acceleration,” IEEE TCAD, 2017.
[7] Y.-H. Chen, T. Krishna, J. Emer, and V. Sze, “Eyeriss: An Energy-Efficient Reconfigurable
Accelerator for Deep Convolutional Neural Networks,” in Proc. IEEE ISSCC, 2016, pp. 262–
263.
[8] Z. Du, R. Fasthuber, T. Chen, P. Ienne, L. Li, T. Luo, X. Feng, Y. Chen, and O. Temam,
“ShiDianNao: Shifting Vision Processing Closer to the Sensor,” in Proc. ACM/IEEE ISCA,
2015, pp. 92–104.
[9] V. Sze, Y.-H. Chen, T.-J. Yang, and J. Emer, “Efficient Processing of Deep Neural Networks:
A Tutorial and Survey,” arXiv:703.09039, 2017.
[10] N. P. Jouppi, A. Borchers, R. Boyle, P.-l. Cantin, C. Chao, C. Clark, J. Coriell, M. Daley,
M. Dau, J. Dean, B. Gelb, C. Young, T. V. Ghaemmaghami, R. Gottipati, W. Gulland, R. Hagmann, C. R. Ho, D. Hogberg, J. Hu, R. Hundt, D. Hurt, J. Ibarz, N. Patil, A. Jaffey, A. Jaworski,
8

[11]

[12]

[13]
[14]

[15]
[16]

A. Kaplan, H. Khaitan, D. Killebrew, A. Koch, N. Kumar, S. Lacy, J. Laudon, J. Law, D. Patterson, D. Le, C. Leary, Z. Liu, K. Lucke, A. Lundin, G. MacKean, A. Maggiore, M. Mahony,
K. Miller, R. Nagarajan, G. Agrawal, R. Narayanaswami, R. Ni, K. Nix, T. Norrie, M. Omernick, N. Penukonda, A. Phelps, J. Ross, M. Ross, A. Salek, R. Bajwa, E. Samadiani, C. Severn, G. Sizikov, M. Snelham, J. Souter, D. Steinberg, A. Swing, M. Tan, G. Thorson, B. Tian,
S. Bates, H. Toma, E. Tuttle, V. Vasudevan, R. Walter, W. Wang, E. Wilcox, D. H. Yoon,
S. Bhatia, and N. Boden, “In-Datacenter Performance Analysis of a Tensor Processing Unit,”
in Proc. ACM ISCA, 2017.
S. Han, J. Kang, H. Mao, Y. Hu, X. Li, Y. Li, D. Xie, H. Luo, S. Yao, Y. Wang, H. Yang,
and W. J. Dally, “ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA,” in
Proc. ACM/SIGDA FPGA, 2016.
V. Rybalkin, N. Wehn, M. R. Yousefi, and D. Stricker, “Hardware Architecture of Bidirectional
Long Short-Term Memory Neural Network for Optical Character Recognition,” in Proc. IEEE
DATE, 2017, pp. 1390–1395.
A. X. M. Chang and E. Culurciello, “Hardware Accelerators for Recurrent Neural Networks
on FPGA,” in Proc. IEEE ISCAS, 2017.
D. Shin, J. Lee, J. Lee, and H. J. Yoo, “DNPU: An 8.1TOPS/W Reconfigurable CNN-RNN
Processor for General-Purpose Deep Neural Networks,” in Proc. IEEE ISSCC, vol. 60, 2017,
pp. 240–241.
S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” Neural Computation, vol. 9,
no. 8, pp. 1735–1780, 1997.
A. Carroll and G. Heiser, “An Analysis of Power Consumption in a Smartphone,” in USENIX
ATC, 2010.

9

