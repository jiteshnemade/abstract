RTMobile: Beyond Real-Time Mobile Acceleration
of RNNs for Speech Recognition
Peiyan Dong,∗ Siyue Wang,∗ Wei Niu,† Chengming Zhang,‡ Sheng Lin,∗ Zhengang Li,∗ Yifan Gong,∗
Bin Ren,† Xue Lin,∗ Yanzhi Wang,∗ and Dingwen Tao‡

arXiv:2002.11474v1 [cs.SD] 19 Feb 2020

∗ Northeastern

University, MA, USA
{dong.pe, wang.siy, lin.sheng, li.zhen}@husky.neu.edu, {xue.lin, yanz.wang}@northeastern.edu
∗ The College of William and Mary, VA, USA
wniu@email.wm.edu, bren@cs.wm.edu
‡ The University of Alabama, AL, USA
czhang82@crimson.ua.edu, tao@cs.ua.edu
Abstract—Recurrent neural networks (RNNs) based automatic
speech recognition has nowadays become prevalent on mobile
devices such as smart phones. However, previous RNN compression techniques either suffer from hardware performance
overhead due to irregularity or significant accuracy loss due
to the preserved regularity for hardware friendliness. In this
work, we propose RTMobile that leverages both a novel blockbased pruning approach and compiler optimizations to accelerate
RNN inference on mobile devices. Our proposed RTMobile is the
first work that can achieve real-time RNN inference on mobile
platforms. Experimental results demonstrate that RTMobile can
significantly outperform existing RNN hardware acceleration
methods in terms of inference accuracy and time. Compared with
prior work on FPGA, RTMobile using Adreno 640 embedded
GPU on GRU can improve the energy-efficiency by about 40×
while maintaining the same inference time.
Index Terms—RNN, pruning, real-time acceleration, mobile

I. INTRODUCTION
Deep neural network (DNN) has evolved to the stateof-the-art technique due to its high prediction accuracy in
many artificial intelligence tasks, such as image recognition
and characterization [1]–[7], speech recognition [8]–[11], and
recommender system [12]. Among various DNN architectures,
recurrent neural networks (RNNs) are widely used for speech
recognition tasks because they can contain cycles to carry
information across neurons when reading inputs. For instance,
Gated Recurrent Unit (GRU) [13], the most recent representative popular type of RNNs, achieve great success in automatic
speech recognition. In recent years, executing DNNs on mobile
platforms has become more and more popular because many
high-end mobile devices are emerging. Several recent studies
have proposed techniques to accelerate large-scale DNNs in
mobile environment. However, due to fairly high computation
complexity and memory consumption when executing RNNs,
it is very challenging to deploy RNNs on current embedded
processors and mobile devices to achieve real-time inference.
DNN model compression provides an effective way to
mitigate the computation and memory challenges bringing
by DNNs [14]. Many model compression techniques have
Corresponding author: Dingwen Tao, Department of Computer Science,
The University of Alabama, Tuscaloosa, AL 35487, USA.

been studied for recent years. For example, weight pruning
can provide a notable reduction ratio in the model size.
Early work [15] on non-structured weight pruning eliminates
weights at arbitrary location, which leads to the pruned model
to be stored in a sparse matrix format, such as compressed
sparse column (CSC) format. Non-structured weight pruning,
however, hurts processing throughput because the indices
in the compressed weight representation result in stalls or
complex workloads on highly parallel architectures, such as
GPUs and FPGAs. On the other hand, structured weight
pruning [16] is more hardware friendly. By exploiting filter
pruning [17] and channel pruning [18], the pruned model is
more regular in terms of the shape, which can eliminate storing
the weight indices. However, structured pruning hurts accuracy
more than non-structured pruning. Moreover, state-of-the-art
model-compression-based RNN acceleration techniques such
as ESE [19] and C-LSTM [20] still suffer from limited inference accuracy and processing throughput, which prevents them
to be implemented on mobile devices. Furthermore, existing
DNN acceleration frameworks for mobile devices such as
TVM [21] do not even support RNN. Therefore, in order to
achieve the real-time inference for RNNs on mobile devices,
it is necessary to develop an end-to-end RNN acceleration
framework that can achieve both high inference accuracy and
high computational efficiency.
In this paper, we propose a real-time RNN acceleration
framework for mobile devices named RTMobile. RTMobile
is composed of two main components: block-based structured
pruning and compiler-assisted performance optimization. Unlike traditional structured pruning methods used on DNNs,
our novel block-based structured pruning approach that can
provide a finer pruning granularity to maintain high inference
accuracy while significantly reducing the RNN model size. We
also propose several compiler-based optimization techniques
to determine the block size and generate the optimal code on
mobiles. Our contributions are summarized as follows.
•

We propose a novel RNN acceleration framework for
mobile devices, namely, RTMobile. To the best of our
knowledge, RTMobile is the first work that achieves

(ﬁlter width) x (ﬁlter height) x (number of channels)

wn,m,0 wn,m,1

wn,m,k

...

...

w1,m,k

...

...

...
...

(b)

...
...
...

As a representative technique in DNN model compression,
DNN weight pruning removes the redundant or less important
weights to reduce the storage and computational costs for
the inference phase. There exist two mainstreams of weight
pruning, i.e., non-structured pruning and structured pruning.
a) Non-structured pruning: Non-structured weight pruning is fine-grained and prunes weights at arbitrary locations.
The early work proposed by Han et al. [22] leverages a heuristic method to iteratively prune weights with small magnitudes.
With the successful applications of the powerful ADMM

Filter Pruning

wn,1,k

(a)
Column Pruning

...
...
...

B. DNN Model Compression Techniques

...

The Gated Recurrent Unit (GRU) is a variation from the
LSTM, proposed by Cho et al. [13]. It combines the forget
and input gates into a single “update gate”. It also merges the
cell state and hidden state, and makes some other changes.
The resulting GRU model is simpler than standard LSTM
models, and has been growing increasingly popular. Fig. 1
shows a single GRU, whose functionality is derived by using
the following equations iteratively from t = 1 to T , where
e h are respectively the update gate, output
symbols z, r, h,
gate, cell state, and cell output. As GRU is a more advanced
version of RNN than LSTM, we mainly focus on GRU model
in this work.

wn,0,k wn,1,0 wn,1,1

...

A. Gated Recurrent Unit

...

In this section, we present some background information
about GRU, DNN model compression, and DNN mobile
acceleration framework, and discuss our research motivation.

w1,m,0 w1,m,1

...

II. BACKGROUND AND M OTIVATION

...

•

...

•

w1,1,k

...

•

real-time RNN inference on mobile devices.
We propose a fine-grained Block-based Structured
Pruning algorithm (BSP) for both high inference accuracy
and high computational efficiency.
We develop a series of compiler-based optimization
techniques to further accelerate RNN inference on mobile platforms, including matrix reorder, load redundant
elimination, and a new compact data format for pruned
model storage (called BSPC, i.e., Block-based Structured
Pruning Compact format).
We compare RTMobile with multiple state-of-the-art
methods based on a representative RNN (GRU) using a
well-known speech recognition dataset. Evaluation results
demonstrate that RTMobile is the first work that can
compress the GRU model by over 10x without losing
accuracy. Experiments also illustrate that RTMobile can
obtain about 50x energy-efficiency improvement over
prior work with the same inference time.

w1,0,k w1,1,0 w1,1,1

...

...

wn,0,0 wn,0,1

Fig. 1: A Single GRU model.

...

w1,0,0 w1,0,1

...

Num of
ﬁlters

Channel m
w0,m,0 w0,m,1 w0,m,k

...

Channel 0
Channel 1
w0,0,k w0,1,0 w0,1,1
w0,1,k
w0,0,0 w0,0,1

Channel Pruning

Fig. 2: (a) To support GEMM computation, the weight tensor representation of a CONV layer is transformed into the weight matrix representation. (b) How different structured weight pruning
schemes are implemented on the weight matrix representation.

optimization framework, existing research works [23], [24]
achieve a very high weight reduction ratio while maintaining
promising accuracy. However, non-structured methods lead to
sparse and irregular weight matrices, which require indices to
be stored in a compressed format. Though saving the storage
cost, the decoding of each stored index requires a search
over the whole activation vector. Consequently, it suffers from
limited acceleration in actual hardware implementation [19].
b) Structured pruning: To overcome the limitations of
non-structured pruning, recent works [16], [18], [25] considered to incorporate regularity in weight pruning with a main
focus on convolutional (CONV) layers of DNNs. Previous
works mainly focus on two types of structured pruning: filter
pruning and channel pruning. Filter pruning, also known
as row pruning, removes the entire filter(s), while channel
pruning removes the whole channel(s). Figure 2 illustrates
the example of transforming convolutional computation into
general matrix multiplication (GEMM) by converting weight
tensors and feature map tensors to matrices [26]. In general,
structured pruning directly reduces the dimension of a weight
matrix and preserves a full matrix format, thereby facilitating hardware implementations. On the downside, the coarsegrained nature of structured pruning hurts the accuracy more
significantly.
C. DNN Acceleration on Mobile Devices
Many efforts target accelerating DNN execution on mobile devices in the past few years, including MCDNN [27],
DeepMon [28], TFLite [29], TVM [21], and Alibaba Mobile
Neural Network [30]. However, most of them do not deeply
exploit model compression techniques as RTMobile. In particular, none of the existing frameworks can even support RNN
acceleration on mobile devices.
D. Research Motivation
Based on the survey of recent research works, we conclude
the following insights: (i) non-structured pruning has the
advantage of very high compression ratio but is typically not
compatible with GPU acceleration for inference; (ii) structured

pruning facilitates hardware implementations but is often subjected to accuracy degradation, especially when it is applied
to time-based RNNs. To overcome the limitations of current
methods, a more flexible and fine-grained pruning policy is
needed. This work specifically focuses on RNN models that
have not been extensively studied.

where N is the total number of weight tensor in recurrent
neural network, f (W, b) is the loss function, and g(W ) is an
indicator function that is zero when the constraint S = { the
number of nonzero weights is less than certain threshold } is
satisfied, but +∞ otherwise.
The augmented Lagrangian formation of problem (1) is
Lp = minimize

III. R ELATED W ORK

{Wi }}

Many existing studies have implemented model compression algorithms for RNN acceleration on FPGAs [11], [19],
[20], [31]–[33]. However, the majority of these works focus on
constructing new RNN architectures [32] rather than software
and hardware co-design framework. Instead, our RTMobile
proposes architecture designs in both software and hardware
level. In this work, we mainly discuss and compare RTMobile
with two most recent and related approaches, i.e., ESE [19]
and C-LSTM [20], which not only address the RNN model
compression problem on algorithm/software but also take into
account the hardware efficiency on hardware (i.e., FPGAs).

N
 X
ρi
kWi − Zi + Ui k2F ,
f {Wi , bi }N
+
i=1
2
i=1
(2)

where ρi is a penalty value, Zi is pruning mask and Ui is
dual variable.
The ADMM algorithm [34] is to iteratively update the
indicated pruning mask and retrain the neural network under
this mask, until a good mask and neural network converge. It
proceed by repeating iteration k = 0, 1, . . . as following:
Wik+1 := arg min Lp ({Wi }, {Zki }, {Uki }),
Zk+1
:= arg min Lp ({Wik+1 }, {Zi }, {Uki }),
i

ESE proposes an optimized LSTM compression framework
on FPGA, which sparses the model through parameter pruning
[15], [22]. Compared with both CPU- and GPU-based implementations, ESE achieves higher energy efficiency on FPGA.
However, the design of ESE has three main limitations: (1)
ESE’s irregular pruning method used for model compression
causes large overhead when performing read/write operations
on hardware; (2) the irregularity of weight matrix storage
in ESE results in inefficient implementations of indices that
consume extra storage cost, thus the computing power of
the FPGA is not fully exerted; and (3) ESE only marginally
improves compression ratio taking into account indices.
B. C-LSTM
In order to solve the problem caused by irregular pruning,
Wang et al. [20] propose an approach (called C-LSTM)
to employ a structured compression technique using blockcirculant matrices to compress the LSTM model. With regular
structure of the block-circulant matrices, C-LSTM can further
reduces both computational and storage complexity compared
with ESE. However, the coarse-grained nature of structured
pruning also cause relatively significant degradation on the
model accuracy. Moreover, the advanced ADMM-based neural
network pruning method, which can effectively handle both
model compression and accuracy, is not supported in the CLSTM training because it requires the most advanced optimizer in stochastic gradient decent (e.g., Adam optimizer).
C. ADMM
The pruning problem can be formulated as the minimization
of f (W, b) + g(W ) by following:
minimize


N 
f {Wi , bi }N
i=1 + g {Wi }i=1 ,

subject to

Wi ∈ Si , i = 1, . . . , N,

{Wi }

(1)

(4)

Zi

.
:= Uki + Wik+1 − Zk+1
Uk+1
i
i

A. ESE

(3)

Wi

(5)

The pruning mask can be trained by Algorithm 1.
IV. P ROPOSED RTM OBILE F RAMEWORK
In this section, we describe in detail RTMobile, our proposed mobile acceleration framework for RNNs.
A. Block-based Structured Pruning
To better facilitate the compression ratio and ensure the
structured model architecture for hardware implementations,
we propose Block-based Structured Pruning (BSP) algorithm.
In general, training a BSP compressed model can be separated
into two main steps: Step 1) row-based column block pruning
and Step 2) column-based row pruning.
The training process starts with splitting the whole weight
matrix W into N umr rows horizontally. For each row, we
divide it into N umc blocks and then perform the structured
pruning using ADMM method (discussed in Section III-C).
Then, we perform column-based row pruning over the entire
weight matrix W in the step 2. Given the constraint of block
number after dividing by N umc and N umr , the pruned model
can achieve a satisfactory performance overhead on hardware.
The training process continues iteratively until all the blocks
are pruned. We identify that by doing so, the training performance is stable, and the whole weight matrix after pruning
is decentralized. Our BSP training approach is summarized in
Algorithm 1.
B. Compiler-assisted RNN Acceleration Framework
After block-based structured pruning, RTMobile relies on
a compiler-assisted RNN acceleration framework to achieve
efficient RNN inference on mobile devices. This compiler
framework consists of three key optimizations that work on
each RNN layer (as shown in Figure 3): matrix reorder, load
redundancy elimination, and a compact data storage format
for pruned RNN matrices, BSPC (i.e., Block-based Structured

Fig. 3: Systematic overview of RTMobile acceleration framework.

achieve balanced processing.
b) Redundant load elimination: Within a group, each
thread processes multiple continuous rows, offering us an
opportunity of eliminating the redundant memory load operations. This optimization is specifically enabled by our blockbased structured pruning, because after such pruning, the
preserved weights in two neighbor rows may share the same
pattern and require the same data in the input feature maps. It
is difficult to explore this optimization opportunity for existing
unstructured weight pruning due to its irregularity.
c) BSPC format: Our proposed block-based structured
pruning also guides us to design a more compact data structure
than traditional CSR format (called BSPC format) to store
RNN weight matrices. This is because within each block the
preserved weights only exist in certain rows and columns,
enabling to further compact the index array in CSR. The BSPC
format also includes the matrix reorder information to match
the corresponding input feature map with the weight matrix.
The BSPC format significantly reduces the memory footprint
thus alleviating the memory-bound issue in RNN execution.
In addition to above optimizations, our compiler framework
also includes an auto-tuning component to perform an offline
search of the best execution configurations like the matrix
tiling size, unrolling size, memory placement, etc. In particular, we employ it to find the best block size that results in an
optimal combination of accuracy and performance.
V. E XPERIMENTAL E VALUATION
Pruning Compact format). These optimizations aim to address
three key challenges in pruned RNN execution: thread divergence and load imbalance among threads, redundant memory
access, and unnecessary zero storage.
a) Matrix reorder: The matrix is executed by multiple
CPU/GPU threads simultaneously. Without a further reorder,
these threads may execute rows with significantly divergent
computations, causing severe load imbalance issue that hurts
thread-level parallelism. Therefore, RTMobile introduces a
matrix reorder optimization to group the rows with the same
(or similar) computation patterns together. After this reorder,
the rows in each group are assigned to multiple threads to

In this section, we evaluate RTMobile by comparing it with
several state-of-the-art methods. There are three evaluation
objectives: 1) comparing RTMobile with other model compression methods and demonstrating that our method outperforms
others in both compression rate and accuracy; 2) showing
RTMobile has both higher computational efficiency and energy
efficiency than a well-known deployment on FPGA (ESE
[19])1 ; and 3) studying the relationship between compression
rate and inference execution time.
1 We compare RTMobile on mobile with ESE on FPGA because (1) none
of the existing RNN acceleration works supports mobile device, and (2) ESE
provides one of the highest inference accuracy among prior works.

TABLE I: Results of Different Model Compression Methods on GRU Using TIMIT Dataset: PER is phone error rate, the lower the
better. Baseline PER is for dense (non-pruned) models and pruned PER is for pruned compressed models. PER Degrad. represents for the
PER degradation, i.e., P ERpruned − P ERbaseline . The rest columns show the column compression rate, row compression rate, the number
of preserved parameters, and the overall compression rate, respectively.
Method
ESE [19]
C-LSTM [20]
C-LSTM [20]
BBS [35]
Wang [36]
E-RNN [37]
BSP (ours)
BSP (ours)
BSP (ours)
BSP (ours)
BSP (ours)
BSP (ours)
BSP (ours)
BSP (ours)
BSP (ours)
BSP (ours)

PER (%)
(baseline - pruned)
20.40 - 20.70
24.15 - 24.57
24.15 - 25.48
23.50 - 23.75
–
20.02 - 20.20
18.80 (w/o pruning)
18.80 - 18.80
18.80 - 19.40
18.80 - 19.60
18.80 - 20.60
18.80 - 21.50
18.80 - 23.20
18.80 - 24.20
18.80 - 24.20
18.80 - 25.50

PER Degrad.

Column Compress. Rate

Row Compress. Rate

Para. No.

Overall Compress. Rate

0.30
0.42
1.33
0.25
0.91
0.18
0
0
0.60
0.80
1.80
2.70
4.40
5.40
5.40
6.70

–
–
–
–
–
–
1
10
16
16
16
20
16
20
20
20

–
–
–
–
–
–
1
1
1.25
2
5
8
16
10
16
20

0.37M
0.41M
0.20M
0.41M
0.81M
1.20M
9.6M
0.96M
0.48M
0.33M
0.22M
0.12M
0.09M
0.06M
0.04M
0.03M

8×
8×
16×
8×
4×
8×
1×
10×
19×
29×
43×
80×
103×
153×
245×
301×

TABLE II: Performance and Energy Evaluation on Mobile GPU and CPU: GOP refers to Giga Operations. GPU/CPU energy efficiency
is calculated as Inf erenceF rames/(P ower × Inf erenceT ime), i.e., the number of frames inferred per unit energy consumption. This
table normalizes our method’s GPU/CPU energy efficiency by the ESE FPGA implementation’s. As our compression rate reaches 245×,
our GPU inference time becomes slightly faster than ESE’s (82.7us). Our GPU implementation uses 16-bit floating point.
Compression Rate
1× (baseline)
10×
19×
29×
43×
80×
103×
153×
245×
301×

GOP

GPU Time / Frame (us)

GPU GOP/s

0.5800
0.0580
0.0330
0.0207
0.0143
0.0080
0.0060
0.0039
0.0028
0.0020

3590.12
495.26
304.11
233.89
186.05
130.00
109.76
97.11
81.64
79.13

161.55
117.11
108.51
88.29
76.86
61.54
54.66
40.16
34.30
25.27

GPU Energy Efficiency
(normalized with ESE)
0.88
6.35
10.35
13.45
16.91
24.2
28.67
32.4
38.54
39.76

CPU Time / Frame (us)

CPU GOP/s

7130.00
1210.20
709.33
464.73
344.77
218.01
202.72
170.74
151.28
145.93

81.35
47.93
46.52
44.43
41.48
36.70
29.59
22.84
18.51
13.71

CPU Energy Efficiency
(normalized with ESE)
0.25
1.48
2.52
3.85
5.19
8.20
8.82
10.47
11.82
12.25

9.6M overall number of parameters.
Evaluation Dataset. We conduct our experiments on the
TIMIT dataset [38], which is widely adopted for evaluating
automatic speech recognition systems. The TIMIT dataset
contains broadband recordings from 630 speakers reading ten
phonetically rich sentences in eight major dialects of American
English, each reading ten phonetically rich sentences.
B. Evaluation Results and Discussion

Fig. 4: Speedup using RTMobile with different compression rates on
mobile platform.

A. Experiment Setup
Experimental Platform. We conduct our experiments using a
Samsung Galaxy S10 with the latest Qualcomm Snapdragon
855 mobile platform, which consists of a Qualcomm Kryo 485
Octa-core CPU and a Qualcomm Adreno 640 GPU.
Model Architecture. We evaluate RTMobile and compare it
with the state-of-the-art methods on the popular GRU RNN
model, which has been widely used in previous studies [19],
[20], [37]. The GRU model contains 2 GRU layers and about

Compression Rate and Accuracy. Table I illustrates the
results (including phone error rate and number of preserved
parameters) of RTMobile with different compression rates and
the comparison with other state-of-the-art methods, including
ESE [19], C-LSTM [20], BBS [35], Wang [36] and E-RNN
[37]. For a fair comparison, we train all models using the
same TIMIT dataset [38]. Benefit from the most advanced
PyTorch-Kaldi Speech Recognition Toolkit [39], the baseline
GRU model for our RTMobile can achieve higher recognition
accuracy than the other methods before pruning, e.g., our PER
is 5.35% lower than C-LSTM’s (18.80% v.s. 24.15%).
We observe that our proposed BSP method can guarantee
no accuracy degradation when the compression rate is not
higher than 10×, which is superior than ESE and C-LSTM
from both compression rate and inference accuracy. We also
observe that BSP can stably keep a high accuracy compared

to the other methods when the compression rate is relatively
high. For instance, when the compression rate is 103×, the
BSP pruned model can even outperform the C-LSTM baseline
model in terms of both compression rate and accuracy. The CLSTM baseline model (with 3.25M parameters) has 36× more
parameters than our BSP pruned model, but its PER is 0.95%
higher than ours (24.15% vs. 23.20%). In addition, we use BSP
to further prune the model until the rate of 301× and observe
that our method can well adapt to ultra-high compression rate
scenario. For example, our model with 245× compression rate
can still maintain the same-level PER as the C-LSTM baseline
model (24.20% vs. 24.15%) and reduce the parameter number
by over 80× (0.04M vs. 3.25M).
Inference Time and Energy Efficiency. Table II presents the
evaluation results of RTMobile’s inference time, Giga Operations Per Second (GOP/s), and energy efficiency (normalized
with ESE method) on mobile GPU and CPU, respectively.
The table illustrates that, when the compression rate is higher
than 245×, RTMobile can outperform in energy efficiency
by about 40× compared with ESE while maintaining the
same inference time (ESE’s inference time is 82.7 us) on
the mobile GPU (ESE uses a large FPGA platform of 41W
power, and thus it is easier to achieve higher energy efficiency
than speed). Please note that this is a clear feat, as it is
typically perceived that FPGA is more energy-efficient than
general-purpose computing devices. This is because of two
main reasons. First, comparing to ESE’s activation calculation by look-up tables that results in limited parallelization
and irregular memory accesses (two key performance factors
on FPGA), RTMobile’s compiler optimizations significantly
improve both the parallelization and memory performance.
Second, RTMobile has a much better compression rate (with
a negligible accuracy loss), resulting in a more significant
computation reduction. Although our compression rates are
significant, we must emphasize that the inefficiency in FPGA
implementation in ESE (especially activation) plays an equally
important, if not more, role. As can be seen from the table, our
GPU energy efficiency (frames in unit energy) is almost the
same as ESE (which uses compression) even when we do not
have any pruning. With increase in the compression rate, the
computation pattern becomes I/O and memory bounded, the
memory access pattern becomes more irregular, which leads
to lower CPU/GPU GOP/s.
Relationship between Compression Rate and Inference
Time. Figure 4 further illustrates the relationship between
inference time and compression rate. The inference time is in
the form of speedups over our own dense CPU/GPU baselines,
respectively. The speedup grows as compression rate increases.
The speedup becomes stable when compression rate reaches to
a certain range (e.g., compression rate reaches 250×). When
the compression rate is 245×, our inference time on mobile
GPU is the same to ESE’s on FPGA.
VI. C ONCLUSION
In this paper, we propose the first RNN acceleration framework for mobiles, called RTMobile. We develop a novel block-

based pruning algorithm and three compiler optimizations to
achieve real-time inference without any accuracy degradation.
Experimental results demonstrate that RTMobile significantly
outperforms the existing RNN hardware acceleration methods
in terms of compression rate, inference accuracy, execution
time, and energy efficiency.
REFERENCES
[1] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from
tiny images,” 2009.
[2] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2016, pp. 770–778.
[3] Y. LeCun, “Lenet-5, convolutional neural networks,” URL: http://yann.
lecun. com/exdb/lenet, 2015.
[4] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
with deep convolutional neural networks,” in Advances in neural information processing systems, 2012, pp. 1097–1105.
[5] S. Wang, X. Wang, P. Zhao, W. Wen, D. Kaeli, P. Chin, and X. Lin, “Defensive dropout for hardening deep neural networks under adversarial
attacks,” in Proceedings of the International Conference on ComputerAided Design. ACM, 2018, p. 71.
[6] X. Wang, S. Wang, P.-Y. Chen, Y. Wang, B. Kulis, X. Lin, and
P. Chin, “Protecting neural networks with hierarchical random switching:
towards better robustness-accuracy trade-off for stochastic defenses,” in
Proceedings of the 28th International Joint Conference on Artificial
Intelligence. AAAI Press, 2019, pp. 6013–6019.
[7] A. Zhao, K. Fu, S. Wang, J. Zuo, Y. Zhang, Y. Hu, and H. Wang,
“Aircraft recognition based on landmark detection in remote sensing
images,” IEEE Geoscience and Remote Sensing Letters, vol. 14, no. 8,
pp. 1413–1417, 2017.
[8] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with
deep recurrent neural networks,” in 2013 IEEE international conference
on acoustics, speech and signal processing. IEEE, 2013, pp. 6645–
6649.
[9] L. Deng, G. Hinton, and B. Kingsbury, “New types of deep neural
network learning for speech recognition and related applications: An
overview,” in 2013 IEEE International Conference on Acoustics, Speech
and Signal Processing. IEEE, 2013, pp. 8599–8603.
[10] R. Collobert and J. Weston, “A unified architecture for natural language
processing: Deep neural networks with multitask learning,” in ICML.
ACM, 2008, pp. 160–167.
[11] Z. Li, C. Ding, S. Wang, W. Wen, Y. Zhuo, C. Liu, Q. Qiu, W. Xu,
X. Lin, X. Qian et al., “E-rnn: Design optimization for efficient recurrent
neural networks in fpgas,” in 25th IEEE International Symposium on
High Performance Computer Architecture, HPCA 2019. Institute of
Electrical and Electronics Engineers Inc., 2019, pp. 69–80.
[12] H. Wang, N. Wang, and D.-Y. Yeung, “Collaborative deep learning for
recommender systems,” in KDD. ACM, 2015, pp. 1235–1244.
[13] K. Cho, B. Van Merriënboer, D. Bahdanau, and Y. Bengio, “On the
properties of neural machine translation: Encoder-decoder approaches,”
arXiv preprint arXiv:1409.1259, 2014.
[14] S. Jin, S. Di, X. Liang, J. Tian, D. Tao, and F. Cappello, “Deepsz:
A novel framework to compress deep neural networks by using errorbounded lossy compression,” in HPDC. ACM, 2019, pp. 159–170.
[15] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural networks with pruning, trained quantization and huffman
coding,” arXiv preprint arXiv:1510.00149, 2015.
[16] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li, “Learning structured
sparsity in deep neural networks,” in NIPS, 2016, pp. 2074–2082.
[17] Y. He, G. Kang, X. Dong, Y. Fu, and Y. Yang, “Soft filter pruning for
accelerating deep convolutional neural networks,” in IJCAI, 2018, pp.
2234–2240.
[18] Y. He, X. Zhang, and J. Sun, “Channel pruning for accelerating very
deep neural networks,” in ICCV. IEEE, 2017, pp. 1398–1406.
[19] S. Han, J. Kang, H. Mao, Y. Hu, X. Li, Y. Li, D. Xie, H. Luo, S. Yao,
Y. Wang, H. Yang, and W. J. Dally, “Ese: Efficient speech recognition
engine with sparse lstm on fpga.” in FPGA, 2017, pp. 75–84.
[20] S. Wang, Z. Li, C. Ding, B. Yuan, Q. Qiu, Y. Wang, and Y. Liang, “Clstm: Enabling efficient lstm using structured compression techniques
on fpgas,” in FPGA. ACM, 2018, pp. 11–20.

[21] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan,
L. Wang, Y. Hu, L. Ceze et al., “TVM: An automated end-to-end
optimizing compiler for deep learning,” in OSDI, 2018.
[22] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and
connections for efficient neural network,” in NIPS, 2015, pp. 1135–1143.
[23] T. Zhang, S. Ye, Y. Zhang, Y. Wang, and M. Fardad, “Systematic weight
pruning of dnns using alternating direction method of multipliers,” arXiv
preprint arXiv:1802.05747, 2018.
[24] A. Ren, T. Zhang, S. Ye, W. Xu, X. Qian, X. Lin, and Y. Wang,
“Admm-nn: an algorithm-hardware co-design framework of dnns using
alternating direction methods of multipliers,” in ASPLOS, 2019.
[25] C. Min, A. Wang, Y. Chen, W. Xu, and X. Chen, “2pfpce: Twophase filter pruning based on conditional entropy,” arXiv preprint
arXiv:1809.02220, 2018.
[26] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,
and E. Shelhamer, “cudnn: Efficient primitives for deep learning,” arXiv
preprint arXiv:1410.0759, 2014.
[27] S. Han, H. Shen, M. Philipose, S. Agarwal, A. Wolman, and A. Krishnamurthy, “Mcdnn: An approximation-based execution framework for
deep stream processing under resource constraints,” in MobiSys. ACM,
2016, pp. 123–136.
[28] L. N. Huynh, Y. Lee, and R. K. Balan, “Deepmon: Mobile gpubased deep learning framework for continuous vision applications,” in
MobiSys. ACM, 2017, pp. 82–95.
[29] https://www.tensorflow.org/mobile/tflite/.
[30] https://github.com/alibaba/MNN/.
[31] S. Li, C. Wu, H. Li, B. Li, Y. Wang, and Q. Qiu, “Fpga acceleration
of recurrent neural network based language model,” in FCCM. IEEE,
2015, pp. 111–118.
[32] E. Nurvitadhi, J. Sim, D. Sheffield, A. Mishra, S. Krishnan, and D. Marr,
“Accelerating recurrent neural networks in analytics servers: Comparison
of fpga, cpu, gpu, and asic,” in FPL. IEEE, 2016, pp. 1–4.
[33] Y. Guan, Z. Yuan, G. Sun, and J. Cong, “Fpga-based accelerator for long
short-term memory recurrent neural networks,” in ASP-DAC. IEEE,
2017, pp. 629–634.
[34] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed
optimization and statistical learning via the alternating direction method
of multipliers,” Foundations and Trends® in Machine Learning, vol. 3,
no. 1, pp. 1–122, 2011.
[35] S. Cao, C. Zhang, Z. Yao, W. Xiao, L. Nie, D. Zhan, Y. Liu, M. Wu,
and L. Zhang, “Efficient and effective sparse lstm on fpga with bankbalanced sparsity,” in FPGA. ACM, 2019, pp. 63–72.
[36] S. Wang, P. Lin, R. Hu, H. Wang, J. He, Q. Huang, and S. Chang,
“Acceleration of lstm with structured pruning method on fpga,” IEEE
Access, vol. 7, pp. 62 930–62 937, 2019.
[37] Z. Li, C. Ding, S. Wang, W. Wen, Y. Zhuo, X. Lin, X. Qian, and Y. Wang,
“E-rnn: design optimization for efficient recurrent neural networks in
fpgas,” in HPCA. IEEE, 2019.
[38] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett,
N. L. Dahlgren, and V. Zue, “Timit acoustic-phonetic continuous speech
corpus,” Linguistic data consortium, vol. 10, no. 5, p. 0, 1993.
[39] M. Ravanelli, T. Parcollet, and Y. Bengio, “The pytorch-kaldi speech
recognition toolkit,” in In Proc. of ICASSP, 2019.

