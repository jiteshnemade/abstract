Distance and Equivalence between Finite State Machines and
Recurrent Neural Networks: Computational results
Reda Marzouk1 and Colin de la Higuera2

arXiv:2004.00478v1 [cs.LG] 1 Apr 2020

1,2

Université de Nantes / Laboratoire des Sciences du Numérique de Nantes (LS2N UMR
CNRS 6004), Nantes, France
April 2, 2020

Abstract

1

The need of interpreting Deep Learning (DL) models has led, during the past years, to a proliferation
of works concerned by this issue. Among strategies
which aim at shedding some light on how information is represented internally in DL models, one consists in extracting symbolic rule-based machines from
connectionist models that are supposed to approximate well their behaviour. In order to better understand how reasonable these approximation strategies
are, we need to know the computational complexity
of measuring the quality of approximation. In this
article, we will prove some computational results related to the problem of extracting Finite State Machine (FSM) based models based on trained RNN Language models. More precisely, we’ll show the following: (a) For general weighted RNN-LMs with a single hidden layer and a ReLu activation: - The equivalence problem of a PDFA/PFA/WFA and a weighted
first-order RNN-LM is undecidable; - As a corollary,
the distance problem between languages generated by
PDFA/PFA/WFA and that of a weighted RNN-LM is
not recursive; -The intersection between a DFA and the
cut language of a weighted RNN-LM is undecidable; The equivalence of a PDFA/PFA/WFA and weighted
RNN-LM in a finite support is EXP-Hard; (b) For consistent weight RNN-LMs with any computable activation function: - The Tcheybechev distance approximation is decidable; - The Tcheybechev distance approximation in a finite support is NP-Hard. Moreover, our
reduction technique from 3-SAT makes this latter fact
easily generalizable to other RNN architectures (e.g.
LSTMs/RNNs), and RNNs with finite precision.

Recurrent Neural Networks and their different variants
represent an important family of Deep Learning models
suitable to learning tasks with sequential data. However, just like all Deep Learning models in general, this
class of models lacks interpretability, which restricts its
applicability to highly critical tasks related for instance
to security and health, where a formal specification of
systems is a mandatory requirement to be approved for
deployment in real-case situations. Due to the crucial
importance of this limitation, the awareness of providing both expressive and interpretable models keeps
growing within the Deep Learning community, resulting in a proliferation of research works focusing on this
topic. In general, two major paradigms have been explored in the literature to tackle this issue:

Mots-clef : Recurrent Neural Networks, Finite State
Machines, Distances, Equivalence.

• Post-hoc methods: Techniques belonging to this
family subsume the existence of an already trained

Introduction

• Interpretable models by design: In this family of models, the idea is to construct deep learning architectures with the concern of interpretability
raised early on the design phase. This change of
the architecture may take the form of adding special components to traditional models whose role is
to leverage the interpretability issue[14] [35], using
interpretation-friendly activation functions, modifying the loss function by injecting a term that favours
a resulting interpretable model[29], or using a variant of the back-propagation algorithm for training,
which enforces that the resulting model is readily
interpretable [43]. Nevertheless, enforcing the constraint of interpretability by design leads inevitably
to a loss of flexibility of the constructed models, and
hence a drop of their expressive power.

1

DL model and the objective is then to design algorithmic and visualization tools [18][41] that attempt
to answer questions related to the interpretability of
the original model such as:
(a) Semantics of hidden units of the network model:
Or, alternatively which role is played by each hidden unit in the network with respect to the learning
task (e.g. does a neuron serve as a counter in RNNs
trained to learn languages recognized by counter
machines[40], a neuron that stores the state of an
RNN trained to accept a regular language etc.)?
(b) Tracing the causal relationship between the predicted output with respect to the input, also called
instance-level interpretability [13][26][27][11]; methods form this class raise the problem of local interpretability and, roughly speaking, aims at designing
algorithmic answers to the following question: What
is the influence degree of each input factor that explains the obtained output?
Another important sub-category of post-hoc techniques concerns methods that attempt to extract
interpretable rule-based machines (e.g. Decision
trees, automata. . . ) from DL models [36][37]. Unlike instance-level interpretability techniques, these
methods are global and the challenge is how to convert the continuous representation of information as
encoded in RNNs into a discrete, symbolic representation, while maintaining a good quality of prediction of these last structures.

-The intersection between a DFA and the cut language
of a weighted RNN-LM is undecidable; - The equivalence of a PDFA/PFA/WFA and weighted RNN-LM
in a finite support is EXP-Hard; (b) For consistent
first-order RNN-LMs with any computable activation
function: - The Tcheybetchev distance approximation
is decidable; - The Tcheybetchev distance approximation in a finite support is NP-Hard.
The rest of this article is organized as follows. Section 2 gives a concise literature overview of issues related to our problematic. Section 3 presents our results for the case of general first-order RNN language
models(RNN-LM) with ReLu activation function. Section 4 is dedicated to the case of consistent RNN-LMs.

2

Related works

The problem of symbolic knowledge extraction from
connectionist models is not a new issue, and one
can trace back works interested in this problem
since the development of the first neural architectures [25][23]. However, with the development of novel
spatio-temporal connectionist models in the nineties,
the most important of which is Ellman RNNs[12],
and their great empirical success on inferring language
models with limited amount of data and with performance results that often outscore rule-based algorithms
traditionally used in the Grammatical Inference field
[9], research interests in this issue has regained more
attention. In fact, these works were mostly driven by
a legitimate motivation: if an RNN-like structure is
trained to recognize a language belonging to a given
class of languages C, and this latter can be recognized
by a class of computing devices M, then there must be
a close connection between the representation of the
target language as encoded in the RNN-like structure
on one hand, and that of the corresponding computing
device in M that is capable of recognizing it on the
other.
This aforementioned motivation raises two fundamental questions, at least from a theoretical viewpoint:

In this work, we are interested in this last family of
interpretability methods. More precisely, we address,
from a computational viewpoint, the issue of extracting FSM-based machines from general RNN language
models.
But this problem would benefit from understanding
better how well a finite-state model can approximate
an RNN. Which in turn requires solving essential
computational problems: can we compute distances
between these language models?
Can we decide
equivalence? These questions have received answers
for PFA [8,10,22]. We aim in this work to extend these
results by including RNN language models into the
picture.

1. What is the expressive power of different classes of
“RNN Machines”, as compared to classical symbolic
Our main results are summarized as follows: (a)
machines (e.g. deterministic/non deterministic fiFor general weighted first-order RNN-LMs with ReLu
nite state automata, deterministic/non determinstic
activation function: 1. The equivalence problem of a
pushdown automata, Turing machines etc.)?
PDFA/PFA/WFA and a weighted first-order RNN-LM
is undecidable; 2- As a corollary, any distance metric
between languages generated by PDFA/PFA/WFA 2. How can we design algorithms that extract symand that of a weighted RNN-LM is also undecidable;
bolic machines from RNN models? What are the
2

functions were Turing Complete. Later on, Kilian
et el. generalized this result to sigmoidal activation
functions [19].
The Turing Completeness of some classes of RNNs has
many consequences with respect to the computational
class to which belong many problems related to them.
In [7], the authors proved that the problem of deciding
whether a RNN language model -RNN-LM- with
ReLu activation function is consistent (encodes a valid
probability distribution) or not is an undecidable
problem. Moreover, the consensus string problem
and finding a minimal RNN-LM equivalent to a given
RNN-LM or testing the equivalence between two
RNN-LMs are also undecidable.

theoretical guarantees of such methods? What is
the computational complexity of such problems?
We should note that these two questions are, in some
sense, interrelated. If a class of RNNs is very powerful –say Turing Equivalent– computational problems
related to the extraction of finite state machines are
more likely to be undecidable. In fact, as a corollary of Rice’s Theorem1 , the equivalence between a
Turing machine and any non-trivial class of computing devices is necessarily undecidable, which means in
practice that no algorithm can exist that can answer
the question of equivalence between symbolic machines
and “Turing- Equivalent” RNN ones. In other words,
from the perspective of the theory of computation, the
trade-off between expressiveness and interpretability2
in connectionist models is unavoidable. As a consequence of the above discussion, we argue that analyzing a class of RNNs as a computational model can give
many insights with regard to its interpretability.
Guided by questions raised above, we divide the rest
of this section into two parts: In the first part, we examine works present in the literature that focused on
the computational power of recurrent neural networks
and its consequences on some computational problems
concerning RNNs. In the second part, we give a brief
overview of existing methods in the literature aimed
at extracting finite state machines from trained RNN
ones.

2.1

Given these pessimistic results about computability
of several important problems related to RNNs, a new
line of research suggests to analyze the practical capabilities computational power of neural nets instead
of the classical “unrealistic” theoretical model, by constraining the amount of memory resources of the RNN
hidden units to be finite [40][24]. Under this constraint,
Korskky et al. [21] proved that RNNs with one hidden
layer and ReLu activation, and GRUs are expressively
equivalent to deterministic finite automata. In [40],
Weiss et al. showed that the class of finite precision
LSTMs were able to simulate counter machines, while
the simple class of Elman RNNs and GRUs can’t.

2.2

Computational power of RNNs

The question of the computational capabilities of
different classes of RNN has been addressed since the
early development of neural systems. To the best of
the author’s knowledge, early works that addressed
this dates back to the middle of the previous century
by McCulloch et al. [23] and Kleene[20], where it was
proven that networks with binary threshold activation
functions are capable of implementing finite state
automata. In [30], Pollack designed a Turing-complete
class of high-order recurrent neural networks with two
types of activation function (linear and Heaviside).
This result was later extended in [32], where authors
relaxed the high-order requirement, and showed that
first-order RNNs with saturated-linear activation

Extraction of automata-based machines from trained RNNs

Early works investigating the problem of extracting
automata-based machines from trained RNNs coincide
with the emergence of novel RNN architectures [12][15]
in early nineteens that have shown promising results
for the task of inferring language models from limited
data. These early works have mainly focused on the extraction of deterministic finite automata (DFAs) from
RNNs trained to recognize regular languages, and most
of which were based on the assumption that a welltrained RNN to recognize a regular language tend to
group hidden states of the RNN into clusters that maps
directly to states of the minimal DFA recognizing the
target regular language. Based on this assumption, the
problem of DFA extraction from RNNs boils down to
a clustering/quantization problem of the RNN’s hidden state space, and many clustering techniques were
proposed for this task: Quantization by Equipartition
[16][38], Hierarchical Clustering [1], k-means [42][31],
fuzzy clustering [6] etc.
During the last few years, as RNN-based architec-

1 The Rice Theorem states that any class of non-trivial languages recognized by a Turing machine is not recursive
2 In our context, we quantify the interpretability of a model
as a measure of the computational difficulty by which one can
extract a finite state machine. A more rigorous formal definition of what is an intepretable model is still an arguably open
question.

3

tures became more sophisticated and thus harder to
be a subject of interpretative analysis, the issue has
gained an increasing interest among researchers, and
new methods were proposed in the literature to extract automata-based machines from different classes
of RNNs. In [39], Weiss et al. proposed an adaptation
of the L∗ algorithm [2] to extract deterministic finite
automata (DFA) from RNN Acceptors, where an RNN
Acceptor model serves as a black box oracle for approximate equivalence and membership queries, hinting that the exact equivalence query is “likely to be
intractable”. Same authors extended their work in [39]
to extract Probabilistic Deterministic Finite Automata
from RNN-LMs. In order to answer the equivalence
query, authors used a sampling strategy of both models, and gave theoretical guarantees of its convergence
in probability under a relaxed notion of equivalence.
Ayache et al. [3] employed the spectral learning framework [4] to extract Weighted Finite Automata(WFA)
from a RNN language model. In [28], Okudono et al.
raised the problem of answering the equivalence query
between a RNN language model and a WFA proposing an empirical regression-based technique to perform
this task. However, no theoretical guarantees were provided to back their method.

3

In Section 4, a 3-SAT formula will be denoted by the
symbol Ψ. A formula is comprised of n Boolean variables denoted x1 , ..xn , and k clauses C1 , ..Ck . For each
clause, we’ll use notation li1 , li2 , li3 to refer to its three
composing literals. For a given string w ∈ {0, 1}n, the
number of clauses satisfied by w will be denoted by Nw .
For the rest of this section, we shall first provide
a formal definition of the class of first-order weighted
RNN-LMs that we’ll study in this work. Also, we’ll give
a brief recall of basic definitions of different automatabased machines that we’ll encounter throughout the
rest of this article.
Definition 3.1. [7] A First-order weighted RNN Language model is a weighted language f : Σ∗ → R and is
defined by the tuple < Σ, N, h(0) , σ, W, (W ′ )Σ$ , E, E ′ >
such that:
•
•
•
•
•

Σ is the input alphabet,
N the number of hidden neurons,
σ : Q → Q is a computable activation function,
W ∈ QN ×N is the state transition matrix,
{Wσ′ }σ∈Σ$ , where each Wσ′ ∈ QN is the embedding
vector of the symbol σ ∈ Σ$ ,
• O ∈ QΣ$ ×N is the output matrix,
• O′ ∈ QΣ$ the output bias vector.

Definitions and Notations

The computation of the weight of a given string w
(where $ is the end marker) by R is given as follows.
(a) Recurrence equations:

Let Σ be a finite alphabet. The set of all finite strings
is denoted by Σ∗ . The set of all strings whose size is
equal (resp. greater than or equal) to n is denoted by
Σn (resp. Σ≥n ). For any string w ∈ Σ∗ , the size of w is
denoted by |w|, and its n-th symbol by wn . The prefix
of length n for any string w ∈ Σ≥n will be referred to
as w:n . The symbol $ denotes a S
special marker. The
symbol Σ$ will refer to the set Σ {$}.

h(t+1) = σ(W.h(t) + Ww′ t )
Et+1 = Oh(t+1) + O′
′
Et+1
= sof tmax2 (Et+1 )

(b) The resulting weight:

|w|+1
Y
Weighted languages: A weighted language f
Ei′
R(w)
=
over Σ is a mapping that assigns to each word
i=0
w ∈ Σ∗ a weight f (w) ∈ R. A WL f is called
consistent, if it encodes a valid probability dis- where w0 = w
|w|+1 = $
tribution, i.e.
satisfies
the
following
properties:
P
Remark that, in order to avoid technical
f (w) = 1. Two WLs f1 , f2
∀w ∈ Σ∗ : f (w) ≥ 0,
w∈Σ∗
issues,
we used softmax base 2 defined as:
2xi
are said to be equivalent if: ∀w ∈ Σ∗ : f1 (w) = f2 (w). sof tmax (x) =
for any x ∈ Rd instead
n
2
i
P
2xj
The Tcheybetchev distance metric between two
j=1
WLs is denoted d∞ (f1 , f2 ), and defined as: of the standard softmax in the previous definition.
max∗ |f1 (w) − f2 (w)|.
Finally, we define, for a In the following, hidden units of the network will be
w∈Σ

given scalar c > 0, the cut-point language of f with designated by lowercase letters n1 , n2 , .., and their
respect to c and denoted Lf,c , as the set of finite words activations at time t by htn . Also, we denote by Rσ the
whose values are greater or equal to c.
class of RNN-LMs when σ is the activation function.
4

For example, an important class of RNN-LMs that
will be used extensively in the rest of the article is
RReLu .

tional viewpoint. Weiss et al. [40] proved both theoretically and empirically that first-order RNNs with
ReLu(.) can simulate counter machines. Korsky et
al.[21] proved that finite precision first-order RNNs
Weighted Finite Automata (WFA). WFAs with ReLu are computationally equivalent to deterrepresent weighted versions of nondeterministic finite ministic finite automata. Moreover, when allowed arbiautomata, where transitions between states, denoted trary precision, they can simulate pushdown automata.
δ(q, σ, q ′ ) where q, q ′ ∈ Q represents states of the WFA Analyzing RNNs with other widely used activation
are labeled with a rational weight T (q, σ, q ′ ), and each functions, such as the sigmoid and the hyperbolic tanof its nodes q ∈ Q is labeled by a pair of rational gent, are left for future research.
numbers (I(q), P (q)) that represents respectively the
initial-state and final-state weight of q. WFAs model 4.1 Turing Completeness of general
weighted languages where the weight of a string w is
weighted-RNNs: Siegelmann’s conequal to the sum of the weights of all paths whose
struction
transitions encode the string w. The weight of a path
p is calculated as the product of the weight labels The basic building block for proving computational reof all its transitions, multiplied by the initial-state sults presented in this part of the article is the work
weight of its staring node and the final-state weight of done by Siegelmann and al. in [32] to prove the Turits ending node.
ing completeness of a certain class of first-order RNNs.
Hence, we propose, in this section, to provide a global
Probabilistic Finite Automata (PFA). A PFA scope of this construction, followed by an equivalent
is a WFA with two additional constraints: First, the reformulation of their main theorem that will be relesum of initial-state weights of all states is a valid vant for our work.
probability distribution over the state space. Second, The main intuition of Siegelmann et al.’s work is that,
for each state, the sum of weights of its outcoming with an appropriate encoding of binary strings, a firstedges added to its finite-state weight is equal to 1. This order RNN with a saturated linear function can readadditional constraint restricts the power of PFAs to ily simulate a stack datastructure by making use of
encode stochastic languages [33], which makes it useful a single hidden unit. For this, they used 4-base enfor representing language models. Interestingly, PFAs coding scheme that represents a binary string w as a
|w|
are proven to be equivalent to Hidden Markov Models
P wi
rational
number:
Enc(w)
=
4i . Backed by this
(HMMs), and the construction of equivalent HMMs
i=1
from PFAs and vice versa can be done in polynomial result, they proved than any two-stack machine can
time[34]. The deterministic version of PFAs, a.k.a be simulated by a first-order RNN with linear satuDeterministic Probabilistic Finite Automata (DPFA), rated function, where the configuration of a running
enforces the additional constraint that for any state q, two-stack machine (i.e. the content of the stacks and
and for any symbol σ there is at most one outgoing the state of the control unit) is stored in the hidden
transition labeled by σ from q.
units of the constructed RNN. Finally, given that any
Turing Machine can be converted into an equivalent
two-stack machine (The set of two-stack machines is
Turing-complete [17]), they concluded their result.
4 Computational results for In the context of our work, two additional remarks
to be noted about Siegelmann’s construction: general RNN-LMs with ReLu need
First, although the class of first-order RNNs examined in their work uses the saturated linear function
activation functions
as an activation function, their result is generalizable
The choice of ReLu in this part of the article is not to the ReLu activation function (or, more generally,
arbitrary. In fact, due to its nice piecewise-linear prop- any computable function that is linear in the support
erty and its wide use in practice, the ReLu(.) function [0,1])? - Second, although not mentioned in their work,
is first choice to analyze theoretical properties of RNN the construction of the RNN from a Turing Machine is
architectures. Recently, Chen et al. [7] provided an polynomial in time. In fact, on one hand, the numextensive study of first-order RNN Language Models ber of hidden units of the constructed RNN is linear in
with ReLu as an activation function from a computa- the size of the Turing Machine, and the construction
5

of transition matrices of the network is also linear in R ∈ RM,w
Relu , there exists T ∈ N, such that ∀t < T :
(T )
time. On the other hand, notice that the 4-base en- h(t)
=
0, and hnhalt = 1.
nhalt
coding map Enc(.) is also computable in linear time.
In light of these remarks, we are now ready to present
4.2 The equivalence problem between
the following theorem:

FSAs and general RNNs

Theorem 4.1. (Theorem 2, [32]) Let φ : {0, 1}∗ →
{0, 1}∗ be any computable function, and M be a
Turing Machine that implements it.
We have,
for any binary string w, there exists N
=
O(poly(|M |)), h(0) = [Enc(w) 0..0] ∈ QN , W ∈
QN ×N , such that for any finite alphabet Σ, ∀σ ∈
Σ$ : Wσ′ ∈ QN , O ∈ Q|Σ$ |×N , O′ ∈ Q|Σ$ | , R =<
Σ, N, ReLu, W, W ′ , O, O′ >∈ RReLu verifies:

The equivalence problem between a DPFA and a general RNN-LMs is formulated as follows:
Problem. Equivalence Problem between a DPFA and
a general RNN
Given a general weighted RNN-LM R ∈ RReLu and a
DPFA A. Are they equivalent?
Theorem 4.3. The equivalence problem between a
DPFA and a general RNN is undecidable

• if φ(w) is defined, then there exists T ∈ N such that
the first element of the hidden vector hT is equal to
Enc(φ(w)), and the second element is equal to 1,

Proof. We’ll reduce the halting Turing Machine problem to the Equivalence problem. Let Σ = {a}. We first
define the trivial DPFA A with one single state q0 , and
T (δq0 ,a,q0 ) = P (q0 ) = 21 , I(q0 ) = 1. This DPFA imple1
.
ments the weighted language f (an ) = 2n+1
Let M be a Turing Machine and w ∈ Σ∗ . Let
R ∈ RM,w
ReLu such that O[nhalt , a] = 1 ,0 everywhere
and O′ is equal to zero everywhere. We construct a
RNN R′ from R by adding one neuron in the hidden
(0)
(t+1)
layer, denoted n′ such that: hn′ = 0, ∀t ≥ 0 : hn′
=
(t)
′
ReLu(hn′ ), O[n , $] = 1.
Notice that, by Corollary 4.2, the TM M never halts
(T )
(T )
on w if and only if ∀T : (hnhalt , hn′ ) = (0, 0), i.e.
1
n
R(a $) = 2n+1 . That is, the TM M doesn’t halt on w
if and only if the DPFA A is equivalent to R′ , which
completes the proof.

• if φ(w) is undefined (i.e. M never halts on w), then
for all t ∈ N, the second element of the hidden vector
ht is always equal to zero.
Moreover, the construction of h0 and W is polynomial
in |M | and |w|.
In the following, we’ll denote by RM,w
ReLu the set of
RNNs in RReLu that simulate the TM M on w. It
is important to note that the construction of a RNN
that simulates a TM on a given string in the previous
theorem is both input and output independent. The
only constraints that are enforced by the construction
are placed on the transition dynamics of the network
and the initial state. In fact, the input string is placed
in the first stack of the two-stack machine before running the computation (i.e. in the initial state h(0) ).
Under this construction, the first stack of the machine
is encoded in the first hidden unit of the network. Afterwards, the RNN Machine runs on this input, and
halts(If It ever halts) when the halting state of the
machine is reached. In theorem 1.1, the halting state
of the machine is represented by the second neuron of
the network. In the rest of the article, we’ll refer to
the neuron associated to the halting state by the name
halting neuron, denoted nhalt .
We present the following corollary that gives a characterization of the halting machine problem3 that relates
it to the class RReLu :

A direct consequence of the above theorem is that
the equivalence problem between PFAs/WFAs and
general RNN-LMs in RReLu is also undecidable, since
the DPFA problem case is immediately reduced to the
general case of PFAs (or WFAs). Another important
consequence is that no distance metric can be computed between DPFA/PFA/WFA and RReLu :
Corollary 4.4. Let Σ = {a}. For any distance metric
d of Σ∗ , the total function that takes as input a description of a PDFA A and a general RNN-LM RReLu
and outputs d(A, R) is not recursive.
This fact is also true for PFAs and WFAs.

Corollary 4.2. Let M be any Turing Machine, and w
be a binary string, M halts on w if and only if for any

Proof. Let d be any distance metric on Σ∗ . By definition of a distance, we’ll have d(A, R) = 0 if and only if
A and R are equivalent. Since the equivalence problem
is undecidable, d(.) can’t be computed.

3 The Halting Machine problem is defined as follows: Given
a TM M and a string w, does M halt on w? This problem is
undecidable.

6

4.3

Intersection of the cut language of The proof is similar to the used for Theorem 4.3. We
a general weighted RNN-LM with are given a TM M , a string w and m ∈ N. Let Σ = {a}.
We construct a general weighted RNN-LM R′ by auga DFA
M,w
′

menting R ∈ RReLu with a neuron n as in Theorem
4.3. By Theorem 4.1, this reduction runs in polynomial
time. On the other hand, let A be the trivial PDFA
with one single state q0 , and T (δq0 ,a,q0 ) = P (q0 ) =
1
′
2 , I(q0 ) = 1. Note that R doesn’t halt in m steps
(T )
if and only if ∀T ≤ m : (nhalt , n′(T ) ) = (0, 0), i.e.
1
R′ (an $) = 2n+1 for the first m running steps on R′ , in
which case the language modeled by R′ is equal to f in
Σ≤m . Hence, A is equivalent to R in Σ≤m if and only
if M doesn’t halt on the string w in less or equal than
m steps.

In this subsection, we are interested in the following
problem:
Problem. Intersection of a DFA and the cut-point
language of a weighted RNN-LM
Given a general weighted
S RNN-LM R ∈ RReLu , c ∈ Q,
and a DFA A, is LR,c LA = ∅? Before proving that
this problem is undecidable, we shall recall first a result
proved in [7]:
Theorem 4.5. (Theorem 9, [7]) Define the highestweighted string problem as follows: Given a weighted
RNN-LM R ∈ RReLu , and c ∈ (0, 1): Does there exist
a string w such that R(w) > c?
The highest-weighted string problem is undecidable.
This problem is also known as the consensus problem
[22] and it is known to be NP-hard even for PFA.

5

Computational results for consistent RNN-LMs with general activation functions

Corollary 4.6. The intersection problem is undecidable.

In the previous section, we have seen that many interesting questions related to measuring the similarity
Proof. We shall reduce the highest-weighted string between weighted languages represented by different
problem from the intersection problem. Let R ∈ classes of weighted automata and first-order RNN-LMs
RReLu a general weighted RNN-LM, and c ∈ (0, 1). with ReLu activation function turned out to be either
Construct theTautomaton A that recognizes Σ∗ . We undecidable, or intractable when restricted to finite
have that LA LR = LR = ∅ if and only if there exist support. In this section, we examine the case where
no string w such that R(w) > c, which completes the trained RNN-LMs are guaranteed to be consistent,
proof.
and we raise the question of approximate equivalence
between PFAs and first-order consistent RNN-LMs
4.4 The equivalence problem in finite with general computable activation functions. For
any computable activation function σ, we formalsupport
ize this question in the following two decision problems:
Given that the equivalence problem between a
weighted RNN-LM and different classes of finite state Problem. Approximating the Tchebychev distance
automata is undecidable, a less ambitious goal is to between RNN-LM and PFA
decide whether a RNN-LM agrees with a finite state Instance: A consistent RNN-LM R ∈ Rσ , a consisautomaton over a finite support. We formalize this tent PFA A, c > 0
problem as follows:
Question: Does there exist |w| ∈ Σ∗ such that
Problem. The EQ-Finite problem between PDFA and |R(w) − A(w)| > c
weighted RNN-LMs
Given a general weighted RNN-LM R ∈ RReLu , m ∈ N Problem. Approximating the Tchebychev distance beand a PDFA A. Is R equivalent to A over Σ≤m ?
tween consistent RNN-LM and PFA over finite support
Instance: A consistent RNN R ∈ Rσ , a consistent
PFA A, c > 0 and N ∈ N+ ,
Question: Does there exist |w| ≤ N such that
|R(w) − A(w)| > c
Note that there is no constraint on the activation function used for consistent RNN-LMs in these defined
problems, provided it is computable. The first fact
is easy to prove:

Theorem 4.7. The EQ-Finite problem is EXP-Hard.
Proof. We reduce the bounded halting problem
the EQ-Finite problem.

4

to

4 The

bounded halting problem is defined as follows: Given
a TM M, a string x and an integer m, encoded in binary form.
Decide if M halts on x in at most n steps? This problem is
EXP-Complete.

7

T
T
∗ If x2 ∈ {li1 , li2 , li3 }, then (qi1
, 1, qi2
) ∈ δA , and
T
F
(qi1 , 0, qi2 ) ∈ δA ,
T
T
∗ If x̄2 ∈ {li1 , li2 , li3 }, then (qi1
, 0, qi2
) ∈ δA , and
T
F
(qi1 , 1, qi2 ) ∈ δA
T
F
∗ Otherwise, ∀a ∈ Σ : (qi1
, a, qi2
) ∈ δA

Theorem 5.1. Approximating the Tcheybechev distance between RNN-LM and PFA is decidable.

Proof. Let R be a consistent RNN-LM and A be a consistent PFA. An algorithm that can decide this problem
runs as follows: enumerate all strings w1 , .. in Σ∗ until
c
c
we reach a string that satisfies this property in which
– Otherwise ∀a ∈ Σ, c ∈ {T, F } : (qi,1
, a, qi,2
) ∈ δA
case the algorithm returns Yes. If there is no such
string, by definition of consistency, there will be a finite • For each clause i and every Boolean variable xj
where j ∈ [2, n − 1]:
T
T
P
P
time T such that
R(wt ) ≥ 1 − c,
A(wt ) ≥ 1 − c
N
S
t=1
t=1
– if xj ∈ {li1 , li2 , li3 }, then (qi,j
, 1, qi,j+1
) ∈ δA
in which case, we have: ∀t > T : R(wt ) < c and
N
T
– if x¯j ∈ {li1 , li2 , li3 }, then (qi,j
, 0, qi,j+1
) ∈ δA
A(wt ) < c which implies ∀t > T : |R(wt − A(wt )| < c.
N
N
When T is reached, the algorithm returns No.
– Otherwise, ∀a ∈ Σ : (qi,j , a, qi,j+1 ) ∈ δA

5.1

• Transition probabilities:

Approximating the Tcheybetchev
distance over a finite support

– ∀i ∈ [1, k], a ∈ Σ, c ∈ {S, N } :
1
ǫ
2k − k

c
TA (q0 , a, qi1
)=

Proving the NP-Hardness of the Tcheybetchev
T
F
– ∀i ∈ [1, k], a ∈ Σ : TA (qin
, a, qin
)=ǫ
distance approximation in finite support is more
– All the other transitions belonging to δA has a
complicated, and we’ll give below the construction of
weight 21 − ǫ
a PFA and a RNN from a given 3-SAT formula which
will help us prove the result. Let ǫ ∈ (0, 21 ) whose
• Final-state probabilities:
value will be specified later.
N
– For each clause i: PA (qin
) = 1 − 2ǫ
• Construction of a PFA A: The construction
– All the other states in A has a final-state probaof our PFA is inspired from the work done in [5], and
bility equal to 2ǫ
illustrated in Figure 1. Intuitively, each clause i in Ψ is
represented by two paths in the PFA, one that encodes
• Construction of a RNN: The RNN R we’ll cona satisfiable assignment of the variables for this clause, struct is simple, and It generates the quantitative lanand the other not. More formally, the PFA A is defined guage R(w) = 2( 21 − ǫ)|w| ǫ. More formally, our RNN is
as:
defined as:
c
• QA = {q0 } ∪ {qi,j
: i ∈ [1, k], j ∈ [1, n], c ∈ {T, F }} • N = 2 (2 hidden neurons),
is the set of states,
!  
(0)
hn1
0
=
•
• Initial probabilities: IA (q0 ) = 1, 0 otherwise,
(0)
0
hn2
• For every i ∈ [1, k], c ∈ {T, F } and a ∈ Σ:


c
1 0
(q0 , a, qi,1
) ∈ δA ,
• Transition matrices: Win =
; W0 = W1 =
0 1
 
T
T
• ∀i ∈ [1, k], j ∈ [2, n − 1], a ∈ Σ : (qi,j , a, qi,j+1 ) ∈ δA
0
W$ =
0
• For each clause i:




1 0
log2 1−2ǫ
T
T
4ǫ
– If x1 ∈ {li1 , li2 , li3 }, then ∀a ∈ Σ : (qi,1 , a, qi,2 ) ∈

• Output matrices: O = 1 0, O′ = log2 1−2ǫ
4ǫ
δA . And:
0 1
0
F
T
∗ If x2 ∈ {li1 , li2 , li3 }, then (qi1
, 1, qi2
) ∈ δA , and
where log2 (.) is the logarithm to the base 2
F
F
(qi1 , 0, qi2 ) ∈ δA ,
1
|w|
F
T
∗ If x̄2 ∈ {li1 , li2 , li3 }, then (qi1
, 0, qi2
) ∈ δA , and What’s left is to show that R(w) = 2( 2 - ǫ) defines
N
N
a consistent language model:
, 1, qi2
) ∈ δA
(qi1
F
F
∗ Otherwise a ∈ Σ : (qi1
, a, qi2
) ∈ δA
Proposition 5.2. For any ǫ < 1 , the weighted lan2

guage model defined as f (w) = 2( 21 − ǫ)|w| ǫ is consistent.

F
T
– If x̄1 ∈ {li1 , li2 , li3 }, then ∀a ∈ Σ : (qi1
, a, qi2
)∈
δA , and:

8

T
q11

T
q12

2

−

1

4

:
1

q0

1:

F
q11

0:
1:

1

−

′

1

0:

0/2ǫ

1/2ǫ

1
2

0, 1 :

ǫ

0/2ǫ

2

−ǫ

0/2ǫ

−ǫ

1
2

F
q12

−ǫ

0/2ǫ

T
q13
1
2

0, 1 :

1:

1

0:

2

−ǫ

0/2ǫ

−ǫ
F
q13

1
2

−ǫ

0/2ǫ

1
2

0, 1 :

1:

1

2

0/1 − 2ǫ

0, 1 : ǫ
−ǫ

F
0/2ǫ q14

ǫ
2
T
q21

1
4 − ǫ
2

T
q22

0:

0

0/2ǫ

1
2

−ǫ

:

1: 1ǫ
1 2−
−ǫ
0: 2

1
ǫ

−

4

2

F
q21

1:

0/2ǫ

1
2

−ǫ

0/2ǫ
F
q22

0/2ǫ

0, 1 :

T
q23
1
2

0, 1 :

1:

1

0:

2

−ǫ

0/2ǫ

−ǫ

1
2

F
q23

−ǫ

0/2ǫ

T
q14

−ǫ

1
2

0:

−ǫ

1
2

0, 1 :

1:

1

0:

2

−ǫ
0/1 − 2ǫ

1
2

−ǫ

1
2

−ǫ

T
q24

−ǫ

1
2

0, 1 : ǫ
−ǫ

F
0/2ǫ q24

0, 1 :

Figure 1: A graphical representation of the PFA constructed from Ψ = (x1 ∨ x2 ∨ x3 ) ∧ (x̄2 ∨ x3 ∨ x4 )
On the other hand, for |w| > n, we have:

Proof. We have:
X
X
f (w) = 2ǫ
w∈Σ∗

X

1
( − ǫ)n
2

Nw 1 − 4ǫ
1
|R(w) − A(w)| = 2ǫ( − ǫ)|w|
2
k 1 − 2ǫ

n∈N w:|w|=n

= 2ǫ

X

(1 − 2ǫ)n

Note that we have for any ǫ < 41 :

n∈N

By applying the equality:

P

n∈N

xn =

1
1−x

∀w ∈ Σ≥n : |R(w) − A(w)| ≤ |R(w:n ) − A(w:n )|

for any |x| <

1 on the sum present in the right-hand term of the
equation above, we obtain the result.

This means that, under this construction, the maximum is reached necessarily by a string whose length is
exactly equal to n. Thus, we obtain:

Proposition 5.3. Let Ψ be an arbitrary 3-SAT formula with n variables and k clauses. Let A be the PFA
1 − 4ǫ
ǫ 1
max Nw
d∞ (R, A) = 2 ( − ǫ)n
constructed from Ψ by the procedure detailed above, the
k 2
2ǫ w∈Σn
probabilistic language generated by A is given as:

Note that Ψ is satisfiable if and only if maxn Nw = k.
1
|w|

if |w| < n
w∈Σ
2( 2 − ǫ) ǫ
k−Nw
As
a
result,
pick
any
s
∈
[k−1,
k),
and
define
cepsilon =
A(w) = 2( 12 − ǫ)|w| ǫ[ Nkw 1−2ǫ
if |w| = n ǫs 1
2ǫ +
k ]

,
the
formula
is
satisfiable
if
and only
2 k ( 2 − ǫ)n 1−4ǫ
 1
k−Nw:n
|w| Nw:n 2ǫ
2ǫ
] else
2( 2 − ǫ) ǫ[ k 1−2ǫ +
k
if d∞ (R, A) > c.
Proposition 5.4. For any rational number ǫ < 41 ,
there exists a rational number cǫ such that Ψ is satis- Theorem 5.5. The Tchebychev distance approximation problem between consistent RNN-LMs and PFAs
fiable if and only if d∞ (R, A) > c
in finite support is NP-Hard.
Proof. For any w such that |w| < n, |R(w)−A(w)| = 0
.
Proof. We reduce the 3-SAT satisfiability problem to
For |w| = n, we have:
our problem. Let Ψ be an arbitrary 3-SAT formula.
Construct a PFA A and a RNN R as specified previNw 1 − 4ǫ
1
(
)
|R(w) − A(w)| = 2ǫ( − ǫ)n
ously. Choose a rational number ǫ < 14 . Let cǫ > 0 be
2
k
2ǫ
9

any rational number as specified in the proof of proposition 5.4, and N = n + 1. By proposition 5.4, Ψ is satisfiable if and only if d∞ (R, A) > c, which completes
the proof
Remarks:
• NP-Hardness for LSTMs/GRUS: Although our
main focus in this article was on first-order weighted
RNNs with one hidden layer, It is worth noting the
the NP-Hardness reduction technique from 3-SAT
problem we employed can easily be generalized to the
case of LSTMs [], and GRUs, two widely used RNN
architectures in practice. Indeed, our reduction relies
on the construction of a memoryless first-order RNN
which makes abstraction of the state of the hidden
units of the network, and exploits only the output
bias vector O′ . Hence, provided we have first-order
output function for a LSTM (or GRU) architecture,
the NP-Hardness result demonstrated above is easy
to extend our proof to these architectures.
• Finite precision RNNs: As said earlier in section II, a new line of work considered the analysis of the computational power of RNNs with
bounded resources, which is a realistic condition in
practice[24][40]. Broadly speaking, a finite-precision
RNN is one whose weights and values of its hidden
units are stored using a finite number of bits (See
[21] for further details). Under our construction, the
same remark raised above about LSTMs/GRUs can
be applied to RNNs with finite precision. In fact, It’s
easy to notice that, with a judicious choice of ǫ, say
1
′
′
10 (in which case O [0] = O [1] = 2), the toy memoryless RNN we constructed in the proof requires only
2 bits to encode a hidden unit and a weight value of
the network. This shows that even approximating
the Tcheybetchev distance in finite support between
a language represented by PFA and that of a finiteprecision first-order RNN with any computable activation function is also NP-Hard.

6

Conclusion and perspectives

In this article, we investigated some computational
problems related to the issue of approximating trained
RNN language models by different classes of finite state
automata. We proved that the equivalence problem of
PDFAs/PFAs/WFAs and general weighted first-order
RNN-LM with ReLu activation function with a single
hidden layer is generally undecidable, and, as a result,
trying to calculate any distance between them can’t be
10

computed. When restricting RNN-LMs to be consistent, we proved that approximating the Tcheybetchev
distance between consistent RNN-LMs with general
computable activation functions and PFAs is decidable,
and that the same problem when restricted to a finite
support is at least NP-Hard. Moreover, we gave arguments that the reduction strategy from 3-SAT problem
we employed to prove this latter result makes this result generalizable to the class of LSTMs/GRUs and
finite precision RNNs.
This work provides first theoretical results of examining equivalence and the quality of approximation problems between automata-based models and RNNs from
a computational viewpoint. Yet, there are still many
interesting problems on the issue that could motivate
future research, such as: Is the equivalence problem
between general RNN-LMs and different classes of finite state machines still undecidable when other highly
non-linear activation functions (e.g. sigmoid, hyperbolic tangent ..) are used instead of ReLus? Is the
equivalence problem between the cut-point language
of an RNN-LM and a DFA decidable? If an RNN-LM
is trained to recognize a language generated by a regular grammar, can we decide if its cut-point language
is indeed regular? etc.

References
[1] Alquezar, R. and Sanfeliu, A. (1994). A hybrid connectionistsymbolic approach to regular grammatical inference based on
neural learning and hierarchical clustering. In Carrasco, R. C.
and Oncina, J., editors, Grammatical Inference and Applications, pages 203–211. Springer.
[2] Angluin, D. (1987). Learning regular sets from queries and
counterexamples. Inf. Comput., 75(2):87–106.
[3] Ayache, S., Eyraud, R., and Goudian, N. (2019). Explaining
black boxes on sequential data using weighted automata. In
Proceedings of The 14th International Conference on Grammatical Inference 2018, volume 93 of Proceedings of Machine
Learning Research, pages 81–103.
[4] Balle, B., Quattoni, A., and Carreras, X. (2014). Spectral
learning techniques for weighted automata, transducers, and
grammars. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, Doha, Qatar. Association for Computational Linguistics.
[5] Casacuberta, F. and de la Higuera, C. (2000). Computational
complexity of problems on probabilistic grammars and transducers. In Oliveira, A. L., editor, Grammatical Inference: Algorithms and Applications, pages 15–24. Springer Berlin Heidelberg.
[6] Cechin, A. L., Regina, D., Simon, P., and Stertz, K. (2003).
State automata extraction from recurrent neural nets using kmeans and fuzzy clustering. In 23rd International Conference

of the Chilean Computer Science Society, 2003. SCCC 2003.
Proceedings., pages 73–78.
[7] Chen, Y., Gilroy, S., Maletti, A., May, J., and Knight, K.
(2018). Recurrent neural networks as weighted language recognizers. In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2261–2271.

[22] Lyngsø, R. B. and Pedersen, C. N. S. (2002). The consensus string problem and the complexity of comparing hidden
markov models. Journal of Computing and System Science,
65(3):545–569.
[23] McCulloch, W. S. and Pitts, W. (1988). A Logical Calculus
of the Ideas Immanent in Nervous Activity, page 15–27. MIT
Press, Cambridge, MA, USA.

[8] Cortes, C., Mohri, M., and Rastogi, A. (2007). lp distance and equivalence of probabilistic automata. International
Journal of Foundations of Computer Science, 18(4):761–779.

[24] Merrill, W. (2019). Sequential neural networks as automata.
In Proceedings of the Workshop on Deep Learning and Formal
Languages: Building Bridges, pages 1–13, Florence. Association for Computational Linguistics.

[9] de la Higuera, C. (2010). Grammatical inference: learning
automata and grammars. Cambridge University Press.

[25] Minsky, M. L. (1967). Computation: Finite and Infinite
Machines. Prentice-Hall, Englewood Cliffs, N. J.

[10] de la Higuera, C., Scicluna, J., and Nederhof, M. (2014).
On the computation of distances for probabilistic context-free
grammars. arXiv preprint arXiv:, 2014.
[11] Du, M., Liu, N., Yang, F., Ji, S., and Hu, X. (2019). On attribution of recurrent neural network predictions via additive
decomposition. In The World Wide Web Conference, WWW
2019, pages 383–393. ACM.
[12] Elman, J. L. (1990). Finding structure in time. COGNITIVE SCIENCE, 14(2):179–211.
[13] Fong, R. C. and Vedaldi, A. (2017). Interpretable explanations of black boxes by meaningful perturbation. 2017 IEEE
International Conference on Computer Vision (ICCV), pages
3449–3457.

[26] Murdoch, W. J., Liu, P. J., and Yu, B. (2018). Beyond
word importance: Contextual decomposition to extract interactions from lstms. In ICLR, 6th International Conference
on Learning Representations, ICLR 2018. OpenReview.net.
[27] Murdoch, W. J. and Szlam, A. (2017). Automatic rule
extraction from long short term memory networks. ArXiv,
abs/1702.02540.
[28] Okudono, T., Waga, M., Sekiyama, T., and Hasuo, I.
(2019). Weighted automata extraction from recurrent neural
networks via regression on state spaces.
[29] Oliva, C. and Lago-Fernandez, L. (2019). On the Interpretation of Recurrent Neural Networks as Finite State Machines,
pages 312–323.

[14] Gers, F. A. and Schmidhuber, J. (2000). Recurrent nets
that time and count. Technical report, Istituto Dalle Molle
Di Studi Sull Intelligenza Artificiale.
[15] Giles, C. L., Chen, D., Miller, C. B., Chen, H. H., Sun,
G. Z., and Lee, Y. C. (1991). Second-order recurrent neural
networks for grammatical inference. In IJCNN-91-Seattle International Joint Conference on Neural Networks, volume ii,
pages 273–281 vol.2.
[16] Giles, C. L., Miller, C. B., Chen, D., Chen, H. H., Sun,
G. Z., and Lee, Y. C. (1992). Learning and extracting finite
state automata with second-order recurrent neural networks.
Neural Comput., 4(3):393–405.
[17] Hopcroft, J. E., Motwani, R., and Ullman, J. D. (2006).
Introduction to Automata Theory, Languages, and Computation (3rd Edition). Addison-Wesley Longman Publishing Co.,
Inc.
[18] Karpathy, A., Johnson, J., and Li, F.-F. (2015). Visualizing and understanding recurrent networks. CoRR,
abs/1506.02078.
[19] Kilian, J. and Siegelmann, H. T. (1993). On the power
of sigmoid neural networks. In Proceedings of the Sixth Annual Conference on Computational Learning Theory, page
137–143.
[20] Kleene, S. C. (1956). Representation of events in nerve
nets and finite automata. In Automata Studies, pages 3–41.
Princeton University Press.
[21] Korsky, S. A. and Berwick, R. C. (2019). On the computational power of rnns.

11

[30] Pollack, J. B. (1987). On connectionist models of natural
language processing. PhD thesis, Computer Science Department, University of Illinois, Urbana.
[31] Schellhammer, I., Diederich, J., Towsey, M., and Brugman,
C. (1998). Knowledge extraction and recurrent neural networks: An analysis of an Elman network trained on a natural
language learning task. In New Methods in Language Processing and Computational Natural Language Learning.
[32] Siegelmann, H. and Sontag, E. (1995). On the computational power of neural nets. Journal of Computer and System
Sciences, 50(1):132 – 150.
[33] Thollard, F., de la Higuera, C., Vidal, E., Casacuberta,
F., and Carrasco, R. C. (2005). Probabilistic finite-state
machines-part i. IEEE Transactions on Pattern Analysis $
Machine Intelligence, 27(07):1013–1025.
[34] Vidal, E., Thollard, F., de la Higuera, C., Casacuberta,
F., and Carrasco, R. C. (2005). Probabilistic finite-state machines - part ii. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 27(7):1026–1039.
[35] Wang, C. and Niepert, M. (2019). State-regularized recurrent neural networks. In Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings
of Machine Learning Research, pages 6596–6606. PMLR.
[36] Wang, Q., Zhang, K., Ororbia, A., Xing, X., Liu, X., and
Giles, C. L. (2018a). A comparison of rule extraction for different recurrent neural network models and grammatical complexity. ArXiv, abs/1801.05420.

[37] Wang, Q., Zhang, K., Ororbia, A. G., Xing, X., Liu, X.,
and Giles, C. L. (2018b). An empirical evaluation of rule
extraction from recurrent neural networks. Neural Comput.,
30(9):2568–2591.
[38] Watrous, R. L. and Kuhn, G. M. (1991). Induction of finitestate automata using second-order recurrent networks. In Proceedings of the 4th International Conference on Neural Information Processing Systems, page 309–316, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
[39] Weiss, G., Goldberg, Y., and Yahav, E. (2018a). Extracting
automata from recurrent neural networks using queries and
counterexamples. In Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 5247–5256.
[40] Weiss,
practical
language
740–745.

G., Goldberg, Y., and Yahav, E. (2018b). On the
computational power of finite precision RNNs for
recognition. In Proceedings of the 56th ACL, pages
ACL.

[41] Zeiler, M. D. and Fergus, R. (2014). Visualizing and understanding convolutional networks. In Fleet, D., Pajdla, T.,
Schiele, B., and Tuytelaars, T., editors, Computer Vision –
ECCV 2014, pages 818–833. Springer.
[42] Zeng, Z., Goodman, R. M., and Smyth, P. (1993). Learning
finite machines with self-clustering recurrent networks. Neural
Comput., 5(6):976–990.
[43] Zhang, Q., Wu, Y. N., and Zhu, S. (2018). Interpretable
convolutional neural networks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
8827–8836.

12

