Deep Architectures for Neural Machine Translation
Antonio Valerio Miceli Barone† Jindřich Helcl? Rico Sennrich†
Barry Haddow† Alexandra Birch†
†
School of Informatics, University of Edinburgh
?
Faculty of Mathematics and Physics, Charles University
{amiceli, bhaddow}@inf.ed.ac.uk
{rico.sennrich, a.birch}@ed.ac.uk
helcl@ufal.mff.cuni.cz
Abstract

arXiv:1707.07631v1 [cs.CL] 24 Jul 2017

It has been shown that increasing model
depth improves the quality of neural machine translation.
However, different
architectural variants to increase model
depth have been proposed, and so far, there
has been no thorough comparative study.
In this work, we describe and evaluate
several existing approaches to introduce
depth in neural machine translation. Additionally, we explore novel architectural
variants, including deep transition RNNs,
and we vary how attention is used in
the deep decoder. We introduce a novel
"BiDeep" RNN architecture that combines
deep transition RNNs and stacked RNNs.
Our evaluation is carried out on the English to German WMT news translation
dataset, using a single-GPU machine for
both training and inference. We find that
several of our proposed architectures improve upon existing approaches in terms
of speed and translation quality. We obtain
best improvements with a BiDeep RNN of
combined depth 8, obtaining an average
improvement of 1.5 B LEU over a strong
shallow baseline.
We release our code for ease of adoption.

1

Introduction

Neural machine translation (NMT) is a wellestablished approach that yields the best results
on most language pairs (Bojar et al., 2016; Cettolo et al., 2016). Most systems are based on the
sequence-to-sequence model with attention (Bahdanau et al., 2015) which employs single-layer recurrent neural networks both in the encoder and in
the decoder.

Unlike feed-forward networks where depth is
straightforwardly defined as the number of noninput layers, recurrent neural network architectures with multiple layers allow different connection schemes (Pascanu et al., 2014) that give rise to
different, orthogonal, definitions of depth (Zhang
et al., 2016) which can affect the model performance depending on a given task. This is further complicated in sequence-to-sequence models
as they contain multiple sub-networks, recurrent
or feed-forward, each of which can be deep in different ways, giving rise to a large number of possible configurations.
In this work we focus on stacked and deep transition recurrent architectures as defined by Pascanu et al. (2014). Different types of stacked architectures have been successfully used for NMT
(Zhou et al., 2016; Wu et al., 2016). However,
there is a lack of empirical comparisons of different deep architectures. Deep transition architectures have been successfully used for language
modeling (Zilly et al., 2016), but not for NMT
so far. We evaluate these architectures, both
alone and in combination, varying the connection scheme between the different components and
their depth over the different dimensions, measuring the performance of the different configurations
on the WMT news translation task.1
Related work includes that of Britz et al. (2017),
who have performed an exploration of NMT architectures in parallel to our work. Their experiments, which are largely orthogonal to ours,
focus on embedding size, RNN cell type (GRU
vs. LSTM), network depth (defined according
to the architecture of Wu et al. (2016)), attention mechanism and beam size. Gehring et al.
(2017) recently proposed a NMT architecture
based on convolutions over fixed-sized windows
1
http://www.statmt.org/wmt17/
translation-task.html

rather than RNNs, and they reported results for
different model depths and attention mechanism
configurations. A similar feedforward architecture which uses multiple pervasive attention mechanisms rather than convolutions was proposed by
Vaswani et al. (2017), who also report results for
different model depths.

...
...
...

2

NMT Architectures

All the architectures that we consider in this work
are GRU (Cho et al., 2014a) sequence-to-sequence
transducers (Sutskever et al., 2014; Cho et al.,
2014b) with attention (Bahdanau et al., 2015). In
this section we describe the baseline system and
the variants that we evaluated.
2.1

Baseline Architecture

As our baseline, we use the NMT architecture implemented in Nematus, which is described in more
depth by Sennrich et al. (2017b). We augment it
with layer normalization (Ba et al., 2016), which
we have found to both improve translation quality
and make training considerably faster.
For our discussion, it is relevant that the baseline architecture already exhibits two types of
depth:
• recurrence transition depth in the decoder
RNN which consists of two GRU transitions
per output word with an attention mechanism
in between, as described in Firat and Cho
(2016).
• feed-forward depth in the attention network
that computes the alignment scores and in the
output network that predicts the target words.
Both these networks are multi-layer perceptrons with one tanh hidden layer.
2.2

Deep Transition Architectures

In a deep transition RNN (DT-RNN), at each time
step the next state is computed by the sequential application of multiple transition layers, effectively using a feed-forward network embedded inside the recurrent cell. In our experiments, these
layers are GRU transition blocks with independently trainable parameters, connected such that
the "state" output of one of them is used as the
"state" input of the next one. Note that each of
these GRU transition is not individually recurrent,
recurrence only occurs at the level of the whole
multi-layer cell, as the "state" output of the last

Figure 1: Deep transition decoder
GRU transition for the current time step is carried
over as the "state" input of the first GRU transition
for the next time step.
Applying this architecture to NMT is a novel
contribution.
2.2.1 Deep Transition Encoder
As in a baseline shallow Nematus system, the encoder is a bidirectional recurrent neural network.
Let Ls be the encoder recurrence depth, then for
the i-th source word in the forward direction the
→
−
→
−
forward source word state h i ≡ h i,Ls is computed as:
 →

→
−
−
h i,1 = GRU1 xi , h i−1,Ls
 →

→
−
−
h i,k = GRUk 0, h i,k−1 for 1 < k ≤ Ls
where the input to the first GRU transition is the
word embedding xi , while the other GRU transitions have no external inputs. Recurrence occurs
→
−
as the previous word state h i−1,Ls enters the computation in the first GRU transition for the current
word.
The reverse source word states are computed similarly and concatenated to the forward ones to
form
the bidirectional
source word states C ≡
nh→
−
←
− io
h i,Ls h i,Ls .
2.2.2 Deep Transition Decoder
The deep transition decoder is obtained by extending the baseline decoder in a similar way. Recall
that the baseline decoder of Nematus already has
a transition depth of two, with the first GRU transition receiving as input the embedding of the previous target word and the second GRU transition
receiving as input a context vector computed by
the attention mechanism. We extend this decoder

...

architecture to an arbitrary transition depth Lt as
follows:
...

...

...

...

...

...

sj,1 = GRU1 (yj−1 , sj−1,Lt )
sj,2 = GRU2 (ATT(C, sj,1 ), sj,1 )
sj,k = GRUk (0, sj,k−1 ) for 2 < k ≤ Lt
where yj−1 is the embedding of the previous target
word and ATT(C, si,1 ) is the context vector computed by the attention mechanism. GRU transitions other than the first two do not have external
inputs. The target word state vector sj ≡ sj,Lt is
then used by the feed-forward output network to
predict the current target word. A diagram of this
architecture is shown in Figure 1.
The output network can be also made deeper by
adding more feed-forward hidden layers.
2.3

Stacked architectures

A stacked RNN is obtained by having multiple
RNNs (GRUs in our experiments) run for the same
number of time steps, connected such that at each
step the bottom RNN takes "external" inputs from
the outside, while each of the higher RNN takes
as its "external" input the "state" output of the one
below it. Residual connections between states at
different depth (He et al., 2016) are also used to
improve information flow. Note that unlike deep
transition GRUs, here each GRU transition block
constitutes a cell that is individually recurrent, as it
has its own state that is carried over between time
steps.
2.3.1

Stacked Encoder

In this work we consider two types of bidirectional
stacked encoders: an architecture similar to Zhou
et al. (2016) which we denote here as alternating
encoder (Figure 2), and one similar to Wu et al.
(2016) which we denote as biunidirectional encoder (Figure 3).
Our contribution is the empirical comparison of
these architectures, both in isolation and in combination with the deep transition architecture.
We do not consider stacked unidirectional encoders (Sutskever et al., 2014) as bidirectional encoders have been shown to outperform them (e.g.
Britz et al. (2017)).
Alternating Stacked Encoder The forward part
of the encoder consists of a stack of GRU recurrent
neural networks, the first one processing words in

Figure 2: Alternating stacked encoder (Zhou et al.,
2016).
the forward direction, the second one in the backward direction, and so on, in alternating directions. For an encoder stack depth Ds , and a source
sentence length N , the forward source word state
→
−
−
wi ≡ →
w i,Ds is computed as:
 →

→
−
−
→
−
w i,1 = h i,1 = GRU1 xi , h i−1,1


→
−
→
−
−
h i,2k = GRU2k →
w i,2k−1 , h i+1,2k
for 1 < 2k ≤ Ds


→
−
→
−
−
w i,2k , h i−1,2k+1
h i,2k+1 = GRU2k+1 →
→
−
w i,j

for 1 < 2k + 1 ≤ Ds
→
−
−
= h i,j + →
w i,j−1
for 1 < j ≤ Ds

→
−
→
−
where we assume that h 0,k and h N +1,k are zero
vectors. Note the residual connections: at each
level above the first one, the word state of the pre−
vious level →
w i,j−1 is added to the recurrent state
→
−
of the GRU cell h i,j to compute the the word state
−
for the current level →
w i,j .
The backward part of the encoder has the same
structure, except that the first level of the stack
processes the words in the backward direction and
the subsequent levels alternate directions.
The forward and backward word states are then
concatenated to form bidirectional word states
−
−
C ≡ {[→
w i,Ds ←
w
i,Ds ]}. A diagram of this architecture is shown in Figure 2.
Biunidirectional Stacked Encoder In this encoder the forward and backward parts are shallow, as in the baseline architecture. Their word
states are concatenated to form shallow bidirec−
− ] that are then
tional word states wi ≡ [→
w i,1 ←
w
i,1
used as inputs for subsequent stacked GRUs which
operate only in the forward sentence direction,
hence the name "biunidirectional". Since residual connections are also present, the higher depth

...

tions between the levels.
sj,1,1 = GRU1,1 (yj−1 , sj−1,1,2 )

...

cj,1 = ATT(C, sj,1,1 )
...

sj,1,2 = GRU1,2 (cj,1 , sj,1,1 )
rj,1 = sj,1,2

...

...

Figure 3: Biunidirectional stacked encoder (Wu
et al., 2016).
...

sj,k,1 = GRUk (rj,k−1 , sj−1,k,1 )
rj,k = sj,k,1 + rj,k−1
for 1 < k ≤ Dt

Note that the higher levels have transition depth
one, unlike the base level which has two.

...
...

Figure 4: Stacked RNN decoder

Stacked rGRU The higher RNNs are GRUs
whose "external" input is the concatenation of the
state below and the context vector from the base
RNN. Formally, the states sj,k,1 of the higher
RNNs are computed as:
sj,k,1 = GRUk ([rj,k−1 , cj,1 ] , sj−1,k,1 )

GRUs have a state size twice that of the base
ones. This architecture has shorter maximum information propagation paths than the alternating
encoder, suggesting that it may be less expressive,
but it has the advantage of enabling implementations with higher model parallelism. A diagram of
this architecture is shown in Figure 3.
In principle, alternating and biunidirectional
stacked encoders can be combined by having Dsa
alternating layers followed by Dsb unidirectional
layers.

for 1 < k ≤ Dt

This is similar to the deep decoder by Wu et al.
(2016).
Stacked cGRU The higher RNNs are conditional GRUs, each with an independent attention
mechanism. Each level has two GRU transitions
per step j, with a new context vector cj,k computed
in between:
sj,k,1 = GRUk,1 (rj,k−1 , sj−1,k,1 )

2.3.2

Stacked Decoder

cj,k = ATT(C, sj,k,1 )
sj,k,2 = GRUk,2 (cj,k , sj,1,1 )

A stacked decoder can be obtained by stacking
RNNs which operate in the forward sentence direction. A diagram of this architecture is shown in
Figure 4.
Note that the base RNN is always a conditional
GRU (cGRU, Firat and Cho, 2016) which has transition depth at least two due to the way that the
context vectors generated by the attention mechanism are used in Nematus. This opens up the possibility of several architectural variants which we
evaluated in this work:

for 1 < k ≤ Dt

Note that unlike the stacked GRU and rGRU, the
higher levels have transition depth two.
Stacked crGRU The higher RNNs are conditional GRUs but they reuse the context vectors
from the base RNN. Like the cGRU there are two
GRU transition per step, but they reuse the context
vector cj,1 computed at the first level of the stack:
sj,k,1 = GRUk,1 (rj,k−1 , sj−1,k,1 )

Stacked GRU The higher RNNs are simple
GRUs which receive as input the state from the
previous level of the stack, with residual connec-

sj,k,2 = GRUk,2 (cj,1 , sj,1,1 )
for 1 < k ≤ Dt

2.4

BiDeep architectures

We introduce the BiDeep RNN, a novel architecture obtained by combining deep transitions with
stacking.
A BiDeep encoder is obtained by replacing the
Ds individually recurrent GRU cells of a stacked
encoder with multi-layer deep transition cells each
composed by Ls GRU transition blocks.
For instance, the BiDeep alternating encoder is
defined as follows:
 →

→
−
−
→
−
w i,1 = h i,1 = DTGRU1 xi , h i−1,1


→
−
→
−
−
h i,2k = DTGRU2k →
w i,2k−1 , h i+1,2k
for 1 < 2k ≤ Ds


→
−
→
−
−
h i,2k+1 = DTGRU2k+1 →
w i,2k , h i−1,2k+1
→
−
w i,j

for 1 < 2k + 1 ≤ Ds
→
−
−
= h i,j + →
w i,j−1
for 1 < j ≤ Ds

where each multi-layer cell DTGRUk is defined
as:

and the number of parameters. For translation quality, we report case-sensitive, detokenized
B LEU, measured with mteval-v13a.pl, on newstest2014, newstest2015, and newstest2016.
We release the code under an open source license, including it in the official Nematus repository.2 The configuration files needed to replicate
our experiments are available in a separate repository.3
3.1

Our first experiment is concerned with layer normalization. We are interested to see how essential layer normalization is for our deep architectures, and compare the effect of layer normalization on a baseline system, and a system with an
alternating encoder with stacked depth 4. Results
are shown in Table 1. We find that layer normalization is similarly effective for both the shallow
baseline model and the deep encoder, yielding an
average improvement of 0.8–1 B LEU, and reducing training time substantially. Therefore we use
it for all the subsequent experiments.
3.2

vk,1 = GRUk,1 (ink , statek )
vk,t = GRUk,t (0, vkt −1 ) for 1 < k ≤ Ls
DTGRUk (ink , statek ) = vk,Ls
It is also possible to have different transition
depths at each stacking level.
BiDeep decoders are similarly defined, replacing the recurrent cells (GRU, rGRU, cGRU or crGRU) with deep transition multi-layer cells.

3

Experiments

All experiments were performed with Nematus
(Sennrich et al., 2017b), following Sennrich et al.
(2017a) in their choice of preprocessing and hyperparameters. For experiments with deep models, we increase the depth by a factor of 4 compared to the baseline for most experiments; in preliminary experiments, we observed diminishing
returns for deeper models.
We trained on the parallel English–German
training data of WMT-2017 news translation task,
using newstest2013 as validation set. We used
early-stopping on the validation cross-entropy and
selected the best model based on validation B LEU.
We report cross-entropy (CE) on newstest2013,
training speed (on a single Titan X (Pascal) GPU),

Layer Normalization

Deep Encoders

In Table 2 we report experimental results for different architectures of deep encoders, while the
decoder is kept shallow.
We find that all the deep encoders perform substantially better than baseline (+0.5–+1.2 B LEU),
with no consistent quality differences between
each other. In terms of number of parameters and
training speed, the deep transition encoder performs best, followed by the alternating stacked
encoder and finally the biunidirectional encoder
(note that we trained on a single GPU, the biunidirectional encoder may be comparatively faster
on multiple GPUs due to its higher model parallelism).
3.3

Deep Decoders

Table 3 shows results for different decoder architectures, while the encoder is shallow. We find that
the deep decoders all improve the cross-entropy,
but the B LEU results are more varied: deep output4
decreases B LEU scores (but note that the baseline
2

https://github.com/EdinburghNLP/
nematus
3
https://github.com/Avmb/
deep-nmt-architectures
4
deep feed-forward output with shallow RNNs in both the
encoder and decoder

encoder

CE

baseline
+layer normalization
alternating (depth 4)
+layer normalization

49.98
47.53
49.25
46.29

2014
21.2
21.9
21.8
22.6

B LEU
2015
23.8
24.7
24.6
25.2

parameters (M)
2016
28.4
29.3
28.9
30.5

98.0
98.1
135.8
135.9

training speed
(words/s)
3350
2900
2150
1600

early stop
(104 minibatches)
44
29
46
29

Table 1: Layer normalization results. English→German WMT17 data.
encoder
shallow
alternating
biunidirectional
deep transition

s. bidir.
1
4
1
1

depth
s. forw.
3
-

CE
trans.
1
1
1
4

47.53
46.29
46.79
46.54

2014
21.9
22.6
22.4
22.9

B LEU
2015
24.7
25.2
25.4
25.4

parameters (M)
2016
29.3
30.5
30.0
30.2

98.1
135.9
173.7
117.0

training speed
(words/s)
2900
1600
1500
1900

Table 2: Deep encoder results. English→German WMT17 data. Parameters and speed are highlighted
for the deep recurrent models.
has already some depth), stacked GRU performs
similarly to the baseline (-0.1–+0.2 B LEU) and
stacked rGRU possibly slightly better (+0.1–+0.2
B LEU).
Other deep RNN decoders achieve higher gains.
The best results (+0.6 B LEU on average) are
achieved by the stacked conditional GRU with independent multi-step attention (cGRU). This decoder, however, is the slowest one and has the most
parameters.
The deep transition decoder performs well (+0.5
B LEU on average) in terms of quality and is the
fastest and smallest of all the deep decoders that
have shown quality improvements.
The stacked conditional GRU with reused attention (crGRU) achieves smaller improvements
(+0.3 B LEU on average) and has speed and
size intermediate between the deep transition and
stacked cGRU decoders.
3.4

Deep Encoders and Decoders

Table 4 shows results for models where both the
encoder and the decoder are deep, in addition to
the results of the best deep encoder (the deep transition encoder) + shallow decoder reported here
for ease of comparison.
Compared to deep transition encoder alone, we
generally see improvements in cross-entropy, but
not in B LEU. We evaluate architectures similar to
Zhou et al. (2016) (alternating encoder + stacked
GRU decoder) and (Wu et al., 2016) (biunidirectional encoder + stacked rGRU decoder), though
they are not straight replications since we used
GRU cells rather than LSTMs and the implementation details are different. We find that the former architecture performs better in terms of B LEU

scores, model size and training speed.
The other variants of alternating encoder +
stacked or deep transition decoder perform similarly to alternating encoder + stacked rGRU decoder, but do not improve B LEU scores over the
best deep encoder with shallow decoder. Applying the BiDeep architecture while keeping the
total depth the same yields small improvements
over the best deep encoder (+0.2 B LEU on average), while the improvement in cross-entropy is
stronger. We conjecture that deep decoders may be
better at handling subtle target-side linguistic phenomena that are not well captured by the 4-gram
precision-based B LEU evaluation.
Finally, we evaluate a subset of architectures
with a combined depth that is 8 times that of the
baseline. Among the large models, the BiDeep
model yields substantial improvements (average
+0.6 B LEU over the best deep encoder, +1.5
B LEU over the shallow baseline), in addition to
cross-entropy improvements. The stacked-only
model, on the other hand, performs similarly to the
smaller models, despite having even more parameters than the BiDeep model. This shows that it is
useful to combine deep transitions with stacking,
as they provide two orthogonal kinds of depth that
are both beneficial for neural machine translation.
3.5

Error Analysis

One theoretical difference between a stacked RNN
and a deep transition RNN is that the distance in
the computation graph between timesteps is increased for deep transition RNNs. While this allows for arguably more expressive computations
to be represented, in principle it could reduce the
ability to remember information over long dis-

decoder
shallow
stacked
stacked
stacked
stacked
deep transition
deep output

high RNN
stacked
GRU
rGRU
cGRU
crGRU
-

decoder RNN depth
trans.
type
1
1
4
1
4
1
4
1
4
1
1
8
1
1

output
depth
1
1
1
1
1
1
4

CE
47.53
46.73
46.72
44.76
45.88
45.98
47.21

2014
21.9
21.8
22.1
22.8
22.5
22.4
21.5

B LEU
2015
24.7
24.6
25.0
25.5
24.7
24.9
24.2

2016
29.3
29.5
29.4
29.6
29.7
30.0
28.7

params.
(M)
98.1
117.0
135.9
164.3
145.4
117.0
98.9

training speed
(words/s)
2900
2250
2150
1300
1750
2200
2850

Table 3: Deep decoder results. English→German WMT17 data. Parameters and speed are highlighted
for the deep recurrent models.
encoder

decoder

decoder high
RNN type
shallow
shallow
deep tran.
shallow
(Zhou et al., 2016) (ours)
alternating
stacked
GRU
(Wu et al., 2016) (ours)
biunidir.
stacked
rGRU
alternating
stacked
rGRU
alternating
stacked
cGRU
deep tran.
deep tran.
BiDeep altern.
BiDeep
rGRU
BiDeep altern.
BiDeep
rGRU
alternating
stacked
rGRU

encoder depth
bidir. forw. trans.
1
1
1
4

decoder depth
stacked
trans.
1
1
1
1

CE
47.53
46.54

2014
21.9
22.9

B LEU
2015
24.7
25.4

2016
29.3
30.2

params.
(M)
98.1
117.0

training speed
(words/s)
2900
1900

4

-

1

4

1

45.89

22.9

25.3

30.1

154.9

1480

1
4
4
1
2
4
8

3
-

1
1
1
4
2
2
1

4
4
4
1
2
4
8

1
1
1
8
4/2
4/2
1

46.15
46.00
44.32
45.52
43.52
43.26
44.32

22.4
23.0
22.9
22.7
23.1
23.4
22.9

24.7
25.7
25.7
25.7
25.5
26.0
25.5

29.6
30.5
29.8
30.1
30.6
31.0
30.5

211.5
173.7
202.1
136.0
145.4
214.7
274.6

1280
1400
970
1570
1480
980
880

Table 4: Deep encoder–decoder results. English→German WMT17 data. Transition depth 4/2 means 4
in the base RNN of the stack and 2 in the higher RNNs. The last two models are large and their results
are highlighted separately.

5
some decisions may not require the information to be
passed on the target side because the decisions may be possi-

1

0.95
accuracy

tances, since each layer may lose information during forward computation or backpropagation. This
may not be a significant issue in the encoder,
as the attention mechanism provides short paths
from any source word state to the decoder, but
the decoder contains no such shortcuts between its
states, therefore it might be possible that this negatively affects its ability to model long-distance relationships in the target text, such as subject–verb
agreement.
Here, we seek to answer this question by testing our models on Lingeval97 (Sennrich, 2017),
a test set which provides contrastive translation
pairs for different types of errors. For the example of subject-verb agreement, contrastive translations are created from a reference translation by
changing the grammatical number of the verb, and
we can measure how often the NMT model prefers
the correct reference over the contrastive variant.
In Figure 5, we show accuracy as a function of
the distance between subject and verb. We find
that information is successfully passed over long
distances by the deep recurrent transition network.
Even for decisions that require information to be
carried over 16 or more words, or at least 128 GRU
transitions5 , the deep recurrent transition network

0.9
shallow GRU
stacked GRU
deep transition GRU
BiDeep GRU

0.85

0.8

0

4

8
distance

12

≥1616

Figure 5: Subject-verb agreement accuracy as a
function of distance between subject and verb.
achieves an accuracy of over 92.5% (N = 560),
higher than the shallow decoder (91.6%), and similar to the stacked GRU (92.7%). The highest accuracy (94.3%) is achieved by the BiDeep network.

4

Conclusions

In this work we presented and evaluated multiple
architectures to increase the model depth of neural
machine translation systems.
We showed that alternating stacked encoders
(Zhou et al., 2016) outperform biunidirectional
ble based on source-side information.

stacked encoders (Wu et al., 2016), both in accuracy and (single-GPU) speed. We showed that
deep transition architectures, which we first applied to NMT, perform comparably to the stacked
ones in terms of accuracy (B LEU, cross-entropy
and long-distance syntactic agreement), and better
in terms of speed and number of parameters.
We found that depth improves B LEU scores especially in the encoder. Decoder depth, however,
still improves cross-entropy if not strongly B LEU
scores.
The best results are obtained by our BiDeep
architecture which combines both stacked depth
and transition depth in both the (alternating) encoder and the decoder, yielding better accuracy for
the same number of parameters than systems with
only one kind of depth.
We recommend to use combined architectures
when maximum accuracy is the goal, or use deep
transition architectures when speed or model size
are a concern, as deep transition performs very
positively in the quality/speed and quality/size
trade-off.
While this paper only reports results for one
translation direction, the effectiveness of the presented architectures across different data conditions and language pairs was confirmed in followup work. For the shared news translation task
of this year’s Conference on Machine Translation
(WMT17), we built deep models for 12 translation directions, using a deep transition architecture
or a stacked architecture (alternating encoder and
rGRU decoder), and observe improvements for the
majority of translation directions (Sennrich et al.,
2017a).

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly
Learning to Align and Translate. In Proceedings of
the International Conference on Learning Representations (ICLR).
Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck,
Antonio Jimeno Yepes, Philipp Koehn, Varvara
Logacheva, Christof Monz, Matteo Negri, Aurelie Neveol, Mariana Neves, Martin Popel, Matt
Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos
Zampieri. 2016. Findings of the 2016 Conference
on Machine Translation (WMT16). In Proceedings
of the First Conference on Machine Translation, Volume 2: Shared Task Papers. Association for Computational Linguistics, Berlin, Germany, pages 131–
198.
Denny Britz, Anna Goldie, Minh-Thang Luong, and
Quoc V. Le. 2017. Massive Exploration of Neural Machine Translation Architectures.
CoRR
abs/1703.03906.
Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa
Bentivogli, and Marcello Federico. 2016. Report on
the 13th IWSLT Evaluation Campaign. In IWSLT
2016. Seattle, USA.
Kyunghyun Cho, B van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014a. On the properties of neural machine translation: Encoder-decoder
approaches. In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST8), 2014.

Acknowledgments

Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014b. Learning Phrase Representations using RNN Encoder–
Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).
Doha, Qatar, pages 1724–1734.

The research presented in this publication was
conducted in cooperation with Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute
Poland.

Orhan Firat and Kyunghyun Cho. 2016.
Conditional Gated Recurrent Unit with Attention
Mechanism.
https://github.com/nyu-dl/dl4mttutorial/blob/master/docs/cgru.pdf.
Published
online, version adbaeea.

This project received funding from the
European Union’s Horizon 2020 research
and innovation programme under grant agreements 645452 (QT21), 644402 (HimL) and
688139 (SUMMA).

Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional Sequence to Sequence Learning. CoRR
abs/1705.03122.

References

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep Residual Learning for Image
Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,
Las Vegas, NV, USA, June 27-30, 2016. pages 770–
778.

Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton.
2016. Layer Normalization. CoRR abs/1607.06450.

Razvan Pascanu, Çağlar Gülçehre, Kyunghyun Cho,
and Yoshua Bengio. 2014. How to Construct Deep
Recurrent Neural Networks. In International Conference on Learning Representations 2014 (Conference Track).
Rico Sennrich. 2017. How Grammatical is Characterlevel Neural Machine Translation? Assessing MT
Quality with Contrastive Translation Pairs. In Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Linguistics: Volume 2, Short Papers. Association for
Computational Linguistics, Valencia, Spain, pages
376–382.
Rico Sennrich, Alexandra Birch, Anna Currey, Ulrich
Germann, Barry Haddow, Kenneth Heafield, Antonio Valerio Miceli Barone, and Philip Williams.
2017a. The University of Edinburgh’s Neural MT
Systems for WMT17. In Proceedings of the Second Conference on Machine Translation, Volume 2:
Shared Task Papers. Copenhagen, Denmark.
Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch, Barry Haddow, Julian Hitschler, Marcin
Junczys-Dowmunt, Samuel Läubli, Antonio Valerio Miceli Barone, Jozef Mokry, and Maria Nadejde. 2017b. Nematus: a Toolkit for Neural Machine
Translation. In Proceedings of the Software Demonstrations of the 15th Conference of the European
Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,
Valencia, Spain, pages 65–68.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural networks. In Advances in neural information processing systems. pages 3104–3112.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention Is All
You Need. arXiv preprint arXiv:1706.03762 .
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
Neural Machine Translation System: Bridging the
Gap between Human and Machine Translation.
CoRR abs/1609.08144.
Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin,
Roland Memisevic, Ruslan R Salakhutdinov, and
Yoshua Bengio. 2016. Architectural Complexity
Measures of Recurrent Neural Networks. In Advances in Neural Information Processing Systems
29. pages 1822–1830.

Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and
Wei Xu. 2016. Deep Recurrent Models with FastForward Connections for Neural Machine Translation. TACL 4:371–383.
Julian Georg Zilly, Rupesh Kumar Srivastava,
Jan Koutník, and Jürgen Schmidhuber. 2016.
Recurrent highway networks.
arXiv preprint
arXiv:1607.03474 .

