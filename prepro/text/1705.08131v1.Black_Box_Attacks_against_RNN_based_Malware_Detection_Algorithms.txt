arXiv:1705.08131v1 [cs.LG] 23 May 2017

Black-Box Attacks against RNN based Malware
Detection Algorithms

Weiwei Hu and Ying Tan∗
Key Laboratory of Machine Perception (MOE), and Department of Machine Intelligence
School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871 China
{weiwei.hu, ytan}@pku.edu.cn

Abstract
Recent researches have shown that machine learning based malware detection
algorithms are very vulnerable under the attacks of adversarial examples. These
works mainly focused on the detection algorithms which use features with fixed
dimension, while some researchers have begun to use recurrent neural networks
(RNN) to detect malware based on sequential API features. This paper proposes
a novel algorithm to generate sequential adversarial examples, which are used to
attack a RNN based malware detection system. It is usually hard for malicious
attackers to know the exact structures and weights of the victim RNN. A substitute
RNN is trained to approximate the victim RNN. Then we propose a generative RNN
to output sequential adversarial examples from the original sequential malware
inputs. Experimental results showed that RNN based malware detection algorithms
fail to detect most of the generated malicious adversarial examples, which means
the proposed model is able to effectively bypass the detection algorithms.

1

Introduction

Machine learning has been widely used in various commercial and non-commercial products, and has
brought great convenience and profits to human beings. However, recent researches on adversarial
examples show that many machine learning algorithms are not robust at all when someone want to
crack them on purpose [29, 6]. Adding some small perturbations to original samples will make a
classifier unable to classify them correctly.
In some security related applications, attackers will try their best to attack any defensive systems to
spread their malicious products such as malware. Existing machine learning based malware detection
algorithms mainly represent programs as feature vectors with fixed dimension and classify them
between benign programs and malware [15]. For example, a binary feature vector can be constructed
according to the presences or absences of system APIs (i.e. application programming interfaces) in a
program [26]. Grosse et al. [8] and Hu et al. [11] have shown that fixed dimensional feature based
malware detection algorithms are very vulnerable under the attack of adversarial examples.
Recently, as recurrent neural networks (RNN) become popular, some researchers have tried to use
RNN for malware detection and classification [25, 30, 14]. The API sequence invoked by a program
is used as the input of RNN. RNN will predict whether the program is benign or malware.
This paper tries to validate the security of a RNN based malware detection model when it is attacked
by adversarial examples. We proposed a novel algorithm to generate sequential adversarial examples.
Existing researches on adversarial samples mainly focus on images. Images are represented as
matrices with fixed dimensions, and the values of the matrices are continuous. API sequences
∗

Prof. Ying Tan is the corresponding author.

consist of discrete symbols with variable lengths. Therefore, generating adversarial examples for API
sequences will become quite different from generating adversarial examples for images.
To generate adversarial examples from API sequences we only consider to insert some irreverent
APIs into the original sequences. Removing an API from the API sequence may make the program
unable to work. How to insert irreverent APIs into the sequence will be the key to generate adversarial
examples.
We propose a generative RNN based approach to generate irreverent APIs and insert them into the
original API sequences. A substitute RNN is trained to fit the victim RNN. Gumbel-Softmax [12] is
used to smooth the API symbols and deliver gradient information between the generative RNN and
the substitute RNN.

2

Adversarial Examples

Adversarial examples are usually generated by adding some perturbations to the original samples.
Szegedy et al. used a box-constrained L-BFGS to search for an appropriate perturbation which can
make a neural network misclassify an image [29]. They found that adversarial examples are able to
transfer among different neural networks. Goodfellow et al. proposed the “fast gradient sign method”
where added perturbations are determined by the gradients of the cost function with respect to inputs
[6]. An iterative algorithm to generate adversarial examples was proposed by Papernot et al. [22]. At
each iteration the algorithm only modifies one pixel or two pixels of the image.
Grosse et al. used the iterative algorithm proposed by Papernot et al. [22] to add some adversarial
perturbations to Android malware on about 545 thousand binary features [8]. For the best three
malware detection models used in their experiments, about 60% to 70% malware will become
undetected after their adversarial attacks.
Previous algorithms to generate adversarial examples mainly focused on attacking feed-forward
neural networks. Papernot et al. migrated these algorithms to attack RNN [23]. RNN is unrolled
along time and existing algorithms for feed-forward neural networks are used to generate adversarial
examples for RNN. The limitation of their algorithm is that the perturbations are not truly sequential.
For examples, if they want to generate adversarial examples from sentences, they can only replace
existing words with others words, but cannot insert words to the original sentences or delete words
form the original sentences.
Sometimes it is hard for the attackers to know the structures and parameters of the victim machine
learning models. For example, many machine learning models are deployed in remote servers or
compiled into binary executables. To attack a black-box victim neural network, Papernot et al. first
got the outputs from the victim neural network on their training data, and then trained a substitute
neural network to fit the victim neural network [21]. Adversarial examples are generated from the
substitute neural network. They also showed that other kinds of machine learning models such as
decision trees can also be attacked by using the substitute network to fit them [20].
Besides substitute network based approaches, several direct algorithms for black-box attacks have
been proposed recently. Narodytska et al. adopted a greedy local search to find a small set of pixels
by observing the probability outputs of the victim network after applying perturbations [19]. Liu et
al. used an ensemble-based algorithm to generate adversarial examples and the adversarial examples
are able to attack other black-box models due to the transferability of adversarial examples [18] .
Several defensive algorithms against adversarial examples have been proposed, such as feature selection [32], defensive distillation [24] and retraining [17]. However, it is found that the effectiveness of
these defensive algorithms is limited, especially under repeated attacks [8, 4, 3].

3

RNN for Malware Detection

In this section we will show how to use RNN to detect malware. Malware detection is regarded as
a sequential classification problem [25, 30, 14]. RNN is used to classify whether an API sequence
comes from a benign program or malware.
2

We will also introduce some variants of RNN in this section. Malware detection model is usually a
black box to malware authors, and they need to take the potential variants into consideration when
developing attacking algorithms.
Each API is represented as a one-hot vector. Assuming there are M APIs in total, these APIs are
numbered from 0 to M − 1. The feature vector x of an API is an M -dimensional binary vector. If
the API’s number is i, the i-th dimension of x is 1, and other dimensions are all zeros.
An API sequence is represented as x1 , x2 , ..., xT , where T is the length of the sequence. After
feeding the input to RNN, the hidden states of RNN can be represented as h1 , h2 , ..., hT .
In the basic version of RNN, the hidden state of the last time step hT is used as the representation
of the API sequence. The output layer uses hT to compute the probability distribution over the two
classes. Then cross entropy is used as the loss function of API sequence classification.
The first variant of the RNN model introduced here is average pooling [2], which uses the average
states across h1 to hT as the representation of the sequence, instead of the last state hT .
Attention mechanism [1] is another variant, which uses weighted average of the hidden states to
represent the sequence. Attention mechanism is inspired by the selective nature of human perception.
For example, when faced with a picture human beings will focus on some meaningful objects in
it, rather than every details of it. Attention mechanism in deep learning makes the model to focus
on meaningful parts of inputs. It has shown to be very useful in machine translation [1] and image
caption [31].
An attention function A is defined to map the hidden state to a scalar value, which indicates the
importance of the corresponding time step. The attention function is usually a feed-forward neural
network. The attention function values across the whole sequence are then normalized according to
T
P
the formula αt = exp(A(ht )))/
exp(A(hs )), where αt is the final weight of time step t.
s=1

The above RNN models only process the sequence in the forward direction, while some sequential
patterns may lie in the backward direction. Bidirectional RNN tries to learn patterns from both
directions [27]. In bidirectional RNN, an additional backward RNN is used to process the reversed
sequence, i.e. from xT to x1 . The concatenation of the hidden states from both directions is used to
calculate the output probability.

4

Attacking RNN based Malware Detection Algorithms

Papernot et al. [23] migrated the adversarial example generation algorithms for feed-forward neural
networks to attack RNN by unrolling RNN along time and regarding it as a special kind of feedforward neural network. However, such model can only replace existing elements in the sequence
with other elements, since the perturbations are not truly sequential. This algorithm cannot insert
irreverent APIs to the original sequences. The main contribution of this paper is that we proposed
a generative RNN based approach to generate sequential adversarial examples, which is able to
effectively mine the vulnerabilities in the sequential patterns.
The proposed algorithm consists of a generative RNN and a substitute RNN, as shown in Figure
1 and Figure 2. The generative model is based on a modified version of the sequence to sequence
model [28], which takes malware’s API sequence as input and generates an adversarial API sequence.
The substitute RNN is trained on benign sequences and the Gumbel-Softmax [12] outputs of the
generative RNN, in order to fit the black-box victim RNN. The Gumbel-Softmax enables the gradient
to back propagate from the substitute RNN to the generative RNN.
4.1

The Generative RNN

The input of the generative RNN is a malware API sequence, and the output is the generated sequential
adversarial example for the input malware. The generative RNN generates a small piece of API
sequence after each API and tries to insert the sequence piece after the API.
For the input sequence x1 , x2 , ..., xT , the hidden states of the recurrent layer are h1 , h2 , ..., hT . At
time step t, a small sequence of Gumbel-Softmax output g t1 , g t2 , ..., g tL with length L is generated
based on ht , where L is a hyper-parameter.
3

Classification

+

Attention
Subtitute RNN

Bidirectional
Layer
Gumbel-Softmax
Sampling

Decoder Layer
Generative RNN

Recurrent Layer

Malware Input

Inserting
Adversarial Example

Black-Box Victim RNN

Cross Entropy

Figure 1: The architecture of the proposed model when trained on malware.

Sequence decoder [5] is used to generate the small sequence. The decoder RNN uses the formula
D
D
D
D
hD
τ = Dec(xτ , hτ −1 ) to update hidden states, where xτ is the input and hτ is the hidden state of
the decoder RNN which is initialized with zero.
Formula 1 is used to get the hidden state when generating g t1 .
D
hD
1 = Dec(ht , h0 = 0).

(1)

When generating the first element at time step t, the input is the hidden state ht .
Then a softmax layer is followed to generate the API. Besides the M APIs, we introduce a special
null API into the API set. If the null API is generated at time step τ , no API will be inserted to the
original sequence at that moment. If we do not use the null API, too many generated APIs will be
inserted into the sequence and the resulting sequence will become too long. Allowing null API will
make the final sequence shorter. Since the M valid APIs have been numbered from 0 to M − 1, the
null API is numbered as M .
The softmax layer will have M + 1 elements, which is calculated as π t1 = sof tmax(W s hD
1 ),
where W s is the weights to map the hidden state to the output layer.
Then we can sample an API from π t1 . Let the one-hot representation of the sampled API be at1 .
The sampled API is a discrete symbol. If we give at1 to the substitute RNN, we are unable to get the
gradients from the substitute RNN and thus unable to train the generative RNN.
Gumbel-Softmax is recently proposed to approximate one-hot vectors with differentiable representations [12]. The Gumbel-Softmax output g t1 has the same dimension with π t1 . The i-th element of
g t1 is calculated by Formula 2.
g it1 =

exp((log(π it1 ) + zi )/temp)
,
M
P
j
exp((log(π t1 ) + zj )/temp)

j=0

4

(2)

where zi is a random number sampled from the Gumbel distribution [9] and temp is the temperature
of Gumbel-Softmax. In this paper we use a superscript to index the element in a vector.
To generate the τ -th API at time step t when τ is greater than 1, the decoder RNN uses Formula 3 to
update the hidden state.
D
hD
τ = Dec(W g g t(τ −1) , hτ −1 ).

(3)

The decoder RNN takes the previous Gumbel-Softmax output as input. W g is used to map g t(τ −1)
to a space with the same dimension as ht , in order to make the input dimension of the decoder RNN
compatible with Formula 1.
Calculating Gumbel-Softmax for τ > 1 can use the same way as τ = 1 (i.e. Formula 2). Therefore,
we omit the formula here.
After generating small sequences from t = 1 to T and inserting the generated sequences to the
original sequence, we obtained two kinds of results.
The first kind of result is the one-hot representation of the final adversarial sequence Sadv :

Sadv = RemoveN ull(x1 , a11 , a12 , ..., a1L , x2 , a21 , a22 , ..., a2L , ......, xT , aT 1 , aT 2 , ..., aT L ).
(4)
The generated null APIs should be removed from the one-hot sequence.
The second kind of result uses Gumbel-Softmax outputs to replace one-hot representations:
SGumbel = x1 , g 11 , g 12 , ..., g 1L , x2 , g 21 , g 22 , ..., g 2L , ......, xT , g T 1 , g T 2 , ..., g T L .

(5)

The null APIs’ Gumbel-Softmax outputs are reserved in the sequence, in order to connect the gradients
of loss function with null APIs. The loss function will be defined in the following sections.
4.2

The Substitute RNN

Malware authors usually do not know the detailed structure of the victim RNN. They do not know
whether the victim RNN uses bidirectional connection, average pooling and the attention mechanism.
The weights of the victim RNN is also unavailable to malware authors.
To fit such victim RNN with unknown structure and weights, a neural network with strong representation ability should be used. Therefore, the substitute RNN uses bidirectional RNN with attention
mechanism since it is able to learn complex sequential patterns. Bidirectional connection contains
both the forward connection and the backward connection, and therefore it has the ability to represent
the unidirectional connection. The attention mechanism is able to focus on different positions of
the sequence. Therefore, RNN with attention mechanism can represent the cases without attention
mechanism such as average pooling and the using of the last state to represent the sequence.
To fit the victim RNN, the substitute RNN should regard the output labels of the victim RNN on the
training data as the target labels. The training data should contain both malware and benign programs.
As shown in Figure 1 and the previous section, for malware input two kinds of outputs are generated
from the generative RNN, i.e. the one-hot adversarial example Sadv and the Gumbel-Softmax output
SGumbel .
We use the victim RNN to detect the one-hot adversarial example, and get the resulting label v. v is a
binary value where 0 represents benign label and 1 represents malware.
Then the substitute RNN is used to classify the Gumbel-Softmax output SGumbel , and outputs the
malicious probability pS .
Cross entropy is used as the loss function, as shown in Formula 6.
LS = −v log(pS ) − (1 − v)log(1 − pS ).
5

(6)

For a benign input sequence, it is directly fed into the victim RNN and the substitute RNN, as shown
in Figure 2. The outputs of the two RNNs v and pS are used to calculate the loss function in the same
way as Formula 6.

Classification

+

Attention
Subtitute RNN
Bidirectional
Layer

Benign Input

Black-Box Victim RNN

Cross Entropy

Figure 2: The architecture of the proposed model when trained on benign programs.

4.3

Training

The training objective of the generative RNN is to minimize the predicted malicious probability
pS on SGumbel . We also add a regularization term to restrict the number of inserted APIs in the
adversarial sequence by maximizing the null API’s expectation probability. The final loss function of
the generative RNN is defined in Formula 7.
LG = log(pS ) − γEt=1∼T,τ =1∼L π M
tτ ,

(7)

where γ is the regularization coefficient and M is the index of the null API.
The training process of the proposed model is summarized in Algorithm 1.
Algorithm 1 Training the Proposed Model
1: while terminal condition not satisfied do
2:
Sample a minibatch of data, which contains malware and benign programs.
3:
Calculate the outputs of the generative RNN for malware.
4:
Get the outputs of the substitute RNN on benign programs and the Gumbel-Softmax output
of malware.
5:
Get the outputs of the victim RNN on the adversarial examples and benign programs.
6:
Minimize LS on both benign and malware data by updating the substitute RNN’s weights.
7:
Minimize LG on malware data by updating the generative RNN’s weights.
8: end while

5

Experiments

Adam [13] was used to train all of the models. LSTM unit was used for all of the RNNs presented in
the experiments due to its good performance in processing long sequences [10, 7].
5.1

Dataset

We crawled 180 programs with corresponding behavior reports from a website for malware analysis
(https://malwr.com/). On the website users can upload their programs and the website will execute
6

the programs in virtual machines. Then the API sequences called by the uploaded programs will be
posted on the website. 70% of the crawled programs are malware.
In real-world applications, the adversarial example generation model and the victim RNN should
be trained by malware authors and anti-virus vendors respectively. The datasets that they collected
cannot be the same. Therefore, we use different training sets for the two models. We selected 30% of
our dataset as the training set of the adversarial example generation model (i.e. the generative RNN
and the substitute RNN), and selected 10% as the validation set of the adversarial example generation
model. Then we selected another 30% and 10% as the training set and the validation set of the victim
RNN respectively. The remaining 20% of our dataset was regarded as the test set.
5.2

The Victim RNNs

To validate the representation ability of the substitute RNN, we used the several different structures for
the black-box victim RNN, as shown in the first column of Table 1. In Table 1, the first LSTM model
uses the last hidden state as the representation of the sequence. BiLSTM represents bidirectional
LSTM. The suffixes “Average” and “Attention” in the last four rows indicate the use of average
pooling and attention mechanism to represent the sequence.
We first tuned the hyper-parameters of BiLSTM-Attention on the validation set. The final learning
rate was set to 0.001. The number of recurrent hidden layers and the number of attention hidden layers
were both set to one and the layer sizes were both set to 128. We directly used the resulting hyperparameters to other victim models. We have tried to separately tune the hyper-parameters for other
victim RNNs but the performance did not improve much compared with using BiLSTM-Attention’s
hyper-parameters.
Table 1 gives the area under curve (AUC) of these victim RNNs before adversarial attacks.
Table 1: AUC of different victim RNNs before attacks.
Algorhthm

Training Set

Test Set

LSTM
BiLSTM
LSTM-Average
BiLSTM-Average
LSTM-Attention
BiLSTM-Attention

94.57%
94.67%
93.07%
91.13%
95.98%
95.02%

91.30%
92.80%
92.66%
91.14%
93.97%
93.83%

Overall, the attention mechanism works better than non-attention approaches, since attention mechanism is able to learn the relative importance of different parts in sequences. LSTM and BiLSTM
only use the last hidden state, and therefore the information delivered to the output layer is limited.
In this case bidirectional connection delivers more information than unidirectional connection, and
AUC of BiLSTM is higher than LSTM. For average pooling and attention mechanism, bidirectional
LSTM does not outperform unidirectional LSTM in AUC. Average pooling and attention mechanism
are able to capture the information of the whole API sequence. Unidirectional LSTM is enough to
learn the sequential patterns. Compared with unidirectional LSTM, bidirectional LSTM has more
parameters, which makes the learn process more difficult. Therefore, the bidirectional connection
does not improve the performance for average pooling and attention mechanism.
5.3

Experimental Results of the Proposed Model

The hyper-parameters of the generative RNN and the substitute RNN were tuned separately for each
black-box victim RNN. The learning rate and the regularization coefficient were chosen by line
search along the direction 0.01, 0.001, et al.. The Gumbel-Softmax temperature was searched in the
range [1, 100]. Actually, the decoder length L in the generative RNN is also a kind of regularization
coefficient. A large L will make the generative RNN have strong representation ability, but the whole
adversarial sequences will become too long, and the generative RNN’s size may exceed the capacity
of the GPU memory. Therefore, in our experiments we set L to 1.
The experimental results are show in Table 2.
7

Table 2: Detection rate on original samples and adversarial examples. “Adver." represents adversarial
examples.

LSTM
BiLSTM
LSTM-Average
BiLSTM-Average
LSTM-Attention
BiLSTM-Attention

Training Set
Original
Adver.

Test Set
Original

Adver.

92.54%
92.21%
93.87%
92.92%
93.67%
93.73%

90.74%
90.93%
93.53%
92.51%
92.45%
92.99%

11.95%
0.95%
1.36%
1.67%
0.51%
3.03%

12.10%
1.06%
1.40%
1.83%
0.44%
3.02%

After adversarial attacks, all the victim RNNs fails to detect most of the malware. For different
victim RNNs, the detection rates on adversarial examples range from 0.44% to 12.10%, while before
adversarial attacks the detection rates range from 90.74% to 93.87%. That is to say, about 90%
malware will bypass the detection algorithms under our proposed attack model.
Except LSTM, the detection rates on adversarial examples of all the victim models are smaller than
3.03%, which means that victim RNNs are almost unable to detect any malware. The victim model
LSTM have detection rates of 12.10% and 11.95% on the training set and the test set respectively,
which are higher than other victim RNNs. We can see that for the LSTM model the substitute RNN
does not fit the victim RNN very well on the training data.
The differences in adversarial examples’ detection rates are very small between the training set and
the test set for these victim RNNs. The generalization ability of the proposed model is quite well for
unseen malware examples. The proposed adversarial example generation algorithm can be applied to
both existing malware and unseen malware.
It can be seen that even if the adversarial example generation algorithm and the victim RNN use
different RNN models and different training set, most of the adversarial examples are still able to
attack the victim RNN successfully. The adversarial examples can transfer among different models
and different training sets. The transferability makes it very easy for malware authors to attack RNN
based malware detection algorithms.

6

Conclusions and Future Works

A novel algorithm of generating sequential adversarial examples for malware is proposed in this
paper. The generative network is based on the sequence to sequence model. A substitute RNN is
trained to fit the black-box victim RNN. We use Gumbel-Softmax to approximate the generated
discrete APIs, which is able to propagate the gradients from the substitute RNN to the generative
RNN. The proposed model has successfully made most of the generated adversarial examples able to
bypass several black-box victim RNNs with different structures.
Previous researches on adversarial examples mainly focused on images which have fixed input
dimension. We have shown that the sequential machine models are also not safe under adversarial
attacks. The problem of adversarial examples becomes more serious when it comes to malware
detection. Robust defensive models are needed to deal with adversarial attacks.
In future works we will use the proposed model to attack convolutional neural network (CNN) based
malware detection algorithms, since many researchers have begun to use CNN to process sequential
data recently [33, 16]. We will validate whether a substitute RNN has enough capacity to fit a victim
CNN, and whether a substitute CNN has enough capacity to fit a victim RNN. The research on the
transferability of adversarial examples between RNN and CNN is very important to the practicability
of sequential malware detection algorithms.

References
[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
8

[2] Y-Lan Boureau, Jean Ponce, and Yann LeCun. A theoretical analysis of feature pooling in
visual recognition. In Proceedings of the 27th international conference on machine learning
(ICML-10), pages 111–118, 2010.
[3] Nicholas Carlini and David Wagner. Defensive distillation is not robust to adversarial examples.
arXiv preprint, 2016.
[4] Xinyun Chen, Bo Li, and Yevgeniy Vorobeychik. Evaluation of defensive methods for dnns
against multiple adversarial evasion models, 2016.
[5] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoderdecoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
[6] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
[7] Klaus Greff, Rupesh K Srivastava, Jan Koutník, Bas R Steunebrink, and Jürgen Schmidhuber.
Lstm: A search space odyssey. IEEE transactions on neural networks and learning systems,
2016.
[8] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel.
Adversarial perturbations against deep neural networks for malware classification. arXiv preprint
arXiv:1606.04435, 2016.
[9] Emil Julius Gumbel and Julius Lieblein. Statistical theory of extreme values and some practical
applications: a series of lectures, 1954.
[10] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735–1780, 1997.
[11] Weiwei Hu and Ying Tan. Generating adversarial malware examples for black-box attacks
based on gan. arXiv preprint arXiv:1702.05983, 2017.
[12] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.
arXiv preprint arXiv:1611.01144, 2016.
[13] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[14] Bojan Kolosnjaji, Apostolis Zarras, George Webster, and Claudia Eckert. Deep learning for
classification of malware system call sequences. In Australasian Joint Conference on Artificial
Intelligence, pages 137–149. Springer, 2016.
[15] J Zico Kolter and Marcus A Maloof. Learning to detect and classify malicious executables in
the wild. The Journal of Machine Learning Research, 7:2721–2744, 2006.
[16] Ji Young Lee and Franck Dernoncourt. Sequential short-text classification with recurrent and
convolutional neural networks. arXiv preprint arXiv:1603.03827, 2016.
[17] Bo Li, Yevgeniy Vorobeychik, and Xinyun Chen. A general retraining framework for scalable
adversarial classification. arXiv preprint arXiv:1604.02606, 2016.
[18] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial
examples and black-box attacks. arXiv preprint arXiv:1611.02770, 2016.
[19] Nina Narodytska and Shiva Prasad Kasiviswanathan. Simple black-box adversarial perturbations
for deep networks. arXiv preprint arXiv:1612.06299, 2016.
[20] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint
arXiv:1605.07277, 2016.
[21] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against deep learning systems using adversarial
examples. arXiv preprint arXiv:1602.02697, 2016.
[22] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and
Ananthram Swami. The limitations of deep learning in adversarial settings. In Security and
Privacy (EuroS&P), 2016 IEEE European Symposium on, pages 372–387. IEEE, 2016.
9

[23] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adversarial input sequences for recurrent neural networks. In Military Communications Conference,
MILCOM 2016-2016 IEEE, pages 49–54. IEEE, 2016.
[24] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation
as a defense to adversarial perturbations against deep neural networks. In Security and Privacy
(SP), 2016 IEEE Symposium on, pages 582–597. IEEE, 2016.
[25] Razvan Pascanu, Jack W Stokes, Hermineh Sanossian, Mady Marinescu, and Anil Thomas.
Malware classification with recurrent networks. In Acoustics, Speech and Signal Processing
(ICASSP), 2015 IEEE International Conference on, pages 1916–1920. IEEE, 2015.
[26] Matthew G Schultz, Eleazar Eskin, Erez Zadok, and Salvatore J Stolfo. Data mining methods for
detection of new malicious executables. In Security and Privacy, 2001. S&P 2001. Proceedings.
2001 IEEE Symposium on, pages 38–49. IEEE, 2001.
[27] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673–2681, 1997.
[28] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural
networks. In Advances in neural information processing systems, pages 3104–3112, 2014.
[29] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199,
2013.
[30] Shun Tobiyama, Yukiko Yamaguchi, Hajime Shimada, Tomonori Ikuse, and Takeshi Yagi.
Malware detection with deep neural network using process behavior. In Computer Software
and Applications Conference (COMPSAC), 2016 IEEE 40th Annual, volume 2, pages 577–582.
IEEE, 2016.
[31] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan Salakhutdinov,
Richard S Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation
with visual attention. In ICML, volume 14, pages 77–81, 2015.
[32] Fei Zhang, Patrick PK Chan, Battista Biggio, Daniel S Yeung, and Fabio Roli. Adversarial
feature selection against evasion attacks. IEEE transactions on cybernetics, 46(3):766–777,
2016.
[33] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text
classification. In Advances in neural information processing systems, pages 649–657, 2015.

10

