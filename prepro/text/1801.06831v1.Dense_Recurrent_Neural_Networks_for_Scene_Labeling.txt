Dense Recurrent Neural Networks for Scene Labeling
Heng Fan Haibin Ling
Department of Computer and Information Sciences, Temple University, Philadelphia, PA USA

arXiv:1801.06831v1 [cs.CV] 21 Jan 2018

{hengfan,hbling}@temple.edu

Abstract
Recently recurrent neural networks (RNNs) have demonstrated the ability to improve scene labeling through capturing long-range dependencies among image units. In this
paper, we propose dense RNNs for scene labeling by exploring various long-range semantic dependencies among
image units. In comparison with existing RNN based approaches, our dense RNNs are able to capture richer contextual dependencies for each image unit via dense connections between each pair of image units, which significantly
enhances their discriminative power. Besides, to select relevant and meanwhile restrain irrelevant dependencies for
each unit from dense connections, we introduce an attention model into dense RNNs. The attention model enables
automatically assigning more importance to helpful dependencies while less weight to unconcerned dependencies. Integrating with convolutional neural networks (CNNs), our
method achieves state-of-the-art performances on the PASCAL Context, MIT ADE20K and SiftFlow benchmarks.

1. Introduction
Scene labeling, or scene parsing, which aims to assign
one of predefined labels to each pixel in an image, is usually formulated as a pixel-level multi-classification problem. Borrowing from the successes of convolutional neural
networks (CNNs) [24] in image classification [23, 42, 19],
there are attempts to apply CNNs on scene labeling [15,
33, 36, 3, 16]. Owing to the powerful feature representation of CNNs, these approaches demonstrate promising
performance on scene parsing. However, a potential problem with these methods is that CNNs only explore limited
contextual cues from a small local field for classification,
which is prone to cause misclassifications for visually similar pixels of different categories. For example, the ‘sand’
pixels can be visually indistinguishable from ‘road’ pixels
even for human with limited context. To alleviate this issue, a natural solution is to leverage richer context information to discriminate locally ambiguous pixels [8, 50, 31].
In these approaches, nevertheless, the long-range contextual

(a) undirected cyclic graph

(b) directed acyclic graph

(c) dense undirected cyclic graph

(d) dense directed acyclic graph

Figure 1. Image (a) shows the image of UCG structure as in [41],
and image (b) is one of four DAG decompositions. Different from
[41], we utilize D-UCG to represent an image as shown in image
(c), and image (d) displays one of the four D-DAGs. Compared
to UCG and DAG, our D-UCG and D-DAG capture richer dependency information flow in images. Best viewed in color.

dependencies among image regions are still not effectively
explored, which are crucial in scene parsing.
Motivated by the capacity of capturing long-range dependency among sequential data, recurrent neural networks
(RNNs) [14] have recently been employed to model semantic dependencies in images for scene labeling [6, 25, 41, 27,
46], allowing us to perform long-range inferences to discriminate ambiguous pixels.
To model the dependencies among image units, a common way [41, 54] is to represent an image with an undirected cyclic graph (UCG) in which the image units are vertices and their interactions are encoded by undirected edges
(see Fig. 1(a)). Due to the loopy structure of UCGs, however, it is difficult to directly apply RNNs to model dependencies in images. To deal with this problem, an UCG is

approximated with several directed acyclic graphs (DAGs)
(see Fig. 1(b)). Then several DAG structured RNNs are
adopted to model the dependencies in these DAGs.
Though these DAG structured RNNs can capture dependencies in images to some extent, quiet a bit of information
are discarded. For example in Fig. 1(a), to correctly distinguish the ‘sand’ unit in red region from the ‘road’ unit,
DAG structured RNNs can use the dependency information
of ‘water’ units in the pink region from its adjacent neighbors. However, the ‘water’ information may be decaying
because it needs to pass through conductors (i.e., the adjacent neighbors of this ‘sand’ unit). Instead, a better way is
to directly leverage the dependency information from ‘water’ units to discriminate ‘sand’ unit from ‘road’ unit.
Recently, DenseNet [20] has demonstrated superior performance in image recognition by introducing dense connections to improve information flow in CNNs. Analogous to CNNs, the DAG structured RNNs can be unfolded
to a feed-forward network, the dependency information in
an image flows from the start vertex at top-left corner to
end vertex at bottom-right corner. To incorporate richer dependency information for each image unit, it is natural to
add more connections to the RNN feed-forward network as
well, as proposed in this paper.

1.1. Contributions
Our first contribution is to propose dense RNNs, which
capture richer dependencies from various abundant connections in images for each image unit. Unlike existing approaches representing an image as an UCG, we formulate
each image as a dense UCG (D-UCG), which is a complete
graph. In D-UCG, each pair of vertexes are connected with
an undirected edge (see Fig. 1(c)). By decomposing the DUCG into several dense DAGs (D-DAGs), we propose the
DAG structured dense RNNs (DD-RNNs) to model dependencies in images (see Fig. 1(d)). Compared to plain DAG
structured RNNs, our DD-RNNs are able to gain richer dependencies from various levels. For instance in Fig. 1(c), to
correctly recognize the ‘sand’ unit in red region, in addition
to the dependencies from its neighbors, our DD-RNNs enable the firsthand use of dependencies from ‘water’ units in
the pink region to improve the discriminative power.
The DD-RNNs are able to capture vast dependencies for
each image unit through dense connections. For a specific
unit, however, certain dependencies are irrelevant to help
improve discriminative power. For example in Fig. 1(d),
the ‘sky’ units in blue region are actually not useful to distinguish a ‘sand’ unit in the red region from a ‘road’ unit.
Instead, the dependencies from ‘water’ units in the pink region are the most crucial cues to infer its label. Thus, more
importance should be assigned to dependencies from ‘water’ units. To this end, we make the second contribution
by introducing an attention model into DD-RNNs. The at-

tention model is able to automatically select relevant and
meanwhile restrain irrelevant dependency information for
each image unit, which further enhances their discriminative power.
Last but not least, our third contribution is to implement an end-to-end scene labeling system by integrating
DD-RNNs with CNNs. For validation, we test the proposed method on three popular benchmarks: PASCAL Context [35], MIT ADE20K [53] and SiftFlow [30]. In these experiments the proposed method significantly improves the
baseline and outperforms other state-of-the-art algorithms.
The code will be released upon the publication.
The rest of this paper is organized as follows. Section
2 briefly reviews the related works of this paper. Section
3 describes the proposed approach in details. Experimental
results are demonstrated in Section 4, followed by conclusion in Section 5.

2. Related Work
2.1. Scene parsing
As one of the most challenging problems in computer
vision, scene parsing has drawn increasing attentions in recent decades. Early efforts mainly focus on the probabilistic
graphical model with hand-crafted features [30, 44, 49, 17].
Despite great progress, these approaches are restricted due
to the use of hand-crafted features.
Inspired by their successes in image recognition [23, 42,
19], deep CNNs have been extensively explored for scene
parsing. Long et al. [33] propose an end-to-end scene labeling method by transforming standard CNNs for classification into fully convolutional networks (FCN), resulting in significant improvement from conventional methods.
To generate desired full-resolution predictions, various approaches are proposed to learn to upsample low resolution
feature maps to high-resolution feature maps for final prediction [36, 3, 28]. In order to alleviate boundary problem
of predictions, graphical models such as Conditional Random Field (CRF) or Markov Random Field (MRF) are introduced into CNNs [8, 52, 32]. As a pixel-level classification problem, context plays a crucial role in scene labeling
to distinguish visually similar pixels of different categories.
The work of [50] proposes to introduce the dilated convolution into CNNs to gather multi-scale context for scene labeling. Liu et al. [31] suggest an additional branch in CNNs
to incorporate global context for scene parsing.

2.2. RNNs on computer vision
Recently, owing to the ability to model spatial dependencies among different image regions, RNNs [14] have
been applied to many computer vision tasks such as image
completion [37], handwriting recognition [18], image classification [54] and so forth. Taking into consideration the

importance of spatial contextual dependencies in images to
distinguish ambiguous pixels, there are attempts to applying
RNNs for scene labeling.
The work of [6] explores the two-dimensional long short
term memory (LSTM) networks for scene parsing by taking into account the complex spatial dependencies of pixels
in an image. In [43], Stollenga et al. introduce a parallel
multi-dimensional LSTM for image segmentation. Liang et
al. [27] propose a graph based LSTM to model the dependencies among superpixels in images. Visin et al. [46] suggest to utilize multiple linearly structured RNNs to model
horizontal and vertical dependencies in images for scene labeling. Li et al. [25] extend this method by replacing RNNs
with LSTM and apply it to RGB-D scene labeling. Qi [38]
proposes to use gated recurrent units (GRUs) to model longrange context. Especially, to exploit more spatial dependencies in images, Shuai et al. [41] propose to represent
an image with an UCG. By decomposing UCG into several
DAGs, they then propose to use DAG structured RNNs to
model dependencies among image units.
Different from the aforementioned approaches, we propose dense RNNs to model richer long-range dependencies
in images from dense connections, which significantly improves the discriminative power for each image unit.

2.3. Attention model
The attention-based model, being successfully applied
in Natural Language Processing (NLP) such as machine
translation [4], sentence summarization [39] and so on, has
drawn increasing interest in computer vision. Xu et al. [48]
propose to leverage an attention model to find out regions
of interest in images which are relevant in generating next
word. In [10], Chen et al. propose scale attention model for
semantic segmentation by adaptively merging outputs from
different scales. In [1], the attention model is utilized to
assign importance to different regions for context modeling
in images. The work of [34] introduces co-attention model
for question answering. Chu et al. [11] propose to utilize
attention model to combine multi-context for human pose
estimation.
To the best of our knowledge, our work is the first to
leverage the attention model in RNNs for scene labeling.
Our attention model automatically selects relevant and restrains irrelevant dependencies for image units from dense
connections, further improving their discriminability.

3.1. Review of DAG structured RNNs
The linear RNNs in [14] are developed to handle sequential data tasks. Specifically, a hidden unit ht in RNNs at
time step t is represented with a non-linear function over
current input xt and hidden layer at previous time step ht−1 ,
and the output yt is connected to the hidden unit ht . Given
an input sequence {xt }t=1,2,··· ,T , the hidden unit and output at time step t can be computed with
ht =φ(U xt + W ht−1 + b)

(1)

yt =σ(V ht + c)

(2)

where U , V and W represent the transformation matrices, b
and c are bias terms, and φ(·) and σ(·) are non-linear functions, respectively. Since the inputs are progressively stored
in the hidden layers as in Eq. (1), RNNs are capable of
preserving the memory of entire sequence and thus capture
long-range contextual dependencies in sequential data.
For an image, the interactions among image units can be
formulated as a graph in which the dependencies are forwarded through edges. The solution in [41] utilizes a standard UCG to represent an image (see again Fig. 1(a)). To
break the loopy structure of UCG, [41] further proposes to
decompose the UCG into four DAGs along different directions (see Fig. 1(b) for a southeast example).
Let G = {V, E} represent the DAG in Fig. 1(b), where
V = {vi }N
i=1 denotes the vertex set of N vertices, E =
{eij }N
i,j=1 represents the edge set, and eij indicates a directed edge from vi to vj . A DAG structured RNN resembles the identical topology of G, with a forward pass formulated as traversing G from the start vertex. In such modeling,
the hidden layer of each vertex is dependent on the hidden
units of its adjacent predecessors, (see Fig. 2(b)). For vertex
vi , its hidden layer hvi and output can be expressed as
X
hvi =φ(U xvi + W
hvj + b)
(3)
vj ∈PG (vi )

yvi =σ(V hvi + c)

(4)

where xvi denotes the local feature at vertex vi and PG (vi )
represents the predecessor set of vi in G. By storing local
inputs into hidden layers and progressive forwarding among
them with Eq. (3), the discriminative power of each image
unit is improved with dependencies from other units.

3.2. Dense RNNs

3. The Proposed Approach
In this section, we describe the proposed approach in details. Section 3.1 briefly reviews the DAG structured RNNs.
Section 3.2 introduces dense RNNs to capture richer dependencies in images. The attention model is applied to dense
RNNs in Section 3.3. Section 3.4 describes the full labeling
system by integrating dense RNNs with CNNs.

In DAG structured RNNs, each image unit receives the
dependencies from other units through recurrent information forwarding between adjacent units. Nevertheless, the
useful dependency information may be potentially degraded
after going through many conductors, resulting in a dependency decaying problem. For instance in Fig. 1(b), the
most useful contextual cues from ‘water’ units have to pass

࣪࣡ ሺ‫ ݅ݒ‬ሻ

‫݅ݒ‬

(a) DAG structured RNNs

(b) Predecessors for ‫ ݅ݒ‬in DAG RNNs
࣪ࣞ ሺ‫ ݅ݒ‬ሻ

of our DD-RNNs resembles the identical topology of D as
shown in Fig. 2(c). In our DD-RNNs, the hidden layer of
each vertex is dependent on the hidden units of its all adjacent and non-adjacent predecessors, which fundamentally
varies from [41] in which the hidden unit of each vertex
only relies on hidden units of its adjacent predecessors (see
Fig. 2(b)). The forward pass at vi in DD-RNNs is expressed
as
X
ĥvi =
hvj
(5)
vj ∈PD (vi )

‫݅ݒ‬

(c) DAG structured dense RNNs (d) Predecessors for ‫ ݅ݒ‬in DD-RNNs

Figure 2. The illustration of difference between DAG structured
RNNs [41] and our DD-RNNs. Image (a) shows the DAG structured RNNs along southeast direction, and in image (b) the hidden
layer of vertex vi relies on its three adjacent predecessors (see the
red region in image (b)). Image (c) is our DD-RNNs, and in image (d) the hidden layer of vi is dependent on all its adjacent and
non-adjacent predecessors (see the red region in image (d)). Best
viewed in color.

through conductors to arrive at the ‘sand’ unit covered in red
region. A natural solution to remedy the problem of dependency decaying is to add additional pathes between hidden
layers of distant units and current image unit.
Inspired by the recent state-of-the-art DenseNet [20], we
propose DAG structured dense RNNs (DD-RNNs) to model
richer dependencies in images. In DenseNet [20], each
layer is connected to every other layer in a feed-forward
fashion, which improves information flow between layers.
Analogous to CNNs, the DAG structured RNNs can be unfolded to a feed-forward network, the dependency information in an image flows from start vertex at top-left corner
to end vertex at bottom-right corner. To capture richer dependencies in images, we introduce more connections in the
RNN feed-forward network, resulting in the proposed DDRNNs.
To achieve dense connections, we in this paper represent
an image with a D-UCG, which is equivalent to a complete
graph (see Fig. 1(c) for illustration). Compared to standard
UCG, the D-UCG allows each image unit to connect with
all of other units. Because of the loopy property of D-UCG,
we adopt the strategy as in [41] to decompose the D-UCG
to four D-DAGs along four directions. One of the four DDAGs along southeast direction is shown in Fig. 1(d).
Let D represent the D-DAG in Fig. 1(d). The structure

hvi =φ(U xvi + W ĥvi + b)

(6)

yvi =σ(V hvi + c)

(7)

where PD (vi ) is the dense predecessor set of vi in D-DAG
D, and it contains both adjacent and non-adjacent predecessors (see Fig. 2(d)). Compared to the DAG structured RNNs
in [41], our DD-RNNs are able to model richer dependencies in images through various dense connections.
A concern arisen naturally from the dense model is the
complexity. In fact, it is unrealistic to directly apply the
DD-RNN to pixels of an image. Fortunately, neither is it
necessary. As described in Section 3.4, we typically apply
DD-RNN to a high layer output of existing CNN models.
Such strategy largely reduces the computational burden –
as summarized in Table 5, our final system runs faster than
state-of-the-arts while achieving better labeling accuracies.

3.3. Attention model in DD-RNNs
For the hidden layer at vertex vi , it receives dependency
information from various predecessors through dense connections. However, the dependency information from different predecessors are not always equally helpful to improve discriminative representation. For example, to distinguish ‘sand’ units from visually alike ‘road’ units in a beach
scene image, the most important contextual cues are probably the dependencies from ‘water’ units instead of other
units such as ‘sky’ or ‘tree’. In this case, we term the relation from ‘water’ units as relevant dependencies while the
information from ‘sky’ or ‘tree’ units as irrelevant ones.
To encourage relevant and restrain irrelevant dependencies for each image unit, we introduce a soft attention
model [4] into DD-RNNs. In [4], the attention model is
employed to softly assign importance to input words in a
sentence when predicting a target word for machine translation. In this paper, we leverage attention model to select
more relevant and useful dependencies for image units. To
this end, we do not use Eq. (5) and Eq. (6) to directly
model the relationships between hvi and its all predecessors. Instead, we employ the following expression to model
the dependency between hvi and one of its predecessors hvj
hvi ,vj = φ(U xvi + W hvj + b)

(8)

prediction

CNNs features
input

CNNs features

CNNs features

D-RNNs features prediction

prediction

Upsampling x 8

Upsampling x 2

DD-RNNs

Upsampling x 2

Conv5 + pooling

Conv4 + pooling

Conv3 + pooling

Conv2 + pooling

Conv1 + pooling

prediction

prediction
labeling

Figure 3. The architecture of the proposed full labeling system. The DD-RNNs are placed on the top of feature maps obtained from the
5th convolutional block to model long-range dependencies in image, and the deconvolution is used to upsample the predictions. Low-level
and high-level features are combined through skip strategy for final labeling (see the green arrows). Best viewed in color.

where hvj represents the hidden layer of one predecessor
vj ∈ PD (vi ) of vi . The hvi ,vj in Eq. (8) models dependency information from hvj for hvi . The final hidden unit
hvi at vi is obtained by summating all hvi ,vj with attention,
and mathematically computed with
X
h vi =
hvi ,vj wvi ,vj
(9)
vj ∈PD (vi )

where the attention weight wvi ,vj for hvj reflects the relevance of the predecessor vj to vi , calculated by
exp(z T hvi ,vj )
wvi ,vj = P
exp(z T hvi ,vk )

(10)

vk ∈PD (vi )

where z T represents a transformation matrix.
With the above attention model, we replace Equations
(5) and (6) with Equations (8) and (9) for a forward pass at
vi in DD-RNNs. With standard stochastic gradient descent
(SGD) method, the attentional DD-RNNs can be trained in
an end-to-end manner.

3.4. Full labeling system
Before showing the full labeling system, we first introduce the decomposition of D-UCG. As in [41], we decompose the D-UCG U into a set of D-DAGs represented with
{Dl }L
l=1 , where L is the number of D-DAGs. Since Eq. (9)
only computes the hidden layer at vertex vi in one of L DDAGs, the final output ŷvi at vi is derived by aggregating
the hidden layers at vi from all D-DAGs. The mathematical
formulation for this process is expressed as
hlvi ,vj =φ(U l xvi + W l hlvj + bl )
X
hlvi =
hlvi ,vj wvl i ,vj

(11)
(12)

vj ∈PDl (vi )

XL
ŷvi =σ(
V l hlvi + c)
l=1

(13)

Through the equations above, we can then utilize the proposed DD-RNNs to model abundant dependencies among
image units.
We develop an end-to-end scene labeling system by integrating our approach with popular CNNs for scene parsing
as shown in Fig. 3. The first five convolutional blocks, borrowed from the VGG network [42], are used to extract highlevel features for local regions. The proposed DD-RNNs are
placed on the top of feature maps obtained from the 5th convolutional block to model long-range dependencies in the
input image, and the deconvolution operations are used to
upsample the predictions. To produce the desired input size
of labeling result, we use the deconvolution [51] to upsample predictions. Taking into consideration both spatial and
semantic information for scene labeling, we adopt the skip
strategy [33] to combine low-level and high-level features.
The whole system is trained end-to-end with the pixel-wise
cross-entropy loss. Finally, we apply conditional random
field [22] to further polish the results.

4. Experimental Results
Implementation details. In our full labeling system, the
parameters for the five convolutional blocks are borrowed
from VGG network [42]. DD-RNNs are employed to model
dependencies among image units in the 5th pooling layer.
The network takes 512 × 512 images as inputs, and outputs
the labeling results with same resolution. When evaluating, the labeling results are resized to the size of original inputs. The dimension of input, hidden and output units for DRNNs is set to 512. The two non-linear activations φ and σ
are ReLU and softmax functions, respectively. The full networks are end-to-end trained with standard SGD method.
For convolutional blocks, the learning rate is initialized to
be 10−4 and decays exponentially with the rate of 0.9 after
10 epochs. For D-RNNs, the learning rate is initialized to
be 10−2 and decays exponentially with the rate of 0.9 after 10 epochs. The batch sizes for both training and testing
phases are set to 1. The results are reported after 50 training

epoches. The networks are implemented in Matlab using
MatConvNet [45] on a single Nvidia GeForce TITAN GPU
with 12GB memory.
Evaluation metrics. In this work, we use three types of
metrics, i.e., global pixel accuracy (GPA), average class accuracy (ACA) and mean intersection over union (IoU), to
evaluate the proposed method. For details of these metrics,
readers are referred to [33].
Baseline. To better analyze the proposed method, we develop a baseline by using plain DAG structured RNNs to
model dependencies. It is worth noticing that the baseline varies from [41] because we do not use class weighting strategy and larger conventional kernel in our labeling
system.

4.1. Results on PASCAL Context
The PASCAL Context [35] dataset consists of 10,103
images. Following the split in [35], 4,998 images are used
for training and the rest for testing. The images are collected from the PASCAL VOC 2010 dataset and re-labeled
into 540 classes for pixel-wise scene labeling. Similar to
other literatures, we in this paper only consider the most
frequent 59 classes in the benchmark for evaluation.
Table 1. Quantitative results and comparisons on PASCAL Context [35] (59 classes). For fair comparisons, we only present algorithms which utilize VGG network [42] for feature extraction.
Algorithm GPA (%) ACA (%) IoU (%)
O2P [7]
CFM [13]
CAMN [1]
PixelNet [5]
FCN-8s [33]
HO-CRF [2]
BoxSup [12]
ParseNet [31]
ConvPP-8 [47]
CNN-CRF [29]
CRF-RNN [52]
DeepLab [8]
DeepLab-CRF [8]
DAG-RNN [41]
DAG-RNN-CRF [41]

n/a
n/a
72.1
n/a
69.4
n/a
n/a
n/a
n/a
71.5
n/a
n/a
n/a
72.7
73.2

n/a
n/a
54.3
51.5
50.5
n/a
n/a
n/a
n/a
53.9
n/a
n/a
n/a
55.3
55.8

18.1
34.4
41.2
41.4
38.2
41.3
40.5
40.4
41.0
43.3
39.3
37.6
39.6
42.6
43.7

Baseline
DD-RNNs w/o CRF
DD-RNNs

71.1
74.8
75.1

53.5
57.6
57.7

41.3
44.9
45.3

The quantitative results and comparisons with state-ofthe-art methods are summarized in Table 1. Benefiting from
the power of CNNs, the FCN-8s [33] achieves promising
result with mean IoU of 38.2%. To alleviate the boundary
issue in FCN-8s, the CRF-RNN [52] and DeepLab-CRF [9]
propose to utilize probabilistic graphical model such as
CRF in CNNs, and obtain better performances with mean
IoUs of 39.3% and 39.6%, respectively. Other approaches

such as CAMN [1] and ParseNet [31] suggest to improve
performance by incorporating global context into CNNs
and obtain mean IoUs of 41.2% and 40.4%. Despite better performance, these methods still ignore the long-range
dependencies in images, which are of importance in inferring ambiguous pixels. The method in [41] employs RNNs
to capture contextual dependencies among image units for
scene labeling and demonstrates outstanding performance
with mean IoU of 42.6%. Moreover, they utilize CRF to improve the result to 43.7%. Different from this method, we
propose DD-RNNs to model richer dependencies in images.
Our baseline method shows the effectiveness of plain DAG
structured RNNs (i.e., no dense connections) for scene labeling with mean IoU of 41.3%. Without any class weighting strategy and post-operations, our DD-RNNs improves
the baseline method from 41.3% to 44.9%, which outperforms the method in [41] by 1.2%, showing the advantage
of DD-RNNs. Further, we apply CRF to polish the result
and achieve the mean IoU of 45.3%.
Fig. 4 demonstrates performance comparisons on each
individual class in PASCAL Context [35] between FCN8s [33] and our methods. From Fig. 4, we can see that using RNNs in our baseline method can improve the performance most categories including visually similar ones such
as ‘dog’ and ‘horse’ with long-range contextual dependencies in images. However, for other similar classes such as
‘ground’ and ‘sidewalk’, the baseline method does not show
significant improvements. For few classes such as ‘bed’ and
‘bench’, the FCN-8s [33] method even performs better than
the baseline. By replacing plain RNNs with dense RNNs,
our DD-RNNs achieves significant gains on performance
for visually similar classes such as ‘mountain’ and ‘rock’
using richer dependency information.
Fig. 5 displays qualitative labeling results on PASCAL
Context [35]. Without considering long-range contextual
dependencies in images, FCN-8s [33] is prone to cause misclassifications (see the third column in Fig. 5). Our baseline approach are able to help alleviate this situation using
RNNs to capture dependencies in images. For example, in
the first two rows in Fig. 5, the ‘water’ can be correctly recognized with the dependencies from ‘boat’. However, the
plain RNNs fail in more complex scenes (see the last three
rows in Fig. 5). For example, in the fourth row in Fig. 5,
most of ‘road’ pixels are mistakenly classified into ‘ground’
pixels because of not full use of dependencies from ‘bus’.
By contrast, the proposed DD-RNNs are capable of recognizing most of ‘road’ pixels by taking advantages of richer
dependencies from ‘bus’ in images. By employing CRF, the
labeling results are further polished (see the last column in
Fig. 5).

IoU of individual class on PASCAL Context

FCN-8s

Baseline

DD-RNNs w/o CRF

1
0.9

The value o IoU

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

0

Figure 4. Comparisons of IoU on PASCAL Context [35] for each individual class. Best viewed in color.

Input

Groundtruth
FCN-8s
Baseline
DD-RNNs w/o CRF
Figure 5. Qualitative labeling results on PASCAL Context [35]. Best viewed in color.

4.2. Results on MIT ADE20K
The recently published MIT ADE20K [53] benchmark
consists of 20,000 images in training set and 2,000 images
in validation set. There are total 150 semantics classes in
the dataset. MIT ADE20K [53] is considered to be one of
the most challenging scene parsing benchmarks because of
its scene varieties and numerous annotated object instances.
Table 2 summarizes the quantitative results and comparisons with other state-of-the-art algorithms. The FCN8s [33] method achieves the result with mean IoU of 29.4%.
To incorporate multi-scale context into CNNs, the work
of [50] proposes the dilated convolution and improves the
performance to mean IoU of 32.3%. To the same end, Hung

DD-RNNs

et al. [21] suggest to embed global context into CNNs to
obtain improvements. Based on FCN-8s [33] method, they
improve the performance to 32.9% with global spatial prior
and to 32.5% with global feature. Though the aforementioned methods take global context of image into consideration, they still ignore the important contextual dependencies in images. In this work, we employ DD-RNNs to
model this dependency information for scene labeling. In
our baseline experiment, the plain DAG structured RNNs
obtain the result with mean IoU of 32.1%, which outperforms FCN-8s [33] with mean IoU of 29.4% and SegNet [3]
with mean IoU of 21.6%. By using dense connections in
RNNs, the result is further significantly improved to mean
IoU of 35.7% without CRF, demonstrating the effectiveness

4.4. Ablation study on attention model

of dense RNNs.
Table 2. Quantitative results on MIT ADE20K [53].
Algorithm GPA (%) ACA (%) IoU (%)
SegNet [3]
FCN-8s [33]
DilatedNet [50]
FCN-8s+Prior [21]
FCN-8s+Feature [21]
Cascade-SegNet [53]
Cascade-Dilated [53]

71.0
71.3
73.6
75.0
74.5
71.8
74.5

31.1
40.3
44.6
n/a
n/a
37.9
45.4

21.6
29.4
32.3
32.9
32.5
27.5
34.9

Baseline
DD-RNNs w/o CRF
DD-RNNs

72.8
75.6
75.9

42.6
46.9
47.3

32.1
35.7
36.3

4.3. Results on SiftFlow
The SiftFlow [30] dataset comprises 2,668 images captured from 8 typical scenes and are annotated with 33
classes. Following the split in [30], 2,488 images are utilized for training and the rest for testing.
Table 3. Quantitative results on SiftFlow [30].
Algorithm GPA (%) ACA (%) IoU (%)
RCNN [26]
RCNN [26]
FCN-8s [33]
Liu et al [30]
ParseNet [31]
ClassRare [49]
Tighe et al [44]
CNN-CRF [29]
DilatedNet [50]
ConvPP-8s [47]
CNN-LSTM [6]
Farabet et al [15]
Sharma et al [40]
DAG-RNN [41]
DAG-RNN-CRF [41]

83.5
79.3
85.9
76.7
86.8
79.8
75.6
88.1
86.8
n/a
70.1
72.3
75.5
87.3
87.8

35.8
57.1
53.9
n/a
52.0
48.7
41.1
53.4
n/a
n/a
22.6
50.8
48.0
60.2
57.8

n/a
n/a
41.2
n/a
40.4
n/a
n/a
44.9
41.5
40.7
n/a
n/a
n/a
44.4
44.8

Baseline
DD-RNNs w/o CRF
DD-RNNs

85.8
88.7
89.3

56.9
60.7
61.1

43.1
45.9
46.3

Table 3 reports the quantitative results on SiftFlow [30].
The FCN-8s [33] approach obtains the result with mean IoU
of 41.2%. By incorporating context, the DilatedNet [50]
and CNN-CRF [29] methods improve the results of mean
IoU to 41.5% and 44.9%. The work of [41] applies DAG
structured RNNs to model dependencies in images for scene
labeling, and obtains the result with mean IoU of 44.4%.
Furthermore CRF is adopted to refine the results and mean
IoU is improved to 44.8%. Without class weighting strategy
or CRF, our DD-RNNs achieve the result with mean IoU of
45.9%, outperforming the approach in [41]. Moreover, the
result is further improved to 46.3% with CRF.

In this paper, we propose the DD-RNNs to model richer
dependencies in images, which significantly enhances discriminability for each image unit. However, different dependencies are not always equally helpful. For example, to
distinguish a ‘sand’ unit in a beach scene image, the most
useful contextual cues are ‘water’ units. Other regions such
as ‘sky’ should be paid less attention. To activate relevant
and restrain irrelevant dependencies, we introduce an attention model into DD-RNNs. In order to demonstrate the effectiveness of attention model, we conduct experiment by
removing attention model from DD-RNNs. Table 4 summarizes the experimental results on three benchmarks. From
Table 4, we can see that the attention model helps to further
improve performance.
Table 4. Analysis on the impact of attention model without CRF.
attention
GPA (%) ACA (%) IoU (%)
model
Pascal Context [35]

7
3

73.7
74.8

56.9
57.6

44.3
44.9

MIT ADE20K [53]

7
3

74.8
75.6

45.7
46.9

34.5
35.7

7
3

88.2
88.7

59.1
60.7

45.2
45.9

SiftFlow [30]

4.5. Study on model complexity
To further analyze our approach, we show the model size
and efficiency in Table 5. The proposed method is based on
the VGG network [42] except for the last fully connected
layers. In addition, to generate full input size prediction
maps, we utilize three deconvolutional layers to upsample
feature maps. Table 5 reports the model complexities (i.e.,
model size and efficiency of one forward pass) and comparisons with other state-of-the-art scene labeling algorithms.
Table 5. Analysis of model size and efficiency.
Model Size Inference
SegNet [3]
FCN-8s [33]
DilatedNet [50]

117 MB
497 MB
513 MB

60 ms
320 ms
360 ms

Ours

190 MB

280 ms

5. Conclusion
In this paper, we propose dense RNNs for scene labeling.
Different from existing methods exploring limited dependencies, our DAG structured dense RNNs (DD-RNNs) exploit abundant contextual dependencies through dense connections in image, which better improves the discriminative
power of image unit. In addition, considering that different
dependencies are not always equally helpful to recognize

each image unit, we propose an attention model, which is
capable of assigning more importance to relevant dependencies. Integrating with CNNs, we develop an end-toend scene labeling system. Extensive experiments on three
benchmarks including PASCAL Context, MIT ADE20K
and SiftFlow demonstrate that our DD-RNNs significantly
improve the baseline and outperform other state-of-the-art
algorithms, evidencing the effectiveness of dense RNNs.

References
[1] A. H. Abdulnabi, B. Shuai, S. Winkler, and G. Wang.
Episodic camn: Contextual attention-based memory networks with iterative feedback for scene labeling. In CVPR,
2017. 3, 6
[2] A. Arnab, S. Jayasumana, S. Zheng, and P. H. Torr. Higher
order conditional random fields in deep neural networks. In
ECCV, 2016. 6
[3] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. TPAMI, 39(12):2481–2495, 2017. 1, 2, 7, 8
[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In ICLR,
2015. 3, 4
[5] A. Bansal, X. Chen, B. Russell, A. G. Ramanan, et al. Pixelnet: Representation of the pixels, by the pixels, and for the
pixels. In CVPR, 2017. 6
[6] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki. Scene labeling with lstm recurrent neural networks. In CVPR, 2015.
1, 3, 8
[7] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Semantic segmentation with second-order pooling. In ECCV,
2012. 6
[8] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. In ICLR, 2015. 1,
2, 6
[9] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI, 2017. 6
[10] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille. Attention to scale: Scale-aware semantic image segmentation.
In CVPR, 2016. 3
[11] X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, and
X. Wang. Multi-context attention for human pose estimation. In CVPR, 2017. 3
[12] J. Dai, K. He, and J. Sun. Boxsup: Exploiting bounding
boxes to supervise convolutional networks for semantic segmentation. In ICCV, 2015. 6
[13] J. Dai, K. He, and J. Sun. Convolutional feature masking for
joint object and stuff segmentation. In CVPR, 2015. 6
[14] J. L. Elman. Finding structure in time. Cognitive science,
14(2):179–211, 1990. 1, 2, 3
[15] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning
hierarchical features for scene labeling. TPAMI, 35(8):1915–
1929, 2013. 1, 8

[16] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 1
[17] S. Gould, R. Fulton, and D. Koller. Decomposing a scene
into geometric and semantically consistent regions. In ICCV,
2009. 2
[18] A. Graves and J. Schmidhuber. Offline handwriting recognition with multidimensional recurrent neural networks. In
NIPS, 2009. 2
[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, 2016. 1, 2
[20] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten.
Densely connected convolutional networks. In CVPR, 2017.
2, 4
[21] W.-C. Hung, Y.-H. Tsai, X. Shen, Z. Lin, K. Sunkavalli,
X. Lu, and M.-H. Yang. Scene parsing with global context
embedding. ICCV, 2017. 7, 8
[22] P. Krähenbühl and V. Koltun. Efficient inference in fully
connected crfs with gaussian edge potentials. In NIPS, 2011.
5
[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classification with deep convolutional neural networks. In
NIPS, 2012. 1, 2
[24] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel. Backpropagation
applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989. 1
[25] Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin. Lstmcf: Unifying context modeling and fusion with lstms for rgbd scene labeling. In ECCV, 2016. 1, 3
[26] M. Liang, X. Hu, and B. Zhang. Convolutional neural networks with intra-layer recurrent connections for scene labeling. In NIPS, 2015. 8
[27] X. Liang, X. Shen, J. Feng, L. Lin, and S. Yan. Semantic
object parsing with graph lstm. In ECCV, 2016. 1, 3
[28] G. Lin, A. Milan, C. Shen, and I. Reid. Refinenet: Multipath refinement networks with identity mappings for highresolution semantic segmentation. In CVPR, 2017. 2
[29] G. Lin, C. Shen, A. van den Hengel, and I. Reid. Efficient
piecewise training of deep structured models for semantic
segmentation. In CVPR, 2016. 6, 8
[30] C. Liu, J. Yuen, and A. Torralba. Sift flow: Dense correspondence across scenes and its applications. TPAMI, 33(5):978–
994, 2011. 2, 8
[31] W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking
wider to see better. In ICLRW, 2016. 1, 2, 6, 8
[32] Z. Liu, X. Li, P. Luo, C.-C. Loy, and X. Tang. Semantic image segmentation via deep parsing network. In ICCV, 2015.
2
[33] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, 2015. 1, 2,
5, 6, 7, 8
[34] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical
question-image co-attention for visual question answering.
In NIPS, 2016. 3
[35] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun, and A. Yuille. The role of context for object

[36]
[37]
[38]
[39]

[40]

[41]

[42]

[43]

[44]
[45]
[46]

[47]

[48]

[49]

[50]
[51]

[52]

[53]

[54]

detection and semantic segmentation in the wild. In CVPR,
2014. 2, 6, 7, 8
H. Noh, S. Hong, and B. Han. Learning deconvolution network for semantic segmentation. In ICCV, 2015. 1, 2
A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel
recurrent neural networks. In ICML, 2016. 2
G.-J. Qi. Hierarchically gated deep networks for semantic
segmentation. In CVPR, 2016. 3
A. M. Rush, S. Chopra, and J. Weston. A neural attention
model for abstractive sentence summarization. In EMNLP,
2015. 3
A. Sharma, O. Tuzel, and M.-Y. Liu. Recursive context propagation network for semantic scene labeling. In NIPS, 2014.
8
B. Shuai, Z. Zuo, B. Wang, and G. Wang. Scene segmentation with dag-recurrent neural networks. TPAMI, 2017. 1, 3,
4, 5, 6, 8
K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
1, 2, 5, 6, 8
M. F. Stollenga, W. Byeon, M. Liwicki, and J. Schmidhuber. Parallel multi-dimensional lstm, with application to fast
biomedical volumetric image segmentation. In NIPS, 2015.
3
J. Tighe and S. Lazebnik. Finding things: Image parsing with
regions and per-exemplar detectors. In CVPR, 2013. 2, 8
A. Vedaldi and K. Lenc. Matconvnet: Convolutional neural
networks for matlab. In ACM MM, 2015. 6
F. Visin, M. Ciccone, A. Romero, K. Kastner, K. Cho,
Y. Bengio, M. Matteucci, and A. Courville. Reseg: A recurrent neural network-based model for semantic segmentation.
In CVPRW, 2016. 1, 3
S. Xie, X. Huang, and Z. Tu. Top-down learning for structured labeling with convolutional pseudoprior. In ECCV,
2016. 6, 8
K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural
image caption generation with visual attention. In ICML,
2015. 3
J. Yang, B. Price, S. Cohen, and M.-H. Yang. Context driven
scene parsing with attention to rare classes. In CVPR, 2014.
2, 8
F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR, 2016. 1, 2, 7, 8
M. D. Zeiler, G. W. Taylor, and R. Fergus. Adaptive deconvolutional networks for mid and high level feature learning.
In ICCV, 2011. 5
S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. H. Torr. Conditional random fields as recurrent neural networks. In ICCV, 2015. 2,
6
B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba. Scene parsing through ade20k dataset. In CVPR, 2017.
2, 6, 7, 8
Z. Zuo, B. Shuai, G. Wang, X. Liu, X. Wang, B. Wang, and
Y. Chen. Learning contextual dependence with convolutional
hierarchical recurrent neural networks. TIP, 25(7):2983–
2996, 2016. 1, 2

