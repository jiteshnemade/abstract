1

Cascaded Recurrent Neural Networks for
Hyperspectral Image Classification

arXiv:1902.10858v1 [cs.CV] 28 Feb 2019

Renlong Hang, Member, IEEE, Qingshan Liu, Senior Member, IEEE,
Danfeng Hong, and Pedram Ghamisi, Senior Member, IEEE

Abstract—This paper has been accepted by IEEE Transactions
on Geoscience and Remote Sensing. By considering the spectral
signature as a sequence, recurrent neural networks (RNNs) have
been successfully used to learn discriminative features from
hyperspectral images (HSIs) recently. However, most of these
models only input the whole spectral bands into RNNs directly,
which may not fully explore the specific properties of HSIs. In this
paper, we propose a cascaded RNN model using gated recurrent
units (GRUs) to explore the redundant and complementary
information of HSIs. It mainly consists of two RNN layers.
The first RNN layer is used to eliminate redundant information
between adjacent spectral bands, while the second RNN layer
aims to learn the complementary information from non-adjacent
spectral bands. To improve the discriminative ability of the
learned features, we design two strategies for the proposed model.
Besides, considering the rich spatial information contained in
HSIs, we further extend the proposed model to its spectral-spatial
counterpart by incorporating some convolutional layers. To test
the effectiveness of our proposed models, we conduct experiments
on two widely used HSIs. The experimental results show that our
proposed models can achieve better results than the compared
models.
Index Terms—Recurrent neural network (RNN), gated recurrent unit (GRU), spectral feature, spectral-spatial feature,
hyperspectral image (HSI) classification.

I. INTRODUCTION
With the development of imaging technology, current hyperspectral sensors can fully portray the surface of the Earth
using hundreds of continuous and narrow spectral bands,
ranging from the visible spectrum to the short-wave infrared
spectrum. The generated hyperspectral image (HSI) is often
considered as a three-dimensional cube. The first two are
spatial dimensions, which record the locations of each object.
The third one is spectral dimension, which captures the
spectral signature (reflective or emissive properties) of each
material in different bands along the electromagnetic spectrum
[1]. Using such rich information, HSIs have been widely
This work was supported in part by the Natural Science Foundation
of China under Grants 61532009 and 61825601, in part by the Natural
Science Foundation of Jiangsu Province, China, under Grant BK20180786.
(Corresponding author: Qingshan Liu.)
R. Hang and Q. Liu are with the Jiangsu Key Laboratory of Big Data Analysis Technology, the School of Automation, Nanjing University of Information
Science and Technology, Nanjing 210044, China (renlong hang@163.com,
qsliu@nuist.edu.cn).
D. Hong is with the Remote Sensing Technology Institute (IMF), German
Aerospace Center (DLR), 82234 Wessling, Germany, and Signal Processing
in Earth Observation (SiPEO), Technical University of Munich (TUM), 80333
Munich, Germany(e-mail: danfeng.hong@dlr.de).
P. Ghamisi is with the Helmholtz-Zentrum Dresden-Rossendorf (HZDR),
Helmholtz Institute Freiberg for Resource Technology (HIF), Exploration, D09599 Freiberg, Germany (e-mail: p.ghamisi@gmail.com).

applied to various applications, such as land cover/land use
classification, precision agriculture, and change detection. For
these applications, one basic but important procedure is HSI
classification, whose goal is to assign candidate class labels to
each pixel.
In order to acquire accurate classification results, numerous
methods have been proposed. For example, one can directly
consider the rich spectral signature as features and feed them
into advanced classifiers, such as support vector machine
(SVM) [2], random forest [3] and extreme learning machine
[4]. However, due to the dense spectral sampling of HSIs,
there may exist some redundant information among adjacent
spectral bands. This easily leads to the so-called curse of
dimensionality (the Hughes effect) which causes a sudden
drop in classification accuracy when there is no balance
between the high number of spectral channels and a limited
number of training samples. Therefore, a large number of
works were proposed to mine discriminative features from
the high-dimensional spectral signature [5]. Popular models
include principle component analysis (PCA), linear discriminant analysis (LDA) [6]–[8], and graph embedding [9]–[11].
Besides, representation-based models have also been employed
to HSI classification in recent years. In [12] and [13], sparse
representation was proposed to learn discriminative features
from HSIs. Similarly, collaborative representation was also
widely explored [14], [15]. In these models, an input spectral
signature is usually represented by a linear combination of
atoms from a dictionary, and the classification result can be
derived from the reconstructed residual without needing to
train extra classifiers, which often costs much time.
Although the aforementioned models have demonstrated
their effectiveness in the field of HSI classification, there
still exist some drawbacks to address. For the traditional
feature extraction models, we need to pre-define a mining
criterion (e.g., maximizing the between-class scatter matrix in
LDA), which heavily depends on the domain knowledge and
experience of experts. For the representation-based models,
their goal is to reconstruct the input signal, leading to suboptimal representation for classification. Additionally, all of
them can be considered as shallow-layer models, which limit
their potentials to learn high-level semantic features. Recently,
deep learning [16], [17], a very hot research topic in machine
learning, has shown its huge superiority in most fields of
computer vision [18]–[21] and natural language processing
[22], [23]. The goal of deep learning is to learn nonlinear, highlevel semantic features from data in a hierarchical manner.
Due to the effects of multi-path scattering and the het-

2

erogeneity of sub-pixel constituents, HSI often lies in a
nonlinear and complex feature space. Deep learning can be
naturally adopted to deal with this issue [24], [25]. In the
past few years, many deep learning models were successfully
applied to HSI classification. For example, in [26]–[28], the
autoencoder model has been used to learn deep features
from high-dimensional spectral signature directly. Similar to
autoencoder, deep belief network was also explored to extract
spectral features [29]–[31]. However, both of them belong
to fully-connected networks, which contain large numbers
of parameters to train. Different from them, convolutional
neural networks (CNNs) have local connection and weight
sharing properties, thus largely reducing the number of training
parameters [32]–[34]. In [35], Hu et al. proposed to use onedimensional CNN to learn and represent the spectral information. This model is comprised of an input layer, a convolutional
layer, a pooling layer, a fully-connected layer and an output
layer. The whole model is trained in an end-to-end manner,
thus achieving satisfying results for HSI classification.
Besides spectral information, HSIs also have rich spatial
information. How to combine them together has been an active
research topic in the field of HSI classification [36], [37].
One potential method is to extend the spectral classification
model into its spectral-spatial counterpart. For instance, in
[38]–[40], a three-dimensional CNN was employed to spectralspatial classification of HSIs. However, due to the simultaneous convolution operators in both spectral domain and
spatial domain, the computational complexity is dramatically
increased. In addition, the number of trainable parameters in
three-dimensional CNNs is also a problem. In order to perform
three-dimensional convolution, the dimensionality of the input
and the dimensionality of the kernel (filter) should be equal.
This heavily increases the number of parameters. Another
candidate method for spectral-spatial classification is the one
based on two-branch networks. One branch is for spectral
classification and the other one for spatial classification. In
[41]–[43], one-dimensional CNN or autoencoder was used
to learn spectral features and two-dimensional CNN was
designed to learn spatial features. These two features are
then integrated together via feature-level fusion or decisionlevel fusion. For two-dimensional CNNs, only a few principal
components were extracted and used as inputs, thus reducing
the computational consuming compared to three-dimensional
CNNs.
Most of existing models can be considered as vectorbased methodologies. Recently, a few works attempted to
regard HSIs as sequential data, so recurrent neural networks
(RNNs) were naturally used to learn features. In [44], Wu et.al
proposed using RNN to extract spectral features from HSIs. In
[45] and [46], a variant of RNN using long short-term memory
(LSTM) units was designed to learn spectral-spatial features
from HSIs. In [47], another variant of RNN using gated
recurrent units (GRUs) was employed. Compared to the widely
explored CNN models, RNNs have many superiorities. For
example, the key component of CNNs is the convolutional operator. Due to the kernel size limitations of it, one-dimensional
CNNs can only learn the local spectral dependency while
easily ignoring the effects of non-adjacent spectral bands.

Different from them, RNNs, especially using GRU or LSTM,
often input spectral bands one by one via recurrent operators,
thus capturing the relationship from the whole spectral bands.
Besides, RNNs often have smaller numbers of parameters to
train than CNNs, so they will be more efficient in the training
and inferring phases.
Benefiting from its powerful learning ability from sequential
data, current RNN-related models often simply input the whole
spectral bands to networks, which may not fully explore
the redundant and complementary properties of HSIs. The
redundant information between adjacent spectral bands will
increase the computational burden of RNNs without improving
the classification results. Sometimes such redundancy may
reduce the classification accuracy since it increases withinclass variances and decreases between-class variances in the
feature space. Besides, it may also increase the difficulties in
learning complementary information. To address these issues,
we propose a cascaded RNN model using gated recurrent
units (GRUs) in this paper. This model mainly consists of
two RNN layers. The first RNN layer focuses on reducing
the redundant information of adjacent spectral bands. These
reduced information are then fed into the second RNN layer
to learn their complementary features. Besides, in order to
improve the discriminative ability of the learned features,
we design two strategies for the proposed model. Finally,
we also extend the proposed model to its spectral-spatial
version by incorporating some convolutional layers. The major
contributions of this paper are summarized as follows.
1) We propose a cascaded RNN model with GRUs for HSI
classification. Compared to the existing RNN-related
models, our model can sufficiently consider the redundant and complementary information of HSIs via two
RNN layers. The first one is to reduce redundancy and
the second one is to learn complementarity. These two
layers are integrated together to generate an end-to-end
trainable model.
2) In order to learn more discriminative features, we design
two strategies to construct connections between the first
RNN layer and the output layer. The first strategy is
the weighted fusion of features from two layers, and
the second one is the weighted combination of different
loss functions from two layers. Their weights can be
adaptively learned from data itself.
3) To capture the spectral and spatial features simultaneously, we further extend the proposed model to its
spectral-spatial counterpart. A few convolutional layers
are integrated into the proposed model to learn spatial
features from each band, and these features are then
combined together via recurrent operators.
The rest of this paper is organized as follows. Section II
describes the details of the proposed models, including a brief
introduction of RNN, and the structure of the proposed model
as well as its modifications. The descriptions of data sets and
experimental results are given in Section III. Finally, Section
IV concludes this paper.

3

First‐layer RNN

Cascaded
RNN Model

RNN
RNN

Second‐layer RNN

RNN
RNN
RNN
Fig. 1: Flowchart of the proposed model.
II. M ETHODOLOGY
As shown in Fig. 1, the proposed cascaded RNN model
mainly consists of four steps. For a given pixel, we firstly
divide it into different spectral groups. Then, for each group,
we consider the spectral bands in it as a sequence, which is
fed into a RNN layer to learn features. After that, the learned
features from each group are again regraded as a sequence
and fed into another RNN layer to learn their complementary
information. Finally, the output of the second RNN layer is
connected to a softmax layer to derive the classification result.
A. Review of RNN

function, C is the number of classes to discriminate. All of
these weight parameters in Equation (1) and (2) can be trained
using the following loss function
L=−

N
1 X
[yi log(ỹi ) + (1 − yi )log(1 − ỹi )]
N i=1

(3)

where N is the number of training samples, yi and ỹi are
the true label and the predicted label of the i-th training
sample, respectively. This function can be optimized using a
backpropagation through time (BPTT) algorithm.
B. Cascaded RNNs

RNN has been widely used for sequential data analysis,
such as speech recognition and machine translation [23], [48].
Assume that we have a sequence data x = (x1 , x2 , · · · , xT ),
where xt , t ∈ {1, 2, · · · , T } generally represents the information at the t-th time step. When applying RNN to HSI
classification, xt will correspond to the spectral value at the
t-th band. For RNN, the output of hidden layer at time t is
ht = φ(Whi xt + Whh ht−1 + bh )

(1)

where φ is a nonlinear activation function such as logistic
sigmoid or hyperbolic tangent functions, bh is a bias vector,
ht−1 is the output of hidden layer at the previous time, Whi
and Whh denote weight matrices from the current input layer
to hidden layer and the previous hidden layer to current hidden
layer, respectively. From this equation, we can observe that via
a recurrent connection, the contextual relationships in the time
domain can be constructed. Ideally, hT can capture most of
the time information for the sequence data.
For classification tasks, hT is often fed into an output layer,
and the probability that the sequence belongs to i-th class can
be derived by using a softmax function. These processes can
be formulated as
OT = Woh hT + bo
eθi OT +bi
P (ỹ = i|θ, b) = PC
θj OT +bj
j=1 e

(2)

where bo is a bias vector, Woh is the weight matrix from
hidden layer to output layer, θ and b are parameters of softmax

HSIs can be described as a three-dimensional matrix X ∈
Rm×n×k , where m, n and k represent the width, height and
number of spectral bands, respectively. For a given pixel x ∈
Rk , we can consider it as a sequence whose length is k, so
RNN can be naturally employed to learn spectral features.
However, HSIs often contain hundreds of bands, making x
a very long sequence. Such long-term sequence increases the
training difficulty since the gradients tend to either vanish or
explode [49]. To address this issue, one popularly used method
is to design a more sophisticated activation function by using
gating units such as the LSTM unit and GRU [50]. Compared
to LSTM unit, GRU has a fewer number of parameters [49],
which may be more suitable for HSI classification because it
usually has a limited number of training samples. Therefore,
we select GRU as the basic unit of RNN in this paper.
The core components of GRU are two gating units that
control the flow of information inside the unit. Instead of using
Equation (1), the activation of the hidden layer for band t is
now formulated as
ht = (1 − ut )ht−1 + ut h̃t

(4)

where ut is the update gate, which can be derived by
ut = σ(wu xt + vu ht−1 )

(5)

where σ is a sigmoid function, wu is a weight value, and vu
is a weight vector. Similarly, h̃t can be computed by
h̃t = tanh(wxt + V(rt

ht−1 ))

(6)

4

RNN

RNN

RNN

RNN
RNN

RNN
RNN

RNN

RNN
C

L

Concatenation

Fig. 2: The first improvement strategy.

L

L

L
L

Loss function

Sum operator

Fig. 3: The second improvement strategy.

where denotes an element-wise multiplication, and rt is the
reset gate, which can be derived by
rt = σ(wr xt + Vr ht−1 )

L

C

RNN

Output layer

L

(7)

Due to the dense spectral sampling of hyperspectral sensors,
adjacent bands in HSIs have some redundancy while nonadjacent bands have some complementarity. In order to take
account of such information comprehensively, we propose a
cascaded RNN model. Specifically, we divide the spectral
sequence x into l sub-sequences z = (z1 , z2 , · · · , zl ), each
of which consists of adjacent spectral bands. Besides the
last sub-sequence zl , the length of the other sub-sequences
is d = floor(k/l), which denotes the nearest integers less
than or equal to k/l. Thus, for the i-th sub-sequence zi , i ∈
{1, 2, · · · , l}, it is comprised of the following bands

(x(i−1)×d+1 , · · · , xi×d ), if i 6= l,
zi =
(8)
(x(i−1)×d+1 , · · · , xk ),
otherwise.
Then, we feed all the sub-sequences into the first-layer
RNNs respectively. These RNNs have the same structure and
share parameters, thus reducing the number of parameters to
train. In the sub-sequence zi , each band has an output from
GRU. We use the output of the last band as the final feature
(1)
representation for zi , which can be denoted as Fi ∈ RH1 ,
where H1 is the size of the hidden layer in the first-layer RNN.
(1)
After that, we can combine Fi , i ∈ {1, 2, · · · , l} together to
(1)
(1)
(1)
generate another sequence F = (F1 , F2 , · · · , Fl ) whose
length is l. This sequence is fed into the second-layer RNN
to learn their complementary information. Similar to the firstlayer RNNs, we also use the output of GRU at the last time l
as the learned feature F(2) . To get a classification result of x,
we need to input F(2) into an output layer whose size equals
to the number of candidate classes C. Both of these two-layer
RNNs have many weight parameters. We choose Equation (3)
as a loss function and use the BPTT algorithm to optimize
them simultaneously.

better than the first-layer RNNs. However, the performance
of the first-layer RNNs will have effects on the second-layer
RNN. In order to improve the discriminative ability of F(2) , an
intuitive method is to construct relations between the first-layer
RNNs and the output layer. Here, we propose two strategies
to achieve this goal. The first strategy is based on the featurelevel connection shown in Fig. 2. Instead of feeding the output
of the second-layer RNN into the output layer only, we attempt
to feed all the output features from the first- and the secondlayer RNNs in a weighted concatenation manner. Specifically,
the input of the output layer is computed as follows
(1)

As described in subsection II-B, the second-layer RNN is
directly connected to the output layer, so it may be optimized

(1)

(1)

(1)

(1)

(9)

(1)

where wi ∈ R1 , i ∈ {1, 2, · · · , l} are fusion weights for
the first-layer RNNs, and w(2) ∈ R1 is the fusion weight for
the second-layer RNN. These weights can be integrated into
the whole network and their optimal values are automatically
learned from data. The same as the original two-layer RNN
model, we also use Equation (3) to construct the loss function
and use the BPTT algorithm to optimize it.
Different from the first improvement strategy, our second
strategy is based on the output-level connection. As shown in
Fig. 3, we feed the features extracted by the first-layer RNNs
into output layers, respectively, so that they can learn more discriminative features. Combining these features together using
the second-layer RNN will result in a better F(2) . In particular,
(1)
for Fi , i ∈ {1, 2, · · · , l}, we can input it into an output
(1)
layer and construct a loss function Li , i ∈ {1, 2, · · · , l}.
(2)
Meanwhile, we also input F
into an output layer and
construct another loss function L(2) . After that, a weighted
summation method can be used to combine them together,
which can be formulated as
l

1 X (1) (1)
w L + w(2) L(2)
L̃ =
l i=1 i i
(1)

C. Improvement for Cascaded RNNs

(1)

F̃ = [w1 F1 , w2 F2 , · · · , wl Fl , w(2) F(2) ]

(10)
(1)

where wi ∈ R1 and w(2) ∈ R1 are fusion weights, Li and
L(2) are derived from Equation (3). The final loss function
L̃ can be optimized by using the BPTT algorithm. In the
prediction phase, we can delete the output layers of the first-

5

Spectral‐spatial
Cascaded RNNs

Convolutional layers
First‐layer RNN

RNN
Second‐layer RNN

RNN
RNN

Fig. 4: Flowchart of spectral-spatial cascaded RNN model.
layer RNNs and use the output from the second-layer RNN
as the final classification result.
D. Spectral-spatial Cascaded RNNs
Due to the effects of atmosphere, instrument noises, and
natural spectrum variations, materials from the same class
may have very different spectral responses, while those from
different classes may have similar spectral responses. If we
only use the spectral information, the resulting classification
maps will have many outliers, which is known as the “salt
and pepper” phenomenon. As a three-dimensional cube, HSIs
also have rich spatial information, which can be used as a
complement to address this issue. Among numerous deep
learning models, CNNs have demonstrated their superiority in
spatial feature extraction. In [38], a typical two-dimensional
CNN is designed to extract spatial features from HSIs. The
input of this model is the first principle component of HSIs.
Inspired from the two-dimensional CNN model, we extend
the cascaded RNN model to its spectral-spatial version by
adding some convolutional layers. Fig. 4 shows the flowchart
of the proposed spectral-spatial cascaded RNN model. For a
given pixel x ∈ Rk , we select a small cube x̂ ∈ Rω×ω×k
centered at it. Then, we split this cube into k matrices
x̂i ∈ Rω×ω , i ∈ {1, 2, · · · , k} across the spectral domain.
For each x̂i , we feed it into several convolutional layers to
learn spatial features. The same as [38], we also use three
convolutional layers, and the first two layers are followed by
pooling layers. The input size ω × ω is 27 × 27. The sizes
of the three convolutional filters are 4 × 4 × 32, 5 × 5 × 64
and 4 × 4 × 128, respectively. After these convolutional
operators, each x̂i will generate a 128-dimensional spatial
feature si . Similar to the cascaded RNN model, we can also
consider s = (s1 , s2 , · · · , sk ) as a sequence whose length is
k. This sequence is divided into l sub-sequences, and they
are subsequently fed into the first-layer RNNs respectively
to reduce redundancy inside each sub-sequence. The outputs
from the first-layer RNNs are combined again to generate
another sequence, which are fed into the second-layer RNN
to learn complementary information.
Compared to the cascaded RNN model, the spectral-spatial
cascaded RNN model is deeper and more difficult to train.

Therefore, we propose a transfer learning method to train it.
Specifically, we firstly pre-train the convolutional layers using
all of x̂i , i ∈ {1, 2, · · · , k}. We replace two-layer RNNs by an
output layer whose size is the number of classes C. Besides,
we assume that the label of x̂i equals to the label of its
corresponding pixel x. Then, we will have N × k samples.
These samples are used to train convolutional layers. After
that, the weights of these convolutional layers are fixed and
the N training samples are used again to train the two-layer
RNNs. Finally, the whole network is fine-tuned based on the
learned parameters.
III. E XPERIMENTS
A. Data Description
Our experiments are conducted on two HSIs, which are
widely used to evaluate classification algorithms.
Indian Pines Data: The first data set was acquired by the
AVIRIS sensor over the Indian Pine test site in northwestern
Indiana, USA, on June 12, 1992. The original data set contains
224 spectral bands. We utilize 200 of them after removing
four bands containing zero values and 20 noisy bands affected
by water absorption. The spatial size of the image is 145 ×
145 pixels, and the spatial resolution is 20 m. The number of
training and test pixels are reported in Table I. Fig. 5 shows
the false-color image, as well as training and test maps of this
data set.
Pavia University Scene Data: The second data set was
acquired by the ROSIS sensor during a flight campaign over
Pavia, northern Italy, on July 8, 2002. The original image was

(a)

(b)

(c)

Fig. 5: Visualization of the Indian Pines data. (a) False-color
image. (b) Training data map. (c) Test data map.

6

TABLE I: Numbers of training and test pixels used in the
Indian Pines data set.
Class No.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
-

Class Name
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Hay-windrowed
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Building-grass-trees
Stone-steel-towers
Alfalfa
Grass-pasture-mowed
Oats
Total

Training
50
50
50
50
50
50
50
50
50
50
50
50
50
15
15
15
695

Test
1384
784
184
447
697
439
918
2418
564
162
1244
330
45
39
11
5
9671

TABLE II: Numbers of training and test pixels used in the
Pavia University data set.
Class No.
1
2
3
4
5
6
7
8
9
-

Class Name
Asphalt
Meadows
Gravel
Trees
Metal sheets
Bare Soil
Bitumen
Bricks
Shadows
Total

Training
548
540
392
524
265
532
375
514
231
3921

Test
6631
18649
2099
3064
1345
5029
1330
3682
947
42776

recorded with 115 spectral channels ranging from 0.43 µm to
0.86 µm. After removing noisy bands, 103 bands are used.
The image size is 610 × 340 pixels with a spatial resolution of
1.3 m. There are nine classes of land covers with more than
1000 labeled pixels for each class. The number of pixels for
training and test are listed in Table II. Their corresponding
distribution maps are demonstrated in Fig. 6.
B. Experimental Setup
In order to highlight the effectiveness of our proposed
models, we compare them with SVM, one-dimensional CNN
(1D-CNN), two-dimensional CNN (2D-CNN), and the original RNN using GRU (RNN). For simplicity, the cascaded
RNN model using GRUs is abbreviated as CasRNN; the
two improvement methods of CasRNN based on feature-level
and output-level connections are abbreviated as CasRNN-F
and CasRNN-O, respectively; the spectral-spatial CasRNN is
abbreviated as SSCasRNN. Some of their explanations are
summarized as follows.
1) SVM: The input of SVM is the original spectrum
signature. We choose Gaussian kernel as its kernel
function. The penalty parameter and the spread of
the Gaussian kernel are selected from a candidate set

(a)

(b)

(c)

Fig. 6: Visualization of the Pavia University data. (a) Falsecolor image. (b) Training data map. (c) Test data map.
{10−3 , 10−2 , · · · , 103 } using a fivefold cross-validation
method.
2) 1D-CNN: The structure of 1D-CNN is the same as that
in [35]. It contains an input layer, a convolutional layer
with 20 kernels whose size is 11 × 1, a max-pooling
layer whose kernel size is 3 × 1, a fully-connected layer
with 100 hidden nodes, and an output layer.
3) 2D-CNN: The structure of 2D-CNN is the same as that
in [38], which consists of three convolutional layers and
two max-pooling layers. Please refer to Table IX in [38]
for the design details of it.
4) RNN: GRU is used as the basic unit of RNN. The
number of hidden nodes is chosen from a candidate set
{24 , 25 , · · · , 210 } via a fivefold cross-validation method.
The deep learning models are constructed with a PyTorch
framework. To optimize them, we use a mini-batch stochastic
gradient descent algorithm. The batch size, the learning rate
and the number of training epochs are set to 64, 0.001 and
300, respectively. For SVM, we use a libsvm package in a
MATLAB framework. All of the experiments are implemented
on a personal computer with an Intel core i7-4790, 3.60GHz
processor, 32GB RAM, and a GTX TITAN X graphic card.
The classification performance of each model is evaluated
by the overall accuracy (OA), the average accuracy (AA), the
per-class accuracy, and the Kappa coefficient. OA defines the
ratio between the number of correctly classified pixels to the
total number of pixels in the test set, AA refers to the average
of accuracies in all classes, and Kappa is the percentage of
agreement corrected by the number of agreements that would
be expected purely by chance.
C. Parameter Analysis
There exist three important hyperparameters in the proposed
models. They are sub-sequence numbers l, as well as the
size of hidden layers in the first-layer RNN and the secondlayer RNN. To test the effects of them on the classification
performance, we firstly fix l and select the size of hidden
layers from a candidate set {16, 32, 64, 128, 256, 384}. Then,
we fix the size of hidden layers and choose l from another
set {2, 4, 6, · · · , 16, 18, 20}. Since the same hyperparameter

7

80

75

75

65

OA (%)

OA (%)

70

60

70

55
65
400

50
400
300

300

400
300

200

400
200

100

100
0

300

200

200

100

100
0

0

0

Fig. 7: Performance of the CasRNN model with different sizes
of hidden layers on the Indian Pines data (Left) and the Pavia
University data (Right).

94

95
90

90

OA (%)

OA (%)

92

88

85
80

86
84
400

75
400
300

400
300

200

200

100

100
0

300

400
300

200

200

100

100
0

0

0

Fig. 8: Performance of the SSCasRNN model with different
sizes of hidden layers on the Indian Pines data (Left) and the
Pavia University data (Right).
95
90

OA (%)

85

CasRNN
CasRNN-F
CasRNN-O
SSCasRNN

80
75

D. Performance Comparison

70

In this section, we will report quantitative and qualitative
results of our proposed models and their comparisons with
the other state-of-the-art models. Table III reports the detailed
classification results of different models on the Indian Pines
data, including OA, AA, Kappa and class specific accuracy.
The bold fonts in each row denote the best results. Several conclusions can be observed from this table. First, if we directly
input the whole spectral bands into RNN, its OA, AA and
Kappa values are 69.82%, 75.42% and 65.87%, respectively,
which are all lower than those achieved by SVM and 1DCNN models. This indicates that RNN cannot fully explore
the long-term spectral sequence of HSIs. On the contrary,
considering the redundant and complementary properties of
spectral signature, our proposed model CasRNN can improve
the performance of RNN by 4 percents, thus outperforming
SVM and 1D-CNN. Second, compared to CasRNN, CasRNNF and CasRNN-O can obtain better results, which validates the
effectiveness of the two improvement strategies. In terms of
each class accuracy, CasRNN-F almost increases all of them
in comparison with CasRNN, so it might be more powerful
than CasRNN-O on the Indian Pines data. Third, compared to
spectral classification models, 2D-CNN significantly improves
the classification results by about 10 percents. It means that

65

5

10
15
Number of subsequences

20

Fig. 9: Performance of different models on the Indian Pines
data with different sub-sequence numbers l.
95
90

OA (%)

values are used for CasRNN and its two improvements (i.e.,
CasRNN-F and CasRNN-O), we only demonstrate the performance of CasRNN here, shown in Fig. 7. In this threedimensional diagram, the first two axes (named Hidden1 and
Hidden2) respectively correspond to the number of hidden
nodes in the first-layer RNN and the second-layer RNN,
while the third axis represents the classification accuracy OA.
From this figure, we can observe that when Hidden1 ≥ 32
and Hidden2 ≥ 128, CasRNN can achieve better OA than
the other values on the Indian Pines data. The best OA
appears when Hidden1 = 128 and Hidden2 = 256. For
the Pavia University data, OA changes a little larger than
the Indian Pines data, but we can still find the best value
when Hidden1 = 256 and Hidden2 = 16. Similarly, Fig. 8
shows OA values achieved by SSCasRNN using different
hidden sizes. We can see the optimal parameter values are
Hidden1 = 128, Hidden2 = 256 for the Indian Pines data,
and Hidden1 = 256, Hidden2 = 256 for the Pavia University
data, respectively.
Fig. 9 and Fig. 10 evaluate the effects of l on classifying the
Indian Pines and the Pavia University data sets, respectively.
In these figures, different colors represent different models.
They are CasRNN, CasRNN-F, CasRNN-O and SSCasRNN.
As l increases, OAs achieved by these models tend to increase
firstly and then decrease. Given the same l, SSCasRNN significantly outperforms the other three models. For the Indian
Pines data, the maximal OAs of four models appear at the
same l, so their optimal l values are set as 10. Different from
the Indian Pines data, four models have different optimal l
values on the Pavia University data. As shown in Fig. 10, the
optimal l value is 4 for SSCasRNN, and 8 for the other three
models.

CasRNN
CasRNN-F
CasRNN-O
SSCasRNN

85
80
75
70

5

10
15
Number of subsequences

20

Fig. 10: Performance of different models on the Pavia University data with different sub-sequence numbers l.

8

TABLE III: Classification results (%) of different models on the Indian Pines data.
Class No.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
OA
AA
Kappa

C૚

SVM
64.31
70.92
84.78
91.05
85.94
93.62
69.17
52.90
76.60
97.53
77.49
73.33
100
87.18
90.91
100
70.55
82.23
66.90

1D-CNN
61.34
60.33
80.43
89.04
90.53
96.13
72.11
54.47
75.71
99.83
80.87
78.48
91.11
94.87
90.91
100
70.79
82.23
67.07

RNN
64.74
61.35
74.46
83.45
77.04
87.70
76.03
60.79
61.17
93.21
81.67
55.45
86.67
69.23
90.91
80.00
69.82
75.24
65.87

CasRNN
68.35
64.8
77.17
91.50
79.34
92.03
74.84
67.41
65.60
95.06
83.28
54.85
93.33
76.92
90.91
100
73.49
79.71
69.91

CasRNN-F
68.93
67.6
83.7
90.60
80.49
92.94
78.54
67.49
67.02
96.91
90.03
67.88
95.56
84.61
90.91
100
75.85
82.70
72.57

CasRNN-O
68.21
67.35
85.87
89.93
80.92
92.94
79.30
66.91
65.43
98.15
86.09
54.55
93.33
76.92
90.91
80
74.60
79.80
71.19

2D-CNN
82.51
88.14
100
94.85
85.80
99.77
82.35
73.86
86.00
100
94.53
97.27
100
97.44
100
100
85.43
92.66
83.49

SSCasRNN
86.99
98.72
100
94.41
97.42
100
87.15
85.98
87.23
100
97.51
99.70
100
100
100
100
91.79
95.94
90.62

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

C૛

C૜

C૝

C૞

C૟

Cૠ

Cૡ

Cૢ

C૚૙ C૚૚

C૚૛ C૚૜

C૚૝

C૚૞

C૚૟

Fig. 11: Classification maps of the Indian Pines data using different models. (a) SVM. (b) 1D-CNN. (c) RNN. (d) CasRNN.
(e) CasRNN-F. (f) CasRNN-O. (g) 2D-CNN. (h) SSCasRNN.

9

TABLE IV: Classification results (%) of different models on the Pavia University data.
Class No.
1
2
3
4
5
6
7
8
9
OA
AA
Kappa

SVM
84.74
64.50
72.56
97.13
99.55
93.30
91.28
91.99
95.56
78.75
87.85
73.62

1D-CNN
80.94
70.37
77.32
85.93
99.70
93.26
95.41
84.47
92.08
79.55
86.61
74.28

RNN
81.51
62.58
64.65
98.89
99.26
88.90
92.63
91.04
95.35
76.58
86.09
71.02

CasRNN
82.34
67.13
60.51
98.63
99.41
84.97
90.60
92.23
94.40
78.03
85.58
72.55

CasRNN-F
83.56
70.65
68.75
98.11
99.55
88.29
76.54
86.04
95.35
79.56
85.21
74.31

CasRNN-O
83.52
71.37
64.51
98.43
99.33
89.08
91.13
93.54
94.72
80.86
87.29
75.93

2D-CNN
77.39
98.89
56.74
92.75
99.78
47.27
80.08
96.69
96.30
86.18
82.88
81.22

SSCasRNN
89.82
96.06
78.89
95.89
100
57.67
80.53
96.80
95.99
90.30
87.97
86.26

C૚
C૛
C૜
C૝

(a)

(b)

(c)

(d)

C૞
C૟
Cૠ
Cૡ
Cૢ

(e)

(f)

(g)

(h)

Fig. 12: Classification maps of the Pavia University data using different models. (a) SVM. (b) 1D-CNN. (c) RNN. (d) CasRNN.
(e) CasRNN-F. (f) CasRNN-O. (g) 2D-CNN. (h) SSCasRNN.
the consideration of spatial information is very important
on the Indian Pines data, because there are many large and
homogeneous objects shown in Fig. 5(c). By incorporating the
spatial information into CasRNN model, our proposed model
SSCasRNN can further increase the performance to above
90 percents. Besides, it can obtain highest accuracies in 15
different classes, which sufficiently certifies the effectiveness
of SSCasRNN.
In addition to the quantitative results, we also visualize
classification results of different models shown in Fig. 11.
Different colors in this figure correspond to different classes.

Compared to the groundtruth map in Fig. 5(c), spectral
classification models (i.e., SVM, 1D-CNN, RNN, CasRNN,
CasRNN-F and CasRNN-O) have many outliers in the classification map due to the spectral variability of materials.
This phenomenon can be alleviated by 2D-CNN, because it
makes use of the spatial contextual information instead of
the spectral information. For homogeneous regions, especially
large objects, 2D-CNN performs very well. However, it will
easily result in an over-smoothing problem especially for small
objects, as demonstrated in Fig. 11(g). Different from 2D-CNN
and spectral models, SSCasRNN takes advantage of spectral

10

and spatial information simultaneously. As shown in Fig. 11
(h), it has significantly fewer outliers than spectral models,
and retains more boundary details of objects than 2D-CNN.
Table IV and Fig. 12 are the classification results of different
models on the Pavia University data. Similar conclusions can
be observed from them. For spectral models, CasRNN is better
than RNN, while CasRNN-F and CasRNN-O are superior to
CasRNN. All of these models have the “salt and pepper”
phenomenon in their classification maps. Compared to the best
spectral model, 2D-CNN can improve OA and Kappa by more
than 5 percents. In addition, it generates fewer outliers and
leads to a more homogeneous classification map. Nevertheless,
without using the spectral information, its performance is not
very high, and the classification map is easily to be oversmoothed. Combining the spectral and spatial information
together, our proposed model SSCasRNN can alleviate these
issues. It improves OA from 86.18% to 90.30%, and generates
more details in the classification map. However, in comparison
with the Indian Pines data, the classification results achieved
by SSCasRNN are still not very high. One possible reason is
that there exist many small objects in the Pavia University data,
which increases the difficulty in exploring spatial features.
IV. C ONCLUSIONS
In this paper, we proposed a cascaded RNN model for
HSI classification. Compared to the original RNN model, our
proposed model can fully explore the redundant and complementary information of the high-dimensional spectral signature. Based on it, we designed two improvement strategies
by constructing connections between the first-layer RNN and
the output layer, thus generating more discriminative spectral
features. Additionally, considering the importance of spatial
information, we further extended the proposed model into its
spectral-spatial version to learn spectral and spatial features
simultaneously. To test the effectiveness of the proposed models, we compared them with several state-of-the-art models on
two widely used HSIs. The experimental results demonstrate
that the cascaded RNN model can obtain higher performance
than RNN, and its modifications can further improve the
performance. Besides, we also thoroughly evaluated the effects
of different hyperparameters on the classification performance
of the proposed models, including the hidden sizes and the
number of sub-sequences. In the future, more experiments will
be conducted to validate the effectiveness of our proposed
models. In addition, more powerful spectral-spatial models
will be explored. Since the sizes and shapes of different objects
vary, using the patches or cubes with same sizes as inputs
easily leads to the loss of spatial information.
REFERENCES
[1] Pedram Ghamisi, Naoto Yokoya, Jun Li, Wenzhi Liao, Sicong Liu, Javier
Plaza, Behnood Rasti, and Antonio Plaza, “Advances in hyperspectral
image and signal processing: A comprehensive overview of the state of
the art,” IEEE Geoscience and Remote Sensing Magazine, vol. 5, no. 4,
pp. 37–78, 2017.
[2] Giorgos Mountrakis, Jungho Im, and Caesar Ogole, “Support vector machines in remote sensing: A review,” ISPRS Journal of Photogrammetry
and Remote Sensing, vol. 66, no. 3, pp. 247–259, 2011.

[3] Mariana Belgiu and Lucian Drăguţ, “Random forest in remote sensing:
A review of applications and future directions,” ISPRS Journal of
Photogrammetry and Remote Sensing, vol. 114, pp. 24–31, 2016.
[4] Wei Li, Chen Chen, Hongjun Su, and Qian Du, “Local binary patterns
and extreme learning machine for hyperspectral imagery classification.,”
IEEE Trans. Geoscience and Remote Sensing, vol. 53, no. 7, pp. 3681–
3693, 2015.
[5] Xiuping Jia, Bor-Chen Kuo, and Melba M Crawford, “Feature mining
for hyperspectral image classification,” Proceedings of the IEEE, vol.
101, no. 3, pp. 676–697, 2013.
[6] Wenzhi Liao, Aleksandra Pizurica, Paul Scheunders, Wilfried Philips,
and Youguo Pi, “Semisupervised local discriminant analysis for feature
extraction in hyperspectral images,” IEEE Transactions on Geoscience
and Remote Sensing, vol. 51, no. 1, pp. 184–198, 2013.
[7] Renlong Hang, Qingshan Liu, Huihui Song, and Yubao Sun, “Matrixbased discriminant subspace ensemble for hyperspectral image spatial–
spectral feature fusion,” IEEE Transactions on Geoscience and Remote
Sensing, vol. 54, no. 2, pp. 783–794, 2016.
[8] Renlong Hang, Qingshan Liu, Yubao Sun, Xiaotong Yuan, Hucheng
Pei, Javier Plaza, and Antonio Plaza, “Robust matrix discriminative
analysis for feature extraction from hyperspectral images,” IEEE Journal
of Selected Topics in Applied Earth Observations and Remote Sensing,
vol. 10, no. 5, pp. 2002–2011, 2017.
[9] Dalton Lunga, Saurabh Prasad, Melba M Crawford, and Okan Ersoy,
“Manifold-learning-based feature extraction for classification of hyperspectral data: A review of advances in manifold learning,” IEEE Signal
Processing Magazine, vol. 31, no. 1, pp. 55–66, 2014.
[10] Renlong Hang and Qingshan Liu, “Dimensionality reduction of hyperspectral image using spatial regularized local graph discriminant embedding,” IEEE Journal of Selected Topics in Applied Earth Observations
and Remote Sensing, vol. 11, no. 9, pp. 3262–3271, 2018.
[11] Wenzhi Zhao, William Emery, Yanchen Bo, and Jiage Chen, “Land
cover mapping with higher order graph-based co-occurrence model,”
Remote Sensing, vol. 10, no. 11, pp. 1713, 2018.
[12] Yi Chen, Nasser M Nasrabadi, and Trac D Tran, “Hyperspectral
image classification using dictionary-based sparse representation,” IEEE
Transactions on Geoscience and Remote Sensing, vol. 49, no. 10, pp.
3973–3985, 2011.
[13] Leyuan Fang, Shutao Li, Xudong Kang, and Jón Atli Benediktsson,
“Spectral–spatial hyperspectral image classification via multiscale adaptive sparse representation,” IEEE Transactions on Geoscience and
Remote Sensing, vol. 52, no. 12, pp. 7738–7749, 2014.
[14] Jiayi Li, Hongyan Zhang, Yuancheng Huang, and Liangpei Zhang,
“Hyperspectral image classification by nonlocal joint collaborative representation with a locally adaptive dictionary,” IEEE Transactions on
Geoscience and Remote Sensing, vol. 52, no. 6, pp. 3707–3719, 2014.
[15] Wei Li and Qian Du, “Joint within-class collaborative representation
for hyperspectral image classification,” IEEE Journal of Selected Topics
in Applied Earth Observations and Remote Sensing, vol. 7, no. 6, pp.
2200–2208, 2014.
[16] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, “Deep learning,”
nature, vol. 521, no. 7553, pp. 436, 2015.
[17] Jürgen Schmidhuber, “Deep learning in neural networks: An overview,”
Neural networks, vol. 61, pp. 85–117, 2015.
[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, “Imagenet
classification with deep convolutional neural networks,” in Advances in
neural information processing systems, 2012, pp. 1097–1105.
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, “Deep
residual learning for image recognition,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp. 770–
778.
[20] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik, “Rich
feature hierarchies for accurate object detection and semantic segmentation,” in Proceedings of the IEEE conference on computer vision and
pattern recognition, 2014, pp. 580–587.
[21] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, “Faster r-cnn:
Towards real-time object detection with region proposal networks,” in
Advances in neural information processing systems, 2015, pp. 91–99.
[22] Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray
Kavukcuoglu, and Pavel Kuksa, “Natural language processing (almost)
from scratch,” Journal of Machine Learning Research, vol. 12, no. Aug,
pp. 2493–2537, 2011.
[23] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, “Sequence to sequence
learning with neural networks,” in Advances in neural information
processing systems, 2014, pp. 3104–3112.
[24] Liangpei Zhang, Lefei Zhang, and Bo Du, “Deep learning for remote
sensing data: A technical tutorial on the state of the art,” IEEE

11

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]

[43]

Geoscience and Remote Sensing Magazine, vol. 4, no. 2, pp. 22–40,
2016.
Xiao Xiang Zhu, Devis Tuia, Lichao Mou, Gui-Song Xia, Liangpei
Zhang, Feng Xu, and Friedrich Fraundorfer, “Deep learning in remote
sensing: a comprehensive review and list of resources,” IEEE Geoscience
and Remote Sensing Magazine, vol. 5, no. 4, pp. 8–36, 2017.
Yushi Chen, Zhouhan Lin, Xing Zhao, Gang Wang, and Yanfeng Gu,
“Deep learning-based classification of hyperspectral data,” IEEE Journal
of Selected topics in applied earth observations and remote sensing, vol.
7, no. 6, pp. 2094–2107, 2014.
Chao Tao, Hongbo Pan, Yansheng Li, and Zhengrou Zou, “Unsupervised
spectral–spatial feature learning with stacked sparse autoencoder for
hyperspectral imagery classification,” IEEE Geoscience and remote
sensing letters, vol. 12, no. 12, pp. 2438–2442, 2015.
Xiaorui Ma, Hongyu Wang, and Jie Geng, “Spectral–spatial classification of hyperspectral image based on deep auto-encoder,” IEEE Journal
of Selected Topics in Applied Earth Observations and Remote Sensing,
vol. 9, no. 9, pp. 4073–4085, 2016.
Yushi Chen, Xing Zhao, and Xiuping Jia, “Spectral–spatial classification
of hyperspectral data based on deep belief network,” IEEE Journal of
Selected Topics in Applied Earth Observations and Remote Sensing, vol.
8, no. 6, pp. 2381–2392, 2015.
Xichuan Zhou, Shengli Li, Fang Tang, Kai Qin, Shengdong Hu, and
Shujun Liu, “Deep learning with grouped features for spatial spectral
classification of hyperspectral images,” IEEE Geoscience and Remote
Sensing Letters, vol. 14, no. 1, pp. 97–101, 2017.
Ping Zhong, Zhiqiang Gong, Shutao Li, and Carola-Bibiane Schönlieb,
“Learning to diversify deep belief networks for hyperspectral image
classification,” IEEE Transactions on Geoscience and Remote Sensing,
vol. 55, no. 6, pp. 3516–3530, 2017.
Yunsong Li, Weiying Xie, and Huaqing Li, “Hyperspectral image
reconstruction by deep convolutional neural network for classification,”
Pattern Recognition, vol. 63, pp. 371–383, 2017.
Wenzhi Zhao, Shihong Du, and William J Emery, “Object-based
convolutional neural network for high-resolution imagery classification,”
IEEE Journal of Selected Topics in Applied Earth Observations and
Remote Sensing, vol. 10, no. 7, pp. 3386–3396, 2017.
Mengmeng Zhang, Wei Li, and Qian Du, “Diverse region-based cnn
for hyperspectral image classification,” IEEE Transactions on Image
Processing, vol. 27, no. 6, pp. 2623–2634, 2018.
Wei Hu, Yangyu Huang, Li Wei, Fan Zhang, and Hengchao Li, “Deep
convolutional neural networks for hyperspectral image classification,”
Journal of Sensors, vol. 2015, 2015.
Lin He, Jun Li, Chenying Liu, and Shutao Li, “Recent advances on
spectral–spatial hyperspectral image classification: An overview and new
guidelines,” IEEE Transactions on Geoscience and Remote Sensing, vol.
56, no. 3, pp. 1579–1597, 2018.
Pedram Ghamisi, Emmanuel Maggiori, Shutao Li, Roberto Souza,
Yuliya Tarablaka, Gabriele Moser, Andrea De Giorgi, Leyuan Fang,
Yushi Chen, Mingmin Chi, et al., “New frontiers in spectral-spatial
hyperspectral image classification: The latest advances based on mathematical morphology, markov random fields, segmentation, sparse representation, and deep learning,” IEEE Geoscience and Remote Sensing
Magazine, vol. 6, no. 3, pp. 10–43, 2018.
Yushi Chen, Hanlu Jiang, Chunyang Li, Xiuping Jia, and Pedram
Ghamisi, “Deep feature extraction and classification of hyperspectral
images based on convolutional neural networks,” IEEE Transactions on
Geoscience and Remote Sensing, vol. 54, no. 10, pp. 6232–6251, 2016.
Ying Li, Haokui Zhang, and Qiang Shen, “Spectral–spatial classification
of hyperspectral imagery with 3d convolutional neural network,” Remote
Sensing, vol. 9, no. 1, pp. 67, 2017.
Cheng Shi and Chi-Man Pun, “Superpixel-based 3d deep neural
networks for hyperspectral image classification,” Pattern Recognition,
vol. 74, pp. 600–616, 2018.
Jingxiang Yang, Yong-Qiang Zhao, and Jonathan Cheung-Wai Chan,
“Learning and transferring deep joint spectral–spatial features for hyperspectral classification,” IEEE Transactions on Geoscience and Remote
Sensing, vol. 55, no. 8, pp. 4729–4742, 2017.
Xiaodong Xu, Wei Li, Qiong Ran, Qian Du, Lianru Gao, and Bing
Zhang, “Multisource remote sensing data classification based on
convolutional neural network,” IEEE Transactions on Geoscience and
Remote Sensing, vol. 56, no. 2, pp. 937–949, 2018.
Siyuan Hao, Wei Wang, Yuanxin Ye, Tingyuan Nie, and Lorenzo
Bruzzone, “Two-stream deep architecture for hyperspectral image
classification,” IEEE Transactions on Geoscience and Remote Sensing,
vol. 56, no. 4, pp. 2349–2361, 2018.

[44] Hao Wu and Saurabh Prasad, “Convolutional recurrent neural networks
forhyperspectral data classification,” Remote Sensing, vol. 9, no. 3, pp.
298, 2017.
[45] Qingshan Liu, Feng Zhou, Renlong Hang, and Xiaotong Yuan,
“Bidirectional-convolutional lstm based spectral-spatial feature learning
for hyperspectral image classification,” Remote Sensing, vol. 9, no. 12,
pp. 1330, 2017.
[46] Feng Zhou, Renlong Hang, Qingshan Liu, and Xiaotong Yuan, “Hyperspectral image classification using spectral-spatial lstms,” Neurocomputing, 2018.
[47] Feng Zhou, Renlong Hang, Qingshan Liu, and Xiaotong Yuan, “Integrating convolutional neural network and gated recurrent unit for hyperspectral image spectral-spatial classification,” in Chinese Conference on
Pattern Recognition and Computer Vision (PRCV). Springer, 2018, pp.
409–420.
[48] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton, “Speech
recognition with deep recurrent neural networks,” in Acoustics, speech
and signal processing (icassp), 2013 ieee international conference on.
IEEE, 2013, pp. 6645–6649.
[49] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua
Bengio, “Empirical evaluation of gated recurrent neural networks on
sequence modeling,” arXiv preprint arXiv:1412.3555, 2014.
[50] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio, “Learning
phrase representations using rnn encoder–decoder for statistical machine
translation,” in Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing, 2014, pp. 1724–1734.

