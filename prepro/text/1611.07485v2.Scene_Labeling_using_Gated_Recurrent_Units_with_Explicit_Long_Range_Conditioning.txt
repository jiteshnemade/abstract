arXiv:1611.07485v2 [cs.CV] 28 Mar 2017

Scene Labeling using Gated Recurrent Units with Explicit Long Range
Conditioning
Qiangui Huang
University of Southern California
Los Angeles, CA

Weiyue Wang
University of Southern California
Los Angeles, CA

qianguih@usc.edu

weiyuewa@usc.edu

Kevin Zhou
Siemens Healthineers
Princeton, NJ

Suya You
University of Southern California
Los Angeles, CA

s.kevin.zhou@gmail.com

suya@usc.edu

Ulrich Neumann
University of Southern California
Los Angeles, CA
uneumann@usc.edu

Abstract
Recurrent neural network (RNN), as a powerful contextual dependency modeling framework, has been widely applied to scene labeling problems. However, this work shows
that directly applying traditional RNN architectures, which
unfolds a 2D lattice grid into a sequence, is not sufficient
to model structure dependencies in images due to the “impact vanishing” problem. First, we give an empirical analysis about the “impact vanishing” problem. Then, a new
RNN unit named Recurrent Neural Network with explicit
long range conditioning (RNN-ELC) is designed to alleviate
this problem. A novel neural network architecture is built
for scene labeling tasks where one of the variants of the
new RNN unit, Gated Recurrent Unit with Explicit Longrange Conditioning (GRU-ELC), is used to model multi
scale contextual dependencies in images. We validate the
use of GRU-ELC units with state-of-the-art performance on
three standard scene labeling datasets. Comprehensive experiments demonstrate that the new GRU-ELC unit benefits
scene labeling problem a lot as it can encode longer contextual dependencies in images more effectively than traditional RNN units.

Figure 1: CNNs have challenges in dealing with local textures in images as shown in the second row. With the help of
Gated Recurrent Units, the model can make globally better
prediction. However, GRUs still struggle in modeling fine
structures in images due to the “impact vanishing” problem.
Our GRU-ELC units can effectively model multi scale contextual dependencies in images and thus successfully preserve local details in predictions, such as the windows and
doors (even not annotated in ground truth) in first row, and
the trees and mountains in the second and third row.

1. Introduction
Scene labeling is a fundamental task in computer vision.
Its goal is to assign one of many pre-defined category la1

bels to each pixel in an image. It is usually formulated
as a pixel-wise multi-class classification problem. Modern scene labeling methods rely heavily on convolutional
neural networks (CNNs) [7, 30, 49, 29]. CNNs are capable of learning scale-invariant discriminative features for
images. These features have proven more powerful than
traditional hand-crafted features on many computer vision
tasks [20, 35, 30]. Specially designed CNN architectures
[7, 30, 1, 48, 49] have shown superior performance on scene
labeling by using end-to-end training.
However, CNNs have challenges in dealing with local
textures and fine structures in images and tend to oversegment or under-segment objects in images. To accurately
segment small components and detect object boundaries,
long range contextual dependencies in images are usually
desired when designing scene labeling algorithm. Many
works have exploited probabilistic graphical models such as
conditional random fields (CRFs) [19] to capture structural
dependencies in images [49, 29]. However, CRFs usually
require carefully designed potentials and their exact inference is usually intractable. In contrast, recurrent neural networks (RNN), as another category of powerful contextual
dependency modeling methods, are free from these disadvantages and can learn contextual dependencies in a datadriven manner.
RNNs have first proven effective in modeling data dependencies in domains such as natural language processing and speech recognition [11, 33]. Recently, there are
some attempts at applying RNNs to images [4, 44, 38, 24].
Because the spatial relationship among pixels in 2D image
data is fundamentally different from the temporal relationship in the 1D data in NLP or speech recognition, variants
of RNN architectures [17, 38, 44, 42, 12, 25, 41] have been
proposed to handle 2D image data, which typically involve
unfolding a 2D lattice grid into a 1D sequence. The unfolded 1D sequence is usually much longer than the data
sequence in NLP or speech recognition. Take a feature map
of size 64 × 64 for example. Its unfolded 1D sequence is of
length 64 × 64 = 4096. One major flaw of applying existing RNN units to sequences of such a long length is that the
“impact vanishing” problem will raise and break the spatial
dependencies in images.
It is well-known that long term dependency is hard to
learn in RNN units due to the gradient exploding or vanishing problems and RNN units such as Long Short Term
Memory (LSTM) or Gated Recurrent Unit (GRU) can effectively avoid these problems. However, in this work, we
empirically show that even in LSTMs or GRUs, the dependency in a extremely long range is still hard to capture due
to the “impact vanishing” problem.
First, this paper studies the “impact vanishing” problem
when traditional RNNs are applied to image data. Then,
we generalize traditional RNN units to RNN units with Ex-

plicit Long-range Conditioning (RNN-ELC) to overcome
this problem. Specifically, two variants, GRU-ELC and
LSTM-ELC, are designed and discussed.
Compared with existing works [17, 38, 44, 42, 12, 25,
41], RNN-ELCs can effectively capture long range contextual dependency in images with the help of their explicit long range conditioning architecture. In the RNNELC units, the present variable is explicitly conditioned on
multiple contextually related but sequentially distant variables. Intuitively, it is adding skip connection between hidden states. Adding skip connections in RNNs to help learn
long term dependency first appears in [26]. Our method
generalizes the idea to model more complex 2D spatial dependencies. The RNN-ELC unit is applicable to both raw
pixels and image features. It can be naturally integrated in
CNNs, thereby enabling joint end-to-end training.
In order to take the benefits of the new RNN units for
scene labeling tasks, we build a novel scene labeling system
using the GRU-ELC units to model long range multi-scale
contextual dependencies in image features.
In summary, our main contributions include: 1) An empirical study of the “impact vanishing” problem, which
commonly exists in traditional RNN units when they are
applied to images; 2) A new RNN unit with Explicit
Long-range Conditioning to alleviate the “impact vanishing” problem; 3) A novel scene labeling algorithm based
on GRU-ELCs. There are a few works utilizing GRUs for
scene labeling. However, we show that our GRU-ELC units
can actually achieve state-of-the-art performances in scene
labeling tasks; and 4) Improved performances on several
standard scene labeling datasets.

2. Related Work
Scene labeling is one of the most challenging problems
in computer vision. Recently, convolutional neural network
based methods achieved great success in this task. Farabet et al. [9] made one of the earliest attempts at applying hierarchical features produced by CNNs to scene labeling. Eigen et al. [7] designed a multi-scale convolutional
architecture to jointly segment images, predict depth, and
estimate normals for an image. Long et al. [30] applied
fully convolutional network (FCN) to this task. Noh et al.
[32] also used deconvolution layers for image segmentation.
They adopted an encoder-decoder architecture, where encoder part consists of convolution and pooling operations
and decoder part consists of deconvolution and unpooling
operations. Badrinarayanan et al. [1] designed a similar
architecture named SegNet. In [48], Yu and Koltun developed a dilated convolutional module to preserve multi-scale
contextual information for image segmentation.
Although CNN based methods introduced powerful
scale-invariant features for scene labeling, they performed
poorly in preserving local textures and fine structures in

predictions. These problems were addressed by combining CNNs with probabilistic graphical models such as conditional random fields (CRFs). Chen et al. [5] suggested
to put a fully connected CRF [19] on top of FCN to capture structural dependencies in images. Zheng et al. [49]
showed that CNN and CRF can be jointly trained by passing the inference errors of CRFs back to CNN. Liu et al.
[29] improved [49] by introducing a more complex pairwise
term for CRF. CRF based methods usually require carefully
designed pair-wise potentials and unfortunately their exact
inference is usually intractable.
RNNs, as another powerful tool for modeling contextual
dependences in data, have achieved tremendous success in
many areas such as speech recognition [11] and natural language processing [33]. There is also a rich literature of using RNNs for image related tasks [24, 46, 21, 34, 8].
Liang et al. [23] designed a graph LSTM to handle image data. However, their method is built on superpixels,
which is computationally expensive and is not directly applicable to image features. Byeon et al. [4] developed
a scene labeling method based on a 2D LSTM network,
which first divided an input image into non-overlapping
patches and then sequentially fed them into LSTM networks
in four different orders. [43] built a RNN segmentation algorithm based on recently proposed ReNet [44]. Their idea
was to to alternatively sweep an image in different directions and then sequentially input each row (or column) into
a RNN. Shuai et al. [37, 38] designed a quaddirectional 2D
RNN architecture for scene labeling, where each pixel was
connected to its 8 nearest neighbors.
Long range contextual dependencies are usually desired
for scene labeling tasks. In previous works [17, 38, 44,
42, 12, 25, 41], the present variable only explicitly conditions on variables within a short range such as its 8 nearest
neighbors. However, as shown later, short range conditioning does not capture long range structural dependencies in
images. Our labeling algorithm is a generalized version of
[43, 37, 38]. But our model is different with them in that (i)
our model is built upon a new RNN unit, GRU-ELC unit,
which is free of “impact vanishing” problem; (ii). our models can effectively utilize multi scale contextual dependencies to provide better scene labeling performances.

probability of current time step output, P (y t |xt , ht−1 , θ),
by the following equations:

ht = σh (xt Wx + ht−1 Wh + bh ),
t

y = σy (h Wy + by ),

it = σi (xt Wxi + ht−1 Whi + wci
t

t

t−1

f = σf (x Wxf + h
t

c = ft

A vanilla RNN unit has two types of dense connections,
namely, input-to-hidden and hidden-to-hidden connections.
At each time step t, the output y t conditions on the input
at current time step xt and the hidden state at previous time
step ht−1 . Mathematically, given a sequence of input data
X = {xt , t = 1, ..., T }, the vanilla RNN unit models the

(2)

Whf + wcf

ct−1 + bi )

(3)

ct−1 + bf )

(4)

t

ct−1 + it

t

σc (x Wxc + ht−1 Whc + bc ) (5)

t

o = σo (x Wxo + ht−1 Who + wco
t

h =o

t

ct + bo )

t

σh (c )

(6)
(7)

GRUs compute the output by the following equations
with Wxr,hr,xu,hu,xc,hc and br,u,c being parameters.

rt = σr (xt Wxr + ht−1 Whr + br )
t

t−1

u = σu (x Wxu + h
t

t

c = σc (x Wxc + r
t

3.1. Recurrent Neural Network

(1)

where θ = {Wx,h,y , bh,y } is the parameter and σh and σy
are the nonlinearity functions of hidden and output layers,
respectively. y t explicitly conditions only on xt and ht−1 ,
but ht−1 explicitly conditions on previous input xt−1 and
hidden state ht−2 ; thus, y t actually implicitly conditions on
all previous inputs and hidden states. Therefore, previous
variables can influence their following variables by passing
information through the hidden states.
However, vanilla RNNs have the notorious gradient vanishing or exploding problem when they are applied to learn
long-term dependencies [33, 2]. In practical applications,
two types of gated RNNs are developed to avoid this problem, Long Short-Term Memory (LSTM) network [15] and
Gated Recurrent Unit (GRU) network [6].
LSTM uses the following equations to update its hidden states. Let it , f t , ct , and ot denote the output of the
input gate, the forget gate, the cell gate, and the output
gate, respectively.
denotes element-wise multiplication.
Wxi,xf,xc,xo,hi,hf,hc,ho , wci,cf,co , and bi,f,c,o are parameters. σi,f,c,o are nonlinearity functions.

t

3. Recurrent Neural Networks with Explicit
Long-range Conditioning

t

t

h = (1 − u )

t

Whu + bu )

t−1

(h

t−1

h

Whc ) + bc )
t

+u

t

c

(8)
(9)
(10)
(11)

LSTMs and GRUs have been widely used in modeling
long term dependencies as they can effectively prevent gradients from vanishing or exploding [3]. However, they are
still limited for image related applications because the “impact vanishing” is hard to avoid when dealing with a very
long sequence..

(a) Original 2D image grid
(a)

(b)

(c)

(d)

(b) Unfolded image grid modeled by traditional RNN unit

Figure 2: Illustration of “impact vanishing” problem in
LSTMs and GRUs. Fluctuation denotes the difference
caused by changing the first data x1 in input sequence.
(c) Unfolded image grid modeled by RNN-ELC units

3.2. Impact Vanishing Problem
Here we design a concise and straightforward toy example to empirically demonstrate the existence of “impact vanishing” problem in LSTM/GRU units.
Assume X = {xt , t = 1, ..., T } is an input sequence.
t
x ∈ RM ×N is the input data at time step t. In this toy
example, xt is generated from a continuous uniform distribution U (0, 1) and all weight parameters W in LSTMs and
GRUs are initialized with a Guassian distribution N (0, 0.1)
and bias parameters are set to zero.
Firstly, the entire sequence X is fed to LSTMs/GRUs
which output Y = {y t , t = 1, ..., T }. Then, the data at
the first time step x1 is replaced with x̂1 which is generated
from same distribution U (0, 1) and get a new input data sequence X̂ = {x̂1 , x2 , ..., xT }. Feed X̂ to the same RNNs
and get the new output Ŷ = {ŷ 1 , ŷ 2 , ..., ŷ T }. If the information of the first input can successfully pass through
hidden states and make impact on following variables, we
should expect ŷ t to be different with y t when x1 changes.
To measure how different ŷ t becomes, we calculate the following fluctuation metric:

1
F =
MN
t

M,N
X

t

t

(y (i, j) − ŷ (i, j))

2

(12)

i=1,j=1

We repeat this process by 20 times, collect all F t , and
report the mean of all 20 F t in the top left plot in Fig.2.
It shows that F t drops dramatically in the first 10 time
steps. When t > 20, F t decreases to zero, which means
that y t stays unchanged when t > 20 regardless of the
initial input x1 . Although ht implicitly depends on h1
and x1 , the dependency between the 1th and tth variables is actually broken when t > 20. In another word,
P (ht |ht−1 , ..., h1 , h0 ) = P (ht |ht−1 , ..., h1 ) when t > 20.

Figure 3: Graphical illustration of unfolding 2D image data
into 1D sequence and applying RNN units.

This phenomenon is referred as “impact vanishing” problem in this paper.
The mechanics behind “impact vanishing” problem is
similar to the gradient vanishing/exploding problem [33].
The tth variable makes impact on following variables by
storing its information in ht and passing it to following hidden states. During the flow, ht will be multiplied by fixed
weight matrixes many times. If the spectral radius of the
weight matrix is smaller than 1, the multiplication results
will vanish to zero. And if the spectral radius is larger than
1, multiplication results will explode to infinity and thus saturates the sigmoid and tanh functions. In both cases, information stored in ht has decreasing impact on ht+k when k
becomes larger.

3.3. RNN units with Explicit Long-range Conditioning
The consequence of “impact vanishing” problem is that
dependencies between spatially related variables are broken. Take the 3 × 3 image grid in Fig.3a for example. x2
and x4 are both degree 1 neighbors of x5 . However, after
unfolding it into a 1D sequence in a given direction (from
left to right and top to bottom) and applying a RNN to it, x5
is only directly conditioned on variable x4 . The information
from x2 needs to flow through two more variables and then
impacts on x5 . From the toy example in previous section,
we know that when a large image neighborhood is involved,
this impact will vanish in practice.
In order to overcome the “impact vanishing” problem
and bring back the contextual dependency, we generalize

Figure 4: Illustration of our scene labeling algorithm described in Section 4. Blue cuboids denote a convolution
layer followed by a pooling layer. Red blocks denote GRUELC units with arrow representing the unfolding direction.
Purple cuboids denote concatenation operation. Yellow
cuboids denote a unpooling layer followed by a convolution
layer. In the GRU-ELC block, S parallel branches are used
to model contextual dependencies between current variable
with its degree 1, 2, ..., s neighbors. In each branch, four
GRU-ELC units are used to model dependency relations in
four different unfolding directions. Note that ⊕ denote concatenation operation here.
existing RNN units to incorporate with long range conditioning. Mathematically, a vanilla RNN-ELC unit has the
following formulation:
1
h = σh (x Wx + (ht−1 + ht−s )Wh + bh ),
2
t

t

(13)

ht−s is the explicit long range conditioning and s is a conditioning skip stride. 12 is a constant term to keep ht stay in the
valid activation area of σh . This RNN-ELC unit models the
following conditional probability: P (y t |xt , ht−1 , ht−s , θ).
Note that no extra weights are introduced here.
In the scenario in Fig.3a, we can set s = 3 and the resulting graphical model is presented in Fig.3c. Now, the information from x2 flows directly into x5 via a skip connection
and the impact from x2 is back now.
In order to test the efficiency of the proposed RNN-ELC
units 1 , we use the same data and weights as in previous
section and run the toy example again. The conditioning
skip stride s is set as 20 here. After repeating each experiment for 20 times, F t is plotted in the top right plot in Fig.2.
Compared with a traditional LSTM/GRU unit, F t of ELCLSTM/GRU unit drops faster due to the constant term 12 .
1 It

is straightforward to generalize equation (13) for an LSTM/GRU
unit. So we don’t present them here for space reason

But, there is a strong peak around time step 20, which does
not exist in tradition RNN units. Intuitively, the peak means
there is a strong dependency between the tth and (t − s)th
variable (t = 20 in this toy example) in RNN-ELC units
now as changing (t − s)th variable has a huge impact on the
output of the tth variable.
In real world scenarios, a long range dependency is usually desired. The tth pixel could be treated as related to all
k pixels vertically above it. In this case, we need to model
the dependency between the tth variable and the (t − s)th ,
(t − 2s)th , ..., (t − ks)th variables.
The top right plot in Fig.2 shows that the conditioning
encoded by equation (13) is still not powerful enough as
the peak around time step 40 is relatively smaller and F t
decreases to near zero again when t > 60. So, equation
(13) is further generalized as below. k can be called as a
conditioning scale.
ht = σh (xt Wx + H t Wh + bh )
Ht =

k
X
1
(ht−1 +
ht−i∗s )
k+1
i=1

(14)
(15)

Equations (14)-(15) are tested against the toy example
again. s is set to be 20 and k is set to be 2 and 3. F t is
reported in the second row in Fig.2. It shows that by explicitly conditioning on related variables in a longer range, the
length of valid dependency becomes longer. Next sections
show how to exploit this property of RNN-ELC units and
build a model for scene labeling which captures a desired
long range contextual dependency.

4. Scene Labeling using Gated Recurrent Units
with Explicit Long Range Conditioning
An overview of our scene labeling algorithm is presented
in Fig.4. It is based on GRU-ELC units2 and CNNs. There
are three blocks: convolution block, GRU-ELC block, and
final prediction block. The convolution block encodes images into features. It is initialized by certain layers from
VGG-16 network [40]. The final prediction block uses unpooling layers followed by convolution layers to make final
predictions of the same resolution as input images. This
block is trained from scratch. The GRU-ELC block models
contextual dependencies over features.
A multi scale contextual dependency is encoded in our
GRU-ELC block. For each variable, its relations with
neighbors from degree 1 to degree S are modeled in our
framework. Take the 2D grid in Fig. 5a for example. Assume the grid is of width w and is unfolded into a 1D sequence from left to right and top to bottom. For variable t,
2 we choose GRU-ELC units here instead of LSTM-ELC units because
GRU-ELC units decay slower than LSTM-ELC units in our toy example
as shown in Fig. 2

Ht =

1 t−s
(h
+ ht−w∗s−s + ht−w∗s + ht−w∗s+s ) (16)
4
rt = σr (xt Wxr + H t Whr + br )
(17)
ut = σu (xt Wxu + H t Whu + bu )
t

(a) Original 2D grid and dependency relation.

(b) Graphical model of traditional GRU unit.

(c) Graphical model of GRU-ELC unit for degree s dependency of
the tth variable.

Figure 5: Graphical model examples of GRU-ELC units.
Note that in (d) only the graphical models of variable t and
its degree 2 neighboring variables are drew here. All other
variables are ignored as they are not implicitly dependent
with variable t. σ, tanh, ⊕, 1- denote the sigmoid activation, tangent activation, element wise addition, and 1 minus
input, respectively. Best viewed with zoom in.

its degree 1 neighbors include the (t − 1)th , (t − w − 1)th ,
(t − w)th , (t − w + 1)th variables and its degree 2 neighbors include the (t − 2)th , (t − 2w − 2)th , (t − 2w)th , and
(t − 2w + 2)th variables 3 .
For dependency in degree k, a group of 4 GRU-ELC
units is utilized, one GRU-ELC unit for one unfolding direction. Four directions are considered in our framework
as inspired by [4, 43, 44]. Take the left-to-right and top-tobottom direction for example. Following the mechanics as
equations (14) - (15), the GRU-ELC unit obeys the following equations.

t

c = σc (x Wxc + r
t

t

h = (1 − u )

t

t

(18)

(H Whc ) + bc )

(19)

t

(20)

t

H +u

t

c

Here w is the width of input grid. For the scenario in
Fig. 5a, graphical model of GRU-ELC unit used for degree
s dependency is presented in Fig. 5c. The blue lines denote the long range conditioning in GRU-ELC units. They
are skip connections which help hidden states from neighboring variables flow directly into current variable. Note
that in equation (16) - (20), variable t only conditions on its
degree s neighboringing variables. Other neighboring variables will be modeled by other groups of GRU-ELC units in
our scene labeling framework. All information will be aggregated together by concatenation operations and then fed
into final prediction block.

5. Experiment
In order to cover both outdoor and indoor scenes, we
select three challenging datasets to test our method, namely
the SiftFlow [27], NYUDv2 [39], and Stanford Background
[10] datasets. Two metrics are used for evaluation, namely
global pixel accuracy (Global) and average per-class accuracy (Class). First, a full exploration of different architecture choices is conducted on SiftFlow. Then, state-of-theart results are presented for NYUDv2, and Stanford Background dataset.
The convolution block in Fig. 4 is initialized by certain
layers from VGG-16 network [40]. Equivalent number of
uppooling and convolution layers are used in final prediction block.
All experiments follow a same training protocol. Images
with original size are used for training/testing. The training process ends after 25 epochs for all models. The initial
learning rate is set as 0.001 and the poly learning policy is
adopted to decrease the learning rate after every epoch. All
models use recently developed Adam solver [18] for gradient descent training. The cross entropy loss is used as objective function. Median frequency balancing [7] is applied
for comparison. Widely used data augmentation, cropping,
flipping, and random jittering are adopted.

5.1. The SiftFlow dataset

3 Note that we only consider neighboring variables in diagonal, vertical,

and horizontal directions. Because conditioning on too many variables in
one RNN-ELC unit will increase k in equation 15 and make the impact
decay faster.

The SiftFlow dataset contains 2,688 images with 33 object categories. All images are of size 256×256 and are captured in outdoor scenes like coast, highway, forest, city, etc.
All experiments follow the same 2,488/200 training/testing
split as convention.

Method
conv4-decoder
conv4-GRU
conv4-ELC-3
conv4-ELC-4
conv4-ELC-5
conv3-decoder
conv3-GRU
conv3-ELC-3
conv3-ELC-4
conv3-ELC-5

No balancing
Global (%) Class (%)
81.3
38.6
84.3
35.2
86.6
45.1
87.6
47.0
87.3
46.6
71.7
22.7
84.8
37.1
85.4
45.9
87.8
46.7
87.4
47.0

Median balancing
Global (%) Class (%)
76.9
52.7
81.0
53.4
83.7
61.8
84.8
62.2
85.0
62.7
68.8
41.9
81.7
54.8
83.6
61.2
84.4
64.0
85.1
63.6

Table 1: Performance comparison of different choices for GRU-ELC block on SiftFlow. The notation convention is: 1).
convX-decoder denotes this model is built on top of the convX 3 layer of VGG 16 net and no GRU-ELC block is used. Only
final prediction block is stacked on top of convolution block; 2). convX − GRU denotes only traditional GRU units are used
in the model; 3) convX-ELC-S denotes GRU-ELC units are used and the dependencies up to degree S is modeled.

Figure 6: Comparison between models with different settings of GRU-ELC block in our system. All models are
trained with median frequency balancing. Note that the
black color denotes unknown categories.

First, we explore different architecture choices for proposed scene labeling algorithm. We build our system on top
of conv4 3 or conv3 3 layer in VGG-16 net. We also tested
the conv5 3 layer but the performance is not comparable so
we don’t report it here. For each feature layer choice, different contextual dependency ranges (S set as 3, 4, and 5) are
tested. We also run experiments for architectures without
GRU units for comparison. All results are reported in Table
1.
Experimental results show that GRU-ELC models have
superior performances compared with models where only

GRU units or no RNN units are used. Some prediction results are visualized in Fig. 6. Generally, models with longer
contextual dependencies have better quantitive and visual
performances. This demonstrates that the newly designed
GRU-ELC units can effectively model desired long range
structure dependencies in images. Specifically, ELC-4 gives
much better performance than ELC-3. But ELC-5 only has
subtle improvements compared to ELC-4. Considering the
fact that ELC-5 requires more computations, ELC-4 actually is a better setting in our system. Note that our models
built upon conv3 3 give better results than conv4 3. However, this is not conclusive as we found conv4 3 layer features could give better results on some other datasets. Another observation to notice is that median frequency balancing helps the model achieve better average per-class accuracy but the global pixel accuracy is sacrificed (which has
also been found in [1, 7, 9, 22]).
Our methods are also compared with other state-of-theart results in Table 2. Comprehensive comparisons show
that our methods can outperform both RNN based and other
state-of-the-art methods. Especially the average class accuracy has been improved by near 6.3%. These results show
that the proposed algorithm with GRU-ELC units can effectively model long range contextual dependencies in images
and thus benefit the scene labeling task a lot. Results produced by our algorithm and FCN-8s [30] are visually compared in Fig.7.

5.2. The NYUDv2 dataset
NYUDv2 is a RGB-D dataset containing 1,449 RGB and
depth image pairs for indoor scenes. Standard split contains
795 training images and 654 testing images. 40 categories
[13] have been widely used to test the performance of la-

Method
Attent to rare class [47]
FCN-16s [30]
Eigen et al. [7]
Eigen et al. [7] MB
ParseNet [28]
RCNN [22]
DAG-RNN [38]
RNN based
Multi-Path [16]
Attention [8]
conv3-LC-GRU-4
conv3-LC-GRU-4 MB

Global (%)
79.8
85.2
86.8
83.8
86.8
84.3
85.3
86.9
86.9
87.8
84.4

Class (%)
48.7
51.7
46.4
55.7
52.0
41.0
55.7
56.5
57.7
46.7
64.0

Table 2: Comparison with state-of-the-art on SiftFlow. Our
method is compared against RNN related and other stateof-the-art methods. MB denotes this model is trained with
median frequency balancing.

petitive against other methods which additionally use depth
or normal information.

5.3. The Stanford background dataset
The Stanford background dataset is composed of 715
images with 8 object categories. The images are captured
in outdoor scenes and most of them are of size 320×240.
Following the standard protocol [22], a 5-fold cross validation is used for measuring the performance, each of them
randomly selects 572 images for training and 143 images
for testing. The conv3-ELC-4 setting is adopted for this
dataset. Results of our method are reported and compared
with other state-of-the-art methods in Table.4. State-of-thearts results demonstrate the effective of long range dependency for scene labeling problems.

6. Conclusion
RNNs are a class of neural network models that have
proven effective in modeling internal data dependencies in
many areas. There are also many works applying RNNs for
image data. In this work, we empirically show that traditional RNN units are not powerful enough to model dependencies in very long sequences due to the “impact vanishing” problem. A new RNN unit with Explicit Long-range
Conditioning is designed to avoid this problem. Based on
the RNN-ELC units, a new scene labeling algorithm is developed in this paper. Various experimental results and
comparisons with other state-of-the-art methods demonstrate that our algorithm can effectively capture long range
structure dependencies in images and thus give better performances in scene labeling.
Potential directions for future works include: 1). Extend our scene labeling algorithm to take multi-modal input
information (like depth or normal information); 2) Apply
our new RNN-ELC unit to other image related applications
such as image inpainting and image generation.

Figure 7: Comparisons of results produced by FCN-8s [30]
and conv3-ELC-4 trained with median frequency balancing.
conv3-ELC-4 can accurately predict small objects in scenes
such the persons, poles, boards, and windows. Note that the
black color denotes unknown categories.

beling algorithms. Besides RGB images, depth information
[30, 14] and normal information [7] can also be used for
scene labeling. Only RGB images are used in our algorithm
in order to get a clear sense about the capacity of our algorithm. The conv4-ELC-4 setting is applied for this dataset.
The comparison between our method and other state-of-theart methods are reported in Table.3. Although only RGB information is used in our method, our results are quite com-

Appendices
A. Network Architecture
Here we give details about our scene labeling network.
The convolution block in our network is initialized by
certain layers from VGG-16 network [40]. All conv3 models are built on top of the conv3 3 layer from VGG-16 network. And all conv4 models are built on top of the conv4 3
layer.
In conv3 models, the final prediction block consists of
U −C256−U −C128−C64−Cn. Here U denotes the upsampling layer and CX denotes a convolutional layer with
X feature maps. n is the number of categories. Kernels of
size 3 × 3 are used in all convolutional layers except the last
one which uses 1×1 kernels. Note that 2 up-sampling layers

Input Information
RGB
RGB
RGB
RGB+depth
RGB+depth
RGB+depth
RGB+depth
RGB+depth
RGB+depth
RGB+depth+normal

Method
FCN 32s RGB [30]
conv4-ELC-4
conv4-ELC-4 MB
Gupta et al.’13 [13]
Gupta et al.’14 [14]
FCN 32S RGBD [30]
FCN 32S RGB-HHA [30]
FCN 16S RGB-HHA [30]
Wang et al. [45]
Eigen et al. [7]

Global (%)
60.0
64.5
62.1
59.1
60.3
61.5
64.3
65.4
65.6

Class (%)
42.2
41.4
45.5
28.4
35.1
42.4
44.9
46.1
47.3
45.1

Table 3: Comparison with state-of-the-art on NYU. MB denotes this model is trained with median frequency balancing.
Method
Sharma et al. [36]
Mostajabi et al. [31]
Liang et al. [22]
Multi-path [16]
conv3-ELC-4
conv3-ELC-4 MB

Global (%)
81.9
82.1
83.1
86.6
87.8
84.7

Class (%)
73.6
77.3
74.8
79.6
81.1
81.5

Table 4: Comparison with state-of-the-art on Stanford
Background Dataset.MB denotes this model is trained with
median frequency balancing.

are used here to upscale feature maps to the same resolution
of input. In all conv4 models, the final prediction block
uses the follow architecture: U − C256 − U − C128 − U −
C128 − C64 − Cn.
In the GRU-ELC block, 128 feature maps are used in
each GRU-ELC unit.

B. Extended Results
B.1. Per-class analysis on the SiftFlow dataset
Here we give a detailed list of per-class accuracy produced by our best model (conv3-ELC-4 M B ) on SiftFlow dataset. They are organized in Table 5 in ascending
order by the data portion in training set. For example, there
are 24.434% of the training data (pixels) are sky but only
0.001% are bird.
Generally, the model performs quite well on categories
with large amount of training data like sky, building, mountain, tree, road, etc. Moreover, our model can also give reasonable performances on some categories where only limited training data are available. For example, our model
has a 96.5% accuracy on sun even when there are only
0.008% of training data are sun. On some other rare categories, such as sign, crosswalk, person, window, sidewalk,

and sand, we have accuracies above average. We argue
that there are two main reasons for the good per-class performances. Firstly, our model can effectively capture long
range contextual dependencies in images and thus yields a
good performance on small objects which are usually rare
categories (such as the persons and poles shown in Fig. 6 in
the main text). Secondly, the median frequency balancing
is used in our conv3-ELC-4 M B model, which helps to
improve performances in rare categories, as also shown in
[1, 7, 9, 22].

B.2. Visual results on the NYUDv2 and Stanford
Background dataset
Some predictions on the NYUDv2 dataset produced by
our conv4-ELC-4 M B model are visualized in Fig. 8. Results produced by [7] and [30] are compared in Fig. 8 as
well. From Fig. 8 we can see that our model performs
much better in local details than other two models in these
samples, such as the monitors in the first sample image, the
TV screen in the second sample image, and the objects on
the desk in the third sample image.
Fig. 9 presents some samples of our results on the Stanford background dataset. These results show that our model
can effectively detect boundaries and accurately segment
small objects in images, such as the poles, animals, persons,
and vehicles in images.

References
[1] V. Badrinarayanan, A. Handa, and R. Cipolla.
Segnet: A deep convolutional encoder-decoder architecture
for robust semantic pixel-wise labelling. arXiv preprint
arXiv:1505.07293, 2015. 2, 7, 9
[2] Y. Bengio, I. J. Goodfellow, and A. Courville. Deep learning.
An MIT Press book in preparation. Draft chapters available
at http://www. iro. umontreal. ca/ bengioy/dlbook, 2015. 3
[3] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term
dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157–166, 1994. 3

Acc (%)
Portion (%)
Acc (%)
Portion (%)
Acc (%)
Portion (%)
Acc (%)
Protion (%)
Acc (%)
Portion (%)
Acc (%)
Portion (%)

bird
28.3
0.001
streetlight
52.8
0.046
balcony
51.4
0.102
window
66.8
0.786
plant
48.7
1.457
road
85.0
6.952

sun
96.5
0.008
sign
75.1
0.062
fence
61.7
0.150
sidewalk
80.9
0.875
river
84.8
1.484
tree
86.6
10.846

bus
6.0
0.020
staircase
55.4
0.080
person
70.4
0.163
rock
34.4
0.921
grass
75.0
1.901
mountain
81.4
11.762

pole
46.8
0.236
awning
51.9
0.081
bridge
24.5
0.193
sand
67.6
1.041
field
53.2
2.663
building
84.0
18.400

boat
48.8
0.0333
crosswalk
71.0
0.083
door
59.2
0.367
car
90.3
1.136
sea
84.8
5.188
sky
96.7
24.434

Table 5: Per-class accuracy achieved by our model (conv3-ELC − 4 M B) on the SiftFlow dataset. Categories are organized
in ascending order by the data portion in training set.

[4] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki. Scene
labeling with lstm recurrent neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3547–3555, 2015. 2, 3, 6
[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv preprint
arXiv:1412.7062, 2014. 3
[6] K. Cho, B. Van Merriënboer, D. Bahdanau, and Y. Bengio.
On the properties of neural machine translation: Encoderdecoder approaches. arXiv preprint arXiv:1409.1259, 2014.
3
[7] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolutional architecture. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2650–2658, 2015. 2,
6, 7, 8, 9, 11
[8] H. Fan, X. Mei, D. Prokhorov, and H. Ling. Multi-level contextual rnns with attention model for scene labeling. arXiv
preprint arXiv:1607.02537, 2016. 3, 8
[9] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning
hierarchical features for scene labeling. IEEE transactions
on pattern analysis and machine intelligence, 35(8):1915–
1929, 2013. 2, 7, 9
[10] S. Gould, R. Fulton, and D. Koller. Decomposing a scene
into geometric and semantically consistent regions. In 2009
IEEE 12th international conference on computer vision,
pages 1–8. IEEE, 2009. 6
[11] A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In 2013 IEEE in-

[12]

[13]

[14]

[15]
[16]

[17]

[18]
[19]

[20]

ternational conference on acoustics, speech and signal processing, pages 6645–6649. IEEE, 2013. 2, 3
A. Graves and J. Schmidhuber. Offline handwriting recognition with multidimensional recurrent neural networks. In
Advances in neural information processing systems, pages
545–552, 2009. 2, 3
S. Gupta, P. Arbelaez, and J. Malik. Perceptual organization
and recognition of indoor scenes from rgb-d images. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 564–571, 2013. 7, 9
S. Gupta, R. Girshick, P. Arbeláez, and J. Malik. Learning rich features from rgb-d images for object detection and
segmentation. In European Conference on Computer Vision,
pages 345–360. Springer, 2014. 7, 9
S. Hochreiter and J. Schmidhuber. Long short-term memory.
Neural computation, 9(8):1735–1780, 1997. 3
X. Jin, Y. Chen, J. Feng, Z. Jie, and S. Yan. Multi-path
feedback recurrent neural network for scene parsing. arXiv
preprint arXiv:1608.07706, 2016. 8, 9
N. Kalchbrenner, I. Danihelka, and A. Graves. Grid long
short-term memory. arXiv preprint arXiv:1507.01526, 2015.
2, 3
D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6
V. Koltun. Efficient inference in fully connected crfs with
gaussian edge potentials. Adv. Neural Inf. Process. Syst,
2011. 2, 3
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classification with deep convolutional neural networks. In

Figure 8: Comparisons of results produced by FCN-8s [30], Eigen et al. [7], and conv4-ELC-4 trained with median frequency
balancing. Note that the black color denotes unknown categories.

Advances in neural information processing systems, pages
1097–1105, 2012. 2
[21] Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin. Lstmcf: Unifying context modeling and fusion with lstms for rgbd scene labeling. In European Conference on Computer Vi-

sion, pages 541–557. Springer, 2016. 3
[22] M. Liang, X. Hu, and B. Zhang. Convolutional neural networks with intra-layer recurrent connections for scene labeling. In Advances in Neural Information Processing Systems,
pages 937–945, 2015. 7, 8, 9

Figure 9: Visualization of prediction results on the Stanford background dataset produced by our conv3-ELC-4 M B model.

[23] X. Liang, X. Shen, J. Feng, L. Lin, and S. Yan. Semantic object parsing with graph lstm. arXiv preprint
arXiv:1603.07063, 2016. 3
[24] X. Liang, X. Shen, D. Xiang, J. Feng, L. Lin, and S. Yan.
Semantic object parsing with local-global long short-term
memory. arXiv preprint arXiv:1511.04510, 2015. 2, 3
[25] X. Liang, X. Shen, D. Xiang, J. Feng, L. Lin, and S. Yan.

Semantic object parsing with local-global long short-term
memory. arXiv preprint arXiv:1511.04510, 2015. 2, 3
[26] T. Lin, B. G. Horne, P. Tino, and C. L. Giles. Learning longterm dependencies is not as difficult with narx recurrent neural networks. 1998. 2
[27] C. Liu, J. Yuen, and A. Torralba. Nonparametric scene parsing: Label transfer via dense scene alignment. In Computer

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]

[43]

Vision and Pattern Recognition, 2009. CVPR 2009. IEEE
Conference on, pages 1972–1979. IEEE, 2009. 6
W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking
wider to see better. arXiv preprint arXiv:1506.04579, 2015.
8
Z. Liu, X. Li, P. Luo, C.-C. Loy, and X. Tang. Semantic image segmentation via deep parsing network. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 1377–1385, 2015. 2, 3
J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pages 3431–3440, 2015. 2, 7, 8, 9, 11
M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich. Feedforward semantic segmentation with zoom-out features. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 3376–3385, 2015. 9
H. Noh, S. Hong, and B. Han. Learning deconvolution network for semantic segmentation. In Proceedings of the IEEE
International Conference on Computer Vision, pages 1520–
1528, 2015. 2
R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty
of training recurrent neural networks. ICML (3), 28:1310–
1318, 2013. 2, 3, 4
P. H. Pinheiro and R. Collobert. Recurrent convolutional
neural networks for scene labeling. In ICML, pages 82–90,
2014. 3
S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Advances in neural information processing systems, pages
91–99, 2015. 2
A. Sharma, O. Tuzel, and M.-Y. Liu. Recursive context propagation network for semantic scene labeling. In Advances in
Neural Information Processing Systems, pages 2447–2455,
2014. 9
B. Shuai, Z. Zuo, and G. Wang. Quaddirectional 2d-recurrent
neural networks for image labeling. IEEE Signal Processing
Letters, 22(11):1990–1994, 2015. 3
B. Shuai, Z. Zuo, G. Wang, and B. Wang.
Dagrecurrent neural networks for scene labeling. arXiv preprint
arXiv:1509.00552, 2015. 2, 3, 8
N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor
segmentation and support inference from rgbd images. In
European Conference on Computer Vision, pages 746–760.
Springer, 2012. 6
K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 5, 6, 8
L. Theis and M. Bethge. Generative image modeling using
spatial lstms. In Advances in Neural Information Processing
Systems, pages 1927–1935, 2015. 2, 3
A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu.
Pixel recurrent neural networks.
arXiv preprint
arXiv:1601.06759, 2016. 2, 3
F. Visin, M. Ciccone, A. Romero, K. Kastner, K. Cho,
Y. Bengio, M. Matteucci, and A. Courville. Reseg: A recurrent neural network-based model for semantic segmentation.

[44]

[45]

[46]

[47]

[48]

[49]

In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition Workshops, pages 41–48, 2016. 3,
6
F. Visin, K. Kastner, K. Cho, M. Matteucci, A. Courville,
and Y. Bengio.
Renet: A recurrent neural network
based alternative to convolutional networks. arXiv preprint
arXiv:1505.00393, 2015. 2, 3, 6
J. Wang, Z. Wang, D. Tao, S. See, and G. Wang. Learning
common and specific features for rgb-d semantic segmentation with deconvolutional networks. In European Conference
on Computer Vision, pages 664–679. Springer, 2016. 9
Z. Yan, H. Zhang, Y. Jia, T. Breuel, and Y. Yu. Combining the best of convolutional layers and recurrent layers: A
hybrid network for semantic segmentation. arXiv preprint
arXiv:1603.04871, 2016. 3
J. Yang, B. Price, S. Cohen, and M.-H. Yang. Context driven
scene parsing with attention to rare classes. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3294–3301, 2014. 8
F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122, 2015.
2
S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. H. Torr. Conditional random
fields as recurrent neural networks. In Proceedings of the
IEEE International Conference on Computer Vision, pages
1529–1537, 2015. 2, 3

