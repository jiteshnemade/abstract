introduction  deep learning has greatly increased the ability for computers to perform complex tasks from a wide range of domains   including image recognition  language modeling  game playing and predicting the future  mnih et al.        lecun et al.         wichers et al.       . however  we have an incomplete understanding of exactly how the deep learning models  learn to perform these tasks. gaining a better understanding  of these models is considered to be one of the most important current challenges in artificial intelligence  samek et al.         garcia et al.       . there are many reasons why a  better understanding is important  ranging from increasing       department of informatics  university of oslo  norway  ritmo  university of oslo  norway. correspondence to  kai  olav ellefsen  uio .       the ability to trust machine learning systems  to the benefits  such understanding would have for continued research and  development of algorithms. there are therefore many recent  studies aiming to make deep learning models more understandable and explainable  for both convolutional neural  networks  cnns   yosinski et al.         recurrent neural  networks  rnns   karpathy et al.        and other architectures  smilkov et al.       .  in this paper  we aim to gain a better understanding of one  specific neural network architecture  mixture density rnns   md rnns . md rnns are recurrent neural networks combined with a mixture density network  bishop        bishop    others         such that that the output parametrizes a  mixture of gaussians distribution  figure   . md rnns  are particularly interesting for tasks involving creative prediction  since the recurrent part allows the modeling and  forecasting of sequences  and the gaussian mixture part  allows predictions to be creative  modeling different types  of scenarios in a single neural network  ha   eck       .  md rnns recently gained significant attention  pearson         yao        in ha and schmidhuber s paper on  world  models   ha   schmidhuber         which demonstrated  that md rnns  can learn to predict the future from a large  number of observations of a simulated world. the internal  models learned in ha and schmidhuber s work represent the  agent s world so well that the authors were able to    train  an agent inside its own internal model  or  said differently   inside its own  dream   and    to be the first agent to solve  the car racing environment in openai gym. despite the  impressive results and the wide attention this work has gotten  we do not have a good understanding of how predictive  md rnns model the world.  md rnns make predictions by sampling from a probability distribution with multiple different sub distributions.  we investigate two hypotheses about the role of these subdistributions  mixture components  when md rnns predict  the future. the hypotheses are     different mixture components model different stochastic events and    different  mixture components model different situations with different  rules   that is  different internal models   figure   .  we train world models in a doom game environment  similarly to  ha   schmidhuber         and let them hallucinate  imagined scenarios. from these scenarios  we extract events     how do md rnns predict the future     figure  . left  md rnns model data with probability distributions composed of several components  parametrized by      and  . right   we investigate the roles of individual components to gain a better understanding of how md rnns make predictions. one possibility   illustrated here  is that different mixture components represent situations governed by different rules.    and situations according to our two hypotheses  and then  measure to which degree different events are produced by  different components of the mixture model.  the main contributions of this paper are    a framework  for automatically measuring the tendency for different components of a gaussian mixture model to generate particular  types of prediction  and    new insights into the roles of the  gaussian components of trained md rnns. in particular   we find evidence for both our hypotheses  including a very  clear demonstration of different mixture components selforganizing to serve as internal models for scenarios with  different rules.     . background   . . md rnns  generative machine learning models for content such as  text  images or sound typically model the generated content  with a probability distribution  goodfellow et al.       .  mixture density networks  mdns  are neural networks that  represent mixture density models  mclachlan   basford          that is  probability distributions which are composed  of several sub distributions  several gaussian distributions  in the models applied here   see figure   . mdns can in  principle represent any conditional probability distribution   and are useful when the modeled phenomenon is not well  represented by a simpler distribution. an example  which  we study here  is learning internal models of an environment   where different events may or may not occur  but where the  average over different events is not meaningful. in this case   the multiple sub distributions of a mixture density model  can help model the fact that the world has multiple possible  states which should not be mixed together or averaged.  in practice  a mixture density network  mdn  operates by  transforming the outputs of a neural network to form the parameters of a mixture distribution  bishop         generally  with gaussian models for each mixture component. these    parameters are the centres     and scales     for each gaussian component  as well as a weight     for each component   see figure   . the mdn usually uses an exponential activation function to transform the scale parameters to be positive  and non zero. for training  the probability density function  of the mixture model is used to generate the negative log  likelihood for the loss function. this involves constructing  probability density functions  pdfs  for each gaussian component and categorical distribution from the mixture weights   see appendix section  .  for details . one advantage of an  mdn is that various component distributions can be used  so long as the pdf is tractable  for instance   d  bishop         or  d  graves        gaussian distributions  or  as in  our case  a multivariate gaussian with a diagonal covariance  matrix.  for inference  results are sampled from the mixture distribution. first  the  s are used to form a categorical distribution  by applying the softmax function. a sample is drawn from  this distribution to determine which gaussian component  will provide the output. the index i of the sampled   is used  to select a gaussian distribution  n   i    i     from which a  sample is drawn to provide the outcome. in some cases   it is advantageous to adjust the diversity of sampling  for  instance  to favour unlikely predictions   in which case the  temperature of the categorical distribution can be adjusted in  the typical way  and the covariance matrices of the gaussian  components may be scaled. we refer the these operations  as adjusting    or   temperature respectively.  an mdn can be applied to the outputs of an rnn  forming an md rnn. this approach has been applied to model   d pen data  such as for handwriting  graves        and  sketches  ha   eck        as well as musical performance  martin   torresen       . other applications include parametric speech synthesis  wang et al.         and  identifying salient locations in video data  bazzani et al.        . ha and schmidhuber applied an md rnn model to  model the future state of a video game screen image and as      how do md rnns predict the future     sist an rl agent  ha   schmidhuber       . in the present  research  we delve into this application to understand what  such a model learns about the virtual worlds and how this  information is represented.    input  sequence of observations    ot    ot       vaeencoder    vaeencoder     . . predicting the future with deep neural networks  progress in deep learning has recently made it possible to  learn to predict future frames of video from observing sequences of video frames  finn et al.        mathieu et al.        . however  most approaches for predicting future visual input from pixels have typically only had the ability  to predict a few frames into the future before predicted images get blurry or static. recently  techniques have been  developed that attempt to mitigate this limitation by first  encoding frames into a compact  high level representation   then predicting how this compact representation develops  over time. finally  decoding the predicted compact representation produces a predicted future image. compared to  predictions made directly in pixel space  such high level  predictions degrade less quickly  demonstrating good prediction performance many seconds into the future  villegas  et al.        wichers et al.        ha   schmidhuber       .  in ha and schmidhuber s recurrent world model  ha    schmidhuber         the predictive model consists of two  components     a visual component  v   which learns an  encoding decoding between a visual scene and a compact  representation and    a memory component  m   which  learns how the compact representation develops over time   figure   . the first component is learned by a variational  autoencoder  vae  kingma   welling          by presenting it with a large collection of pictures from the visual  scene. the second is learned by an md rnn. this world  model was demonstrated to be able to predict many frames  into the future  and in fact to  dream  whole episodes of  agent experience. it is  however  not clear what role the  different mixture components in the md rnn play in predicting the future.   . . architectures for multiple internal models  one of our hypotheses suggests that the different gaussian  components learn to model different situations with different   rules   that is  situations where predictions need to be so  different that they are best modeled separately. humans  show a remarkable ability to learn internal models  mental  simulations  of a wide range of different situations  objects  and people  without a high degree of conflict or interference  between them.  one theory suggests that this ability is facilitated by the  modular organization of our central nervous system. neural  modularity may be a key to allow multiple internal models to  coexist  enabling the selection of the appropriate actions for  the current context  ghahramani   wolpert        wolpert    zt  md rnn    ht    p  z  t         z t     md rnn    h t       p  z  t         sampling    sampling     z t         z t        vaedecoder    vaedecoder    o  t       o  t       output  sequence of predictions  figure  . world model predicting future frames by combining a  variational autoencoder and an md rnn. we follow the architecture suggested in  ha   schmidhuber       .    et al.       . computational models built around this idea  have indeed demonstrated the ability to learn and maintain  multiple internal models  and select the appropriate model  for a given context  wolpert   kawato        haruno et al.         demiris   khadhouri       . these models work by  dividing learning experiences into multiple modules  where  different modules compete to represent different situations.  after a number of learning episodes  this causes different  modules to specialize at representing different internal models  allowing the system to model situations with different  rules  with minimal interference. our hypothesis suggests  that the gaussian components of the mdn self organize  to perform a similar task  allowing scenarios with different  rules to be modeled with little interference.     . methods   . . world model md rnn  ha and schmidhuber s world model  ha   schmidhuber         combines an md rnn and a vae to predict future  states of a video game screen  figure   . by training the  vae to compress representations of visual scenes  the mdrnn has a more manageable job of predicting how scenes  unfold in the future.     how do md rnns predict the future     training the model happens in two steps  first  the vae  is trained on examples of images from the environment  in which we wish to learn to make predictions. the vae  compresses each image    x   pixels  with   color channels in our setup  into a latent vector  z     floating point  numbers in our case . it then attempts to reconstruct the  same image from the latent vector. the vae is trained to  both reconstruct images as well as possible  and to keep  the representations of similar inputs close together in latent  space  details are found in appendix section  .  . this  allows small changes to the latent vector to give meaningful  changes in the compressed images.  after the vae has learned to compress images of the world  into latent vectors  the md rnn can be trained on sequences of latent vectors. we follow  ha   schmidhuber        in applying a single layer lstm  hochreiter    schmidhuber         trained by seeing examples of sequences of images as input  and the same sequence  shifted  by one time step  as outputs. thereby  the lstm learns to  predict the next latent vector from a sequence of previous  observations. more details on the world model md rnn  are found in the appendix section  . .   . . . data collection and training  the data collection and training process follows  ha    schmidhuber         except we do not train a controller   since we are here analyzing predictions  and not using them  for agent control. the process can be summarized in the  following steps  more details in appendix section    .    rectangular room  where a player is facing monsters on an  opposite wall. monsters will fire exploding fireballs at the  player  and the player attempts to survive as long as possible  by moving left and right  dodging the incoming projectiles.  agents receive  d images of the scene ahead of them as  input  and make only one decision at each timestep  move  to the left  move to the right or stay in the same place.  this scenario serves as a useful test for our hypotheses  since  it has both stochastic events  e.g.  monsters may or may  not launch fireballs  and different situations governed by  different rules  an exploding fireball behaves very differently  than an incoming fireball mid air .   . . measuring how md rnns make predictions  after training md rnns  we analyze the predictions they  make when  dreaming  about the future. we insert an initial  latent vector  representing the real initial state from the  game  into the md rnn  and then repeat the steps below  for as long as we want to predict  figure   illustrates the  steps  following the same numbering as the list     . produce a probability distribution over the next latent  vector  p  z t    at   z t   ht   parametrized by the mdnparameters       and  . store    the vector indicating  the weight of each gaussian component .   . sample a latent vector z t   from the probability distribution  and decode it into a predicted frame with the  vae.     . train a vae to encode each frame into a length     latent vector z  and to decode z back to the same image.     . analyze the predicted frame to measure which events  are depicted  see below . store the list of events for  the current frame together with  . together  these can  tell us whether different mixture components generate  different events.     . generate latent vectors z for each frame from the simulated episodes. further training can now be done  without the actual images.     . repeat the process  starting from point    with the  sampled z t   as the rnn input  to predict the next  latent vector p  z t    at     z t     ht         . simulate       episodes with a random policy. store  all actions taken and frames observed.     . train an md rnn to model p  zt    at   zt   ht    that  is  the probability distribution for next latent vector   given the current latent vector and action  as well as  the rnn s hidden state.   . . training scenario  we follow  ha   schmidhuber        in training the predictive mdn rnns to model the vizdoom  kempka et al.         take cover scenario  . this scenario takes place in a     experiment code is available at  http   doi.org   .       zenodo.            https   gym.openai.com envs   doomtakecover v      every round through this process generates a new predicted  latent vector  which is next used as input to predict the  latent vector following it. storing latent vectors and mdrnn parameters allows us to subsequently analyze the way  the md rnn has learned to represent the world and make  predictions about it.   . . . m easuring events in predicted frames  our hypotheses suggest that different mixture components  represent either different stochastic events  or different situations where different rules apply. in the world we make  predictions about  we identify two stochastic events      monsters may appear  and    they may launch a fireball     how do md rnns predict the future     figure  . our proposed framework for analyzing how md rnns make predictions    towards the player. note that monsters never disappear  in the modeled world  and fireballs disappearing is not a  stochastic event  since once a fireball has been fired  it will  deterministically disappear after reaching the other end of  the room.  for our second hypothesis  we identify three situations  where the rules for how frames evolve in a time sequence  are very different     the normal situation  player is facing  monsters  who sometimes launch fireballs      an explosion takes place in front of the player and    the player is  next to a wall. situation   and   are so different from the  normal situation that internal models of the three different  situations could benefit from some separation. explosions  cover a large portion of the screen  and unfold according to  a specific sequence  which has little to do with the way a  normal scene unfolds  see figure   . walls next to the agent  result in unique dynamics  since they require a large portion  of the screen to move sideways  in the opposite direction   as the player moves.  since we are dealing with a quite simple and limited world   we can measure events from frames with straightforward  image processing methods from the python package scikitimage  . the methods we apply to measure the presence of  monsters  fireballs  walls and explosions are documented in  appendix section    and also made available online  .          https   scikit image.org   http   doi.org   .     zenodo.            . results  as previously discussed  we have two main hypotheses  about the roles of different mixture components in the mdrnn     different components learn to model different  possible futures  allowing them to creatively sample what  will happen next  and    different components learn to form  different internal models of the environment  that is  they  specialize to model situations governed by a specific set of  rules. below  we analyze md rnn predictions along with  the weights of mixture components to shed light on these  hypotheses.   . . common parameters  in our main experiments  we test   independently trained  md rnns  all with the same architecture  see appendix  section     to reduce the chance that results are specific to  one trained model. in practice  we found results to be very  similar when training the same model multiple times with  shuffled data. for all five  we generate multiple  dreams  by  predicting future latent vectors  and feeding each prediction  in as the input vector to the rnn for the next time step   along with a randomly sampled action. this allows us  to dream up long prediction sequences which  although  not always realistic  illuminate how mixture components  relate to predicted events. in our main experiments  we  generate    dreams for each of our   models  each dream       time steps long. tests of statistical significance apply  the mann whitney u test.     how do md rnns predict the future      . . . s tochastic events  our first hypothesis suggests that different mixture components represent different stochastic events  allowing creative  predictions about the future by sampling from different  gaussian components. we test this hypothesis by dreaming  up many different futures as described above  and measuring     different stochastic events in the dreams and    the weight  assigned to each component in the mixture model. as mentioned above  there are two different stochastic events in this  scenario  fireballs appearing and monsters appearing.  to confidently say that a specific mixture component is  particularly responsible for producing one event  we need  to measure whether that component has produced the event  more frequently than one would expect if events were evenly  distributed among components. for instance  if we find  that one component is responsible for     of the fireball  appearances  but that component is also responsible for      of all generated frames  then we do not have any clear  evidence. we therefore measure the relationship between  components and events as follows     proportion of frames  from main component     . . analyzing frames produced in prediction  sequences     .     current event  any event            .      .      .      .     fireball  appears    monster  appears    generated event    figure  . the tendency for different stochastic events to be produced by one specific gaussian component  blue  vs the tendency for that component to be responsible for events overall   orange .       indicates significant differences with p    .   .     . produce    different dreams with each of the   trained  md rnns  resulting in a total of    dreams.   . for each time step of a dream  measure a  the presence  of the events described above  and b  which component  is currently the most active  the one with the highest    value output by the md rnn .   . within one dream  the component that produced an  event most frequently  is denoted as the  main component  for that event. this is the gaussian that is most  likely responsible for generating the given event.   . measuring the proportion of the event produced by the   main component  versus the other components across  all n dreams yields the leftmost boxes in the pairs in  figure  .   . to be sure the  main component  is specifically responsible for the specific event situation  we also measure  the proportion of all frames produced by that component. this yields the rightmost boxes.   . a significantly higher value in the leftmost than the  rightmost box thus indicates that one component is  producing the relevant event situation more frequently  than one would expect by looking at the proportion of  all events generated by that component.  as we can see in figure    there is a strong tendency for  fireball appearances to be produced more by one specific    figure  . top  a monster launching a fireball at the player. bottom   an explosion unfolding in front of the player. the two situations  are governed by very different rules. images down sampled to the  same resolution    x    used during training.    component. there is no similar tendency for monster appearances.   . . . d ifferent internal models  our second hypothesis is that different mixture components  represent different internal models  that is  models of scenarios where the rules are different. to study this  we repeated  the calculations outlined above  measuring the presence of  such scenarios  rather than stochastic events. as discussed  above  we identify   scenarios in this game where the rules  of how to generate the next frame are very different from  the normal situation  facing monsters and any fireballs       having a wall on the left     having a wall on the right and     getting hit by an exploding fireball.     the result of this calculation is shown in figure  . there  are statistically significant differences  p    .     between  the main component s tendency to generate the specific situations and their tendency to generate frames overall  for  explosions and walls on either side. we also show that the  same effect is not generally present for situations containing  fireballs. we hypothesize that this is because fireballs are  very common  and do not drastically change the way the  world changes from one frame to the next. there should  therefore be less need for modeling them in a separate mixture component.    component weight   i     how do md rnns predict the future      .    .    .    .    .    .        proportion of frames  from main component                                                 timestep            .      a  the weights of the   different mixture components   i for i                     during a     timestep dream.     .    .    .    .   current event  any event     .   explosion    left wall    right wall    fireball    generated event    figure  . the tendency for different scenarios to be produced by  one specific gaussian component  blue  vs the tendency for that  component to be responsible for events overall  orange .       indicates significant differences with p    .   .     . . plotting component weights and dreams  to further illuminate the role of different mixture components  we let a single trained md rnn dream up    timestep predictions  while plotting the weights of all components  to see which one is currently most responsible for  making predictions. an example of such a plot is shown in  figure  . notice one specific mixture component dominates  from around timestep     the same time that an explosion is  present in the frame. in other repetitions of the experiment   we found a different component tends to dominate when  the agent is near a wall. in  normal  situations  no nearby  walls or explosions   we tend to see different components  being active together  without any clear dominance. this  supports our second hypothesis  that different components  specialize to model scenarios where the rules for generating  future frames are different.   . . committing to one mixture  as a final test of the role of different mixture components in  making predictions  we generated dreams while committing  to a single component during an entire dream. we condi      b  the resulting dream generated by sampling according to the  mixture weights in  a . numbers indicate the current timestep.  figure  . the weights over time of each of the   mixture components output by the md rnn and the corresponding dreams  produced by sampling according to those weights. until timestep      several components are similarly weighted  and responsible for  making predictions together. after timestep     one component  dominates  and takes over in generating predictions. the resulting  prediction is an exploding fireball.     how do md rnns predict the future     tioned the md rnn with a random start image  and made a  dream predicting      steps into the future  sampling only  from the first mixture component. we then repeated this for  each of the five mixture components in the md rnn. since  different trained models may not represent the same events  in the exact same mixture components  we base this analysis  on ten      timestep dreams for each component  from a  single trained model.  the results are shown in figure  . there is a clear tendency  for this model to generate explosions with the second mixture component  and walls  both right and left  with the fifth  component. visualizing the conditional dreams  we observe  something interesting  the components that do not produce  explosions result in dreams where fireballs approach the  player  but stop and hover mid air  or even reverse and return to the monsters. presumably  these components have  never learned to model explosions  and can therefore not  produce them when being responsible for generating dreams  alone.    total frame count            gaussian                                            explosion    fireball    left wall    right wall    generated event    figure  . the events generated in dreams  when committing to a  single gaussian component during the entire prediction sequence.     . discussion  the results give support for both our initial hypotheses  the  different gaussian components in the md rnn specialize  to model both different stochastic events  figure    and different internal models  figure   . for the stochastic events   we saw a strong tendency for fireball appearances to be  generated more frequently by a specific mixture component   but the same was not true for monster appearances. detecting monsters in the generated predictions is more difficult  than the other elements we detect  see appendix section      and we cannot rule out that a relationship exists here that  we could have measured if monster appearances were less  ambiguous.  for the second hypothesis  we observed very clear evidence    that situations where the rules governing predictions are  different were produced by separate gaussian components.  this was seen clearly both when measuring which components were most active in the different situations  figures    and     and when sampling from a single component during  an entire prediction sequence  figure   .     . conclusion  through automatic classification of predicted frames  we  have shed light on the way mixture density rnns predict  the future. we started out with two hypotheses for the role  of the different components in the gaussian mixture models  and found some evidence in support of both. first  we  found evidence that different components produce different  stochastic events more frequently  supporting the hypothesis  that different components of the mixture models represent  different potential directions for the predicted future. this  is a valuable property for systems modeling creative predictions  such as in generation of artistic text  music and  images   since it allows them to strike a balance between  modeling an observed phenomenon and improvising by  choosing between several possible predicted futures.  we found even more solid evidence for our second hypothesis  there is a very strong tendency for different components  of the mixture model to be responsible for producing events  that are governed by different  rules   that is  events that  require different internal models. building machine learning systems that can represent different internal models is  a long standing challenge  since learning of very different  skills tends to cause interference or forgetting  ellefsen et al.        . one way this challenge has been handled in the past   is by building modular systems  where different modules  compete to represent different internal models  demiris    khadhouri        haruno et al.       . our results suggest that mixture density rnns self organize to separate  different internal models into different components.  this ability of md rnns opens up a further hypothesis   since these networks can automatically self organize multiple internal models  they should be well equipped to model  different scenarios with a low degree of interference. in  future studies  we plan to examine this further by training  md rnns on multiple different environments  studying  the effect of the number of components in the mdn on the  observed interference.     . acknowledgments  this work is partially supported by the research council  of norway as a part of the engineering predictability with  embodied cognition  epec  project  under grant agreement        .     how do md rnns predict the future     