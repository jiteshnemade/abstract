introduction    recurrent neural networks  rnns  are the dominant  model class for high dimensional  regularly sampled time  series data  such as text or speech. however  they are an  awkward fit for irregularly sampled time series data  common in medical or business settings. a standard trick for  applying rnns to irregular time series is to divide the  timeline into equally sized intervals  and impute or aggregate observations using averages. such preprocessing  destroys information  particularly about the timing of  measurements  which can be informative about latent  variables  lipton et al.        che et al.       .    standard rnn    rnn decay    neural ode    ode rnn  an approach which better matches reality is to construct  a continuous time model with a latent state defined at all  times. recently  steps have been taken in this direction   time  defining rnns with continuous dynamics given by a simple exponential decay between observations  che et al.   figure    hidden state trajectories. ver      cao et al.        rajkomar et al.        mei and  tical lines show observation times. lines  eisner       .  show different dimensions of the hidden  we generalize state transitions in rnns to continuous  state. standard rnns have constant or  time dynamics specified by a neural network  as in neural undefined hidden states between observaodes  chen et al.       . we call this model the ode  tions. the rnn decay model has states  rnn  and use it to contruct two distinct continuous time which exponentially decay towards zero   models. first  we use it as a standalone autoregressive and are updated at observations. states  model. second  we refine the latent ode model of chen of neural ode follow a complex trajecet al.        by using the ode rnn as a recognition tory but are determined by the initial state.  network. latent odes define a generative process over the ode rnn model has states which  time series based on the deterministic evolution of an obey an ode between observations  and  initial latent state  and can be trained as a variational are also updated at observations.  autoencoder  kingma and welling       . both models  preprint. under review.     naturally handle time gaps between observations  and remove the need to group observations into  equally timed bins. we compare ode models to several rnn variants and find that ode rnns can  perform better when the data is sparse. since the absence of observations itself can be informative   we further augment latent odes to jointly model times of observations using a poisson process.         background    recurrent neural networks a simple way to handle irregularly timed samples is to include the  time gap between observations  t   ti   ti   into the update function of the rnn   hi   rnncell hi      t   xi         however  this approach raises the question of how to define the hidden state h between observations.  a simple alternative introduces an exponential decay of the hidden state towards zero when no  observations are made  che et al.        cao et al.        rajkomar et al.        mozer et al.          hi   rnncell hi     exp     t    xi         where   is a decay rate parameter. however  mozer et al.        found that empirically  exponentialdecay dynamics did not improve predictive performance over standard rnn approaches.  neural ordinary differential equations neural odes  chen et al.        are a family of  continuous time models which define a hidden state h t  to be the solution to an ode initial value  problem  ivp    dh t     f   h t   t  where h t      h        dt  in which the function f  specifies the dynamics of the hidden state  using a neural network with  parameters  . the hidden state h t  is defined at all times  and can be evaluated at any desired times  using a numerical ode solver   h    . . .   hn   odesolve f    h     t    . . .   tn          chen et al.        showed how adjoint sensitivities can be used to compute memory efficient gradients  w.r.t.    allowing black box ode solvers to be used as a building block in larger models. they also  conducted toy experiments in a time series model in which the latent state follows a neural ode.  chen et al.        used time invariant dynamics in their time series model  dh t  dt   f   h t     and  we follow the same approach  but adding time dependence would be straightforward if necessary.         method    in this section  we use neural odes to define two distinct families of continuous time models  the  autoregressive ode rnn  and the variational autoencoder based latent ode.   .     constructing an ode rnn hybrid    following mozer et al.         we note that an rnn with exponentially decayed hidden state  implicitly obeys the following ode dh t   dt      h with h t      h    where   is a parameter of the  model. the solution to this ode is the pre update term h    exp     t   in    . this differential  equation is time invariant  and assumes that the stationary point  i.e. zero valued state  is special. we  can generalize this approach and model the hidden state using a neural ode. the resulting algorithm  is given in algorithm  . we define the state between observations to be the solution to an ode   h i   odesolve f    hi      ti     ti    and then at each observation  update the hidden state using a  standard rnn update hi   rnncell h i   xi  . our model does not explicitly depend on t or  t when  updating the hidden state  but does depend on time implicitly through the resulting dynamical system.  compared to rnns with exponential decay  our approach allows a more flexible parameterization of  the dynamics. a comparison between the state dynamics of these models is given in table  .  autoregressive modeling with the ode rnn the ode rnn can straightforwardly be used  n  to probabilistically model sequences. consider a series of observations  xi  n  i   at times  ti  i   .  autoregressive models make a one step ahead  prediction conditioned on the history of observations   q  i.e. they factor the joint density p x    i p   xi  xi     . . .   x   . as in standard rnns  we can use  an ode rnn to specify the conditional distributions p   xi  xi   ...x     algorithm   .        algorithm   the ode rnn. the only difference  highlighted in blue  from standard rnns is that  the pre activations h  evolve according to an ode between observations  instead of being fixed.  input  data points and their timestamps   xi   ti   i  ..n  h       for i in       . . .   n do  . solve ode to get state at ti  h i   odesolve f    hi      ti     ti     hi   rnncell h i   xi    . update hidden state given current observation xi  end for  oi   outputnn hi   for all i    ..n  return   oi  i  ..n   hn   .     latent odes  a latent variable construction    autoregressive models such as rnns and the ode rnn presented above are easy to train and allow  fast online predictions. however  autoregressive models can be hard to interpret  since their update  function combines both their model of system dynamics  and of conditioning on new observations.  furthermore  their hidden state does not explicitly encode uncertainty about the state of the true  system. in terms of predictive accuracy  autoregressive models are often sufficient for densely sampled  data  but perform worse when observations are sparse.  an alternative to autoregressive models are latent variable models. for example  chen et al.         proposed a latent variable time series model  where the generative model is defined by ode whose  initial latent state z  determines the entire trajectory   z    p z     z    z    . . .   zn   odesolve f    z     t    t    . . .   tn     each xi    indep.         p xi  zi      i         . . .   n                     we follow chen et al.        in using a varia  encoder decoder models  encoder decoder  tional autoencoder framework for both training  ode rnn  ode  and prediction. this requires estimating the ap  latent ode  ode enc.   latent ode  rnn enc.   rnn  ode  n  proximate posterior q z    xi   ti  i    . inference rnn vae  rnn  rnn  and prediction in this model is effectively an  encoder decoder or sequence to sequence archi  table    different encoder decoder architectures.  tecture  in which a variable length sequence is  encoded into a fixed dimensional embedding  which is then decoded into another variable length  sequence  as in sutskever et al.       .  chen et al.        used an rnn as a recognition network to compute this approximate posterior.  we conjecture that using an ode rnn as defined above for the recognition network would be a  more effective parameterization when the datapoints are irregularly sampled. thus  we propose using  an ode rnn as the encoder for a latent ode model  resulting in a fully ode based sequence tosequence model. in our approach  the mean and standard deviation of the approximate posterior  q z    xi   ti  n  i     are a function of the final hidden state of an ode rnn   n  q z    xi   ti  n  i       n   z     z    where  z     z    g ode rnn    xi   ti  i                where g is a neural network translating the final hidden state of the ode rnn encoder into the mean  and variance of z  . to get the approximate posterior at time point t    we run the ode rnn encoder  backwards in time from tn to t  . we jointly train both the encoder and decoder by maximizing the  table    definition of hidden state h t  between  observation times ti   and ti in autoregressive  models. in standard rnns  the hidden state does  not change between updates. in ode rnns   the hidden state is defined by an ode  and is  additionally updated by another network at each  observation.    model  standard rnn  rnn decay  gru d  ode rnn         state h ti   between observations  hti    hti   e    t  hti   e    t  odesolve f    hi      ti     t       gru    gru gru    gru    q z   x  ..xn           ode         z     ode solve f  z     t  ..tn       z     zi    zn    ode    xn  tn    xi    x   t     x     x i    x      t     t     x    t     x n  tn    figure    the latent ode model with an ode rnn encoder. to make predictions in this model  the  ode rnn encoder is run backwards in time to produce an approximate posterior over the initial  state  q z    xi   ti  n  i    . given a sample of z    we can find the latent state at any point of interest by  solving an ode initial value problem. figure adapted from chen et al.       .  evidence lower bound  elbo            elbo         ez   q   z    xi  ti  n   log p   x    . . .   xn       kl q   z    xi   ti  n       i      p z     i        this latent variable framework comes with several benefits  first  it explicitly decouples the dynamics  of the system  ode   the likelihood of observations  and the recognition model  allowing each to be  examined or specified on its own. second  the posterior distribution over latent states provides an  explicit measure of uncertainty  which is not available in standard rnns and ode rnns. finally  it  becomes easier to answer non standard queries  such as making predictions backwards in time  or  conditioning on a subset of observations.  poisson process likelihoods    tstart    i      where tstart and tend are the times at which observations started and  stopped being recorded.    we augment the latent ode framework with a poisson process over  the observation times  where we parameterize   t  as a function  of z t . this means that instead of specifying and maximizing  the conditional marginal likelihood p x    . . .   xn  t    . . .   tn       we  can instead specify and maximizing the joint marginal likelihood  p x    . . .   xn   t    . . .   tn      . to compute the joint likelihood  we  can evaluate the poisson intensity   t   precisely estimate its integral   and the compute latent states at all required time points  using a  single call to an ode solver.    diastolic arterial blood pressure  inferred rate    the fact that a measurement was made at a particular time is often  informative about the state of the system  che et al.       . in the  ode framework  we can use the continuous latent state to parameterize the intensity of events using aninhomogeneous poisson point  process  palm        where the event rate   t  changes over time.  poisson point processes have the following log likelihood   z tend  n  x  log p t    . . .   tn  tstart   tend            log   ti        t dt                                            time  hours           partial pressure of arterial o   inferred rate     .                                          time  hours           figure    visualization of  the inferred poisson rate   t    green line  for two selected  features of different patients  from the physionet dataset.  vertical lines mark observation times.    mei and eisner        used a similar approach  but relied on a fixed time discretization to estimate  the poisson intensity. chen et al.        showed a toy example of using latent odes with a poisson  process likelihood to fit latent dynamics from observation times alone. in section  .   we incorporate  a poisson process likelihood into a latent ode to model observation rates in medical data.   .     batching and computational complexity    one computational difficulty that arises from irregularly sampled data is that observation times can  be different for each time series in a minibatch. in order to solve all odes in a minibatch in sync  we  must we must output the solution of the combined ode at the union of all time points in the batch.        taking the union of time points does not substantially hurt the runtime of the ode solver  as the  adaptive time stepping in ode solvers is not sensitive to the number of time points  t  ...tn   at which  the solver outputs the state. instead  it depends on the length on the time interval  t    tn   and the  complexity of the dynamics.  see suppl. figure   . thus  ode rnns and latent odes have a similar  asymptotic time complexity to standard rnn models. however  as the ode must be continuously  solved even when no observations occur  the compute cost does not scale with the sparsity of the data   as it does in decay rnns. in our experiments  we found that the ode rnn takes     more time  than the standard gru to evaluate  and the latent ode required roughly twice the amount of time to  evaluate than the ode rnn.   .     when should you use an ode based model over a standard rnn     standard rnns are ignore the time gaps between points. as such  standard rnns work well on  regularly spaced data  with few missing values  or when the time intervals between points are short.  models with continuous time latent state  such as the ode rnn or rnn decay  can be evaluated  at any desired time point  and therefore are suitable for interpolation tasks. in these models  the  future hidden states depend on the time since the last observation  also making them better suited  for sparse and or irregular data than standard rnns. rnn decay enforces that the hidden state  converges monontically to a fixed point over time. in ode rnns the form of the dynamics between  the observations is learned rather than pre defined. thus  ode rnns can be used on sparse and or  irregular data without making strong assumptions about the dynamics of the time series.  latent variable models versus autoregressive  models we refer to models which iteratively comq  pute the joint distribution p x    i p   xi  xi    r. .q  .   x    as autoregressive models  e.g. rnns and  ode rnns . we call models of the form p x     i p xi  z   p z   dz  latent variable models  e.g.  latent odes and rnn vaes .  in autoregressive models  both the dynamics and the conditioning on data are encoded implicitly  through the hidden state updates  which makes them hard to interpret. in contrast  encoder decoder  models  latent ode and rnn vae  represent state explicitly through a vector zt   and represent  dynamics explicitly through a generative model. latent states in these models can be used to compare  different time series  for e.g. clustering or classification tasks  and their dynamics functions can be  examined to identify the types of dynamics present in the dataset.         experiments     .     toy dataset    we tested our model on a toy dataset of       periodic trajectories with variable frequency and the  same amplitude. we sampled the initial point from a standard gaussian  and added gaussian noise  to the observations. each trajectory has     irregularly sampled time points. during training  we  subsample a fixed number of points at random  and attempt to reconstruct the full set of     points.       observed points       observed points       observed points   .          x         x    x    x          .    .               time                   time                   time     a  conditioning on increasing number of observations          .      .      .     time     .      .      b  prior samples    figure     a  a latent ode model conditioned on a small subset of points. this model  trained  on exactly    observations per time series  still correctly extrapolates when more observations are  provided.  b  trajectories sampled from the prior p z      normal z       i of the trained model   then decoded into observation space.        conditioning on sparse data latent odes can often reconstruct trajectories reasonably well given  a small subset of points  and provide an estimate of uncertainty over both the latent trajectories and  predicted observations. to demonstrate this  we trained a latent ode model to reconstruct the full  trajectory      points  from a subset of    points. at test time  we conditioned this model on a subset  of        or    points. conditioning on more points results in a better fit as well as smaller variance  across the generated trajectories  fig.   . figure   b  demonstrates that the trajectories sampled from  the prior of the trained model are also periodic.  extrapolation next  we show that a time invariant ode can recover stationary periodic dynamics  from data automatically. figure   shows a latent ode trained to condition on    points in the       .   interval  red area  and predict points on   .      interval  blue area . a latent ode with  an ode rnn encoder was able to extrapolate the time series far beyond the training interval and  maintain periodic dynamics. in contrast  a latent ode trained with rnn encoder as in chen et al.         did not extrapolate the periodic dynamics well.   b  latent ode with ode rnn encoder     a  latent ode with rnn encoder     .     x    x             .    .                     time                                      time                      figure     a  approximate posterior samples from a latent ode trained with an rnn recognition  network  as in chen et al.       .  b  approximate posterior samples from a latent ode trained with  an ode rnn recognition network  ours . at training time  the latent ode conditions on points in  red area  and reconstruct points in blue area. at test time  we condition the model on    points in red  area  and solve the generative ode on a larger time interval.   .     quantitative evaluation    we evaluate the models quantitavely on two tasks  interpolation and extrapolation. on each dataset   we used     for training and     for test. see the supplement a detailed description.  baselines in the class of autoregressive models  we compare ode rnns to standard rnns. we  compared the following autoregressive models      ode rnn  proposed      a classic rnn where   t is concatenated to the input  rnn  t       an rnn with exponential decay on the hidden  states h   e    t  rnn decay      an rnn with missing values imputed by a weighted average of  previous value and empirical mean  rnn impute   and     gru d  che et al.        which combines  exponential decay and the above imputation strategy. among encoder decoder models  we compare  the latent ode to a variational autoencoder in which both the encoder and decoder are recurrent  neural nets  rnn vae . the ode rnn can use any hidden state update formula for the rnncell  function in algorithm  . throughout our experiments  we use the gated recurrent unit  gru   cho  et al.       . see the supplement for the architecture details.  interpolation the standard rnn and the ode rnn are straightforward to apply to the interpolation task. to perform interpolation with a latent ode  we encode the time series backwards in time   compute the approximate posterior q z    xi   ti  n  i     at the first time point t    sample the initial state  of ode z    and generate mean observations at each observation time.  extrapolation in the extrapolation setting  we use the standard rnn or ode rnn trained on the  interpolation task  and then extrapolate the sequence by re feeding previous predictions. to encourage  extrapolation  we used scheduled sampling  bengio et al.         feeding previous predictions instead  of observed data with probability  .  during training. one might expect that directly optimizing for  extrapolation would perform best at extrapolation. such a model would resemble an encoder decoder  model  which we consider separately below  the rnn vae . for extrapolation in encoder decoder  models  including the latent ode  we split the timeline in half. we encode the observations in the  first half forward in time and reconstruct the second half.         .     mujoco physics simulation    next  we demonstrated that ode based models can learn an approximation to simple newtonian  physics. to show this  we created a physical simulation using the  hopper  model from the deepmind  control suite  tassa et al.       . we randomly sampled the initial position of the hopper and initial  velocities such that hopper rotates in the air and falls on the ground  figure   . these trajectories are  deterministic functions of their initial states  which matches the assumptions made by the latent ode.  the dataset is    dimensional  and we model it with a    dimensional latent state. we generated         sequences of     regularly sampled time points each.  we perform both interpolation and extrapolation tasks on the mujoco dataset. during training  we  subsampled a small percentage of time points to simulate sparse observation times. for evaluation   we measured the mean squared error  mse  on the full time series.    enc dec autoreg    table    test mean squared error  mse           on the mujoco dataset.  model    interpolation    observed pts.                         extrapolation    observed pts.                         rnn  t  rnn gru d  ode rnn  ours      .      .      .        .      .      .        .      .      .        .      .      .        .       .       .        .       .       .        .       .       .         .      .       .       rnn vae  latent ode  rnn enc.   latent ode  ode enc  ours      .      .      .        .      .      .        .      .      .        .      .      .        .      .      .        .      .      .        .      .      .        .      .      .       table   shows mean squared error for models trained on different percentages of observed points.  latent odes outperformed standard rnn vaes on both interpolation and extrapolation. our odernn model also outperforms standard rnns on the interpolation task. the gap in performance  between rnn and ode rnn increases with sparser data. notably  the latent ode  an encoderdecoder model  shows better performance than the ode rnn  an autoregressive model .  all autoregressive models performed poorly at extrapolation. this is expected  as they were only  trained for one step ahead prediction  although standard rnns performed better than ode rnns.  latent odes outperformed rnn vaes on the extrapolation task.  interpretability of the latent state figure   shows how the norm of the latent state time derivative  f   z  changes with time for two reconstructed mujoco trajectories. when the hopper hits the ground   there is a spike in the norm of the ode function. in contrast  when the hopper is lying on the ground   the norm of the dynamics is small.  figure   shows the entropy of the approximate posterior q z    xi   ti  n  i     of a trained model conditioned on different numbers of observations. the average entropy  uncertainty  monotonically  decreases as more points are observed. figure   shows the latent state z  projected to  d using  umap  mcinnes et al.       . the latent state corresponds closely to the physical parameters of the  true simulation that most strongly determine the future trajectory of the hopper  distance from the  ground  initial velocity on z axis  and relative position of the leg of the hopper.  truth  latent  ode    f  z      ode      h     rnn   time    time    figure    top row  true trajectories from mujoco dataset. second row  trajectories reconstructed  by a latent ode model. third row  norm of the dynamics function f  in the latent space of the latent  ode model. fourth row  norm of the hidden state of a rnn trained on the same dataset.                                                 number of time points    figure    entropy of the approximate posterior over z  versus number of observed time points. the  line shows the mean  shaded area  shows     and     percentiles estimated over      trajectories         h                     .      .               h     h     h z       .                                 h    a  height                                     h    b  velocity                             h    c  hip position         figure    nonlinear projection of latent space of z  from a  latent ode model trained on the mujoco dataset . each point  is the encoding of one time series. the points are colored  by the  a  initial height  distance from the ground   b  initial  velocity in z axis  c  relative initial position of the hip of the  hopper. the latent state corresponds closely to the physical  parameters of the true simulation.    physionet    we evaluated our model on the physionet challenge      dataset  silva et al.         which contains       time series  each containing measurements from the first    hours of a different patient s  admission to icu. measurements were made at irregular times  and of varying sparse subsets of the     possible features.  k  lactate  mg  map  mechvent  na  nidiasabp   .      .       .       .       .   time  hours       .       .       .       .     figure    observation times of a subset of features for one patient in the physionet dataset. black  lines indicate observation times  whose number and timing vary across patients.  most existing approaches to modeling this data use a coarse discretization of the aggregated measurements per hour  che et al.         which forces the model to train on only one twentieth of  measurements. in contrast  our approach  in principle  does not require any discretization or aggregation of measurements. to speed up training  we rounded the observation times to the nearest minute   reducing the number of measurements only   fold. hence  there are still              possible  measurement times per time series under our model s preprocessing  while the previous standard  was to used only    possible measurement times. we used    latent dimensions in the latent ode  generative model. see supplement for more details on hyperparameters.  tables   and   report mean squared error averaged over runs with different random seeds  and their  standard deviations. we run one sided t test to establish a statistical significance. best models are  marked in bold. ode based models have smaller mean squared error than rnn baselines on this  dataset.  finally  we constructed binary classifiers based on each model type to predict in hospital mortality.  we passed the hidden state at the last measured time point into a two layer binary classifier. due  to class imbalance    .    samples with positive label   we report test area under curve  auc   instead of accuracy. table   shows that the ode rnn  latent ode and gru d achieved the similar  classification auc. a possible explanation is that modelling dynamics between time points does not  make a difference for binary classification of the full time series.  we also included a poisson process likelihood on observation times  jointly trained with the latent  ode model. figure   shows the inferred measurement rate on a patient from the dataset. although  the poisson process was able to model observation times reasonably well  including this likelihood  term did not improve classification accuracy.        table    test mse  mean   std  on  physionet. autoregressive models.  model  interp           rnn  t  rnn impute  rnn decay  rnn gru d  ode rnn  ours      .       .      .       .      .       .      .       .      .       .       table    test mse  mean   std  on physionet.  encoder decoder models.  model  interp          extrap           rnn vae  latent ode  rnn enc.   latent ode  ode enc   latent ode   poisson    table    per sequence classification.  auc on physionet.  method  auc  rnn  t  rnn impute  rnn decay  rnn gru d  rnn vae  latent ode  rnn enc.   ode rnn  latent ode  ode enc   latent ode   poisson   .      .       .      .       .        .       .      .       .        .       .      .       .      .       .      .       .       table    per time point classification.  accuracy on human activity.  method  accuracy     .       .      .       .      .       .      .       .      .       .      .       .       rnn  t  rnn impute  rnn decay  rnn gru d  rnn vae  latent ode  rnn enc.      .       .      .       .      .       .       ode rnn  latent ode  ode enc      .       .      .       .      .       .      .       .      .       .      .       .        .       .      .       .       human activity dataset    we trained the same classifier models as above on the human activity dataset  which contains  time series from five individuals performing various activities  walking  sitting  lying  etc. the  data consists of  d positions of tags attached to their belt  chest and ankles     features in total .  after preprocessing  the dataset has      sequences of     time points  details in supplement . the  task is to classify each time point into one of seven types of activities  walking  sitting  etc. . we  used a    dimensional latent state  more details in the supplement . table   shows that the latent  ode based classifier had higher accuracy than the ode rnn classifier on this task.         related work    standard rnns treat observations as a sequence of tokens  not accounting for variable gaps between  observations. one way to accommodate this is to discretize the timeline into equal intervals  impute  missing data  and then run an rnn on the imputed inputs. to perform imputation  che et al.         used a weighted average between the empirical mean and the previous observation. others have used  a separate interpolation network  shukla and marlin         gaussian processes  futoma et al.          or generative adversarial networks  luo et al.        to perform interpolation and imputation prior to  running an rnn on time discretized inputs. in contrast  lipton et al.        used a binary mask to  indicate the missing measurements and reported that rnns performs better with zero filling than  with imputed values. they note that such methods can be sensitive to the discretization granularity.  another approach is to directly incorporate the time gaps between observations into rnn. the  simplest approach is to append the time gap  t to the rnn input. however  mozer et al.         suggested that appending  t makes the model prone to overfitting  and found empirically that it did  not improve predictive performance. another solution is to introduce the hidden states that decay  exponentially over time  che et al.        cao et al.        rajkomar et al.       .  mei and eisner        used hidden states with exponential decay to parametrize neural hawkes processes  and explicitly modeled observation intensities. hawkes processes are self exciting processes  whose latent state changes at each observation event. this architecture is similar to our ode rnn.  in contrast  the latent ode model assumes that observations do not affect the latent state  but only  affect the model s posterior over latent states  and is more appropriate when observations  such as  taking a patient s temperature  do not substantially alter their state.             discussion and conclusion    we introduced a family of time series models  ode rnns  whose hidden state dynamics are  specified by neural ordinary differential equations  neural odes . we first investigated this model as  a standalone refinement of rnns. we also used this model to improve the recognition networks of a  variational autoencoder model known as latent odes. latent odes provide relatively interpretable  latent states  as well explicit uncertainty estimates about latent states. neither model requires  discretizing observation times  or imputing data as a preprocessing step  making them suitable for  the irregularly sampled time series data common in many applications. finally  we demonstrate that  continuous time latent states can be combined with poisson process likelihoods to model the rates at  which observations are made.  acknowledgments  we thank chun hao chang  chris cremer  quaid morris  and ladislav rampasek for helpful  discussions and feedback. we thank the vector institute for providing computational resources.    