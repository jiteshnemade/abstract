introduction    recurrent neural networks  rnn  are artificial neural networks with recurrent connections  allowing  them to learn temporal regularities and model sequences. long short term memory  lstm       is a  recurrent architecture that overcomes the problem of gradients exponentially vanishing          and  allows rnns to be trained many time steps into the past  to learn more complex programs     . now   with increased compute power and large training sets  lstms and related architectures are proving  successful not only in sequence classification                   but also in sequence generation in  many domains such as music                 text           handwriting       images       machine  translation       speech synthesis      and even choreography    .  however  most current applications of sequence generation with rnns is not a real time  interactive  process. some recent implementations have used a turn based approach  such as the online text editor  word synth    . this allows a user to enter a  seed  phrase for the rnn   priming  it such that the  next phrase generated is conditioned on the seed. although a very useful approach  this still does not  provide real time continuous control in the manner required for the creation of expressive interfaces.         method    an ensemble of models is usually used to improve overall prediction accuracy. the common  motivation behind this approach is that training multiple diverse models  using different architectures   parameters and or algorithms  and then combining their predictions  through weighted or unweighted   voting  or averaging  is likely to minimise bias and undesired variance  and thus is more likely to  provide more accurate results    . usually in these cases  all models are trained on the same training  data.  we propose a method of using an rnn ensemble  containing models trained on vastly different  datasets  and dynamically altering the models  mixture weights in real time to control the output    th conference on neural information processing systems  nips        barcelona  spain.     figure    example screen output and generated text. the user can select multiple models with over     to choose from. the chosen models are instantly available to interact with. the blue red vertical  bars visualise the probability distributions at the current time step t  one row per active model  yti for  the ith model   and the joint probability distribution  t  the top row . the horizontal dark grey bars  on each row visualise the mixture weights for each model  stored in the vector  t .     style    . while this method can potentially be applied to many different domains  we choose to  first demonstrate it on character based text models for a number of practical reasons  i  the data is  relatively low dimensional and has modest computational requirements  processing power  memory  requirements  training times etc.   ii  training data is very easy to find  iii  the output is simple to  judge qualitiatively and unambigously  iv  it has been demonstrated that lstms are successful in this  domain         .   .     training data    we train an ensemble consisting of n lstm networks  each trained on a different corpus of text  representing a unique style. the styles were selected due to the ease with which each can be  characterised with respect to language use and structure. they include the works of shakespeare   baudelaire  nietzsche  jane austen  donald trump speeches  the king james bible  assorted love  song lyrics  linux kernel c code  latex source  the chilcot report of the iraq inquiry and many more.  the amount of training data varies for each corpus  ranging from    kb to   mb.   .     training    we use different architectures for each model depending on the size of the training data  ranging from  a single lstm layer with     dimensions  to three lstm layers each with     dimensions. we use  lstm cells with input  output and forget gates      without peepholes or skip connections between  layers. we use dropout regularization as described in      with a dropout probability ranging from      to     depending on the model and architecture.  in order to provide cross model compatibility of inputs and outputs  we use a consistent mapping  between characters and indices. so we choose standard ascii codes with each model having input  and output dimensions of      with a softmax on the output to provide a probability distribution over  the     characters. we train each model to minimize the negative log likelihood of the next character  given a sequence of characters  of maximum length      as described in     .       we use the term  style  very liberally here.          settings   model mixture weights    server    osc    console   daemon    model     model     model     probability distributions    visualiser  interactive   opengl    sensor    model...    figure    software architecture for the interactive prediction and visualisation system. the visualiser  is an opengl application which continually calculates model mixture weights  t at each time step  t  either via mouse input  tracking the user s hands using a leapmotion device or using an external  midi controller. the visualiser sends  t via the osc protocol to the server  which runs each of the  models independently on the same input xt to receive probability distributions from each model yti  for the next character. the server then sends each yti back to the vizualizer which calculates the joint   t weighted by  t . separating the two processes allows transparently switching between running  both processes on the same computer  or running the backend on a remote  more powerful server.     .     interactive prediction and visualisation    once trained  the system loads and runs each of the models independently with the same character  input represented as a one hot vector xt . each model   parameterized by  i where the superscript  i denotes the model index  i       n     predicts a probability distribution for the next character  conditioned on the current history of inputs and can be written as  yti   p  xt    x    x    ...  xt    i             these are stored in a conditional probability matrix  t where the ith row contains yti   the distribution  predicted by the ith model  i.e. conditioned on the ith dataset. the system then calculates the joint  distribution at time t by mixing each model s output via a model mixture weights vector  t   which  can be thought of as the marginal distribution p   i     with     t        t    t    joint distribution    t    t              where            t   conditional probability matrix. ith row contains    yti         i     t   model mixture weights  i.e. marginal distribution p                     where the denominator is simply a normalising factor. finally  the system samples a character from   t   prints it to the screen  and feeds it back into the system at the next time step as xt with t   t    .  while the sequence is being generated  a user can steer the output towards different models by  interacting with the system and dynamically shifting the mixture weights  t . interaction is through  clicking on the screen  hand tracking with a leapmotion device  or through the use of an external  midi controller   . as an optimization  at every time step we only run models which have a mixture  weight     . depending on the number of models active  the system outputs characters at around       chars second on a high end gaming laptop.  with this method we are able to guide the system to morph between the different models  output with  relatively smooth transitions between different styles.       we are also developing new interaction mechanisms such as using a multilayer perceptron to map the user s  facial expressions or hand gestures to different configurations of mixture weights.           .     software architecture    the interactive prediction system consists of two standalone processes as seen in figure   that  communicate with each other using the open sound control  osc  network protocol     . this  allows the interaction and visualisation frame rate to be independent of the sequence generation  frame rate. it also allows us to run the server and visualiser on different  networked  computers if  need be  e.g. a powerful gpu based server for running the models  and a less powerful front end  computer for visualisation and interaction .         results and discussion    in this study we train an ensemble of lstm rnns  with each model trained on a different corpus   and we build an interactive prediction and visualisation system which mixes each model s predicted  probability distributions via mixture weights  controlled in real time via a user s gestures.  the system works as desired and allows users to continuously  steer  the output while text is being  generated  seamlessly morphing between styles  in effect  conducting  the generation of text. figure    shows an example output.  we also observe some interesting behaviour. when multiple models are active with roughly equal  mixture weights  and the system is fed a sequence containing words or phrases that are common to  all models  the probabilities for the common characters accumulate whilst probabilities specific to  individual models are suppressed  i.e. when multiple models are active the system tends towards  common words and phrases.  sometimes  while a sequence is being generated  a particular model might output a spiking probability  distribution  i.e. very high confidence for a particular character . if at that point other models output  wider distributions  i.e. lower confidence aross multiple characters   then the first model will  overpower and dominate the sequence generation.  e.g.  if at any time step the input sequence ends with  the house    the bible  model predicts the letter o with very high confidence  to eventually lead on to   the house of  judah jeremiah n oah isaac etc...   . even if at that time step  the bible  model has a lower mixture weight than the other models  it is probable that it might overpower the  other models  probability distributions and cause the o to be dominant in the final joint probability  distribution. this is quite likely to start a positive feedback loop and that model will stay in control  of the sequence generation until it reaches a point where its probability distribution widens  and  another model spikes. so it s very possible to see hints of love songs  philosophy or poetry within c  comments and variable names or latex equations. it seems there are  hand over  words or sequences  which are common to many models  but have stronger connotations in some models over others.  this is of course further guided by the user s actions  who can choose to push further towards the  emerging theme  or pull towards another style and seamlessly go from one style to another over these  hand over words.  it is also worth noting  that like most character based lstm models  the output is quite nonsensical.  the only long term dependencies which are preserved are in formatting and syntax  and there is only  meaning within the space of a few neighbouring words. nevertheless  it s still very interesting to  see the model produce words and phrases very much in the style of the associated texts  with correct  formatting  punctuation  indentation etc. there are also some nonsensical words  spelling mistakes  and incorrect punctuation. as well as being due to mixing probability distributions  this behaviour  is also observed in single models  and is most likely due to the relatively small training set  a few  hundred kb for some  and  unclean  data. with more time dedicated to collecting more training data  and cleaning it  this is likely to be improved.   .     future work    mixing models with approximate equal weights generally works when the number of models is low   e.g. n     . when we go beyond that  the sequence occasionally diverges away from comprehensible  words  towards what appears to be random sequences of characters. this is accentuated by the fact  that the system is predicting on a character by character level with no foresight beyond that. in order  to overcome this problem  we are planning on implementing a beam search     with limited depth         whereby we sample multiple times per time step  and explore  i.e. resample  each sample a few  time steps into the future  scoring each path on the sum of the log probabilities accumulated along  the way  then pruning and selecting accordingly.  as opposed to training many models independently on different corpora  another approach we are  looking at is using a single model trained on the entire corpora. we would then look to control  the output via manipulating the internal state of the lstm. this has advantages and disadvantages   particularly when it comes to adding a new corpus  i.e.  style   to the system.  with our current approach  adding a new  style  is relatively quick  since we only need to train a  new model on just the new corpus. however scalability becomes an issue during deployment. since  all of the models are run during prediction  having too many models can be a bottleneck. we have  implemented an optimization such that if the mixture weight of a model is less than    at a particular  time step  we don t run the model. this allows us to have many       models loaded in the system  and still retain real time performance if not all models are mixed in at every time step. however   as we dynamically mix in more models  the rate of sequence generation drops from around        char second  for     models  to   char second     models  on a high end laptop. n.b. since the  visualiser is a separate process to the server running the models  the visualiser framerate is always  real time at   fps  so the response of the interactivity and visualisation doesn t suffer  but characters  are output at a slower rate.  with a single monolithic model  prediction performance is less of an issue  since we will always be  running a single model. however  in this case training performance can become an issue. to add  a new  style   we will have to incrementally train the model on the new corpus  while makig sure  it maintains prediction accuracy on the previous collection of corpora. as we add more and more  styles  this is likely to have a big impact on training times and memory requirements.  finally  we are currently working with character based text models because the dimensions are  relatively low and discrete  training data is easily accessible and judging the outputs is quite straightforward. however we are planning on applying these techniques to higher dimensional and continuous  domains such as music  sound and vector graphics.         acknowledgements    in addition to my ongoing research in this field as part of my phd  this work was further supported  by a placement at google s artists and machine intelligence program. in that capacity i d like to  especially thank mike tyka  kenric mcdowell  blaise aguera y arcas and andrea held for the  organization  inspiration and support  and doug fritz  douglas eck  jason yosinki  ross goodwin   hubert eichner and john platt for the inspiring conversations and ideas.  the server is implemented with keras      using the theano      backend  while the visualiser is  implemented with openframeworks  a c   framework for creative development     . this work  wouldn t have been possible without these wonderful opensource toolkits.    