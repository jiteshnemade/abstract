introduction  time series are ubiquitous. data from diverse sources such as sensors  financial markets  demographics  audio and uber rides have a time component. time series  prevalence implies a need to  develop powerful forecasting methods. however  there are two major reasons why popular machine  learning algorithms have failed to outperform as they have in other domains. first  the time component breaks the independent and identically distributed assumption by introducing auto correlation.  machine learning has struggled overcome this assumption gap and offer step wise improvement over  pure statistical methods. second  data on many of these processes is prohibitively expensive  or  impossible  to obtain and so at most a handful of observations will exist in a given time as in the  case of a country s gdp or a company s sales. hence  theres a strong need to develop methods that  can cope with these two difficulties.  the m  competition  the continuation of three previous ones organized by spyros makridakis   is the latest challenge on time series forecasting for different types of time series  mco . these  competitions have attracted great interest in both the academic literature and among practitioners  and have provided objective evidence of the most appropriate way of forecasting various variables  of interest. the purpose of the m  competition is to replicate the results of the previous three ones  and extend them in two directions. first  the number of series is increased to          and second   machine learning  neural network  forecasting methods are included.   . all three authors contributed equally          the recurring result of the m competitions is that simple methods outperform complex ones.  this year s m  competition generally reproduced this result with one exception  the winner of the  competition  by a significant margin  was slawek smyls hybrid exponential smoothing recurrent  neural network  es rnn  method a synthesis of exponential smoothing techniques with recurrent  nueral networks  rnn .  smyl et al.         our task is to make state of the art forecasting fast  accessible  and generalizable. first  we  achieve fast training by porting smyl s original c   submission to pytorch. this vectorization  enables the use of a gpu  providing up to a    x training speedup. second  python code makes  the model more accessible to the forecasting and machine learning community.  finally  the use of  pytorch allows easier generalization of the model as the library contains several architectures that  could complement the es rnn for application in non m  time series. we expect our contribution  to speed up the adoption of hybrid models in time series forecasting.     . related work  the field of time series analysis has not strongly deviated from statistical foundations laid in the      s. seminal among these are the arima method  box et al.        and the exponential smoothing method  brown   winters and r.       . in addition to innovations resulting from the m   competition  there has been research into using deep neural network  dnn  architectures for time  series forecasting. the primary focus of this research has been to transfer techniques from other  non temporal dnn structures to time series forecasting.  qin et al.        have added attention to the prediction algorithm by developing a dual stage  architecture. others have incorporated learnings from the field of computer vision by implementing  residual lstms  kim et al. . as well as dilated lstms  chang et al. . the latter having seen  significant success in its cnn formulation  yu and koltun .  the popularity of dnns in time series forecasting has also been verified by the development  pre trained weights that leverage a large number of time series.  malhotra et al.        this latter  development has not only shown strong generalization but also has enabled dnns to be applied  to environments with relatively fewer training samples. the applications of such improvements in  dnn  time series architectures has been leveraged heavily in a myriad of industries from predicting  power demand  torres et al.        to predicting extreme events at uber  laptev et al. .     . model description  our main contribution is a new es rnn implementation. in order to achieve our objective of a fast   accessible  and general forecasting engine  we re engineer the smyl s m  competition c   submission for gpu computation. in contrast to the first version of es rnn  we exploit pytorch s eager  execution by initializing per series parameters which are dynamically included in the computational  graph.  this technique enables us to vectorize es rnn and train on a gpu using the nvidia cuda  library. the results in section   show an important reduction in computation times. furthermore   accessibility and generality are derived from the use of python and pytorch  respectively.  while the hybrid es rnn algorithm is discussed in detail in a blog post by smyl et al.         we  provide a condensed description of the learning process. the algorithm is divided into two distinct  layers  a pre processing layer that uses exponential smoothing and a lstm layer that updates the  per series parameters of the holts winter model.   . python is not only more popular    .   vs   .    but also the most wanted language for two years in a row   stack overflow       .   . we can see the parameters required for each series in equations      and  .           .  pre processing layer  in the pre processing layer  holts winter exponential smoothing  winters and r.        with multiplicative seasonality and trend is applied via  yt             lt   bt    st m  lt  bt                  bt    lt    yt  st               st m  lt   bt    lt         y t h   lt bht st m h   m                          where l is a state variable for level  b is a state variable for trend and s is a multiplicative  seasonality coefficient.      and   are smoothing coefficients between zero and one. an h step  forecast is given by equation  .  smyl et al.        used constant size input and output windows. the output windows were  defined by the prediction horizon. conversely  the input window was determined heuristically as  described in smyl et al.       .  smyl et al.        further details how the holt winters and the rnn methods were merged to  create the final model. since the model no longer considers local linear trend  equation   is replaced  by an rnn from the model as follows   y t  ...t h   rn n  xt     lt   st  ...t h  yi  xi    lt si                xt is a vector of normalized  de seasonalized features of which a scalar component xt is calculated  via equation  .   .  deep learning layer  the neural network architectures used by smyl et al.        are detailed in figure  . a summary  of the different models for quarterly  monthly and yearly time periods is presented in table  . the  model at its base consists of a lstm layer with skip connections to form the dilated lstm network  as described in chang et al.. the main advantages of this lstm structure  when compared to  a vanilla lstm  are that it greatly increases computational efficiency and allows the network to  remember information from earlier time instances. for example  in figure    the rnn has dilations         and       . this means that the first lstm hidden weights are inputs to the next cell in the  layer  however the second layer has a dilation of two which means that the hidden weights and the  bias weights are forwarded two cells forward and so on and so forth. the rnn layers also have a  residual connections that helps stabilize the network s training. finally  we have a simple linear layer  at the end for adapting the rnn output to the output prediction window in the form of normalized  and de seasonalized data  an important distinction which we discuss further in subsection  . .  it is important to note that the rnn and the classical holts winters parameters detailed in  section  .  are jointly trained. the power of smyl et al.        lies in the co training of both the  per time series holts winters parameters and the general rnn parameters.   .  training of the network  the training of this hybrid model is different from most neural network training. first  in order  to have an initial estimate of the level and seasonality coefficients  we compute a primer estimate        time frame  monthly  quarterly  yearly    dilations    lstm size                                                                     table    summary of network parameters    figure    nn architecture by smyl et al.         following the classical holts winters equations   and  . note that if we have n series in our dataset   the model will store n        s  holt winters parameters  where s is the length of the seasonality   as each series have its own level and seasonality smoothing parameters along with initial seasonality  values  s .  in smyl et al.        s cpu implementation  the network is trained per series however  the  use of dynamic gpu computational frameworks such as pytorch and  recently  tensorflow enable  batching. in figure    the input to the left most input box can be thought of as a matrix of batch  size x longest input window length in batch. we will discuss further in sections   and   why we  need to pad and chunk each time series into windows.   .  testing of the network  for generating the output of the network  we feed the output of the lstm through a non linear  layer with a tanh activation function and finally through a final linear layer to get our predictions.  this output is a de seasonlized and normalized output which is not the case of our truth data. to        re seasonalized and de normalize our outputs  we use the holts winters equations and the parameter  estimates we have for each series to arrive at the desired output.   .  loss metrics  to calculate our loss  we first unpad and mask the output as we do not include these loss values in  our calculation. the metrics used in the m  competition were symmetric mean absolute percent  error  smape  and mean absolute scaled error  mase . detailed explanations of these metrics are  included in referenced post by  smyl et al.       . however  since the metrics are non differentiable   we employ a surrogate loss function called pin ball loss as defined in  takeuchi et al.       .     . data description  the m  competition data comprises    k time series with yearly  quarterly  monthly  weekly   daily and hourly data. moreover  the dataset was sampled from real data   categories encompass  demographic  financial  industrial  macro  and micro economic and other. the series counts  broken out by frequency and type are in table  . however  the competition is more heavily focused  on yearly  quarterly and monthly data frequencies. we are of the opinion that this data is commonly  generated from expensive or irreplicable processes  such as census or financial data.  frequency    demographic    finance    industry    macro    micro    other    total    yearly  quarterly  monthly  weekly  daily  hourly  total                                                                                                                                                                                                                                                                                                                              table    m  data by type and series frequency  the dataset has two significant characteristics  first  the data is purely uni variate with no  guaranteed links between series other than broad sampling category. this lack of temporal or other  connections between observations makes the forecasting challenge more difficult for complex methods  as the sample itself is the only representation of signal. second  each series is not only variable length  but at times very small for an appropriate input window into the output forecasting horizon.     . data preparation   .  validation data sets  in time series analysis the prediction is defined as the most likely output given the series data. this  means that to maximize the training data utilization we must use the latter part of our data for  hold out validation. we extracted the last outputsize time steps from the end of our training set  to create a validation set. thus our data set takes the following form     t rain ...n  o       v aln  o  ...n  o     t estn  o...n  where o is the size of the forecast horizon and n is the length of the dataset.                  .  series length equalization  the series in the m  data are variable length. to simplify the vectorization implementation  we  equalize all series to a fixed length by frequency. we further disregard all series below that specific  length. in section    we discuss future work that will enable use of variable length series.  key in determining an appropriate length to equalize series is the trade off between removing  too many series as a result of setting the minimum length too high or removing too much series  history by making the minimum length too low. hence  we visualized a histogram of all frequency  series  length to understand what may be be best threshold by frequency. table   shows statistics  around series length in the m  dataset.  the heuristic we used to determine an appropriate threshold was one that maximized the data  retention. this value usually fell somewhere in the second quartile. we used    as minimum series  value for both quarterly and monthly time series frequencies. the cut off value also provided several  years of observed seasonality for both frequencies.    yearly  quarterly  monthly  weekly  daily  hourly    mean    std dev    min                         max                                                                                                                                                                                                                            table    frequency series length statistics  after equalizing the series length within frequency our full set of data is as follows   t rainn  o   c...n  o       v aln  o  ...n  o     t estn  o...n           where c is the minimum length threshold.   .  data windowing  prior to passing the data through the second  rnn layer defined in section  . . we perform a  windowing over the dataset  normalizing and deseasonalizing using the output from the es layer  defined in section  .   as shown in figure  .  in addition to the input window  we concatenate a one hot representation of the time series  category.     . results  ever since the first m competition  simple models such as exponential smoothing  es  and arima  have outperformed more complex ones   including machine learning. these type of models were also  the benchmark for the m  competition  a simple average of simple  holt and damped es models  makridakis et al.        named comb. the comb model is a tough to beat benchmark  with a  rank of    in the m  competition. moreover  the second best submission by hyndman        in  the m  competition is a meta learner that ensembles classical statistical model selection based on  the r forecast package routines for automatic arima  automatic es  theta  naive  seasonal naive   random walk  tbats from de livera et al.        and a shallow mlp.  table   shows the results of the es rnn model by smyl et al.         that of hyndman         and our implementation of smyl et al.        s es rnn on the gpu. furthermore  table   shows        figure    time series normalization and deseasonalization as defined in smyl et al.         the running times of our implementation on the gpu and the running time of smyl et al.         on cpu. note that for monthly data  smyl et al.        were running the algorithm of   pairs of    workers and for quarterly data    pairs of   workers were used.  finally  table   breaks down the  smape achieved by our implementation in each of the frequencies  and corresponding models  for  the six different data categories.  model  benchmark  smyl et al.         hyndman         our implementation    yearly    smape by frequency  quarterly monthly average      .       .       .       .        .      .      .       .        .       .       .       .        .      .      .      .        improvement     .     .      .      table    comparison of results to the m  baseline model    time period  yearly  quarterly  monthly    time taken for    epochs  mins   smyl et al.        our implementation                 .     .      .      speed improvement     x     x    table    comparison of run times for    epochs     . discussion  most significant to this project is the increase in speed. we observe that our gpu implementation  performs up to    x faster on the quarterly dataset over the cpu approach. this speed up is driven   . we were not able to get the running time on yearly data from smyl et al.        at this time and we will update  the table when we have their reports.          data category  demographic  finance  industry  macro  micro  other  overall    yearly    quarterly    monthly      .     .      .      .      .      .      .        .      .     .     .      .     .      .      .      .      .      .     .      .      .      table    breakdown of smape by time period and category  by the vectorized implementation  which enables batching and parallelization.  our implementation  no longer requires running each time series data individually. as a result we could use batch sizes  up to      time series.  as shown in table    smyl et al.        achieved superior results over the yearly and quarterly  time interval. however  for monthly data our implementation surpasses the original model and  benchmark by a strong margin. we believe there are two possible reasons for such a result. first   the longer training times of the original implementation likely made iteration over the parameter  space prohibitive. second  the series length equalization discussed in subsection  .  likely simplified  the problem to the model in some instances and omitted harder  shorter sequences in others. overall   our implementation has the lowest weighted smape score due to the mentioned improvement on  the monthly dataset.  we are pleased to see that our implementation is close to the m  implementation for most  frequencies. however  the large disparity in scores achieved by smyl et al.        for the yearly  dataset could be attributed to the original author s use of attentive lstm on top of the residual  and dilated lstm  which our current implementation does not include. furthermore  smyl et al.         did not use any seasonality parameters for the yearly data which our implementation was  not designed to handle.     . future work extensions   .  variable length series  in section  .   we discussed how we equalized series length by frequency to simplify batch implementation. the next step is to support variable length series. this feature is critical as results both  improve with the addition of more data and weaken as shorter series are introduced. the result of  these competing effects will better inform whether batching is appropriate for this model structure  we anticipate this feature will require the use of padded sequences together with masking just  before the loss function aggregation. this mask will zero those gradients that are derived from  padded observations outside the series  real data. we recognize that there is risk to this approach  as series reaches the frequency maximum length as padding will dramatically increase the number  of required iterations when performing the exponential smoothing pre processing. therefore further development in this area may require a smaller batch size to hedge risk against unnecessary  computation.   . our implementation is in pytorch  but is possible in any other deep learning framework with similar dynamic  graph computation features.           .  multiple seasonality  smyl s m  submission uses more than one seasonality. this is particularly important in the hourly  frequency as the data may follow both a    and      hour cycles. gould et al.        demonstrated  that multiplicative multiple seasonality is achieved by maintaining a second set of seasonality smoothing and initial values  and when de seasonalizing the data dividing by the corresponding seasonalities  one after the other.  secondarily  the model needs to be tuned to handle no seasonality in situations where no obvious  cycle exists  i.e. yearly data .   .  hyper parameter tuning  one major purpose of this work was to validate that the primary components of es rnn can  be vectorized and ported to dynamic frameworks. as a result  we kept all of smyl s m  winning  parameters wherever possible. however  we don t know which parameters were set out of necessity  given the cpu training structure and which were actually tuned in preparation for the competition.  for example  in the winning submission  the hidden size for all rnns was no more than   . as a  result  our gpu utilization was very low relative to other deep learning tasks that are often tuned  to values in the hundreds.   .  additional penalization  the m  winning  dynet c   model includes several layers of loss penalization that were not  included in our work.  first  as the rnn is modeling the series trend  i.e. the change in level from t to t       the  likelihood of over fitting could be reduced by penalizing dramatic changes in the level. this change  would causes the model to favor a smoother forecast over a more variant forecast ceteris paribus.  second  krueger and memisevic have shown that penalizing abrupt movements in the hidden  state can encourage a model to be more stable over long forecast horizons. rather than penalizing  the change from t to t      smyl s model penalized the average squared magnitude of the first layer s  cell state within each stack. these penalties applied to the internals of the model favor a model that  will likely be more stable after appropriate training  and could improve es rnn performance.   .  additional frequencies  we built out vectorized  es rnn functionality for a subset of m  competition dataset as a proof of  concept  but our approach will hypothetically scale to other time series provided we overcome the  challenges presented by section  .  as well as implement the other network structures smyl used for  the various frequencies shown in figure    noteworthy differences between already the implemented quarterly and monthly frequencies and  yearly  weekly  daily and hourly frequencies are the use of attention in yearly and weekly and the  multiple seasonalities defined in hourly series.  note  daily is the same structure as quarterly and  monthly .     . conclusion  in this project  we successfully implement the state of the art es rnn algorithm in a fast  accessible   and generalizable forecasting framework. the main challenge we overcame was the training of  per time series parameters. this proved difficult due to the direct implementation of the original  submission on a cpu. our work focused on the vectorization of the per time series parameters to  enable gpu computation in a framework that supports eager execution  e.g. pytorch . we obtained  similar results to those of the original submission  but with orders of magnitude less training time.          figure    nn architecture by smyl et al.         we anticipate that our contribution will enable a strong adoption of state of the art algorithms over  univariate series  and facilitate the generalization of the model towards specific problems where  covariates are available.    