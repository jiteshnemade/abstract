introduction    image captioning  bernardi et al.        has  emerged as an important testbed for solutions to the  fundamental ai challenge of grounding symbolic  or linguistic information in perceptual data  harnad        roy and reiter       . most captioning systems focus on what hodosh et al.        refer to as concrete conceptual descriptions  that is   captions that describe what is strictly within the image  although recently  there has been growing interest in moving beyond this  with research on visual  question answering  antol et al.        and imagegrounded narrative generation  huang et al.         among others.  approaches to image captioning can be divided  into three main classes  bernardi et al.            kenneth p. camilleri  deptartment of systems  and control engineering  university of malta  kenneth.camilleri um.edu.mt     . systems that rely on computer vision techniques to extract object detections and features  from the source image  using these as input to  an nlg stage  kulkarni et al.        mitchell  et al.        elliott and keller       . the latter is roughly akin to the microplanning and  realisation modules in the well known nlg  pipeline architecture  reiter and dale       .   . systems that frame the task as a retrieval problem  where a caption  or parts thereof  is identified by computing the proximity relevance of  strings in the training data to a given image.  this is done by exploiting either a unimodal   ordonez et al.        gupta et al.        mason and charniak        or multimodal  hodosh et al.        socher et al.        space.  many retrieval based approaches rely on neural models to handle both image features and  linguistic information  ordonez et al.         socher et al.       .   . systems that also rely on neural models  but  rather than performing partial or wholesale  caption retrieval  generate novel captions using a recurrent neural network  rnn   usually a long short term memory  lstm . typically  such models use image features extracted  from a pre trained convolutional neural network  cnn  such as the vgg cnn  simonyan  and zisserman        to bias the rnn towards  sampling terms from the vocabulary in such a  way that a sequence of such terms produces  a caption that is relevant to the image  kiros  et al.      b  kiros et al.      a  vinyals et     al.        mao et al.      a  hendricks et al.        .  this paper focuses on the third class. the key  property of these models is that the cnn image features are used to condition the predictions of the best  caption to describe the image. however  this can be  done in different ways and the role of the rnn depends in large measure on the mode in which cnn  and rnn are combined.  it is quite typical for rnns to be viewed as  generators . for example  bernardi et al.        suggest that  the rnn is trained to generate the next  word  of a caption    a view also expressed by lecun et al.       . a similar position has also been  taken in work focusing on the use of rnns as language models for generation  sutskever et al.         graves       . however  an alternative view is possible  whereby the role of the rnn can be thought  of as primarily to encode sequences  but not directly  to generate them.   a  conditioning by injecting the image means injecting the image into the same rnn that processes  the words.    dictions conditioned by the image. a different architecture keeps the encoding of linguistic and perceptual features separate  merging them in a later multimodal layer  at which point predictions are made   figure  b . in this type of model  the rnn is functioning primarily as an encoder of sequences of word  embeddings  with the visual features merged with  the linguistic features in a later  multimodal layer.  this multimodal layer is the one that drives the generation process since the rnn never sees the image  and hence would not be able to direct the generation  process.  while both architectural alternatives have been attested in the literature  their implications have not  to  our knowledge  been systematically discussed and  comparatively evaluated. in what follows  we first  discuss the distinction between the two architectures   section    and then present some experiments comparing the two  sections   and   . our conclusion is  that grounding language generation in image data is  best conducted in an architecture that first encodes  the two modalities separately  before merging them  to predict captions.          b  conditioning by merging the image means merging the image with the final state of the rnn in a   multimodal layer  after processing the words.    figure    the inject and merge architectures for  caption generation. the rnn s previous state going  into the rnn is not shown. legend  rnn   recurrent neural network  ff   feed forward layer.  these two views can be associated with different  architectures for neural caption generators  which  we discuss below and illustrated in figure  . in one  class of architectures  image features are directly incorporated into the rnn during the sequence encoding process  figure  a . in these models  it is natural  to think of the rnn as the primary generation component of the image captioning system  making pre     background  neural caption  generation architectures    in a neural language model  an rnn encodes a prefix  for example  the caption generated so far  and  either itself predicts the next item in the sequence  with the help of a feed forward layer or else it passes  the encoding to the next layer which will make the  prediction itself. this new item is added to the prefix  at the next iteration to predict another item  until an  end of sequence symbol is reached. typically  the  prediction is carried out using a softmax function to  sample the next item according to a probability distribution over the vocabulary items  based on their  activation. this process is illustrated in figure  .  one way to condition the rnn to predict image  captions is to inject both visual and linguistic features directly into the rnn  depicted in figure  a.  we refer to this as  conditioning by inject   or inject for short . different types of inject architectures  have become the most widely attested among deep  learning approaches to image captioning  chen and  zitnick        donahue et al.        hessel et al.         karpathy and fei fei        liu et al.            figure    how rnns work  each state of the  rnn encodes a prefix  which incorporates the output word derived from the previous state. in practice the neural network does not output a single word  but a probability distribution over all known words  in the vocabulary. legend  ff   feedforward layer    beg    the start of sentence token   end    the  end of sentence token.  yang et al.        zhou et al.       .  given training pairs consisting of an image and a caption  the  rnn component of such models is trained by exposure to prefixes of increasing length extracted from  the caption  in tandem with the image.  an alternative architecture   which we refer to  as  conditioning by merge   figure  b    treats the  rnn exclusively as a  language model  to encode  linguistic sequences of varying length. the linguistic vector resulting from this encoding is subsequently combined with the image features in a separate multimodal layer. this amounts to viewing the  rnn as primarily an encoder of linguistic information. this type of architecture is also attested in the  literature  albeit to a lesser extent than the inject architecture  mao et al.        mao et al.      a  mao  et al.      b  song and yoo        hendricks et al.         you et al.       . a limited number of approaches have also been proposed in which both architectures are combined  lu et al.        xu et al.        .  notice that both architectures are compatible with  the inclusion of attentional mechanisms  xu et al.        . the effect of attention in the inject architec     see tanti et al.        for an overview of different versions  of the inject architecture and a systematic comparison among  models. in this paper we focus on parallel inject.    ture is to combine a different representation of the  image with each word. in the case of merge  a different representation of the image can be combined  with the final rnn state before each prediction. attentional mechanisms are however beyond the scope  of the present work.  the main differences between inject and merge  architectures can be summed up as follows  in an inject model  the rnn is trained to predict sequences  based on histories consisting of both linguistic and  perceptual features. hence  in this model  the rnn  is primarily responsible for image conditioned language generation. by contrast  in the merge architecture  rnns in effect encode linguistic representations  which themselves constitute the input to a  later prediction stage that comes after a multimodal  layer. it is only at this late stage that image features  are used to condition predictions.  as a result  a model involving conditioning by inject is trained to learn linguistic representations directly conditioned by image data  a merge architecture maintains a distinction between the two representations  but brings them together in a later layer.  put somewhat differently  it could be argued that  at a given time step  the merge architecture predicts what to generate next by combining the rnnencoded prefix of the string generated so far  the   past  of the generation process  with non linguistic  information  the guide of the generation process .  the inject architecture on the other hand uses the full  image features with every word of the prefix during  training  in effect learning a  visuo linguistic  representation of each word. one effect of this is that  image features can serve to further specify or disambiguate the  meaning  of words  by disambiguating tokens of the same word which are correlated  with different image features  such as  crane  as in  the bird versus the construction equipment . this  implies that inject models learn a larger vocabulary  during training.  the two architectures also differ in the number  of parameters they need to handle. as noted above   since an inject architecture combines the image with  each word during training  it is effectively handling a larger vocabulary than merge. assume that  the image vectors are concatenated with the word  embedding vectors  inject  or the final rnn state   merge . then  in the inject architecture  the number     of weights in the rnn is a function of both the caption embedding and the images  whereas in merge   it is only the word embeddings that contribute to the  size of this layer of the network. let e be the size  of the word embedding  v the size of the vocabulary   i the image vector size and s the state size of the  rnn. in the inject case  the number of weights in  the rnn is w    e   i    s  whereas it is w   e   s  in merge. the smaller number of weights handled  by the rnn in merge is offset by a larger number of  weights at the final softmax layer  which has to take  as input the rnn state and the image  having size     s   i    v.  a systematic comparison of these two architectures would shed light on the best way to conceive of the role of rnns in neural language generation. apart from the theoretical implications  concerning the stage at which language should be  grounded in visual information  such a comparison  also has practical implications. in particular  if it  turns out that merge outperforms inject  this would  imply that the linguistic representations encoded in  an rnn could be pre trained and re used for a variety of tasks and or image captioning datasets  with  domain specific training only required for the final feedforward layer  where the tuning required to  make perceptually grounded predictions is carried  out. we return to this point in section  . .  in the following sections  we describe some experiments to conduct such a comparison.         experiments    to evaluate the performance of the inject and merge  architectures  and thus the roles of the rnn  we  trained and evaluated them on the flickr k  hodosh et al.        and flickr  k  young et al.         datasets of image caption pairs. for the purposes of these experiments  we used the version  of the datasets distributed by karpathy and fei fei          . the dataset splits are identical to that used  by karpathy and fei fei         flickr k is split  into       images for training        for validation   and       for testing whilst flickr  k is split into         images for training        images for validation  and       images for testing. each image       http   cs.stanford.edu people karpathy   deepimagesent      a  the merge architecture.     b  the inject architecture.    figure    an illustration of the different architectures that are tested in this paper. the numbers or  letters at the bottom of each box refer to the vector  size output of a layer.  x  is an arbitrary layer size  that is varied in the experiments and  v  is the vocabulary size which is also varied in the experiments.   dense  means fully connected layer with bias.    in both datasets has five different captions.      element image feature vectors that were extracted  from the pre trained vgg cnn  simonyan and zisserman        are also available in the distributed  datasets. we normalised the image vectors to unit  length during preprocessing.  tokens with frequency lower than a threshold in  the training set were replaced with the  unknown   token. in our experiments we varied the threshold  between   and   in order to measure the performance of each model as vocabulary size changes.  for thresholds of       and    this gives vocabulary  sizes of               and       for flickr k and                      and for flickr  k.  since our purpose is to compare the performance  of architectures  we used the  barest  models possible  with the fewest number of hyperparameters.  this means that complexities that are usually introduced in order to reach state of the art performance   such as regularization  were avoided  since it is difficult to determine which combination of hyperparameters do not give an unfair advantage to one architecture over the other.  we constructed a basic neural language model  consisting of a word embedding matrix  a basic  lstm  hochreiter and schmidhuber         and a     softmax layer. the lstm is defined as follows   in   sig xn wxi   sn   wsi   bi             fn   sig xn wxf   sn   wsf   bf             on   sig xn wxo   sn   wso   bo             gn   tanh xn wxc   sn   wsc   bc             cn   fn    cn     in           s n   on    tanh cn      gn           where xn is the nth input  sn is the hidden state after  n inputs  s  is the all zeros vector  cn is the cell state  after n inputs  c  is the all zeros vector  in is the  input gate after n inputs  fn is the forget gate after  n inputs  on is the output gate after n inputs  in is  the input gate after n inputs  gn is the modified input  used to calculate cn after n inputs  w   is the weight  matrix between   and    b  is the bias vector for     is the elementwise vector multiplication operator   and  sig  refers to the sigmoid function. the hidden  state and the cell state always have the same size.  in the experiments  this basic neural language  model is used as a part of two different architectures  in the inject architecture  the image vector  is concatenated with each of the word vectors in a  caption. in the merge architecture  it is only concatenated with the final lstm state. the layer sizes  of the embedding  lstm state  and projected image  vector were also varied in the experiments in order  to measure the effect of increasing the capacity of  the networks. the layer sizes used are           and     . the details of the architectures used in the experiments are illustrated in figure  .  training was performed using the adam optimisation algorithm  kingma and ba        with default hyperparameters and a minibatch size of     captions. the cost function used was sum crossentropy. training was carried out with an early stopping criterion which terminated training as soon as  performance on the validation data started to deteriorate  validation performance is measured after  each training epoch . initialization of weights was  done using xavier initialization  glorot and bengio         and biases were set to zero.  each architecture was trained three separate  times  the results reported below are averages over  these three separate runs.  to evaluate the trained models we generated captions for images in the test set using beam search    with a beam width of   and a clipped maximum  length of    words. the mscoco evaluation code   was used to measure the quality of the captions  by using the standard evaluation metrics bleu           papineni et al.         meteor  banerjee and lavie         cider  vedantam et al.          and rouge l  lin and och       . we also calculated the percentage of word types that were actually  used in the generated captions out of the vocabulary  of available word types. this measure indicates how  well each architecture exploits the vocabulary it is  trained on.  the code used for the experiments was implemented with tensorflow and is available online  .         results    table   reports means and standard deviations over  the three runs of all the mscoco measures and the  vocabulary usage. since the point is to compare  the effects of the architectures rather than to reach  state of the art performance  we do not include results from other published systems in our tables.  across all experimental variables  dataset  vocabulary  and layer sizes   the performance of the merge  architecture is generally superior to that of the inject architecture in all measures except for rougel and bleu  rouge l is designed for evaluating  text summarization whilst bleu is criticized for its  lack of correlation with human given scores . in  what follows  we focus on the cider measure for  caption quality as it was specifically designed for  captioning systems.  although merge outperforms inject by a rather  narrow margin  the low standard deviation over the  three training runs suggests that this is a consistent  performance advantage across train and test runs. in  any case  there is clearly no disadvantage to the  merge strategy with respect to injecting image features.  one peculiarity is that results on flickr k are  better than those on flickr  k. this could mean  that flickr k captions contain less variation  hence  are easier to perform well on. preliminary results  on the larger dataset mscoco  lin et al.          currently in progress  show cider results over  .           https   github.com tylin coco caption  https   github.com mtanti rnn role     layer                                                 vocab.                                                            vocabulary  merge  inject    .      .      .      .       .      .     .      .       .      .     .      .       .      .      .      .       .      .      .      .       .      .     .      .       .      .      .      .       .      .      .      .       .      .     .      .       cider  merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .       meteor  merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .       rouge l  merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .        a  flickr k    of vocabulary used  cider  meteor and rouge l results.  layer                                                 vocab.                                                          bleu    merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .       bleu    merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .       bleu    merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .       bleu    merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .        b  flickr k  bleu n scores.  layer                                                 vocab.                                                            vocabulary  merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .       cider  merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .       meteor  merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .       rouge l  merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .        c  flickr  k    of vocabulary used  cider  meteor and rouge l results.  layer                                                 vocab.                                                          bleu    merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .       bleu    merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .       bleu    merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .       bleu    merge  inject   .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .      .      .     .      .        d  flickr  k  bleu n scores.    table    results on the captions generated using the inject and merge architectures. values are means over  three separately retrained models  together with the standard deviation in parentheses. legend  layer   the  layer size used   x  in figure     vocab.   the vocabulary size used.  which means that either flickr k is too easy or  flickr  k is too hard when compared to the much  larger mscoco.    the best performing models are merge with state  size of     on flickr k  and merge with state size      on flickr  k  both with minimum token fre      quency threshold of  . inject models tend to improve with increasing state size  on both datasets   while the relationship between the performance of  merge and the state size shows no discernible trend.  inject therefore does not seem to overfit as state size  increases  even on the larger dataset. at the same  time  inject only seems to be able to outperform the  best scores achieved by merge if it has a much larger  layer size. therefore  in practical terms  inject models have to have larger capacity to be at par with  merge. put differently  merge has a higher performance to model size ratio and makes more efficient  use of limited resources  this observation holds even  when model size is defined in terms of number of  parameters instead of layer sizes .  given the same layer sizes and vocabulary  the  number of parameters for merge is greater than for  inject. the difference becomes greater as the vocabulary size is increased. for a vocabulary size of        and layer size of      merge has about     more parameters than inject whilst for a vocabulary  size of       and layer size of      merge has about      more parameters. however  the foregoing remarks concerning over  and under fitting also apply  when the difference between the number of parameters is small. that is  the difference in performance  is due at least in part to architectural differences  not  just to differences in number of parameters.  merge models use a greater proportion of the  training vocabulary on test captions. however  the  proportion of vocabulary used is generally quite  small for both architectures  less than     for  flickr k and less than    for flickr  k. overall  the  trend is for smaller proportions of the overall training vocabulary to be used  as the vocabulary grows  larger  suggesting that neural language models find  it harder to use infrequent words  which are more  numerous at larger vocabulary sizes  by definition .  in practice  it means that reducing training vocabularies results in minimal performance loss.  overall  the evidence suggests that delaying the  merging of image features with linguistic encodings  to a late stage in the architecture may be advantageous  at least as far as corpus based evaluation  measures are concerned. furthermore  the results  suggest that a merge architecture has a higher capacity than an inject architecture and can generate  better quality captions with smaller layers.         discussion    if the rnn had the primary role of generating captions  then it would need to have access to the image  in order to know what to generate. this does not  seem to be the case as including the image into the  rnn is not generally beneficial to its performance  as a caption generator.  when viewing rnns as having the primary role  of encoding rather than generating  it makes sense  that the inject architecture generally suffers in performance when compared to the merge architecture.  the most plausible explanation has to do with the  handling of variation. consider once more the task  of the rnn in the image captioning task  during  training  captions are broken down into prefixes of  increasing length  with each prefix compressed to a  fixed size vector  as illustrated in figure   above.  in the inject architecture  the encoding task is  made more complex by the inclusion of image features. indeed  in the version of inject used in our  experiments   the most commonly used solution in  the caption generation literature    image features  are concatenated with every word in the caption.  the upshot is  a  a requirement to compress caption  prefixes together with image data into a fixed size  vector and  b  a substantial growth in the vocabulary size the rnn has to handle  because each image word is treated as a single  word . this problem is alleviated in merge  where the rnn encodes  linguistic histories only  at the expense of more parameters in the softmax layer.  one practical consequence of these findings is  that  while merge models can handle more variety  with smaller layers  increasing the state size of the  rnn in the merge architecture is potentially quite  profitable  as the entire state will be used to remember a greater variety of previously generated words.  by contrast  in the inject architecture  this increase  in memory would be used to better accommodate information from two distinct  but combined  modalities.     we are referring to architectures that inject image features  in parallel with word embeddings in the rnn. in the literature   when this type of architecture is used  the image features might  only be included with some of the words or are changed for  different words  such as in attention models .          conclusions    this paper has presented two views of the role of  the rnn in an image caption generator. in the first   an rnn decides on which word is the most likely  to be generated next  given what has been generated  before. in multimodal generation  this view encourages architectures where the image is incorporated  into the rnn along with the words that were generated in order to allow the rnn to make visuallyinformed predictions.  the second view is that the rnn s role is purely  memory based and is only there to encode the sequence of words that have been generated thus far.  this representation informs caption prediction at a  later layer of the network as a function of both the  rnn encoding and perceptual features. this view  encourages architectures where vision and langauge  are brought together late  in a multimodal layer.  caption generation turns out to perform worse  in  general  when image features are injected into the  rnn. thus  the role of the rnn is better conceived  in terms of the learning of linguistic representations   to be used to inform later layers in the neural network  where predictions are made based on what has  been generated in the past together with the image  that is guiding the generation. had the rnn been  the component primarily involved in generating the  caption  it would need to be informed about the image in order to know what needs to be generated   however this line of reasoning seems to hurt performance when applied to an architecture. this suggests that it is not the case that the rnn is the main  component of the caption generator that is involved  in generation.  in short  given a neural network architecture that  is expected to process input sequences from multiple modalities  arriving at a joint representation   it would be better to have a separate component to  encode each input  bringing them together at a late  stage  rather than to pass them all into the same rnn  through separate input channels. with respect to  the question of how language should be grounded  in perceptual data  the tentative answer offered by  these experiments is that the link between the symbolic and perceptual should be established late  once  encoding has been performed. to this end  recurrent networks are best viewed as learning represen     tations  not as generating sequences.   .     future work    the experiments reported here were conducted on  two separate datasets. one concern is that results on  flickr k and flickr  k are not entirely consistent   though the superiority of merge over inject is clear  in both. we are currently extending our experiments  to the larger mscoco dataset  lin et al.       .  the insights discussed in this paper invite future  research on how generally applicable the merge architecture is in different domains. we would like  to investigate whether similar changes in architecture would work in sequence to sequence tasks such  as machine translation  where instead of conditioning a language model on an image we are conditioning a target language model on sentences in a  source language. a similar question arises in image  processing. if a cnn were conditioned to be more  sensitive to certain types of objects or saliency differences among regions of a complex image  should  the conditioning vector be incorporated at the beginning  thereby conditioning the entire cnn  or would  it be better to instead incorporate it in a final layer   where saliency differences would then be based on  high level visual features   there are also more practical advantages to merge  architectures  such as for transfer learning. since  merge keeps the image separate from the rnn  the  rnn used for captioning can conceivably be transferred from a neural language model that has been  trained on general text. this cannot be done with an  inject architecture since the rnn would need to be  trained to combine image and text in the input. in future work  we intend to see how the performance of  a caption generator is affected when the weights of  the rnn are initialized from those of a general neural language model  along lines explored in neural  machine translation  ramachandran et al.       .    acknowledgments  this work was partially funded by the endeavour  scholarship scheme  malta   part financed by the  european social fund  esf .     