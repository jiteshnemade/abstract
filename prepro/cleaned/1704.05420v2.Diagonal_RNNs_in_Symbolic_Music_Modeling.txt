introduction  during the recent resurgence of neural networks in     s   recurrent neural networks  rnns  have been utilized in a  variety of sequence learning applications with great success.  examples include language modeling      machine translation      handwriting recognition      speech recognition       and symbolic music modeling    .  in this paper  we empirically show that in symbolic music  modeling  using a diagonal recurrent matrix in rnns results  in significant improvement in terms of convergence speed  and test likelihood.  the inspiration for this idea comes from multivariate  gaussian mixture models  in gaussian mixture modeling   or gaussian models in general  it is known that using a diagonal covariance matrix often results in better generalization  performance  increased numerical stability and reduced computational complexity       . we adapt this idea to rnns by  using diagonal recurrent matrices.  we investigate the consequences of using diagonal recurrent matrices for the vanilla rnns  and for more popular long short term memory networks  lstms         and  gated recurrent units  grus      . we empirically observe  that using diagonal recurrent matrices results in an improvement in convergence speed in training and the resulting test  likelihood for all three models  on four standard symbolic  music modeling datasets.  this work was supported by nsf grant         .     . recurrent neural networks  the vanilla rnn  vrnn  recursion is defined as follows   ht      w ht     u xt   b             where ht   rk is the hidden state vector with k hidden  units  and xt   rl is the input vector at time t  which has  length l . the u   rk l is the input matrix that transforms the input xt from an l to k dimensional space and  w   rk k is the recurrent matrix  factor  that transforms  the previous state. finally  b   rk is the bias vector. note  that  in practice this recursion is either followed by an output  stage on top of ht to get the outputs as yt       v ht     rl    or another recursion to obtain a multi layer recurrent neural network. the hidden layer non linearity     .  is usually  chosen as hyperbolic tangent. the choice of the output nonlinearity     .  is dependent on the application  and is typically softmax or sigmoid function.  despite its simplicity  rnn in its original form above is  usually not preferred in practice due to the well known gradient vanishing problem     . people often use the more involved architectures such as lstms and grus  which alleviate the vanishing gradient issue using gates which filter the  information flow to enable the modeling of long term dependencies.   . . lstm and gru  the gru network is defined as follows   ft    wf ht     uf xt     wt    ww ht     uw xt     ct   tanh w  ht     wt     u xt     ht  ht     ft        ft     ct             where   denotes element wise  hadamard  product    .   is the sigmoid function  ft   rk is the forget gate  and  wt   rk is the write gate  if ft is a zeros vector  the current  state ht depends solely on the candidate vector ct . on the  other extreme where ft is a ones vector  the state ht   is          ieee workshop on applications of signal processing to audio and acoustics    carried over unchanged to ht . similarly  wt determines how  much ht   contributes to the candidate state ct . notice that  if wt is a ones vector and ft is a zeros vector  the gru  architecture reduces to the vrnn architecture in equation     . finally  note that we have omitted the biases in the  equations for ft   wt   and ct to reduce the notation clutter.  we will omit the bias terms also in the rest of this paper.  the lstm network is very much related to the gru  network above. in addition to the gates in gru  there is the  output gate ot to control the output of the rnn  and the forget gate is decoupled into gates ft and wt   which blend the  previous state and the candidate state ct    ft    wf ht     uf xt     wt    ww ht     uw xt       where wf   ww   w   rk . similarly for lstm  we obtain  the following   ft    wf   ht     uf xt     wt    ww   ht     uw xt     ot    wo   ht     uo xt     ct   tanh w   ht     u xt     h t  h t     ft   wt   ct    ht  ot   tanh h t              where again wf   ww   wo   w   rk . one more thing to  note is that the total number of trainable parameters in this  model scales as o k  and not o k     like the regular full  architectures  which implies lower memory and computation  requirements.   . . intuition on diagonal rnns    ot    wo ht     uo xt     ct   tanh w ht     u xt     h t  h t     ft   wt   ct    ht  ot   tanh h t       october              new paltz  ny           also notice the application of the tangent hyperbolic on h t  before yielding the output. this prevents the output from  assuming values with too large magnitudes. in      it is experimentally shown that this output non linearity is crucial  for the lstm performance.     . . diagonal rnns  we define the diagonal rnn as an rnn with diagonal recurrent matrices. the simplest case is obtained via the modification of the vrnn. after the modification  the vrnn  recursion becomes the following     in order to gain some insight on how diagonal rnns differ  from regular full rnns functionally  let us unroll the vrnn  recursion in equation     ht     w   w ht     u xt       u xt                w   w   w ht     u xt       u xt       u xt       w   w   . . . w   w h    u x      . . .     u xt       u xt    so  we see that the rnn recursion forms a mapping from  x  t    x    . . .   xt     xt   to ht . that is  the state ht is a  function of all past inputs and the current input. to get an  intuition on how the recurrent matrix w interacts with the  inputs x  t functionally  we can temporarily ignore the   .   non linearities   ht  w t h    w t   u x    w t   u x            u xt   w t h       t  x    w t k u xk .           k      ht      w   ht     u xt              where this time the recurrent term w is a length k vector   instead of a k   k matrix. note that element wise multiplying the previous state ht   with the w vector is equivalent  to having a matrix vector multiplication wdiag ht   where  wdiag is a diagonal matrix  with diagonal entries set to the  w vector  and hence the name for diagonal rnns. for the  more involved gru and lstm architectures  we also modify the recurrent matrices of the gates. this results in the  following network architecture for gru   ft    wf   ht     uf xt     wt    ww   ht     uw xt     ct   tanh w   ht     wt   u xt     ht  ht     ft        ft     ct             although this equation sacrifices from generality  it gives a  notion on how the w matrix effects the overall transformation  after the input transformation via the u matrix  the  inputs are further transformed via multiple application of w  matrices  the exponentiated w matrices act as  weights  on  the inputs. now  the question is  why are the weights applied via w are the way they are  the input transformations  via u are sensible since we want to project our inputs to a  k dimensional space. but the transformations via recurrent  weights w are rather arbitrary as there are multiple plausible  forms for w .  we can now see that a straightforward alternative to the  rnn recursion in equation     is considering linear transformations via diagonal  scalar and constant alternatives for the  recurrent matrix w   similar to the different cases for gaussian covariance matrices    . in this paper  we explore the  diagonal alternative to the full w matrices.          ieee workshop on applications of signal processing to audio and acoustics    one last thing to note is that using a diagonal matrix does  not completely eliminate the ability of the neural network  to model inter dimensional correlations since the projection  matrix u gets applied on each input xt   and furthermore  most  networks typically has a dense output layer.   . experiments    october              new paltz  ny      number of hidden units per hidden layer  uniform samples from     . . .       for lstm  and uniform samples  from     . . .       for gru  and uniform samples from      . . .       for vrnn.    learning rate  log uniform samples from the range                .    momentum  for rms prop   uniform samples from the  range       .    we trained vrnns  lstms and grus with full and  as noted in the aforementioned url  we used the perdiagonal recurrent matrices on the symbolic midi  frame negative log likelihood measure to evaluate our modmusic datasets.  we downloaded the datasets from  els. the negative log likelihood is essentially the crosshttp   www etud.iro.umontreal.ca  boulanni icml      entropy between our predictions and the ground truth. per  which are originally used in the paper    . the learning  frame negative log likelihood is given by the following exgoal is to predict the next frame in a given sequence using  pression   the past frames. all datasets are divided into training  test   t   x  and validation sets. the performance is measured by the  per frame negative log likelihood      yt log y t    per frame negative log likelihood on the sequences in the  t t    test set.  where yt is the ground truth for the predicted frames and y t  the datasets are ordered in increasing size as  jsb  is  the output of our neural network  and t is the number of  chorales  piano midi  nottingham and musedata. we did  time steps  frames  in a given sequence.  not apply any transposition to center the datasets around a  in figures          and   we show the training iterations vs  key center  as this is an optional preprocessing as indicated  negative  test log likelihoods for top   hyperparameter conin    . we used the provided piano roll sequences provided  figurations on jsb chorales  piano midi  nottingham and  in the aforementioned url  and converted them into binary  musedata datasets  respectively. that is  we show the negamasks where the entry is one if there is a note played in the  tive log likelihoods obtained on the test set with respect to the  corresponding pitch and time. we also eliminated the pitch  training iterations  for top   hyper parameter configurations  bins for which there is no activity in a given dataset. due to  ranked on the validation set according to the performance atthe large size of our experiments  we limited the maximum  tained at the last iteration. the top rows show the training  sequence length to be      we split the sequences longer than  iterations for the adam optimizer and the bottom rows show      into sequences of length     at maximum  to take adthe iterations for the rmsprop optimizer. the curves show  vantage of gpu parallelization  as we have noticed that this  the negative log likelihood averaged over the top   configuraoperation does not alter the results significantly.  tions  where cyan curves are for full model and black curves  we randomly sampled    hyper parameter configuraare for diagonal models. we use violin plots  which show  tions for each model in each dataset  and for each optimizer.  the distribution of the test negative log likelihoods of the top  we report the test accuracies for the top   configurations     configurations. we also show the average number of paranked according to their performance on the validation set.  rameters used in the models corresponding to top   configufor each random hyper parameter configuration  we trained  rations in the legends of the figures. the minimum negative  the given model for     iterations. we did these experiments  log likelihood values obtained with each model using adam  for two different optimizers. overall  we have   different  and rmsprop optimizers are summarized in table  .  models  vrnn full  vrnn diagonal  lstm full  lstm diwe implemented all models in tensorflow        agonal  gru full  gru diagonal   and   different datasets   and our code can be downloaded from our github page  and   different optimizers  so this means that we obtained  https   github.com ycemsubakan diagonal rnns.                        training runs      iterations each. we  all of the results presented in this paper are reproducible  trained our models on nvidia tesla k   gpus.  with the provided code.  as optimizers  we used the adam optimizer      with the   . conclusions  default parameters as specified in the corresponding paper   and rmsprop     . we used a sigmoid output layer for all    we see that using diagonal recurrent matrices results in  models. we used mild dropout in accordance with      with  an improvement in test likelihoods in almost all cases  keep probability  .  on the input and output of all layers.  we have explored in this paper. the benefits are exwe used xavier initialization      for all cases. the sampled  tremely pronounced with the adam optimizer  but with  hyper parameters and corresponding ranges are as follows   rmsprop optimizer we also see improvements in train  number of hidden layers  uniform samples from      .  ing speed and the final test likelihoods. the fact that this          ieee workshop on applications of signal processing to audio and acoustics    october              new paltz  ny    table    minimum negative log likelihoods on test data  lower is better  with adam and rmsprop optimizers. f stands  for full models and d stands for diagonal models.  dataset optimizer  rnn f rnn d lstm f lstm d gru f gru d  jsb chorales adam   .     .     .     .     .     .    piano midi adam   .     .     .     .     .     .    nottingham adam   .     .     .     .     .     .    musedata adam   .     .     .     .     .     .    jsb chorales rmsprop   .     .     .     .     .     .    piano midi rmsprop   .     .     .     .     .     .    nottingham rmsprop   .     .     .     .     .     .    musedata rmsprop   .     .     .     .     .     .                     training iteration                                             training iteration           modification results in an improvement for three different models and two different optimizers strongly suggests that using diagonal recurrent matrices is suitable  for modeling symbolic music datasets  and is potentially  useful in other applications.    except the nottingham dataset  using the diagonal recurrent matrix results in an improvement in final test likelihood in all cases. although the final negative likelihoods  on the nottingham dataset are larger for diagonal models  we still see some improvement in training speed in  some cases  as we see that the black curves lie below the  cyan curves for the most part.    we see that the average number of parameters utilized  by the top   diagonal models is in most cases smaller  than that of the top   full models  in these cases  we  observe that the diagonal models achieve comparable  if  not better  performance by using fewer parameters.                         training iteration                        log likelihood     log likelihood                               training iteration                                                training iteration           f  p  . e   d  p  . e                                       gru         log likelihood    f  p  . e   d  p  . e                         lstm         log likelihood     log likelihood                 figure    training iterations vs test negative log likelihoods  on jsb chorales dataset for full and diagonal models. top  row is for the adam optimizer and the bottom row is for rmsprop. black curves are for the diagonal models and cyan   gray in grayscale  curves are for full  regular  models. left  column is for vrnn  middle column is for lstm and right  column is for gru. legends show the average number of  parameters used by top   models  f is for full  d is for diagonal models . this caption also applies to figures           with corresponding datasets.                        training iteration                              training iteration                  figure    training iterations vs test negative log likelihoods  on piano midi dataset.  vanilla rnn         f  p  . e   d  p  . e                                          training iteration                  vanilla rnn    f  p  . e   d  p  . e                                     training iteration                          training iteration           f  p  . e   d  p  . e                                      lstm         f  p  . e   d  p  . e                                training iteration    gru                             f  p  . e   d  p  . e                                               gru                              f  p  . e   d  p  . e                        lstm          log likelihood                                 log likelihood                             f  p  . e   d  p  . e                                     training iteration                                             training iteration                  figure    training iterations vs test negative log likelihoods  on nottingham dataset.  vanilla rnn          f  p  . e   d  p  . e                                              training iteration           f  p  . e   d  p  . e                                              training iteration           f  p  . e   d  p  . e                                vanilla rnn          lstm                       f  p  . e   d  p  . e                                              training iteration           f  p  . e   d  p  . e                                lstm                                training iteration    gru         log likelihood                       f  p  . e   d  p  . e                                training iteration    vanilla rnn          f  p  . e   d  p  . e                      gru                              f  p  . e   d  p  . e                                     training iteration    gru         log likelihood           gru                             log likelihood                   training iteration                  log likelihood                         training iteration    f  p  . e   d  p  . e            log likelihood                     lstm           log likelihood    f  p  . e   d  p  . e                  lstm                                 log likelihood                         training iteration          log likelihood                      log likelihood    f  p  . e   d  p  . e                         log likelihood           vanilla rnn         log likelihood                   training iteration    f  p  . e   d  p  . e                      vanilla rnn          f  p  . e   d  p  . e            log likelihood          gru         log likelihood                   f  p  . e   d  p  . e            log likelihood     log likelihood          lstm         log likelihood    f  p  . e   d  p  . e            log likelihood    vanilla rnn                                           training iteration                  figure    training iterations vs test negative log likelihoods  on musedata dataset.  overall  in this paper we provide experimental data which  strongly suggests that the diagonal rnns can be a great alternative for regular full recurrent matrix rnns.          ieee workshop on applications of signal processing to audio and acoustics     . 