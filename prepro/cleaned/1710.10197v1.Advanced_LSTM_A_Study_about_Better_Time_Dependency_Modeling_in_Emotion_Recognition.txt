introduction  recurrent neural network is recently used as a dynamic  model for sequential input. long short term memory  lstm   is usually adopted as basic units in rnn because it is able  to solve the gradients vanishing and exploding problems in  rnn training    . it uses memory cell and gates to control  whether information will be memorized  output or forgotten.  the lstm takes two inputs  output from lower layer and  output from previous time step in current layer. this configuration implies an assumption that the current state depends  on the state of previous time step. this assumption of time  dependency may constraint the modeling capability of rnn.  in this paper  we propose a new variation of lstm  advanced  lstm  a lstm   to address this issue. in a lstm  current  state depends on multiple states of different time steps. this  releases the constrains in conventional lstm and provides  better time dependency modeling capability.  this paper presents our early study on a lstm. we explore the modeling capability of a lstm in the application  of voice based emotion recognition. recognizing emotion  based on audio in real world will improve the user experience of voice based artificial intelligent  ai  product  like  siri  alexa. the input voice to system in real application may   this work was done during the author s summer internship at alibaba  group  u.s.  inc.    contains long silence  or pause  or non speech voice filler   the conventional low level statistics feature like interspeech       paralinguistic challenge feature set  is    or gemaps          may be failed. weighted pooling based on attention  mechanism is an appealing solution for these cases      which  relies on rnn. we built an attention based weighted pooling  framework with multi task learning for emotion recognition  in this study. when we apply a lstm in this framework  it  gains  .   relative improvement compared with conventional  lstm.  the remaining of the paper is organized as following  structure. section   reviews previous work. section   introduces the iemocap corpus which is used in this paper.  besides  the acoustic feature extraction is also described. section   describes the details of the proposed approach. section    describes the experiments and analysis of results. section    concludes the work and leads to the future direction of the  work.   . related work      shows that temporal information is beneficial for emotion identification.           shows that the performance of  the neural network will be improved when higher layers can  see more time steps from lower layer. these works rely on  dnn rather than rnn. they do not discuss the timing sequence modeling.         proposed solutions to having alternative connections between layers in dnn. these solutions are different from the conventional connections within  network.          modify the lstm architecture relying on  residual or highway connection. however  the modifications  in these papers are focusing on connecting the memory cells  between lower and higher layers. they do not modify the connection within the same layer.      modifies the output hidden  value to higher layer by a weighted summation.      follows  similar idea. it uses weighted pooling of the hidden values of  multiple historic time steps at each time steps which improves  the information richness to higher layer. this is equivalent  to allow higher layer see more time steps. but they do not  modify the memory cell which means the time dependency is  not changed.      shows that the combination of near time  steps may not improve the system a lot. the combination  should contain a long term range. besides  they do not combine the multiple states at each step  which is different from     submitted to      ieee international conference on acoustics  speech and signal processing  the high order rnn. for emotion recognition      recently  proposed attention based weighted pooling rnn to extract  acoustic representation. the work shows the weighted pooling rnn can outperform conventional pooling approach  like  mean  maximum  or minimum. it also shows the rnn framework can capture the section of interest. multi task learning recently shows its advantage in emotion recognition task          . but in these papers  the regression of valence and  arousal values are normally set as auxiliary tasks  which is  hard to obtain.   . corpus description and feature  extraction  we apply a lstm in the application of categorical emotion  classification. we used iemocap      corpus in this study  which has   sections and    actors in total. in each section   there were two actors  one male and one female  involved in  scripted or spontaneous scenarios to perform specific emotions. the utterances were segmented and with one categorical label  which is among angry  fear  excited  neutral  disgust   surprised  sad  happy  frustrated  other and xxx. xxx was  the case that the annotators were not able to have agreement  on the label. the corpus has       utterances with average  duration of  .  s per utterance    .   hr in total . the distribution of emotion classes is not balanced. in this study   we select   classes  neutral  happy  angry and sad. the total  number of utterances used is     .  the corpus has video and audio channels. we only used  audios in this study. the audio was collected by high quality microphones  schoeps cmit  u  at the sample rate of     khz. we downsampled them to    khz and extract a   d  acoustic feature. the acoustic feature includes   d mfccs   zero crossing rate  zcr   energy  entropy of energy  spectral  centroid  spectral spread  spectral entropy  spectral flux  spectral rolloff    d chroma vector  chroma deviation  harmonic  ratio and pitch. the extraction was performed within a    ms  window whose shifting step size was    ms      fps . the  acoustic feature sequence was z normalized within each utterance.   . proposed approach   . . attention based weighted pooling rnn  attention based weighted pooling rnn is a data driven  framework to learn utterance representation from data  which  can be suitable for practical application    . it relies on the attention mechanism      to learn the weight of each time step.  the weighted summation is then computed as the representation of the whole utterance. multi task learning incorporates  several aspects of knowledge into training  therefore it can  learn better representation.  the system diagram is shown in figure  . the diagram  has two parts  trunk and branch  two dashed boxes in the  diagram . the branch is the part for different tasks  which  includes emotion  speaker and gender classifications in this    fig.  . the attention based weighted pooling rnn. the  lstm layer is unrolled along the time axis  time t  to tn .  the trunk part has the layers that are shared by all the tasks.  on top of the trunk part  there is branch part for tasks. the  main task is emotion classification. the auxiliary tasks are  speaker and gender classifications.  study. the trunk is the shared part of all tasks. the attention  based weighted pooling is computed as equation    where  ht is the hidden value output from the lstm layer at time  t   and at is a scalar number representing the corresponding  weight at time t . at is computed in a softmax fashion following equation    where w is a parameter need be learned.  exp w   ht   represents the potential energy at time t . this  is similar to attention mechanism. if the frame at time t has  high potential energy  its weight will be high and therefore  gain high  attention   if the potential energy is low  the weight  and  attention  will also be low. by this way  the model can  learn to assign weights to different time steps from data.  if weights at all time steps are same  the weighted pooling  is equal to arithmetic mean.  w eightedp ooling      tn  x    at   ht           t  t     exp w   ht         at   ptn  t  t  exp w   ht    in this study  we define that trunk part has two hidden  layers. the first layer is fully connected layer which has      relu neurons. the second one is a bidirectional lstm   blstm  layer with     neurons. the hidden values go to  weighted pooling layer after the lstm layer. in the branch  part  each task has one hidden fully connected layer with      relu neurons and one softmax layer performing classification.   . . advanced lstm  conventional lstm tasks take the output from lower layer  and previous time step as input and feed value to higher layer.  the gating mechanism is used to control information flow by  point wise multiplication  denoted as operation in the following contents . there is a cell to memorize information  within the unit. the diagram is shown in figure  .     submitted to      ieee international conference on acoustics  speech and signal processing  ing equation    where t is the set of selected time steps to  be combined. in figure    the selected time steps is t       t     and t for time t    . t is therefore denoted as a set of         . in the remaining contents  we follow the same naming convention to show our configuration of a lstm. wct  is a scalar number as corresponding weight at a specific time  step. it is learned from equation  . candidate value of h at  time t is computed following equation   . it is same as equation   except the cell value now is updated to c   . after ht is  fig.  . the unrolled conventional lstm. unrolling is along  obtained  the computation of h  is computed following equathe time axis. the c is the cell memory  x is the values from  tion    and   . the equations are similar to c   computation.  lower layer  and h is the hidden values to higher layer. states  in equation   and     w is shared  which is the parameter to  at time t depends on the one at time t     in conventional  be learned from data. in this study  c   and h  are computed  lstm.  every max t   steps rather than every step. for example  in  the case of figure    they are computed every   steps.  a lstm is able to allow more flexible time dependency  modeling capability. it makes the cell to recall far back historic records. recalling every once in a while will be like  the human learning mechanism  which makes learning better.  therefore the cell memory can memorize information better  compared with conventional lstm.  fig.  . the unrolled a lstm. unrolling is along the time  axis. the c is the cell memory  x is the values from lower  layer  and h is the hidden values to higher layer. the dashed  box is a weighted summation operation to combine the states  at time t      t     and t. c   and h  is new cell memory and  hidden value after combination. they are passed to compute  the states at time t    .  the cell is updated as equation    where ft and it are the  et is new candidate  forgetting and inputting gates at time t. c  cell values. it is computed as equation    where tanh is the  activation function  wc is a set of weights to be learned  bc is  the bias  and  ht     xt   is the concatenation of the values from  previous time step  h value  and lower layer  x value . h value  at time t is computed by equation    where ot is outputting  gate. it can be seen that the states at time t depends on the  states at time t      because ct is computed from ht   and  ct   . the computation about controlling gates are omitted  for simplification.  c t   ft    ct     it    et  c    et   tanh wc    ht     xt     bc    c                ht   ot tanh ct         the a lstm is different from the conventional one. it releases the assumption that time t state depends on time t      state. it use weighted summation of multiple states at different time steps to compute cell  c value  and hidden value  h  value . the diagram is shown in figure  .  in a lstm  equation   is modified to equation    and  equation   is modified to equation  . c   is computed follow        ct      it    ct   ft    et  c    et   tanh wc    h t     xt     bc    c  x  c     wct   ct                     t    exp w   ct    wc t   p  t exp w   ct    ht   ot tanh ct     x  h     wht   ht                       t    exp w   ht    wht   p  t exp w   ht               . experiments and results  we evaluate our proposed a lstm on selected utterances  from iemocap corpus  which belonged to neutral  happy   angry and sad classes. we run two sets of experiments. in  the first one  we compared different types of lstms. all the  systems were based on weighted pooling rnn framework.  in the second one we compared rnn framework with a deep  neural network  dnn  framework  which represents current  state of the art system on iemocap. multi task learning  was applied during all the systems. the weights for emotion   speaker and gender classification were     .    .  respectively. we randomly selected   male and   female as testing  subjects. the data from other subjects were used as training  data.     of the training data was used as validation data to  check whether we need early stopping. the early stopping  criteria was that in continuous   epochs  the accuracy on the  validation data was lower than the highest accuracy.     submitted to      ieee international conference on acoustics  speech and signal processing  table  . the performance of baseline and proposed systems.  table  . the comparison between dnn and rnn framemaf is macro average f score. map is macro average preworks.  is    is interspeech      feature set.  seq  is the  cision.  sequential acoustic feature.  rnn dnn  is the fusion result.  approach    maf    map    accuracy    approach    feature    maf    map    accuracy    conventional lstm      .       .       .     mean lstm  advanced lstm      .     .       .     .       .     .     dnn  rnn  rnn dnn    is    seq  is   seq      .     .     .       .     .     .       .     .     .     macro average f score  maf   also named as unweighted  average f score  macro average precision  map   also  named as unweighted average precision  and accuracy were  used as performance metrics. the metrics were computed  with the open source tool  scikit learn     . since the classes  were imbalanced  we mainly rely on the maf for performance evaluation.   . . weighted pooling rnn results  we built up two baseline systems for comparison under the  rnn framework. the first one used conventional lstm.  the second one used recurrent unit that was similar to the alstm structure except that wct and wht were fixed. they  were determined to same values which made the combination  equivalent to arithmetic mean of the states at selected time  steps. we therefore name it as  mean lstm . the proposed  framework used a lstm as recurrent unit. the parameter  details of the neural network has been described in section   . . dropout was used in all the layers in the network except  the attention based weighted pooling layer and the parameter  of w in equation   and   . the dropout rate was  . . the set  of t for a lstm used in this experiment was          . the  time steps were selected every   time points. it was observed  in pilot experiment that the training would be difficult when  too many times steps in t   we therefore fixed to   selected  time steps. adam      was used as optimizer. the batch size  for all systems was   .  the performance of the two baseline systems and the proposed systems are listed in table  . comparing a lstm and  conventional lstm shows that the a lstm is able to outperform the conventional lstm by  .   in terms of maf.  since the weighted pooling layer can see the hidden values  from all time steps  this improvement is not from the benefit  of seeing more time steps in higher layer. it leveraged the advantage of the flexible time dependency modeling capability  of a lstm. this is especially useful in emotion recognition   because emotion is usually shown a state within a range of  time steps rather than at a time step instantly. in this study   we have     neurons in the blstm  each direction has      neurons   so we only need add     parameters  which is the  w size  to achieve this improve. this cost can be ignored  compared with about     k parameters of network.  the results also show that there is no improvement when  we fixed the weights. comparing mean lstm and a lstm  implies that learnable weights are better. learning weights  as a framework of data driven assignment allows the model  to make the assignment according to different situations. it    is better because time dependency may vary at different time  steps.   . . comparison between rnn and dnn frameworks  we also built a dnn with multi task learning for comparison. the network has two parts  shared part and separate  part. the former part is shared by all the tasks  which has    fully connected layers with      relu neurons per layer.  the later part has   separate sub networks respectively for    tasks. each sub network has   fully connected layers with       relu neurons. on top of that  there is a softmax layer  for classification. the batch size was    and dropout rate  was  . . the optimizer was stochastic gradients descending   sgd . we used is   feature set extracted with opensmile       as input because it was suitable for the three tasks. is    was z normalized based on the mean and variance from training part. we also used the tool of focal      to fuse the results  from these two frameworks.  the results of the experiment are shown in table  . it  is shown that the rnn framework is about   .    worse  than dnn framework. there are two reasons here. first   we have very limited data  which is only about      training  utterances. this amount may not train rnn framework sufficiently  especially training rnn is more difficult than dnn.  second  all the utterances were well segmented in iemocap.  it may not have long silence and pause as the situation in real  world. the fusion result shows combining the two frameworks is better than either single one. it indicates that rnn  framework can complement the dnn even with few training  data. besides  there are about    m parameters in dnn which  is about     times as the one in rnn which means that rnn  will have low hardware requirement when it is employed.   . conclusion and future work  we proposed a new type of lstm  a lstm  in this paper. this was a early study of a lstm. we applied it in  the weighted pooling rnn for emotion recognition. it is  shown that the a lstm can outperform the conventional  lstm under weighted pooling rnn framework with few  extra parameters. the improvement leverages the advantage  of flexible time dependency modeling capability in a lstm.  even though the weighted pooling rnn framework can not  beat the state of the art dnn framework on iemocap  it  can complement the dnn to achieve better performance. it  also has the advantage in practical application in real world.  future work is necessary to explore a lstm in other  tasks. the idea of combining states at multiple time steps can     submitted to      ieee international conference on acoustics  speech and signal processing  also be extended to gated recurrent unit  gru  in the future.       y. zhang  g. chen  d. yu  k. yao  s. khudanpur  and  more data is also needed for training the rnn framework.  j. glass   highway long short term memory rnns for  distant speech recognition   in      ieee international  conference on acoustics  speech and signal processing   . 