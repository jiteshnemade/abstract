introduction    overfitting in machine learning is addressed by restricting the space of hypotheses   i.e. functions   considered. this can be accomplished by reducing the number of parameters or using a regularizer  with an inductive bias for simpler models  such as early stopping. more effective regularization  can be achieved by incorporating more sophisticated prior knowledge. keeping an rnn s hidden  activations on a reasonable path can be difficult  especially across long time sequences. with this  in mind  we devise a regularizer for the state representation learned by temporal models  such as  rnns  that aims to encourage stability of the path taken through representation space. specifically   we propose the following additional cost term for recurrent neural networks  rnns           t   x   kht k    kht   k      t t      where ht is the vector of hidden activations at time step t  and   is a hyperparameter controlling the  amounts of regularization. we call this penalty the norm stabilizer  as it successfully encourages the  norms of the hiddens to be stable  i.e. approximately constant across time . unlike the  temporal  coherence  penalty of jonschkowski   brock         our penalty does not encourage the state  representation to remain constant  only its norm.  in the absence of inputs and nonlinearities  a constant norm would imply orthogonality of the hiddento hidden transition matrix for simple rnns  srnns . however  in the case of an orthogonal transition matrix  inputs and nonlinearities can still change the norm of the hidden state  resulting in  instability. this makes targeting the hidden activations directly a more attractive option for achieving norm stability. stability becomes especially important when we seek to generalize to longer  sequences at test time than those seen during training  the  training horizon  .  the hidden state in lstm  hochreiter   schmidhuber        is usually the product of two squashing nonlinearities  and hence bounded. the norm of the memory cell  however  can grow linearly  when the input  input modulation  and forget gates are all saturated at  . nonetheless  we find that  the memory cells exhibit norm stability far past the training horizon  and suggest that this may be  part of what makes lstm so successful.        published as a conference paper at iclr         table    lstm performance  bits per character  on penntreebank for different values of  .    penalize hidden state  penalize memory cell          .     .               .     .                .     .      the activation norms of simple rnns  srnns  with saturating nonlinearities are bounded. with  relu nonlinearities  however  activations can explode instead of saturating. when the transition  matrix  whh has any eigenvalues   with absolute value greater than    the part of the hidden state  that is aligned with the corresponding eigenvector will grow exponentially to the extent that the  relu or inputs fails to cancel out this growth.  simple rnns with relu  le et al.        or clipped relu  hannun et al.        nonlinearities  have performed competitively on several tasks  suggesting they can learn to be stable. we show   however  that irnns performance can rapidly degrade outside of their training horizon  while the  norm stabilizer prevents activations from exploding outside of the training horizon allowing irnns  to generalize to much longer sequences. additionally  we show that this penalty results in improved validation performance for irnns. somewhat surprisingly  it also improves performance  for lstms  but not tanh rnns.  to the best of our knowledge  our proposal is entirely novel. pascanu et al.        proposed vanishing gradient regularization  which encourages the hidden transition to preserve norm in the direction  of the cost derivative. like the norm stabilizer  their cost depends on the path taken through representation space  but the norm stabilzer does not prioritize cost relevant directions  and accounts  for the effects of inputs as well. a hard constraint  clipping  on the activations of lstm memory  cells was previously proposed by sak et al.       . hannun et al.        use a clipped relu  which  also has the effect of limiting activations. both of these techniques operate element wise however   whereas we target the activations  norms. several other works have used penalties on the difference  of hidden states rather than their norms  jonschkowski   brock        wen et al.       . other  regularizers for rnns that do not target norm stability include weight noise  jim et al.        and  dropout  pham et al.        pachitariu   sahani        zaremba et al.       .         e xperiments     .     c haracter  l evel l anguage modeling on p enn t reebank    we show that the norm stabilizer improves performance for character level language modeling on  penntreebank  marcus et al.        for lstm and irnns     but not tanh rnns. we present results  for                 . we found that values of         could slightly improve performance   but also resulted in much longer training time on this task. scheduling   to increase throughout  training might allow for faster training. unless otherwise specified  we use           units for  lstm srnn  and sgd with learning rate .     momentum .    and gradient clipping  . we  train for a maximum of      epochs and use sequences of length    taken without overlap. when  we encounter a nan in the cost function  we divide the learning rate by    and restart with the  previous epoch s parameters.  for lstms  we either apply the norm stabilizer penalty only to the memory cells  or only to the  hidden state  in which case we remove the output tanh  as in  gers   schmidhuber        . although greff et al.        found the output tanh to be essential for good performance  removing it  gave us a slight improvement in this task. we compare to tanh and relu  with and without bias    with a grid search across cost weight  gradient clipping  and learning rate. for simple rnns  we  found that the zero bias relu  i.e. trec  konda et al.        with threshold    gave the best performance. the best performance for relu activation functions is obtained with the penalty applied.  for tanh rnns  the best performance is obtained without any regularization. results are better with  the penalty than without for   out of    experiment settings.       as in le et al.         we initialize whh to be an identity matrix in our experiments          published as a conference paper at iclr         figure    learning curves for lstm with different values of  . penalty is applied to the hidden  state  left   or the memory cells  right .     . .     a lternative costs    we compare   alternatives to the norm stabilizer cost on penntreebank for irnns without biases   see table     using the same setup as in  . . these include relative error  l  norm  absolute difference  and penalties that don t target successive time steps. the following two penalties performed     very poorly and were not included in the table     kht k     kht k  .  we find that our proposal of penalizing successive states  norms gives the best performance  but  some alternatives seem promising and deserve further investigation. in particular  the relative error  could be more appropriate  unlike the norm stabilizer cost  it cannot be reduced simply by dividing  all of the hidden states by a constant. the value   was chosen as a target for the norms based on  the value found by our proposed cost  in practice it would be another hyperparameter to tune. the  success of the other regularizers which encourage  l    norm stability indicates that our inductive  bias in favor of stable norms is useful.  table    performance with and without norm stabilizer penalty for different activation functions.  gradients are clipped at   in the first and third  and     in the second and fourth columns.    tanh         tanh           relu         relu           trec         trec             lr   .     gc       .     .     .     .     .     .      lr   .      .     .     .     .     .     .      lr   .      gc       .     .     .     .     .     .      lr   .       .     .     .     .     .     .      table    performance  bits per character  of zero bias irnn with various penalty terms designed to  encourage norm stability.                         ht      .     .         kht k       .            kht k     kht k          kht k       .     .       .     .            khk          .     .       kh  k    kht k       .     .       published as a conference paper at iclr         table    phoneme error rate  per  on timit for different experiment settings  average of   experiments. norm stabilized networks achieve the best performance. the regularization parameters  are      norm stabilizer  p   dropout probability      standard deviation of additive gaussian weight  noise.    test  dev   .               p      .     .                  p      .     .                   p      .     .              .    p      .     .               p   .     .     .                  p   .     .     .                   p   .     .     .              .    p   .     .     .     p honeme r ecognition on timit    we show that the norm stabilizer improves phoneme recognition on the timit dataset  outperforming networks regularized with weight noise and or dropout. for these experiments  we use a similar  setup to the previous state of the art for an rnn on this task  graves et al.         with ctc  graves  et al.        and bidirectional lstms with   layers of     hidden units  for each direction . we  train with adam  kingma   ba        using learning rate .    and gradient clipping    . unlike  graves et al.         we do not use beam search or an rnn transducer. we early stop after    epochs  without improvement on the development set.  we apply norm stabilization to the hidden activations  in this case we do use the output tanh as  is standard  with                   and use standard deviation .   for weight noise and p .  for  dropout. we try all pair wise combinations of the regularization techniques. we run   experiments  for each of these    settings  and report the average phoneme error rate  per . combining weight  noise and norm stabilization gave poor performance  with some networks failing to train  these results are omitted. adding dropout had a minor effect on results. norm stabilized networks had the  best performance  see figure   and table   . inspired by these results  we decided to train larger  networks with more regularization  and observed further performance improvements  see table   .  we also used a higher  patience  for our early stopping criterion here  terminating after     epochs  without improvement. unlike previous experiments  we only ran one experiment with each of these  settings. the network with     hidden units and          gave the best performance on the development set  with dev test per of   .     .  . this is competitive with the state of the art results  on this task from graves et al.        and we evaluate without beam search or rnn transducer. although to th        achieved   .     .   using convolutional neural networks. the network with       hidden units and          achieved dev test per of   .     .  .    figure    average per on timit core test set for different combinations or regularizers. the  norm stabilizer     shows a clear positive effect on performance. weight noise  wn  also improves  performance but less so. combining weight noise with norm stabilization gives poor results.   .     a dding task    the adding task  hochreiter   schmidhuber        is a toy problem used to test an rnn s ability  to model long term dependencies. the goal is to output the sum of two numbers seen at random        published as a conference paper at iclr         table    phoneme error rate  per  on timit for experiments with n hidden units and more normstabilizer regularization    . networks regularized with weight noise     .   when      . the  network with     units and          achieved the best dev per    .   .    test  dev         n          .     .              n          .     .               n          .     .                n          .     .          n          .     .              n          .     .               n          .     .               n          .     .     time steps during training  inputs at other time steps carry no information. each element of an input  sequence consists of a pair  n  i   where n          is chosen at uniform random and i           indicates which two numbers to add. we use sequences of length    . in le et al.         none  of the models were able to reduce the cost below the  short sighted  baseline set by predicting the     first  or second  of the indicated numbers  which gives an expected cost of       for this sequence  length. we are able to solve this task more successfully. we use uniform initialization in   .    .      learning rate .    gradient clipping  . we compare across nine random seeds with and without the     norm stabilizer  using       . the norm stabilized networks reduced the test cost below     in      cases  averaging .    mse. the unregularized networks averaged .    mse  and only outperformed  the short sighted baseline in     cases  also failing to improve over a constant predictor in     cases.   .     v isualizing the effects of norm   stabilization    to test our hypothesis that stability helps networks generalize to longer sequences than they were  trained on  we examined the costs and hidden norms at each time step.  comparing identical srnns trained with and without norm stabilizer penalty  we found lstms and  rnns with tanh activation functions continued to perform well far beyond the training horizon. although the activations of lstm s memory cells could potentially grow linearly  in our experiments  they are stable. applying the norm stabilizer does significantly decrease their average norm and the  variability of the norm  however  see figure   . irnns  on the other hand  suffered from exploding  activations  resulting in poor performance  but the norm stabilizer effectively controls the norms  and maintains a high level of performance  see figure  . norm stabilized irnns  performance and  norms were both stable for the longest horizon we evaluated         time steps .    figure    norm  y axis  of lstm memory cells  left  and hidden states  right  for different values  of    across time steps  x axis . non zero values dramatically reduce the mean and variance of the  norms. lstm memory cells have the potential to grow linearly  but instead exhibit natural stability.    for more insight on why the norm stabilizer outperforms alternative costs  we examined the hidden  norms of networks trained with values of   ranging from   to     on a dataset of      length     sequences taken from wikipedia  hutter       . when we penalize the difference of the initial and  final norms  or the difference of the norms from some fixed value  increasing the cost does not change  the shape of the norms  they still begin to explode within the training horizon  see figure   . for  the norm stabilizer  however  increasing the penalty significantly delayed  but did not completely  eradicate  activation explosions on this dataset.        published as a conference paper at iclr         we also noticed that the distribution of activations was more concentrated in fewer hidden units when  applying norm stabilization on penntreebank. similarly  we found that the forget gates in lstm  networks had a more peaked distribution  see figure     while the average across dimensions was  lower  so the network was forgetting more on average at each time step  but a small number of units  were forgetting less . finally  we found that the eigenvalues of regularized irnn s hidden transition  matrices had a larger number of large eigenvalues  while the unregularized irnn had a much larger  number of eigenvalues closer to   in absolute value  see figure   . this supports our hypothesis that  orthogonal transitions are not inherently desirable in an rnn. by explicitly encouraging stability   the norm stabilizer seems to favor solutions that maintain stability via selection of active units  rather  than restricting the choice of transition matrix.    figure    top  average logarithm of hidden norms as a function of time step. bottom  average cost  as a function of time step. solid blue            dashed red        . notice that irnn s activations  explode exponentially  linearly in the log scale  within the training horizon  causing cost quickly go  to infinity outside of the training horizon     time steps .    figure    hidden norms as a function of time step for values from   to     of the norm stabilizer   left and center  vs. a penalty on the initial and final norms  right . the norm stabilizer delays the  explosion of activations by changing the shape of the curve  extending the flat region.          published as a conference paper at iclr         figure    left  sorted distribution of average forget gates for different memory cells in lstm.  right  sorted absolute value of eigenvalues of whh in irnn. blue          green                   c onclusion    we introduced norm based regularization of rnns to prevent exploding or vanishing activations.  we compare a range of novel methods for encouraging or enforcing norm stability. the best performance is achieved by penalizing the squared difference of subsequent hidden states  norms. this  penalty  the norm stabilizer  improved performance on the tasks of language modeling and addition  tasks  and gave state of the art rnn performance on phoneme recognition on the timit dataset.  future work could involve     exploring the relationship between stability and generative modeling with rnns    applying norm regularized irnns to more challenging tasks    applying similar regularization techniques to feedforward nets  acknowledgments  this research was developed with funding from the defense advanced research projects agency   darpa  and the air force research laborotory  afrl  . the views  opinions and or findings  expressed are those of the authors and should not be interpreted as representing the official views or  policies of the department of defense or the u.s. government. we appreciate the many k   gpus  provided by computecanada. the authors would like to thank the developers of theano  bastien  et al.        and blocks  van merrie nboer et al.       . special thanks to alex lamb  amar shah   asja fischer  caglar gulcehre  cesar laurent  dmitriy serdyuk  dzmitry bahdanau  faruk ahmed   harm de vries  jose sotelo  marcin moczulski  martin arjovsky  mohammad pezeshki  philemon  brakel  and saizhen zhang for useful discussions and or sharing code.    