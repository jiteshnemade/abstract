introduction  in the area of signal integrity  eye diagrams have become important metrics to assess the performance of a highspeed channel. in order to generate an eye diagram  transient  waveforms are first obtained from a circuit simulator and  then overlaid. generating eye diagrams by using a circuit  simulator can be very computationally intensive  especially in  the presence of nonlinearities. as shown in figure    there are  often multiple newton like iterations involved at every time  step when a spice like circuit simulator handles a nonlinear  system in the transient regime    . given the size of a practical  and large scale circuit  the runtime of a circuit simulator on  modern processors can be hours  days  or even weeks. there  thong nguyen and jos  schutt ain  are with the university of illinois at urbana   champaign  illinois  usa  e mail   tnnguye    jesa  illinois.edu  tianjian lu and ken wu are with google inc.       amphitheatre pkwy  mountain view  ca        usa  e mail   tianjianlu  kenzwu  google.com.    fig.    flow chart of a circuit simulator    .    are many efforts in seeking novel numerical techniques to  improve the computation efficiency of a circuit simulator. for  example  people are using hardware accelerators including  fpgas     and gpus     to achieve the acceleration of matrix  factorization in a circuit simulator. there are also works in  efficiently generating eye diagrams  for example  using shorter  bit patterns instead of the pseudo random bit sequence as input  sources to simulate the worst case eye diagram    . in this  work  we propose taking a different route and using machine  learning methods  to be specific  recurrent neural network   rnn   to improve the efficiency of a circuit simulator.  recently  many remarkable results are reported on modern  time series techniques by using rnn in the fields such as  language modeling  machine translation  chatbot  and forecasting        . there are also a number of prior attempts in  incorporating rnn into modeling and simulating electronic  devices and systems. for example  researchers propose combining a narx  nonlinear auto regressive network  topology  with a feedforward neural network in modeling nonlinear  rf devices    . a variant of rnn  known as elman rnn   ernn   is applied in simulating digital designs           .  more recently  researchers present an ernn based model in  simulating electrostatic discharge  esd      . the aforementioned two topologies  to be specific  narx rnn and ernn           will be discussed in details in the following sections. it is also  worth mentioning that machine learning methods in general  have been seen into many applications related to electronic  designs such as modeling high speed channels            replacing computationally expensive full wave electromagnetic  simulations             and building macro model from sparameters           .  through the proposed approach  a modern ernn is first  trained and then validated on a relatively short sequence  generated from a circuit simulator. once the training completes  the rnn can be used to make predictions on the  remaining sequence in order to generate an eye diagram. the  training cost can also be amortized when the trained rnn  starts making predictions. as the time domain waveforms are  generated from rnn through inference instead of iterations  of solving linear systems involved in a circuit simulator  it  significantly improves the computation efficiency. besides  the  proposed approach requires no complex circuit simulations nor  substantial domain knowledge. we demonstrate through two  examples that the proposed approach can meet the accuracy  of transistor level simulation while the run time can be dramatically reduced.  in this work  we also investigate the performance of ernns  built with different recurrent units  to be specific  vanilla  recurrent neural network  vrnn   long short term memory   lstm  unit  and gated recurrent unit  gru  in generating  accurate eye diagrams. it is shown that the lstm network  outperforms vrnn in terms of both convergence and accuracy. the numerical issue of gradient vanishing or explosion  during back propagation in the vrnn is also well resolved  in the lstm network. the activation function in this work is  chosen as the rectified linear unit  relu       as it enables  better numerical stability and higher efficiency in training.  adam      optimizer is found to stand out among investigated  optimizers such as stochastic gradient descent  sgd         root mean square propagation  rmsprop       for fast  convergence in training. it is also shown that with the training  scheme proposed in this paper  training sample length plays  an important role to the convergence of the rnn.  ii. r ecurrent neural network  understanding rnn cannot be separated from the feedforward neural network  fnn   which consists of multiple  layers of neurons. unlike a fnn  in which the signal flows  unidirectionally from the input to the output  a rnn has  in  addition  a feedback loop from the output to the input. the  fnn is a universal approximator  which can be written as the  following  y   fl   fl     ...   f    x        where x   rn and y   rm represent the input and the output   respectively  fl  l       ...  l  is the weighted activation  and    denotes the composition operation. as a comparison  the  rnn can be understood as a universal turing machine in the  form of     ht   gh  xt   ht           yt   go  ht        where ht and xt are the hidden state and the input at time t   respectively  and gh and go are weighted activations.  similar to that in a dynamical system  the concept of state is  employed to describe the temporal evolution of a system  the  power of a rnn in dealing with time series tasks arises from  the special variable  namely  the hidden  internal  state ht . in  system identification  the mappings including both gh and go in  equation     are learnt via a least square alike approximation  process during which a set of pre defined parameters are tuned.  similar models to the one described by equation     can  be found in autoregressive  ar  family  which are also very  popular for time series tasks. the models of a ar family can  often be implemented with  yt   g  xt i   yt j          i   kx       j   ky             where kx and ky are known as the memory length of the  input and output  respectively. it can be seen from equation      that there is no explicit hidden state  instead  the feedback  comes from the delayed versions of the output. in order to  differentiate the mechanism described in equation      the  rnn with explicitly defined hidden states are often called  the elman rnn  ernn      . in this work  we use rnn  to denote ernn for simplicity. the term narx rnn and  output feedback rnn will be used interchangbly to refer to  ar based rnn.  it is often beneficial to unroll a rnn  which will ease the  understanding for why the learning process of a rnn could  be computationally intractable and how it is made tractable.  as shown in figure    the rnn is unrolled such that it can  be fed with an input sequence of k time steps. the signal  propagating through a unit in the unrolled rnn can thus be  written as  ht    h  wih xt   whh ht           and the output of the rnn unit is given by  yt   ht             where  h is the nonlinear activation function and w contains  the tunable weights. it is worth mentioning that one can always  add a fully connected layer to yt in equation     to transform  it into the desired form  which is also the reason why modern  formulation of rnn takes the current state as the output.  the unrolled rnn looks likes a deep fnn  dnn   but the  weights are shared across the units over time. it is an advantage  of rnn over fnn as by unrolling the rnn  one obtains  a dnn of the same number of layers but with much fewer  parameters. unfortunately  this also leads to disadvantages of  rnn  which can be understood in the following. the gradient  of the loss e at output with respect to a parameter   can be  written as  k  x   e    e                           where         x  e    y    h   hj   e      .        y    h   hj     j             the parameters in the rnn are updated through the backpropagation of the calculated gradients. the backpropagation of the           a  sigmoid    fig.    an unrolled rnn with input sequence of k steps with  y   and e  representing the prediction and the corresponding  loss  error  at time step   .    gradients from time   are done through all possible routes  toward the past  which is also known as backpropagation  through time  bptt .  one disadvantage on bptt is the computation efficiency  because at any time step     the calculation of the loss e   depend on all previous quantities. it can be seen that with  bptt  the longer the sequence with which the rnn is trained   the more challenging the computation becomes considering  both the degraded convergence rate and the increased demand  on computing resources. another numerical issue associated  with the gradients with bptt is that as the span of the  temporal dependencies increases  the gradients tend to vanish  or explode. the jacobian term in the gradient of the loss   h   function   in equation     can be proved to be upper   hj  bounded by a geometric series         h    hj           j       b  tanh     c  relu           where   is a constant determined by the norm of the nonlinearity in rnn. when the hyperbolic tangent function tanh is  chosen as the activation function  we have        and for the  sigmoid function       .       . therefore  the gradient either  explodes or vanishes. we can use a numerical experiment to  demonstrate the gradient vanishing and explosion. as shown  in figure    an input signal whose magnitude ranges from      to    is passed through various types of activation functions  in multiple times. after the third time  the signal is flattened  when the sigmoid function is taken as the activation function.  due to the vanishing of the gradients  the sigmoid function  cannot be used as the activation function in a rnn unit. in  contrast  as shown in figure   c  when relu is taken as the  activation function  the signal remains as its original shape  after being passed through the unit for iterations  which is  also the reason why relu is very popular in modern rnn  structures.    fig.    multiple passes through the same activation function.    one remedy to the problem of gradient vanishing or exploding is known as truncated backpropagation through time   tbptt              which is a modified version of bptt.  a tbptt processes the sequence one step at a time  and  after every k  time steps  it calls bptt for k  time steps.  a common configuration of tbptt is that the forward and  backward processes share the same number of steps such  that k    k  . another remedy utilizes a more sophisticated  activation function with gating units to deal with problem of  gradient vanishing or explosion  for example  the long shortterm memory  lstm  unit      and the gated recurrent unit   gru      . both lstm unit and gru own gates  which  allow the rnn cell to forget. the working mechanism of a  lstm network is based on             it                f     t          gt    ot                ct             ht         wii xt   whi ht         wif xt   whf ht      tanh  wig xt   whg ht         wio xt   who ht      ft ct     it gt  ot tanh  ct               where ht is hidden state at time t  ct is the cell state   and it   ft   gt   ot are the input  forget  cell and output gates  respectively.  as a comparison to a lstm unit  there is no cell component  in gru and the working mechanism of a gru network is as  follows     rt      wir xt   whr ht               zt      wiz xt   whz ht            nt   tanh  win xt   rt whn ht               ht        zt   nt   zt ht        fig.    readout training.    where zt and rt are called the update and the reset gates. both  zt and rt function as control signals within the unit. the gru  employs a new way to calculate the memory nt by using the  current input and the past hidden state. it can be seen that a  lstm unit requires a more complex implementation on the  gating functions than a gru. however  both lstm unit and  gru are able to store and retrieve relevant information from  the past by using gating control signal  which resolves the  issue of gradient vanishing or explosion     .  iii. t raining an rnn  in this section  we will review three different training  schemes  namely  readout  teacher force  and professor force.  the most trivial way to train an rnn is the readout training as  shown in figure  . by taking advantage of the recurrent nature  of an rnn  the readout training takes the output at previous  time steps as the input. the ground truth y k is only used  in calculating loss with the corresponding prediction yk . the  rnn is fed with what it generated  which is also the reason  it is called readout. readout technique is mostly adopted in  inference  i.e. when predictions are being made on the unseen  data. however  training in readout mode often takes longer  time on convergence because the model has to make a lot of  mistakes  being penalized for many times before it eventually  learns to generate accurate predictions. therefore  teacher  force training is often preferred over readout. in teacher force  training as illustrated in figure    the ground truth values are  fed into an rnn as input. teacher forcing can ensure an rnn  learn faster but not necessarily better.  similar to the mechanism behind overfitting  the underlying  distribution of the input data in teacher force training may be  very different from that during its readout mode inference. in  that case  teacher force training may have worse performance  on unseen data comparing to its performance on the training  set. to filter out the potential bias in training  a scheduling  process can be adopted     . a good analogy of the scheduling  process is the event of flipping a coin  we can imagine that  a coin is flipped every time before the previous output is fed  into an rnn as the input. the coin used in the scheduling  process is biased  for the first few training epochs  the coin    fig.    teacher force training.    is biased towards the training data distribution such that the  training is more into a teacher force mode  as the training  evolves  the coin becomes biased towards the distribution of  the predicted data  in other words  in a readout mode. the  scheduling technique has lead to a significant improvement  on the generalization capability of an rnn model in speech  recognition     . in this paper  we use the teach force training  with scheduling for the transient channel simulation example  shown in section iv.  there is an another training scheme called professor force       technique which enforces the similar behaviors of the  network during training and test. the professor force training  uses the concept from generative modeling  to be specific  generative adversarial network  gan  to fill the gap between the  training distribution and the predictive distribution  providing  better generalization performance.    iv. n umerical examples  the robustness of the proposed method using rnn to  model high speed channels will be illustrated through two  different types of rnn presented in section ii using a pam   and a pam  driver circuit. different training conditions such  as optimization method  memory length and recurrent cell  topology etc. will be investigated.          a. pam  channel simulation with output feedback rnn   narx rnn   in this section  we illustrate the training procedures of the  rnn  with which the predictions can be made on the voltage  waves arriving at the receiver of a high speed channel using  narx rnn. as demonstrated in equation      the narxrnn does not have a hidden state explicitly defined in the  model. the current output response is determined only using  the current and past values of the input and the past values  of the output. the set up is shown in figure    vt x  is the  output voltage of the transmitter  tx  when it is terminated  with a    ohm resistor  and vt x and vrx are voltages at the  immediate output of tx and the input of rx in the presence  of the channel. in this example  we use vt x and vt x  of the  current time step and vrx of the past to predict vrx of the  current time step.    fig.    training data collected with the setup shown in  figure  .    fig.    simulation setup for data collection..  the data is normalized and segmented into sequences of  length k. sample sequences after normalization of all the  signals of interest are depicted in figure  . this number k  represents the memory dependency of the system. the larger  the k is  the longer the memory the system keeps. a portion  of the data       is reserved for test. in this example  a  stack of four lstm cells of    hidden units is used. the  optimization method used is adam with  .  dropout regularization. throughout our experiments  increasing k not only  improves the convergence but also achieves higher accuracy.  however  once k reaches the underlying memory length of  the system under learning  a further increase does not offer  better convergence nor higher accuracy. we use k      in the  following numerical experiments. the time steps for training is         and the model converges in about    epochs. accurate  predictions are achieved on unseen sequence as shown in  figure  .  figure   and figure    show the comparison between  lstm network and vanilla rnn in terms of their capability  of handling the long term memory. the same network architecture is adopted in this comparison including the number of  layers  the layer width  and the regularization. it is shown that  when the memory is relatively short with k      the vanilla  rnn cells fails to capture the signal evolution whereas the  lstm network makes pretty accurate predictions. when the  memory is sufficiently long  the vanilla rnn starts making  comparably accurate predictions as the lstm network does   which is shown in figure   . from this comparison  it also  reveals that training with adam optimizer achieves better  performance than the sgd optimizer regardless of the memory  length.    fig.    predicted voltage at the receiver vrx with a lstm  network.    it is worth mentioning that while using the lstm and  gru networks  one needs to pay particular attention upon  the selection of activation functions. for example  the gating  signals ft and it in equation     controls the percentage of the  memory passing through the gates  which ranges from   to  .  in this case  the activation function associated with ft and it  has to be the sigmoid function. besides  the gating signal gt in  equation     allows both addition and subtraction operations  between the input and the forget gates and the hyperbolic  tangent function is appropriate. as for a vrnn  the selection  of activation functions is only based on the nonlinearities. as  shown in figure     using the hyperbolic tangent function  as the activation function in a vanilla rnn achieves more  accurate predictions than that with relu.    in addition  sgd optimizer does not work for the proposed  rnn structure under the aforementioned settings for training. adding momentum for sgd does not help the learning  process either. however  adam optimizer achieves accurate  predictions. we also investigate rmsprop optimizer  which          fig.    comparison between vanilla rnn and lstm network  in handling relative short memory when the memory length  k is chosen as  .    fig.     the impact from different types of activation functions  on the prediction accuracy in a vanilla rnn.    confirms that for k      networks trained by rmsprop make  accurate predictions on the output waveform. however  setting  a high momentum to deploy adaptive learning rate degrades  the performance of the network  as can be seen in figure      the prediction accuracy becomes worse with momentum added  in rmsprop.    fig.     comparison between vanilla rnn and lstm network  in handling sufficiently long memory when the memory length  k is chosen as   .    has been used for rnns long before adam is invented  to  train the same architecture in terms of both vrnn and lstm  network. it is found that for short memory such as k       using rmsprop optimizer does not achieve convergence  as  the memory length k goes beyond    rmsprop optimizer  performs as well as adam. the result shown in figure       fig.     performance of the same architecture using different  rnn cells  trained by rmsprop when k              fig.     setup to obtain training data for pam  example.    fig.     a training sample by windowing the training sequence  with k      .    fig.     voltages used to train ernn in pam  example.    b. pam  channel simulation with ernn  the limitation of the output feedback rnn used in the  pam  example is that it strictly requires the output of the  current time step before it can make predictions on one future  time step  which can be seen from equation    . the neural  network model  while being used in this way  cannot utilize  batch inference. in this example  it is shown that using a deeper  and wider network  an rnn based model can be developed  to utilize batch inference  which can dramatically reduce the  run time for long transient simulation.  to prepare the training data  first  the transmitter output is  measured when it is opened  denoted as vt x  . this signal is  the thevenin source to the combined  channel and receiver   system of interest. when the transmitter is connected to the  channel and the receiver  the input to the channel from the  transmitter vt x and the input to the receiver after the channel  vrx are both collected for training purpose. besides  the  output voltage from the receiver vro is also captured and  included in the training set. the setup for data collection is  shown in figure   . the data in this example comes from a  pam  transceiver circuit transmitting data at    gbps. an  lstm network is trained on about         time points of  time domain response of as shown in figure   . a training  waveform sample is shown in figure   .  we first investigate the impact from memory length on the  training process. the memory length depends on not only the  nonlinearity of the transmitter and receiver but also the delay  of the channel. as for the training setup  adam is used as  the optimizer with initial learning rate of  .    and dropout  regularization is fixed at  . . the lstm network has six  layers each with    hidden units. the memory length k is  varied with everything else remaining the same. figure     demonstrates the training performance under various memory  lengths with the same network topology. by showing the    results at different epochs  figure    also reveals the fact  that the learning ability of the lstm network evolves as  the training progresses. for example  at the    th epoch  the  lstm network learned the switching pattern of the waveforms  at the     th epoch  the same network is able to make  accurate predictions on vt x in terms of both the pattern and  the amplitude.  with the memory length chosen as k      and at the     th  epoch  the predictions made with the lstm network on vro  are less accurate than those on vt x   as shown in figure   .  the reason that obtaining accurate predictions on vro is more  challenging than that for vt x is because the former requires a  much better knowledge of the delay imposed by the channel.  it seems the memory length set by k      dose not provide  adequate data on the channel delay. after the memory length  is increased to k       the predictions on vro become much  more accurate  as shown in figure   c. a further increase of  the memory length to k       does not further improve the  performance as shown in figure   d. it is worth mentioning  that the increase of memory length demands more computation  resources.  to further validate the model  we employ a much longer  prbs than the training one and generate eye diagrams. in  figure     it shows a very good agreement between the eye  diagram generated from traditional spice like simulation and  the one from the proposed rnn based model. for example   both eye diagrams point out that the optimal sampling point  is about   .     s.  c. accumulation of numerical error  one limitation of the proposed method with rnn is the  accumulation of numerical error. during the training process   tbptt gives a noisy gradient information to the optimizer   which translates to the numerical error in the solution. this  numerical error  though initially very small  gradually accumulates as the prediction goes on with the input sequence.  a longer input sequence leads to a larger numerical error  in the predicted results. figure    shows the performance  of the trained model in the previous section on a very long  prbs. initially  the predicted results from the rnn model  agree well with those obtained from the circuit simulation.  however  as the prediction progresses  the numerical error due           a  when k       trained in     epochs.     b  when k       trained in       epochs.     c  when k      trained in       epochs.     d  when k        trained in       epochs.    fig.     training error  most left  and test performance of rnn model in pam  transceiver example.           a  from spice    network outperforms the vanilla rnn in terms of accuracy.  we also investigates the impacts of training schemes and  tunable parameters on both the accuracy and the generalization  capability of an rnn model through examples.  understanding the memory length of the data for training is  important in achieving a balance between the computational  cost and the accuracy of an rnn model. this remains an  open problem in causal inference domain and the selection of a  sufficient memory length to train an rnn model heavily relies  on prior experience and substantial domain knowledge. one  future work is to divide the full channel modeling task into  blocks where each block is represented by a standalone rnn  model. individual blocks can thus be swapped in and out to  combine with different channel designs without retraining the  rnn. another future work is to take the equalization settings  as inputs such that the rnn models can completely replace  the transceivers circuit models.  acknowledgment     b  from rnn    fig.     eye diagram obtained in pam  transceiver example.  to tbptt accumulates and degrades the performance of the  rnn model. the accumulation of the numerical error is a  well known limitation of rnn trained by tbptt  which at the  same time leaves room for improvement in the future work on  the proposed method with advanced techniques for sequence  modeling such as attention mechanism     .  v. c onclusion and f uture work  in this paper  we propose using rnn for transient highspeed link simulation. it shows that using rnn based model  for circuit simulation is promising in terms of both the accuracy and the capability of improving computation efficiency.  through the proposed approach  an rnn model is trained  and validated on a relatively short sequence generated from  a circuit simulator. after the training completes  the rnn  can be used to make predictions on the remaining sequence  to generate an eye diagram. using rnn model significantly  enhances the computation efficiency because the transient  waveforms are produced through inference  which saves iterations in solving nonlinear systems required by a circuit  simulator. an rnn differs from a fnn by the fact that its  parameters are shared across time. this is an advantage of an  rnn because the number of tunable parameters is significantly  reduced comparing to a fnn. however  it also becomes a  challenge to train an rnn as the regular back prop does  not work anymore  instead  back prop through time must be  employed. two topologies of the rnn  namely  ernn and  narx rnn  are investigated and compared in terms of the  performance in high speed link simulation. through examples   it is demonstrated that ernn without output feed back is  preferable in high speed link simulation owing to its capability  of batch learning and inference. it is also found out that lstm    this material is based upon work supported by the national  science foundation under grant no. cns           the u.s  army small business innovation research  sbir  program  office and the u.s. army research office under contract  no.w   nf    c      and by zhejiang university under  grant zju research       .  