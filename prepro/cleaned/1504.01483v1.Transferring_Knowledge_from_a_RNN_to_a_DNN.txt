introduction  deep neural networks  dnns  combined with hidden markov  models  hmms  have been shown to perform well across many  automatic speech recognition  asr  tasks          . dnns  accept an acoustic context  e.g.  a window of fmllr features   as inputs and models the posterior distribution of the acoustic  model. the  deep  in dnn is critical  state of the art dnn  models often contain multiple layers of non linearities  giving  it powerful modelling capabilities       .  recently  recurrent neural networks  rnns  have demonstrated even more potential over its dnn counterparts          .  rnn models are neural network models that contain recurrent  connections or cycles in the connectivity graph. rnn models when unrolled  can actually be seen as a very special case  of dnn. the recurrent nature of the rnn allows us to model  temporal dependencies  which is often the case in speech sequences. in particular  the recurrent structure of the model allows us to store temporal information  e.g.  the cell state in  lstm      within the model. in       rnns were shown to  outperform dnns in large commercial asr systems. and in       rnns have been shown to provide better performance over  dnns in robust asr.  currently  there has been much industry interest in asr  for embedded platforms  for example  mobile phones  tablets  and smart watches. however  these platforms tend to have limited computational capacity  e.g.  no limited gpu and or low  performance cpu   limited power availability  e.g.  small bat     teries  and latency requirements  e.g.  asking a gps system for  driving directions should be responsive . unfortunately  many  state of the art dnn and rnn models are simply too expensive  or impractical to run on embedded platforms. traditionally  the  approach is simply to use a small dnn  reducing the number  of layers and the number of neurons per layer  however  such  approaches often suffer from word error rate  wer  performance degradations     . in our paper  we seek to improve the  wer of small models which can be applied to embedded platforms.  dnns and rnns are typically trained from forced alignments generated from a gmm hmm system. we refer to this  as a hard alignment  the posterior distribution is concentrated  on a single acoustic state for each acoustic context. there has  been evidence that these gmm alignment labels are not the optimal training labels as seen in         . the gmm alignments  make various assumptions of the data  such as independence of  acoustic frames given states     . in this paper  we show soft  distribution labels generated from an expert is potentially more  informative over the gmm hard alignments leading to wer  improvements. the effects of the poor gmm alignment quality  may be hidden away in large deep networks  which have sufficient model capacity. however  in narrow shallow networks   training with the same gmm alignments often hurts our asr  performance     .  one approach is to change the training criteria  rather than  trying to match our dnn to the gmm alignments  we can instead try and match our dnn to the distribution of an expert  model  e.g.  a big dnn . in       a small dnn was trained  to match the output distribution of a large dnn. the training  data labels are generated by passing labelled and unlabelled  data through the large dnn  and training the small dnn to  match the output distribution. the results were promising        achieved a  .    wer reduction over their baseline systems.  another approach is to train an model to match the softmax  logits of an expert model. in       an ensemble of experts were  trained and used to teach a  potentially smaller  dnn. their  motivation was inference  e.g.  computational cost grows linearly to the number of ensemble models   however the principle  of model compression applies     .      also generalized the  framework  and showed that we can train the models to match  the logits of the softmax  rather than directly modelling the distributions which could yield more knowledge transfer.  in this paper  we want to maximize small dnn model performance targeted at embedded platforms. we transfer knowledge from a rnn expert to a small dnn. we first build a  large rnn acoustic model  and we then let the small dnn  model learn the distribution or soft alignment from the large  rnn model. we show our technique will yield improvements     in wer compared to the baseline models trained on the hard  gmm alignments.  the paper is structured as follows. section    begins with  an introduction of a state of the art rnn acoustic model. in  section    we describe the methodology used to transfer knowledge from a large rnn model to a small dnn model. section    is gives experiments  results and analysis. and we finish in  section   with our conclusion and future work discussions.     . . kl divergence  we can match the output distribution of our dnn to our rnn  by minimizing the kullback leibler  kl  divergence between  the two distributions. namely  given the rnn posterior distribution p and the dnn posterior distribution q  we want to  minimize the kl divergence dkl  p   q    dkl  p  s x   q s x        x    p  si  x  ln    i     . deep recurrent neural networks      h p  q    h p      there exist many implementations of rnns       and lstm  is a particular implementation of rnn that is easy to train and  does not suffer from the vanishing or exploding gradient problem in backpropagation through time  bptt      . we follow           in our lstm implementation   it  ft  ct  ot  ht        wxi xt   whi ht          wxf xt   whf ht        ft cst     it tanh wxc xt   whc ht          wxo xt   who ht        ot tanh ct      p  si  x   q si  x                                this particular lstm implementation omits the the bias and  peephole connections. we also apply a cell clipping of   to ease  the optimization to avoid exploding gradients. lstms can also  be extended to be a bidirectional lstm  blstm   to capture  temporal dependencies in both set of directions    .  rnns  and lstms  can be also be extended into deep rnn  architectures     . there has been evidence that the deep rnn  models can perform better than the shallow rnn models             . the additional layers of nonlinearities can give the  network additional model capacity similar to the multiple layers  of nonlinearities in a dnn.  we follow       in building our deep rnn  to be exact  the  particular rnn model is actually termed a tc dnn blstmdnn model. the architecture begins with a time convolution   tc  over the input features  e.g.  fmllr      . this is followed by a dnn signal processor which can project the features into a higher dimensional space. the projected features  are then consumed by a blstm  modelling the acoustic context sequence. finally a dnn with a softmax layer is used to  model the posterior distribution.      s model gave more than     relative improvement over previous state of the art dnns  in the wall street journal  wsj  eval   task. in this paper   we use the tc dnn blstm dnn model as our deep rnn  to generate the training alignments from which the small dnn  will learn from.     . methodology  our goal is to transfer knowledge from the rnn expert to a  small dnn. we follow an approach similar to     . we transfer knowledge by training the dnn to match the rnn s output  distribution. note that we train on the soft distribution of the  rnn  e.g.  top k states  rather than just the top   state  e.g.  realigning the model with the rnn . in this paper we will show  the distribution generated by the rnn is more informative over  the gmm alignments. we will also show the soft distribution  of the rnn is more informative over taking just the top   state  generated by the rnn.                where  si   s are the acoustic states  h p  q     p   x  ln q si  x  is the cross entropy term and  i  p  sip  h p     i p  si  x  ln p  si  x  is the entropy term. we can  safely ignore the h p   entropy term since its gradient is zero  with respect to the small dnn parameters. thus  minimizing  the kl divergence is equivalent to minimizing the cross entropy error  cse  between the two distributions   x  h p  q      p  si  x  ln q si  x        i    which we can easily differentiate and compute the pre softmax  activation a  e.g.  the softmax logits  derivative    j    q si  x    p  si  x    ai            . . alignments  in most asr scenarios  dnns and rnns are typically trained  with forced alignments generated from gmm hmm models to  model the posterior distribution. we refer this alignment as a  hard gmm alignment because the probability is concentrated  on only a single state. furthermore  the alignment labels generated from gmm hmm model are not always the optimal for  training dnns     . the gmm hmm makes various assumptions that may not be true  e.g.  independence of frames . one  possible solution is to use labels or alignments from another expert model  for example in      an ensemble of experts was used  to teach one model. in this paper  we generate labels from an  expert rnn which provide better training targets compared to  the gmm alignments.  one possibility is to generate hard alignments from a rnn  expert. this is done by first training the rnn with hard alignments from the gmm hmm model. after the dnn is trained   we then realign the data by taking hard alignments  e.g.  top  probability state  from the trained rnn. the alignment is  hard as it takes only the most probable phoneme state for each  acoustic context  and the probability is concentrated on a single  phoneme state.  on the other hand  we could utilize the full distribution or  soft alignment associated with each acoustic frame. more precisely  for each acoustic context  we take the full distribution of  the phonetic states and their probabilities. however  this suffers  from several problems. first  during training  we need to either  run the rnn in parallel or pre cache the distribution on disk.  running the rnn in parallel is an expensive operation and undesirable. the alternative is caching the distribution on disk   which would require obscene amounts of storage  e.g.  we typically have several thousand acoustic states . for example  in  wsj  it would take over    tib to store the full distribution of  the si    dataset. we also run into bandwidth issues when loading the training samples from the disk cache. finally  the entire  distribution may not be useful  as there will be many states with     gmm hard alignments    gmm hmm    rnn soft alignments    rnn expert    small dnn    figure    we use the hard gmm alignments to first train a rnn  after which we use the soft alignments from the rnn to train our  small dnn.  near zero values  intuition suggests we can just discard those  states  e.g.  lossy compression .  our solution sits inbetween the two extremes of taking only  the top   state or taking the full distribution. we find that the  posterior distributions are typically concentrated on only a few  states. therefore  we can make use of almost the full distribution by storing only a small portion of the states probability  distribution. we take the states that contains the top     of the  probability distribution. note  this is different than taking the  top k states  we take at least n states where we can capture at  least     of the distribution  and n will vary per frame. we  then re normalize the probability per frame to ensure the distribution sums up to  . this lossy compression method losses up  to    of the original probability mass.     . experiments and results  we experiment with the wsj dataset  we use si    with approximately    hours of speech as the training set  dev   as our  development set and eval   as our test set. we observe the wer  of our development set after every epoch  we stop training once  the development set no longer improves. we report the converged dev   and the corresponding eval   wers. we use the  same fmllr features generated from the kaldi s  recipe        and our decoding setup is exactly the same as the s  recipe  e.g.   big dictionary and trigram pruned language model . we use the  tri b gmm alignments as our hard forced alignment training  targets  and there are a total of      acoustic states. the gmm  tri b baseline achieved a dev and test wer of  .   and  .    respectively.   . . optimization  in our dnn and rnn optimization procedure  we initialized  our networks randomly  e.g.  no pretraining  and we used  stochastic gradient descent  sgd  with a minibatch size of     . we apply no gradient clipping or gradient projection in  our lstm. we experimented with constant learning rates of    .    .     .     and geometric decayed learning rates with  initial values of   .    .    with a decay factor of  . . we report the best wers out of these learning rate hyperparameter  optimizations.   . . big dnn and rnn  we first built several baseline  big  dnn and rnn systems.  these are the large networks and not suitable for deployment  on mobile platforms. we followed the kaldi s  recipe and built  a   layer dnn and      neurons per hidden layer with dbn  pretraining and achieves a eval   wer of  .       . we also  followed      and built a   layer relu dnn with      neurons  per hidden layer and achieves a eval   wer of  .  . our rnn  model follows       consists of      neurons per layer for the  dnn layers  and     bidirectional cells for the blstm. the  rnn model achieves a eval   wer of  .    significantly better    table     models.    wall street journal wers for big dnn and rnn  model  gmm kaldi  dnn kaldi s   dnn relu  rnn         dev   wer   .     .     .     .      eval   wer   .     .     .     .      than both big dnn models. each network has a softmax output  of      states matching the gmm model. table   summarizes  the results for our baseline big dnn and big rnn experiments.   . . small dnn  we want to build a small dnn that is easily computable by  an embedded device. we decided on a   layer network    hidden layers   wherein each hidden layer has     relu neurons  and a final softmax of      acoustic states matching the gmm.  since matrix matrix multiplication  mmm  is an o n    operation  the effect is approximately a     times reduction in number of computations for the hidden layers  when comparing the    hidden layers of      neurons vs. a   hidden layers of      neurons . this will allow us to perform fast interference on embedded platforms with limited cpu gpu capacity.  we first trained a small relu dnn using the hard gmm  alignments. we achieved a  .   wer compared to  .   wer  of the big relu dnn model on the eval   task. the dev    wer is  .   for small model vs  .   for the large model  the  big gap in dev   wer suggests the big dnn model is able to  optimize substantially better. the large dnn model has significantly more model capacity  and thus yielding its better results  over the small dnn.  next  we experimented with the hard rnn alignment. we  take the top   state of the rnn model and train our dnn towards this alignment. we did not see any improvement  while  the dev   wer improves from  .   to  .    the eval   wer  degrades from the  .   to  .  . this suggests  the rnn hard  alignments are worse labels than the original gmm alignments.  the information provided by the rnn when looking at only  the top state is no more informative over the gmm hard alignments. one hypothesis is our dnn model overfits towards the  rnn hard alignments  since the dev   wer was able to improve  while the model is unable to generalize the performance  to the eval   test set.  we now experiment with the rnn soft alignment  wherein  we can add the soft distribution characteristics of the rnn to  the small dnn. we take the top     percentile of probabilities of from the rnn distribution and renormalize them  e.g.   ensure the distribution sums up to   . we minimize the kl divergence between the rnn soft alignments and the small dnn.  we see a significant improvement in wer. we achieve a dev    wer of  .   and eval    .  . in the eval   scenario  our wer     table    small dnn wers for wall street journal based on  different training alignments.  alignment  hard gmm  hard rnn  soft rnn  soft dnn    dev   wer   .     .     .     .      eval   wer   .     .     .     .      improves by over     relative compared to the baseline gmm  hard alignment. we were almost able to match the wer of the  big dnn of  .    off by  .   relative   despite the big dnn  have many more layers and neurons. the rnn soft alignment  adds considerable information to the training labels over the  gmm hard alignments or the rnn hard alignments.  we also experimented training on the big dnn soft alignments. the big dnn model is the dnn relu model mentioned in table    wherein it achieved a eval   wer of  .  .  once again  we generate the soft alignments and train our small  dnn to minimize the kl divergence. we achieved a dev    wer of  .   and eval   wer of  .  . there are several things  to note  first  we once again improve over the gmm baseline by   .   relative. next  the dev   wer is very close to the rnn  soft alignment  less than    relative   however  the gap widens  when we look at the eval   wer  more than    relative . this  suggests the model overfits more under the big dnn soft alignments  and the rnn soft alignments provide more generalization. the quality of the rnn soft alignments are much better  than big dnn soft alignments. table   summarizes the wers  for the small dnn model using different training alignments.   . . cross entropy error  we compute the cse of our various models against the gmm  alignment for the dev   dataset. we measure the cse against  dev   since that is our stopping criteria and that is the optimization loss. the cse will give us a better indication of the optimization procedure  and how our models are overfitting. table    summarizes our cse measurements. there are several observations  first the big rnn is able to achieve a lower cse  compared to the big dnn. the rnn model is able to optimize  better than the dnn as seen with the better wers the rnn  model provides. this is as expected since the big rnn model  achieves the best wer.  the next observation is that the small dnns trained off the  soft alignment from the large dnn or rnn achieved a lower  cse and compared to the small dnn trained on the gmm hard  alignment. this suggests the soft alignment labels are indeed  better training labels in optimizing the model. the extra information contained in the soft alignment helps us optimize better  towards our dev   dataset.  the small dnn trained on the soft rnn alignments and  soft dnn alignments give interesting results. these models  achieved a lower cse compared to the large rnn and large  dnn models trained on the gmm alignments. however  the  wers are worse than the large rnn and large dnn models.  this suggests the small model trained on the soft distribution  is overfitting  it is unclear if the overfitting occurs because the  smaller model can not generalize as well as the large model  or  if the overfitting occurs because of the quality of the soft alignment labels.    table    cross entropy error  cse  on wsj dev   over our  various models.  alignment  gmm  gmm  hard rnn  soft rnn  soft dnn    model  big rnn  big dnn  small dnn  small dnn  small dnn    cse   .        .        .        .        .          . conclusion and discussions  the motivation and application of our work is to extend asr  onto embedded platforms  where there is limited computational  capacity. in this paper we have introduced a method to transfer  knowledge from a rnn to a small dnn. we minimize the kl  divergence between the two distributions to match the dnn s  output to the rnn s output. we improve the wer from  .    trained on gmm forced alignments to  .   on the soft alignments generated by the rnn. our method has resulted in more  than     relative improvement in wer with no additional inference cost.  one question we did not answer in this paper is whether  the small dnn s model capacity or the rnn s soft alignment is  the bottleneck of further wer performance. we did not measure the effect of the small dnn s model capacity on the wer   would we get similar wers if we increased or decreased the  small dnn s size  if the bottleneck is in the quality of the soft  alignments  then in princple we could reduce the small dnn s  size further without impacting wer  much   however  if model  capacity is the issue  then we should not use smaller networks.  on a similar question  we did not investigate the impact of  the top probability selection in the rnn alignment. we threshold the top     of the probabilities out of convenience  however  how would selecting more or less probabilities affect the  quality of the alignments. in the extreme case  wherein we only  selected the top   probability  we found the model to perform  much worse compared to the     soft alignments  and even  worse than the gmm alignments  this evidence definitely shows  the importance of the information contained in the soft alignment.  we could also extend our work similar to      and utilize  vast amounts of unlabelled data to improve our small dnn. in        they applied unlabelled data to their large dnn expert to  generate vast quantities of soft alignment labels for the small  dnn to learn from. in principle  one could extend this to an  infinite amount of training data with synthetic data generation   which has been shown to improve asr performance     .  finally  we did not experiment with sequence training        sequence training has almost always shown to help       it  would be interesting to see the effects of sequence training on  these small models  and whether we can further improve the  asr performance.     . acknowledgements  we thank won kyum lee for helpful discussions and proofreading this paper.      . 