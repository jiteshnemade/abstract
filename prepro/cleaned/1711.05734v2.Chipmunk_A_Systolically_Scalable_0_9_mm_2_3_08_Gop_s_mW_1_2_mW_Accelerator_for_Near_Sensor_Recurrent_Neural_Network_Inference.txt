introduction    in the last few years  we have witnessed an  artificial intelligence  revolution that has been fueled  by the concurrent availability of huge amounts of training data  computing power to learn upon it   and evolution of  smart  algorithms  in particular those based on deep learning. within this field   recurrent neural networks  rnns   particularly long short term memory  lstm  and gated recurrent units  gru   are receiving increasing attention  they have shown state of the art accuracy  in tasks such as speech recognition        and language translation      making them the forefront of  the  intelligent  user interfaces of products such as amazon alexa  google assistant  apple siri   microsoft cortana and others.  one of the key limitations of the current generation of commercial products based on rnns is that  these embedded  edge devices depend on remote servers taking care of the computational workload  necessary for the deployment of these algorithms. moreover  when rnns are used as a component  of human machine interfaces  the intrinsic latency of network communication can also be problematic  as people expect the  smart  devices to reply not only accurately  but also timely.  for these reasons  it is very attractive to integrate rnn capabilities locally in embedded mobile  and wearable platforms  making them capable of state of the art voice and speech recognition autonomously and independent from external servers. nonetheless  while much attention has recently  been dedicated to the deployment of embedded low power inference accelerators for forward only  deep networks deployment        making rnns energy efficient is a fundamentally harder problem   the necessity to keep and update an internal state and the widespread usage of densely connected  layers translate to very large memory footprint and high bandwidth requirements.          integrated systems laboratory  eth zurich.  energy efficient embedded systems laboratory  university of bologna.     in this work  we present a twofold contribution towards the deployment of rnn based algorithms in  devices such as smartphones  smartwatches and wearables. first  we designed c hipmunk  a small  and low energy hardware accelerator engine targeted at real time speech recognition and capable to  operate autonomously on moderate size lstm networks. we present silicon results from a prototype  chip containing a c hipmunk engine  which has been fabricated in umc    nm technology  the  chip can achieve up to  .  gop s at maximum efficiency operating point    .   v   consuming  only  .   mw.  second  we conceived a scalable computing architecture  apt to operate on bigger lstm models  as well. as the main limitation to the deployment of big rnns in embedded scenarios stems from  their memory boundedness  we designed the c hipmunk engines so that they can be replicated  in a systolic array  cooperating on a single bigger lstm network. this methodology allows the  acceleration of large scale rnns  which can be made fast enough to operate in real time under  realistically tight time  memory and battery constraints without requiring complex  power hungry  and expensive high bandwidth main memory interfaces.         related work    a recent thorough survey of efforts on hardware acceleration and design of efficient shows that few  efforts have been focused on rnn inference    . we thus focus on this application  surveying stateof the art implementations from data center to ultra low power accelerators in the remainder of this  section.  data center workloads for rnns are often offloaded to gpus or specialized semi independent coprocessors such as google s tensor processing unit  tpu       consuming in the order of        w.  the tpu is a unified architecture to target dnns with convolutional and densely connected layers  as well as lstms. however  tpus suffer from low utilization when running rnns. yet     of the  workload running on google s tpus is devoted to rnn inference       showing their relevance in  commercial applications.  in a lower power range  tens of watts   several fpga implementations can be found. the efficient speech recognition engine  ese       targets the deployment of rnns on a xilinx ultrascale  fpga. to maximize efficiency and address the memory boundedness of rnns  it heavily focuses  on network quantization and pruning of the recurrent topologies and thus this accelerator engine  is mainly targeted at sparse matrix vector operations. rybalkin et al.      also target bidirectional  lstms in their fpga accelerator. bidirectional lstms have been shown to obtain better accuracy  in some cases     but are less attractive for an online  real time scenario as they inherently increase  the network latency. finally  deepstream      is a small hardware accelerator deployed on a xilinx  zynq      targeted at text recognition with rnns. it requires to continuously stream in weights   which makes it impractical for big rnn topologies with millions of weights.  the only published ultra low power  few mw  implementation  the dnpu       uses two separate  special purpose engines for convolutional layers  called cp   on one side  and fully connected and  recurrent ones on the other  called frp . the frp does not include any particular facilities to address  the stateful nature of rnns  and it includes only a small amount of memory     kb  making external  memory accesses necessary for even small rnns  thus limiting peak performance by introducing a  serious bandwidth bottleneck.        .     architecture  operating principle    long short term memory  lstm  network layers      are often described with the following set of  canonical equations   it  ft  ct  ot  ht        wxi xt   whi ht     wci ct     bi        wxf xt   whf ht     wcf ct     bf      ft ct     it tanh wxc xt   whc ht     bc        wxo xt   who ht     wco ct   bo      ot tanh ct                                 where x is the input state vector  i  f   o are called input  forget and output gates respectively  c  and h are the cell and hidden states. the subscript indicates either the current state t or the previous        xt    ht         wxf         whf    ct            wxi         whi    x       ft            wxc         whc    bc              chip       y     w         chip        w         chip       y     w         ct    wco    who    bo  computed across all chips    chip        w         it  tanh    wxo    x       wci    bi       systolic matrix multiplication    wcf    bf            tanh         ot         ht       why         yt    comp. only in right most chips    figure    data dependency graph of a lstm. the majority of computations are the vector matrix  mult.  green  and can be distributed across multiple chips. top right  distribution of a vector matrix  mult. to a systolic array of chips.  t      and denotes element wise multiplication  . the characteristic dimensions of all vectors and  matrices depend on the size of the input state  nx   and on that of the hidden state  nh  . multiple  lstm layers can be connected by using the hidden state of one layer as input of the next. finally   lstm networks often include a final densely connected layer without recurrence  yt     why ht  .  in c hipmunk  we exploit two distinct observations regarding lstms. first  all compute steps  are based on the same set of basic operations  i  matrix vector products  ii  element wise vector  products  and iii  element wise non linear activations. the internal datapath of c hipmunk can be  configured to execute these three basic operations  section  .   and the lstm state parameters are  stored on chip. second  the vast amount of data required to compute one time step of a rnn are  the weights. storing them on chip is thus essential to achieve high energy efficiency. to this end   we a large share of the overall chip area is dedicated to sram to keep the weights local. for larger  lstms not fitting on a single chip  we allow operation in a systolic mode where the weights are  split across multiple connected chips and only the much smaller intermediate results are exchanged  as further discussed in section  . .   .     tile architecture    a product between a matrix of size a   b and a vector of size b is composed of two nested loops   i.e. in pseudo code   for a in range    a      row loop  for b in range    b     column loop  z    w a b    x b   in c hipmunk  the row loop is executed on multiple parallel units  while the inner loop is executed  sequentially.  fig.  a shows a high level diagram of the c hipmunk lstm datapath that implements this functionality. nlstm parallel lstm units are used to execute all the iterations of the row loop at the same  time. each lstm unit is composed of an embedded memory bank to store weights  w    registers  for storing the ot   ft   it and ct values locally  a multiply accumulate unit and two lookup tables to  implement the non linear activation functions. xt and ht are kept outside of the lstm units  in a  bank of nlstm registers. at each cycle of a column loop  one element of the input state and one  of the hidden state are selected depending on the iteration index and broadcast to all lstm units.  fig.  b shows the basic operation loops composing a lstm network deployed on c hipmunk.     in most literature eqs.          and     use matrix notation for wci   wcf and wco   however as these  matrices are diagonal by construction  we use the element wise product notation here for consistence with what  is actually implemented in the c hipmunk hardware.           a  lstm datapath.     b  sequence of datapath basic operation loops.    figure    lstm datapath used in c hipmunk and typical sequence of operations. the datapath can  be used to implement the operations in eqs.     to     by appropriately controlling the muxes and  clearing the register states.  all state variables use   bit fixed point precision  while    bits are used within the multiplyaccumulate block to minimize overflows. i o is performed via an input stream port and an output  stream port  each consisting of   bits of data and   bits to enable a simple ready valid handshake.  weights are loaded at the beginning of the computation of a lstm layer  and inputs are streamed in  sequentially. the internal state of the lstm cell in terms of cell state and hidden state is retained  between consecutive lstm input  frames  to implement the recurrent nature of the network. a  c hipmunk engine can be used to implement a full lstm network with nx   nh   nlstm storing  the weights on chip. larger networks require to stream them in from an external source.   .     systolic scaling    as the main target of the c hipmunk accelerator is to enable ultra low latency applications such  as on device real time speech recognition  the computing power of a single engine might not be  sufficient. a single engine cannot be arbitrarily scaled up  lstm units are all coupled to the same  set of registers via simple multiplexers  making it impractical to increase nlstm above a few hundred  units. instead  to provide a more scalable and elegant solution  we designed c hipmunk so that  multiple engines can be connected as tiles and share the burden of the rnn computation in a spatial  fashion.  fig.   shows how the computation is split between multiple tiles in the case of a     array. the input  state is split into vectors of size nlstm and each vector is broadcast vertically along a column. the        figure    c hipmunk tile i o and operation of a     systolic array during the load of the input state  xt   computation of the new it   ot   ft   ct   ht state values  and redistribution of the updated hidden  state ht .  chipmunk    tile   weight  sram       weight  sram       weight  sram       weight  sram       weight  sram       weight  sram        weight  sram        weight  sram       weight  sram       core pitch   .   mm    lstm cells    weight  sram       weight  sram       weight  sram       technology    umc   nm  hvt std cells    area  core      .    mm     area  die      .    mm     i o       logic pins    power pins    .  kb    memory    performance    gop s   .   v  power  core   op. range     .  gop s   .  v     mw   .   v   .  mw   .  v   .  v    .  v    die pitch   .   mm    figure    microphotograph of a c hipmunk die.  new value for the internal gates states is computed by accumulating the results computed by each  row. finally  the last column can compute the output hidden state  which is broadcasted vertically  to the columns for the next iteration  cf. fig.  c . for a given network size systolic configuration   these connections can be hard wired such that no external multiplexing is required.         results   discussion     .     silicon prototype   comparison with state of the art    we designed and built a silicon prototype based on a single c hipmunk tile as described in section   . . the prototype chip was fabricated in umc    nm technology  using high voltage threshold  cells to minimize leakage. it features nlstm      lstm units  which hold their weight and bias  parameters in    separate sram banks    .  kb in total . the full chip  shown in fig.    occupies   .   mm  including the pads. the chip exposes the interface described in section  .  for tile to tile  communication  so that it would be possible to prototype a systolic array using many discrete chips.  fig.   shows the experimental results obtained by testing the c hipmunk prototype at room temperature      c . the prototype is fully functional in an operating range between  .   v  limited  by sram failure  and  .   v  corresponding to a range of    to     mhz of maximum clock  frequency and from  .   to    mw of power consumption. the peak performance in terms of operations per second  of one c hipmunk chip is   .  gop s  at  .   v  and the peak energy efficiency    .   gop s mw  is reached at  .   v.  table   compares architectural parameters and synthetic results between c hipmunk and the existing vlsi and fpga based implementations for which performance and energy numbers have been  published. our work reaches comparable performance with the dnpu proposed by shin et al.     .  performance is obviously below that claimed by google tpu       but this is mostly due to the  different size. in fact  despite the tpu uses    nm integration  c hipmunk has  .   better area       as customary for neural network accelerators  we count   multiply accumulate as   operations.                              umc    nm cmos  core   .   mm   die   .   mm      kb       bit       .   v    .   v           mhz    .      .   mw    .     .  gop s   .      .   gop s mw    .  gop s mm        nm cmos  core     .  mm   die    .  mm      kb      bit       .  v    .   v           mhz        .  mw        .   gop s   .      .   gop s mw    .  gop s mm     dnpu          nm cmos     die       mm      mb       bit    k         mhz        w   .   .  top s    .   gop s mw    .   .  gop s mm     google tpu       xilinx xcku        k lut     k ff   .  mb     bit  pruned            mhz     w   .  top s  equiv.     .    gop s mw        han et al.         xilinx z        k lut    k ff     dsp      kb       bit            mhz      w      gop s   .     gop s mw       rybalkin et al.         xilinx z       . k lut    k ff     dsp        bit            mhz   .  w   .    gop s   .       gop s mw       chang et al.         the dnpu is a mixed cnn rnn processor. we report here only the figures related to the rnn subunit.  we present here the values from      based on the two lstms for which they measured the performance. for both  the tpu is severely memory bandwidth  limited.  they assume a well structured sparsity of   .   in the weight matrices. reported numbers are dense equivalent throughput. underlying compute throughput       gop s.    on chip memory  arithmetic  number of macs  core voltage  frequency  power  peak performance  energy efficiency  area efficiency    technology  area    this work    table    comparison to existing vlsi and fpga implementations      .      .     highest performance     gop s    .   v       mhz     .    .      .     core power  mw     core voltage  v      .      .    .      .    .      .     highest efficiency   .   gop s mw    .   v      mhz                                  frequency  mhz                           .      .      .      .      .      .     energy efficiency  gop s mw      .      .    .     core voltage  v     figure    frequency  power and performance of the c hipmunk prototype versus operating voltage  at room temperature      c . the left shmoo plot shows core voltage versus operating frequency   the color shade corresponds to the core power consumption  darker less power . the right plot  shows energy efficiency versus core voltage  the color shade of the scattered dots corresponds to the  core power  while their size is proportional to the maximum frequency.  table    ctc  l    h uni speech recognition lstm executed on c hipmunk with a    ms  constraint  configuration    perf   .   v    eff   .   v    execution time    systolic        systolic      single     .   ms   .   ms    .   ms     .   ms    .   ms     .   ms    peak power    systolic        systolic      single        .   mw     .   mw    .   mw       .   mw    .   mw   .   mw    systolic        systolic          .   mw    .   mw      .   mw       average power    efficiency   and a performance wise  tpu equivalent  array with      c hipmunk engines would  consume only  .   w  an order of magnitude less than the tpu. c hipmunk advances the state ofthe art energy efficiency with respect to the dnpu  showing a     improvement. moreover  the  dnpu does not include any provision to address the fundamental memory boundedness of rnns   which c hipmunk addresses via systolic scaling. all fpga implementations         are at least  two orders of magnitude less energy efficient.  in terms of arithmetic precision we have chosen to use   bit fixed point representations for storage  and perform the mac operations with    bit precision. this is in line with google s tpu and higher  than the     bit of the dnpu.   .     real world speech recognition    to evaluate c hipmunk on a real world problem  we targeted ctc  l    h uni  a   layer     hidden units per layer lstm topology introduced by graves et al.      which takes as input a stream  of     mel frequency cepstral coefficients  mfccs  extracted from an audio stream and identifies  phonemes with an error rate of   .    evaluated on the timit database. the mfcc input  frames   are produced with a    ms rate  which means that any embedded low latency real time rnn implementation should be able to elaborate the full network in less than this time. we evaluate three  different c hipmunk configurations  a systolic array of    units  divided in   sub arrays of        engines  a single array of       engines  and a single c hipmunk engine. the largest configuration  can host the full topology in a spatial fashion  each of the sub arrays hosts one layer of the rnn.  after the initial programming phase  it does not need any reprogramming. the smaller arrays need        to be reconfigured at each new layer  in the       array case  or multiple times per layer  in the  single unit case .  table   reports execution time and power for these three configurations. execution times include  both computation and reconfiguration  excluding only the initial configuration which doesn t need  to be repeated for each new frame layer. bold time power values indicate configurations that can  meet the    ms deadline. as the ctc  l    h uni topology has    .        weights  a            systolic configuration is best used  all weights stored locally . smaller configurations imply a        overhead for reloading weights.  average power  also shown in table    is computed under the assumption that the array is perfectly  duty cycled when not in use over the    ms window. even in the assumption that the c hipmunk  array is always on  the   .   mw required to process this network would only add     to idle  power on a typical smartphone      to     mw      . adding a filter to drop clearly uninteresting  input  e.g. silence  would likely decrease this overhead by an order of magnitude.         conclusion    we have presented an architecture and silicon measurement results for a small   .  mm    rnn hardware accelerator providing  .  gop s at  .  mw in    nm digital cmos technology  resulting in new  state of the art energy and area efficiencies of  .   gop s mw and   .  gop s mm  . the systolic  design is scalable to accommodate also large rnns efficiently by connecting multiple identical  chips on the circuit board.    acknowledgements  this work was supported in part by the eu project exanode under grant h             and in  part by the swiss national science foundation project micropower deep learning.    