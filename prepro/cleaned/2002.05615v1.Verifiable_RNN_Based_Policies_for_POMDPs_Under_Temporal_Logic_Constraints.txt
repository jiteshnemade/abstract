introduction  research in the reinforcement and supervised learning communities has demonstrated the utility of recurrent neural  networks  rnns  in synthesizing control policies in domains that exhibit temporal behavior  tsoi and back         bakker        heess et al.       . the internal memory  states of rnns  such as in long short term memory   lstm  architectures  hochreiter and schmidhuber          effectively account for temporal behavior by capturing the  history from sequential information  pascanu et al.       .       contact authors    furthermore  in applications that suffer from incomplete information  rnns leverage history to act as either a state or  value estimator  s rensen        wierstra et al.        or as  a control policy  hausknecht and stone       .  in safety critical systems such as autonomous vehicles   policies that are guaranteed to prevent unsafe behavior  are necessary. we seek to provide formal guarantees for  policies represented by rnns with respect to temporal  logic  pnueli        or reward specifications. such a verification task is  in general  hard due to the complex  often  non linear  structures of rnns  mulder et al.       . existing work directly employs satisfiability modulo theories   smt   wang et al.        or mixed integer linear program  milp  solvers  akintunde et al.         however  such  methods not only scale exponentially in the number of variables but also rely on constructions using only rectified linear  units  relus .  we take an iterative and model based approach  summarized in fig.  .  in particular  we extract a policy in the form of a so called finite state controller   fsc   poupart and boutilier        from a given rnn.  first  we employ a modification of a discretization technique called quantized bottleneck insertion  introduced  in  koul et al.       . basically  the discretization facilitates  a mapping of the continuous memory structure of the rnn  to a pre defined number of discrete memory states and transitions of an fsc.  however  this standalone fsc is not sufficient to prove  meaningful properties. the proposed approach relies on the  exact behavior a policy induces on a specific application   e.g. construction of verifiable policies for partially observable markov decision processes  pomdps .we apply the extracted fsc directly to a formal model  e.g. a pomdp that  represents such an application. the resulting restricted model  is amenable for efficient verification techniques that check  whether a specification is satisfied  baier and katoen       .  if the specification does not hold  verification methods typically provide diagnostic information on critical  parts of the model in the form of so called counterexamples. we propose to utilize such counterexamples to  identify improvements in the extracted fsc or in the underlying rnn. first  increasing the amount of memory  states in the fsc may help to approximate the behavior  of the rnn more precisely  koul et al.       . second      the rnn may actually require further training data to induce higher quality policies for the particular application.  existing approaches rely  for example  on loss visualization  goodfellow and vinyals         but we strive to exploit  the information we can gain from the concrete behavior of  the rnns with respect to a formal model. therefore  in  order to decide whether more data are needed in the training of the rnn or whether first the number of memory  states in the fsc should be increased  we identify those critical decisions of the current fsc that are  arbitrary . basically  we measure the entropy  cover and thomas         biondi et al.        of each stochastic choice over actions the  current fsc based policy makes at critical states. that is  if  the entropy is high  the decision is deemed arbitrary despite  its criticality and further training is required.  we showcase the applicability of the proposed method on  pomdps. with their ability to represent sequential decisionmaking problems under uncertainty and incomplete information  these models are of particular interest in planning and  control  cassandra       . despite their utility as a modeling formalism and recent algorithmic advances  policy synthesis for pomdps is hard both theoretically and practically  meuleau et al.       . for reasons outlined earlier   rnns have recently emerged as efficient policy representations for pomdps  hausknecht and stone         but the  task of verifying the induced behavior is far more difficult  than that for fscs. we detail the proposed approach on  pomdps and combine the scalability and flexibility of an  rnn representation with the rigor of formal verification to  synthesize pomdp policies that adhere to temporal logic  specifications.  we demonstrate the effectiveness of the proposed synthesis approach on a set of pomdp benchmarks. these  benchmarks allows for a comparison to well known pomdp  solvers  both with and without temporal logic specifications.  the numerical examples show that the proposed method      is more scalable  by up to   orders of magnitude  than wellknown pomdp solvers and     achieves higher quality re     critical information  for network retraining  entropy  test    increase  precision  diagnostics on  induced behavior    rnn based  policy    extraction  as an fsc    formal  verification  system model   e.g.  a pomdp     figure    high level iterative policy extraction process.    sults in terms of the measure of interest than other synthesis  methods that extract fscs.  related work. closest to the proposed method  is  carr et al.         which introduced a verificationguided method to train rnns as pomdp policies. in  particular   carr et al.        extracts polices from the rnns  but unlike the proposed method  the extracted policies do not  directly exhibit the memory structure of the rnns and were  instead handcrafted based on knowledge about the particular  application.  there are three lines of related research. the first one concerns the formal verification of neural network based control policies. two prominent approaches  huang et al.         katz et al.        for the class of feed forward deep neural  networks rely on encoding neural networks as smt problems  through adversarial examples or relus architectures respectively.  akintunde et al.        concerns the direct verification of rnns with relu activation functions using smt or  milp. however  the scalability of these solver based methods  suffer from the size of the input models. we circumvent this  shortcoming by our model based approach where verification  is restricted to concrete applications followed by potential improvement of the rnns.  the second relevant direction concerns the direct synthesis of fscs for pomdps without neural networks.  for example   meuleau et al.        uses a branch andbound method to compute optimal fscs for pomdps.   chatterjee et al.        uses a sat based approach to compute fscs for qualitative properties.  junges et al.         constructs an fsc using parameter synthesis for markov  chains.  third  existing work that concerns the extraction of fscs  from neural networks  zeng et al.        weiss et al.         finnegan and song        michalenko et al.         does not  integrate with formal verification to provide guarantees for  extracted policies or to generate diagnostic information.      preliminaries  a probability distribution  over a set x is a function     x    p           r with x x   x      x     . the set of all  distributions on x is distr x . the support of a distribution    is supp       x   x  p    x      . the entropy of a  distribution   is h         x x   x  log x    x .    pomdps. a markov decision process  mdp  m is a tuple  m    s  act  p  with a finite  or countably infinite  set s of  states  a finite set act of actions  and a transition probability  function p   s   act   distr  s . the reward function for  states and actions is given by r   s   act   r. a finite path    of an mdp m is a sequence of states and actions  last     is the last state of  . the set of all finite paths is pathsm  fin .  definition    pomdp . a pomdp is a tuple m     m   z  o   with m the underlying mdp of m  z a finite  set of observations  and o   s   z the observation function.  for pomdps  observation action sequences are based on a  finite path     pathsm  fin of m and have the form  o       a   a   o s       o s             o sn  . the set of all finite  observation action sequences for a pomdp m is obsseqm  fin .     definition    pomdp policy . an observation based policy  for a pomdp m is a function     obsseqm  fin    distr act       such that supp   o       act last    for all      m  pathsm  fin .  z is the set of observation based policies for m.  a policy for a pomdp resolves the nondeterministic  choices in the pomdp  based on the history of previous observations  by assigning distributions over actions. a memoryless observation based policy      m  z is given by     z    distr  act   i. e.  decisions are based on the current observation only. a pomdp m together with a policy   yields an  induced discrete time markov chain  mc  m  . an mc does  not contain any nondeterminism or partial observability.  our definition restricts pomdp policies to finite memory   which are typically represented as fscs.  definition    finite state controller  fsc  . a k fsc for a  pomdp is a tuple a    n  ni         where n is a finite set  of k memory nodes  ni   n is the initial memory node    is  the action mapping     n   z   distr act   and   is the  memory update     n   z   act   n .  an fsc has the observations z as input and the actions  act as output. upon an observation  depending on the current  memory node the fsc is in  the action mapping   returns a  distribution over act followed by a change of memory nodes  according to  . fscs are an extension of so called moore  machines  moore         where the action mapping is deterministic  that is      n   z   act  and the memory update      n   z   n does not depend on the choice of action.  definition    specifications . we consider linear time temporal logic  ltl  properties  pnueli       . for a set of  atomic propositions ap   which are either satisfied or violated by a state  and a   ap   the set of ltl formulas is         a                                  u    .    intuitively  a path   satisfies the proposition a if its first  state does            is satisfied  if   satisfies both    and          is true on   if   is not satisfied. the formula   holds  on   if the subpath starting at the second state of   satisfies       satisfies     if all suffixes of   satisfy  . finally     satisfies     u      if there is a suffix of   that satisfies    and  all longer suffixes satisfy    .     abbreviates  true u   .  for pomdps  one wants to synthesize a policy such that  the probability of satisfying an ltl property respects a given  bound  denoted     p       for                  and            . in addition  undiscounted expected reward properties      e      a  require that the expected accumulated cost  until reaching a state satisfying a respects     r   .  a specification   is satisfied for pomdp m and   if it is  satisfied in the mc m   m       .  policy network. we now define a general notion of an rnn  that represents a pomdp policy.  definition    policy network . a policy network for a  pomdp is a function      obsseqm  fin   distr  act .  the underlying rnn which receives sequential input in the  form of  finite  observation sequences from obsseqm  fin   the  output is a distribution over actions  see fig.  a. to be more  precise  we identify the main components of such a network.    definition    components of a policy network . a policy  network    is sufficiently described by a hidden state update  function      r   z   act   r and an action mapping   h   r   distr  act .  consider the following observation sequence   a    a                o si    o s        o      o s                 the policy network receives an observation and returns an  action choice. throughout the execution of the sequence  the  rnn holds a continuous hidden state h   r  occasionally described as an internal memory state  which captures previous  information. on each transition  this hidden state is updated  to include the information of the current state and the last action taken under the hidden state transition function   . from  the prior observation sequence in      the corresponding hidden state sequence would be defined as   a    o s       a    o s               h          h                hi  additionally  the output of the policy network is expressed  by the action distribution function  h   which maps the value  of hidden state to a distribution over the actions. at internal memory states hi   we have    hi   o si    ai     hi   and   h  hi         act  for state si on path  . note that a policy  network characterizes a well defined pomdp policy.      problem statement  we attempt to solve two separate but related problems       for a pomdp m  a policy network    and a specification     the problem is to extract an fsc a      m  z such that  ma       .     if the extraction process fails to produce a  suitable candidate  then we improve the policy network       for which we can solve    .     .     outline    fig.   illustrates the workflow of the proposed approach for a  given pomdp m  policy network    and specification  . we  summarize the individual steps below and provide the technical details in the subsequent sections.  fsc extraction. we first quantize the memory nodes of the  policy network     that is  we discretize the memory update of  the continuous memory state h. from this discrete representation of the memory update  we construct an fsc a      m  z .  the procedure has as input the number bh of neurons which  defines a bound on the number of memory nodes in the fsc.  verification. we use the fsc a   to resolve partial information and nondeterministic choices in the pomdp m   resulting in an induced mc ma   . we evaluate whether  the given specification   is satisfied for this induced mc  using a formal verification technique called model checking  baier and katoen       . if the specification   holds   then the synthesis is complete with output policy a   . however  if   does not hold  then we decide if we shall increase  the bound bh on the number of memory nodes or if the network needs retraining. in particular  we examine whether or  not the entropy over the fsc s action distribution is above a  prescribed threshold.     counterexamples  set critm  a   of  critical states    yes    increment  discretization  bh   bh        up    recurrent  neural network  policy network                   fsc extraction  memory nodes  n      bh    s   s     down    up  down  s            s     a    s     a    up    down     a    state pomdp    no    entropy check  h critm  a             finite policy  policy a      up       up    p    unsat    model checking  ma           induced model  dtmc ma          blue     p    sat    concrete model  pomdp m  specification      figure    procedural flow for the iterative fsc extraction and rnnbased policy improvement.    policy improvement. in the high entropy case  we increase  the discretization level  that is  we increase bh   and construct  the fsc a   with additional memory states at its disposal.  whereas in the other case  additional memory nodes may  cause the extracted fsc to be drawn from extrapolated information and we instead seek to improve the policy network.  for that  we use diagnostic information in the form of counterexamples to generate new data  carr et al.       .        blue         blue     down       down   b    fsc     c    fsc    figure     a  pomdp for example   with  b    fsc and  c    fsc.  both fscs are defined for observing  blue  and subsequent action  choices that may result in a change of memory node for the   fsc.    we can create an fsc a  that ensures the satisfaction of        up  with probability          blue     down with probability        up  with probability          blue     down with probability          blue  up             blue  down     .      policy extraction  example  . we consider the pomdp in fig.   as a motivating example for the necessity of memory based fscs. the  pomdp has three observations   blue   s  and s    where  observation  blue  is received upon visiting s    s    and s  .  that is  the agent is unable to distinguish between these  states. the specification is     pr  .     s     so the agent  is to reach state s  with at least probability  . . in a   fsc   i.e. one memory node     we can describe an fsc a  by          blue          up  down         z  a         with probability p   with probability     p      z   z  a   act.    a   fsc with two memory nodes    and     see fig.  c  allows  for greater expressivity  i.e. the policy can base its decision  on larger observation sequences. with this memory structure     in this section we describe how we adapt the method called  quantized bottleneck insertion  koul et al.        to extract  an fsc from a given rnn. let us first explain the relationship between the main components of a policy network      definition    and an fsc a  definition   . in particular   the hidden state update function      r z  act   r takes as  input a real valued hidden state of the policy network  while  the memory update function of an fsc takes a memory node  from the finite set n . the key for linking the two is therefore  a mechanism that encodes the continuous hidden state h into  a set n of discrete memory nodes.  policy network modification. to obtain the above linkage   we leverage an autoencoder  goodfellow et al.        in the  form of a quantized bottleneck network  quantized bottleneck  network  qbn    koul et al.       . this qbn  consisting  of an encoder and a decoder  is inserted into the policy network directly before the softmax layer  see fig.  b. in the encoder  the continuous hidden state value h   r is mapped to  an intermediate real valued vector rbh of pre allocated size  bh . the decoder then maps this intermediate vector into a     recurrent  layer    entropy of the extracted fsc    softmax  layer    a   z    a          rnn    a    a  rnn policy.    encoder    decoder        rnn    h        h          b  rnn block and associated qbn of bh     with quantized activation      r             .  figure    policy network structure without and with a qbn.    discrete vector space defined by           bh . this process   illustrated in fig.  b  provides a mapping of the continuous  hidden state h into  bh possible discrete values. we denote  the discrete state for h by h  and the set of all such discrete  states by h . note  that  h      bh since not all values of  the hidden state may be reached in an observation sequence.   koul et al.        has another qbn for a continuous observation space  however  we focus on discrete observations and  can neglect the additional autoencoder.  fsc construction. after the qbn insertion we simulate a  series of executions  querying the modified rnn for action  choices  on the concrete application  e.g. using a pomdp  modelx. we form a dataset of consecutive pairs  h t   h t      of discrete states  the action at and the observation zt   that  led to the transition  h t   at   zt     h t     at each time t during  the execution of the policy network. the number of accessed  memory nodes n   h  corresponds to the number of different discrete states h    h  in this dataset. the deterministic  memory update rule   nt   at   zt       nt   is obtained by  constructing a n     z     act   transaction table  for a detailed description see  koul et al.       . we can additionally  construct the action mapping     n   z   distr  act  with    nt   zt         distr  act  by querying the softmax output  layer  see fig.  a  for each memory state and observation.    average entropy h  a      input       bh      bh      bh         .    .    .    .                    number of samples           figure    entropy of the extracted fscs from an rnn as it is trained  with more samples. for each sequence we fix the discretization  ignore the inner loop and add more samples guided by the counterexamples. note  the behavior at the right edge of the figure is due to  the fact that this represents the entropy of the entire fsc  which will  differ from the entropy over the components of the counterexamples.    extracted fsc a   regarding  . model checking provides the  probability  or the expected reward  to satisfy a specification  for all states s   s via solving linear equation systems.  example    cont. . consider the case in the   fsc a    fig.  b  where p      the probability of reaching the state  s  in the induced mc is pr   s         . clearly  the behavior induced by this   fsc violates the specification and  formal verification provides two counterexamples of critical  memory state pairs for this policy a         s    and     s   .  after model checking  if the specification does not hold   the policy may require refinement. as discussed before  on  the one hand we can increase the upper bound bh on the number of memory nodes to extract a new fsc. at each iteration  of the inner loop in fig.    we modify the qbn for the new   increased  level of discretization and obtain a new fsc using  the process outlined in sect  . on the other hand  we may  decide via a formal entropy check whether new data need to  be generated to actually improve the policy.      policy evaluation and improvement    improving the policy network. our goal is to determine  whether a policy network requires more training data or not.  existing approaches in supervised learning methods leverage benchmark comparisons between a train test set using a  loss function  baum and wilczek       . loss visualization   proposed by  goodfellow and vinyals        yu et al.         provides a set of analytical tools to show model convergence. however  such approaches are generally more suited  to classes of continuous functions than the discrete representations we seek. more importantly  we want to leverage  the information we gain from employing a model based approach.    evaluation using formal verification. we assume that for  pomdp m    m   z  o  and specification    we have an  extracted fsc a      m  z as in definition  . we use the policy  a   to obtain the induced mc ma   . for this mc  formal verification through model checking checks whether ma         and thereby provides hard guarantees about the quality of the    counterexamples. we first determine a set of states that  are critical for the specification under the current strategy.  consider the sequences of memory nodes and observations  at    a               nt   zt   from the pomdp m under   n    z        the fsc a   . for each of these sequences  we collect the  states s   s underlying the observations  e.g.  o s    zi for     problem  maze     maze     maze     maze      grid     grid     grid     grid      grid      navigation      navigation      navigation       navigation          s   z  type        min        min        min        min       min        min        min         min         min          max          max          max   .            max    extraction approach  memory value time  s       .      .        .       .        .       .        .       .        .      .        .       .        .       .        .       .         .       .        .       .        .       .        .       .        .       .      handcrafted  memory value time  s       .      .        .      .        .      .          .       .        .      .        .      .       .       .        .       .          .       .        .      .        .       .        .       .        .       .      prism pomdp  value time  s    .     .     .     .       .        .    mo  mo   .     .      .       .    mo  mo  mo  mo  mo  mo   .        .    mo  mo  mo  mo  mo  mo    solvepomdp  value time  s    .     .     .     .      .       .    mo  mo   .     .     .     .     .     .    mo  mo  mo  mo  na  na  na  na  na  na  na  na    table    synthesizing strategies for examples with expected reward and ltl specifications.        i   t. as we know the probability or expected reward for  these states to satisfy the specification from previous model  checking  we can now directly assess their criticality regarding the specification. we collect all pairs of memory nodes  and states from n   s that contain critical states and build  m  the set crita    n   s that serves us as a counterexample.      these pairs carry the joint information of critical states and  memory nodes from the policy applied to the mc and may be  formalized using a so called product construction.  entropy measure. the average entropy across the distrim  butions over actions at the choices induced by crita  is      our measure of choice to determine the level of training for  the policy network. put differently  for each pair  n  s     m  crita    we collect the distribution     distr act   over ac    tions that a   returns for the observation o s  when it is in  memory node n. then  we define the evaluation function h  using the entropy h    of the distribution     h   critm  a            with h n  s    h              for high values of h  the distribution is uniform across all actions and the associated policy network is likely extrapolating  from unseen inputs.  in fig.    we observe that when there are fewer samples  and higher discretization  the extracted fsc tends to perform  m  arbitrarily. we define the function h for the full set crita         m  h crita                critm  a        x    h n  s             n s  critm  a          we compare the average entropy over all components of the  counterexample against a threshold             that is  if  m  h crita         we will provide more data.      example    cont. . under the working example  the policy a  was the   fsc with p      fig.  b   which  produces two counterexample memory and state pairs   critm  a         s         s    . the procedure would then examine the policy s average entropy at these critical components  n  s    critm  a    which in this trivial example is given  by h critm  a       p log   p         p  log       p      from     . the average entropy is below a prescribed threshold        .   and thus we increase the number of memory nodes   which results in the satisfying fsc a  in fig.  c.    collecting new training data. to perform retraining we  take a policy for the underlying mdp m that satisfies    and  we use that policy to generate observation action sequences   which are initialized at the states s in the critical set critm  a   .  these executions  with observations as inputs and actions as  labels  form a batch for retraining the rnn policy network   .      experiments  we evaluate the proposed verification and synthesis approach by comparing to a series of benchmark examples that are subject to either ltl or expected cost specifications. for both sets we compare to two synthesis  tools  prism pomdp  norman et al.        and solvepomdp  walraven and spaan        from the respective formal methods and planning communities. we further compare to another rnn based synthesis procedure with a handcrafted memory structure for fscs  carr et al.       . for  proper comparison with the synthesis tools we adopt the experiment setup of  carr et al.         whereby we always iterate for    instances of retraining the rnn from counterexample data. similarly  the proposed approach is not guaranteed  to reach the optimum  but shall rather improve as far as possible within    iterations.  implementation and setup. we provide a python  toolchain that employs the probabilistic model checker  prism. we train and encode the rnn policy networks     using the deep learning library keras. all experiments are  run on a  .  ghz machine with a    gb memory limit and a  maximum computation time of     seconds.  settings. we analyze the method on three different settings  maze c   grid c  and navigation c   for detailed descriptions of these examples see  norman et al.        and   carr et al.         respectively. in each of these settings the  policies have an action space of the cardinal directions navigating through a grid based environment. for the former two  examples  we attempt to synthesize a policy that minimizes an  expected cost subject to reaching a goal state a  em  min    a .  in the latter example  we seek a policy that maximizes the  probability of satisfying an ltl specification  in particular  avoiding obstacles x  both static and randomly moving  until  reaching a target area a  pm  max   x u a .  discussion. the results are shown in table  . the proposed extraction approach scales to significantly larger exam      ples than both state of the art pomdp solvers which compute near optimal policies. while the handcrafted approach  scales equally well  the extraction method produces higherquality policies   within    of the optimum. that effect  is due to our automatic extraction of suitable fscs. note  that an optimal policy for maze    can be expressed using    memory states. the fsc structure employed by the handcrafted method uses this structure and consequently  for the  small maze environments  the handcrafted method synthesizes higher values. yet  with larger environments the fixed  memory structure produces poor policies as more memory  states are beneficial to account for the past behavior.      conclusion  we introduced a novel synthesis procedure for extracting and  verifying rnn based policies. in comparison to other approaches to verify rnns  we take a model based approach  and provide guarantees for concrete applications modeled by  pomdps. based on the verification results  we propose a  way to either improve the extracted policy or the rnn itself.  our results demonstrate the effectiveness of the approach and  that we are competitive to state of the art synthesis tools.    