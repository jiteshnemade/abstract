introduction    rnns have been widely adopted for its high accuracy on temporal  sequence analysis  such as machine translation      speech recognition      or even stock price prediction     . however  the increasingly large model size and tremendous computational workload  of the rnn hampers its deployment on embedded  edge  devices   which strictly demand realtime processing with limited hardware resources. to address this issue  weight pruning techniques               have been proposed  which shrink the model size  reduce storage  demand  and provide higher potential hardware performance by  eliminating the redundant  close to zero  weights and the corresponding arithmetic operations in inference.  the weight pruning schemes in some existing works           are in a non structured fashion and with extreme irregularities in  the computation  which is unfriendly to either the modern parallel device or the hardware architecture design. thus the performance degradation from the hardware inefficiency encroaches  upon the gains from model compression. therefore  researchers  start to explore other pruning approaches  i.e.  structured pruning                in which the regular computation patterns are maintained. although these structured pruned models are relatively  hardware friendly  the coarse pruning granularity  structure  leads  to either a significant degradation on model accuracy or a limited  pruning rate  the weight count ratio of the original model to pruned  model . to keep the accuracy loss acceptable  the attainable pruning  rates delivered in the existing structured pruning schemes are far  lower than that the ones with the non structured pruning  wasting  potential pruning opportunities.  in this paper  we aim to overcome the above limitations. we  first propose a novel fine grained structured pruning technique   csb pruning  that provides a similar compression rate  and test  accuracy  as non structured pruning while offering a higher potential for hardware acceleration than the non structured methods.  during the training phase  each weight matrices are divided into  fine grained blocks  and a structured pruning is conducted on every     ics      june    july          barcelona  spain  cell type  gru  zt    wiz xt   whz ht    ht         xl    zt      ht    x  x  x               cell type  lstm         bz             br        zt    embedded  vector     latexit sha  base     mlx  henkof  dxdtydljupvei   aaadonicfvjnb naef  qev ssury oilkhqzjdkcegq mixskspfefwer  v   rd xs l d hhv djwiudchhlb cohzskfsnz  rm r x aa  boc  tl  y fwd    n v hdx   w otcwuzwmqhvtnadfm iyngyng  lija emwqxb v  jipzwx aey m ukznjekqfd fvwcndrliof ppx nm tknfswkkqvrokr ki r wkcp  nesik  cit gezvp igd v  ip  ipg xutpxak ouemiai dhzkxcngmpegyeb rekwu  rfx wyyt ysjo kctd z xs  v qfnloclfg  dv oypzzf tldzxn onbb entjpzufy utehzblqqraeuk osjao fazqeivmoaexjgztazosmj rl df wegcxeklfkywet vamkqdbzndcv sb dq mb pnc hezuqe qwwjdzgusewsfy ixxyxsiiuqgekm mxtqhilawr  jlubu  j chy df mj  f e ftovyq  qu rhlnqjtte nejjrk p jfrh xt mx t  zv ts  p hqonsp  bw cbnu   latexit     rt    wir xt   whr ht  e  ht     wig xt   whg  rt    r. shi  p. dong  t. geng  y. ding  x. ma  h.k. h. so  m. herbordt  a. li and y. wang    sequence length  l   input frames   e.g.  words  audio  video    ht       e  ht  input    ft    wf x xt   wf h ht  it    wix xt   wih ht  ct   ft    ct           it     latexit sha  base    yii yyhwtd   v x li i   nvu   aaadbhicdvjda swffxsfxtzr nudxtlibzwgsqc rw l hzxvbywdiu mbkry fzcti  nb gu cw  cxvzb j sa osgtcl ovzjlx cgxxipn ew  p     vpog fpt ye gb  cy jrnqfsshuzec ez geoah mstg kiwaxt qappvzglucy wipn  qsmx zssbc v vo  cxgui paddjrfpuqosakkuwymzwji  jcv xjdpqujre vpflfgyimoxhoaj dr  px fssc  pgtotrsntyduzbqyw  i ejasqmunr rg   i grhtp ezqnojb mvtdhldv zhlvy l d  oonwshqg t  m eb mhaum hpwifpiwkcuacql zpdymbuigfpbyn qajytekwwbgbtjkrmz   lcv bzefjqwyjwnco dfgjjqvuoj  nnp drfbitnisgfj  pmslyblthojcyjc  jy  iprecubekq  yppqhshypezbz bxx  m k gftvxydftodnh vv eeh bu qj  h  qz sojoj fjm znpnhfpbfeyeuc bvqfxvnmkouee gfntbuf  latexit                a  element wise pruning   non structured       bf        bi        wcx xt   wch ht    ot    wox xt   woh ht  ht   o t                bc        bo      sru   li gru   future  types      c  fine grained  structured      ct      rnn  cell  context link    hl  output    csb pruning   this work     h  h  h            high pruning rate    hardware unfriendly    output frames   e.g.  translation  prediction    figure    computation flow of rnn inference. note that  there are multiple rnn cell types. the main workload is  matrix vector multiplication  mvm .  block independently. the pruned blocks are encoded in a novel  compressed structured block  csb  sparse format for inference acceleration  which significantly reduces the weight storage demand  while retaining the fine grained content in the original model.  to realize a realtime inference with parallel hardware  there are  still multiple challenges to design an architecture that can exploit  the benefits of csb pruning in a seamless manner. in particular   the parallel architecture should handle massive fine grained blocks  with imbalanced workloads  sparsity  but maintain a high hardware  efficiency  utilization . meanwhile  the architecture should be programmable for various rnn cell types  e.g.  lstm       gru       although the existing rnn architectures are designed for a particular  cell type. to address the issues above  we propose an architecturecompilation co design to realize the best flexibility and acceleration  performance. a programmable rnn dataflow architecture is designed that supports existing rnn cell types. in particular  the  csb engine in our architecture is designed with a novel workload  sharing technique. with the one shot compilation  the workload is  automatically balanced among processing elements  pes  in csbengine  which improves the hardware efficiency to a near theoretical value.  the major contributions are summarized as follows     we present csb rnn  an optimized full stack rnn acceleration  framework  which facilitates running various types of rnns with  faster than realtime latency. csb rnn includes three innovations      an adaptive and fine grained structured compression  technique  csb pruning      a programmable rnn dataflow architecture equipped with csb engine      a compiler design with  optimizations to achieve almost perfect workload balance.    the proposed csb pruning technique provides ultra high   .       pruning rates without any loss on accuracy. furthermore   csb pruning does not incur high degree computational irregularities  making highly efficient hardware acceleration possible.    an architecture compilation co design is proposed to sufficiently  exploit the benefits of csb pruning and provide close to theoretical  peak performance with automatic workload balancing.    with experiments on    rnn models from various application  domains  csb pruning demonstrates  .       lossless pruning  rate  which is  .   to  .   over existing designs. with the proposed architecture compilation co design applied  the csb rnn  delivers faster than realtime inference with the latency of  .   s .   s in an fpga implementation. the proposed framework     b  row column wise  pruning  structured       high pruning rate    hardware friendly      low pruning rate    hardware friendly    figure    csb pruning takes advantage of both nonstructured  random  pruning  a  and coarse grained structured  row column  pruning  b .  contributes to  .      .    lower latency  with even fewer computation resources  and  .      .    improvement on powerefficiency over the state of the art.      background   .  temporal sequence processing with rnn  the recurrent neural networks  rnns  deliver high accuracy in  the temporal sequence processing. a typical schematic of rnn  computation is depicted in fig.  . successive frames  e.g.  word   phoneme  from the temporal sequence  e.g.  sentence  voice  are  embedded as input neuron vectors  x t    and then sent to rnn cells  for inference computation. t represents the time point. the output  neuron vector  ht   contains the inference results  e.g.  translation   prediction  that may have different dimensions with x t .  multiple rnn cell types exist that are composed of different computational dataflow but almost the same arithmetic primitives. fig.    lists the arithmetic of two widely used rnn cells  gru     and  lstm     . the significant workload is matrix vector multiplication   mvm  between the weight matrices and input hidden neurons   and the rest workload is element wise operations  including sigmoid      tanh      activation function  element wise multiplication      and addition. in particular  the rnn cell computation at time t  invokes the intermediate vector c t    and output vector ht    from  the previous timestamp. the data dependency results in a context  link between two successive rnn cell iterations  which helps to  memorize the temporal feature during inference.     .     rnn weight pruning techniques     . .  non structured pruning v.s. structured pruning. the pruning  technique has been proposed for deep learning models to reduce redundant  close to zero  weights and thus the computation workload.  the early non structured pruning      achieves a high pruning rate   however  the random sparse model  fig.   a   brings a high degree  of irregularity to the inference computation  which is unfriendly  to either the modern parallel device or the hardware architecture  design. some existing works         address this issue by pruning  model with region balanced sparsity  between non structured and  structured sparsity   which reduced the attainable pruning rate. as  fig.   b   the structured pruning schemes         were proposed for  hardware friendly purpose that the entire row column is removed  as a whole in pruning. although the pruned model maintains the  regularity and can even be compacted to a dense matrix  the pruning rate with this scheme is relatively low due to the coarse pruning     csb rnn  a faster than realtime rnn acceleration framework with compressed structured blocks    minimize   wi     bi      s .t .    f   wi  in     bi  in      wi   si   i      ...  n    f   wi  in     bi  in           wi     bi      s .t .    n        i  zi      i              wi   zi   i      ...  n    where zi is an auxiliary variable for subproblem decomposition   and the indicator function  eqn.    is used to replace the original  constraint on feasible set.      i  zi           if wi   si       otherwise.           then the eqn.   can be decomposed to two subproblems listed in  eqn.   and eqn.   with the formation of augmented lagrangian    .  minimize    f   wi  in     bi  in          minimize     i  zi         wi     bi       zi      n      i     wi   zit   uit    f      i       n      i     wit      zi   uit    f      i                   where t denotes the iteration index in the admm process  ui is  the dual variable that is updated in each iteration through uit    uit     wit  zit . following the admm process  the two subproblems  are iteratively solved till convergence. the first subproblem  eqn.     is solved by the classical sgd method  and the solution for the  second subproblem  eqn.    is obtained by  zit      proj wit      uit                   block     block                                      block     block            si    where proj is the euclidean projection onto constraint set si   which  guarantees the weight matrices exhibit the specific sparse pattern  defined in the constraint si for each layer. in this work  we propose  a new type of structured sparse matrix with the novel csb sparse  format  which is the target pattern  si   in our rnn weight pruning  method. the detailed projection process for csb formated weight  will be given in   . .                                         block                                        m           n               kernel matrix   c  csb format    kernel matrix size  n                           for   blocks  m                         non zero row column index  rowidx                                where function f represents inference loss on the given dataset   si is the feasible set of wi   which is subject to the user constraints.  in the regular rnn training  si is r  i.e.  no constraint   and thus  the optimal weights  wi   and bias  bi   for each layer can be obtained by classical stochastic gradient descent  sgd  method    .  however  once the weight pruning is conducted along with the  training process  the constraint of weight sparsity represented by  si becomes combinatorial and no longer convex  which prevents  the eqn.   from being solved by classical sgd. the advanced alternating direction method of multipliers  admm  method     is  leveraged in our csb pruning scheme. the admm separates the  weight pruning  during training  problem into two subproblems   which are iteratively solved until convergence. first  the problem  is reformulated as   minimize    n         h   n  blocks      . .  model training with admm based pruning technique. the  training process is to find a proper set of weight values that reach  the minimal classification loss compared to the ground truth. the  objective of training an n  layer rnn can be formulated as      a  weight matrix partition   b  sparsify the block in row column wise  m  sparsified columns  non zero  zero  sparsified rows    granularity. with the advantages of both the non structured and  coarse grained structured pruning methods  the csb pruning in  this work is a fine grained structured method that not only achieves  a high pruning rate but also makes the hardware acceleration possible.    ics      june    july          barcelona  spain    block  block     block     block     w   m  blocks     colidx                       non zero values  val                                            block     block     figure    a novel structured sparse matrix  csb  with its dedicated storage format  which benefits both the pruning flexibility and hardware parallelism.      csb pruning technique   .  a novel structured sparse weight format   . .  definition of csb. we propose the compressed structured  block  csb   a novel structured sparse matrix for model pruning  that benefits both the pruning flexibility and the hardware parallelism in inference acceleration. fig.   illustrates the csb matrix  and the dedicated storage format. as fig.   a   we consider the  csb structured matrix  with a size of w   h   to be composed of  multiple blocks with the size m   n . each block is sparsified in the  row column wise  as fig.   b   in which the certain rows columns  are set to zero as a whole. by doing so  the non zero elements are  located at the cross points of the un sparsified rows columns only.  a significant benefit of this structured sparsity is the non zero elements in each block compose a dense kernel matrix that provides a  higher potential for parallel hardware acceleration than the random  sparsity. corresponding to this particular sparsity  a new sparse  matrix format is developed for efficient storage and computation.  as fig.   c   the csb format contains five arrays in three groups   i   array n   and m   are the row and column counts of the kernel matrix in each block   ii  array rowidx   and colidx   store the index  of un sparsified  non zero  rows and columns  respectively  note  that  the index count for each block equals to the corresponding  value in n   or m     iii  the non zero values in successive blocks   row major order  are concatenated and stored continuously in the  array val  . because the inference computation accesses the sparse  blocks in sequential  the offset for arbitrary access is omitted in the  csb format.   . .  advantages and challenges of pruning with csb. we adopt  the csb structured sparsity in pruning the rnn models  which  integrates two fold advantages of both the non structured pruning  and coarse grained structured pruning in fig.  . on one hand  csb  provides adequate pruning flexibility  because each block is pruned  independently  and the pruning rate varies among blocks that helps  to preserve the weights with important information. physically   each element in the weight matrix represents the synapses  connection  between input neurons  matrix column  and output neurons   matrix row . the pruning process is zeroing the synapses between     ics      june    july          barcelona  spain    r. shi  p. dong  t. geng  y. ding  x. ma  h.k. h. so  m. herbordt  a. li and y. wang    two neurons without a strong connection. the csb pruning automatically groups the strongly connected neurons into blocks with  high density while leaving the weakly connected ones in the lowdensity blocks. further  the pruning granularity is adjustable  via changing the block size  such that different weight matrices in  rnn model can be pruned with various granularities. the above  flexibilities enable a high pruning rate while maintaining the model  accuracy. on the other hand  the un pruned weight values in each  block compose a dense kernel matrix that makes the inference computation friendly to parallel hardware. nevertheless  the blocks  may have different sized kernel matrices that result in a workload imbalance issue while mapping computation of blocks to  parallel hardware. this paper carefully addresses this issue with an  architecture compilation co design in    and   .    achieved via the progressive pruning. the entire csb pruning flow  is presented in algorithm   with carefully specified annotations. initially  the baseline model  with dense weight matrix w  is obtained  via classical sgd training and input to the flow. note that the bias  vector  b  is omitted as the csb pruning flow does not touch it. the  lossless accuracy  accu  is given as the constraint of the progressive  pruning. two input parameters  initial pruning rate  initpr  and  initial step of pruning rate reduction  initprstep  are set for tuning  the pruning rate in the progressive flow. we use the progressive  increase manner in approaching the maximum value of lossless  pruning rate. therefore  we set initpr to a small value  e.g.      as  the starting point  which surely meets the lossless constraint. the  variables prunerate and stepprunerate are initialized to initpr  and initprstep  respectively  at the beginning. in each progressive  iteration  the flow performs re training and pruning on the model  with multiple epochs  e.g.      in algorithm    to obtain the csbformatted weight matrix  z  with the admm pruning fashion. in  each epoch  two subproblems are alternatively solved following the  principle of the admm pruning technique in   . . . the function  sgdtrain updates the weights with classical sgd   st subproblem   eqn.     and the subsequent process prunes the weight matrix and  projects it to csb constrained set   nd subproblem  eqn.   . the  process in algorithm   details the projection corresponding to the  general representation in eqn.  . first  the weight from sgdtrain  is partitioned to multiple blocks zi  j following the csb method in    . . then the rowprune process is applied to each block column  independently. specifically  for each block column  the     norm  accumulate the square of all elements  of each row  with the size of m   is obtained  then  a row wise pruning is conducted referring to the      norm values. subsequently  the columnprune is applied to each  block row with the same behavior to rowprune. note    that the pruning rate in both rowprune and columnprune is        prunerate   which results in the target prunerate after the combined processes.  once the csb formatted weight matrix z is obtained  it will be sent  to sgdtrain of the next epoch  along with un pruned weight matrix  w  and accumulated difference matrix u. with multiple epochs   weight z will eventually meet the csb pattern constraints and  achieve good accuracy.  after each progressive iteration  the csb pruned model is evaluated  eval z   and compared to the lossless accu. the prunerate  is increased by stepprunerate in the next iteration if the accu is  achieved. once eval z    accu  the model is over pruned and the  optimum pruning rate is just between the prunerate of the two  neighboring iterations. therefore  we reduce stepprunerate by  half and reduce the prunerate by this new step to further approach  the optimum point. the progressive csb pruning flow terminates  until the pruning rate reaches a target precision. for instance  as  the last line in algorithm    the flow terminates when the pruning  rate precision  stepprunerate       initprstep.     .     csb pruning flow with admm    algorithm    auto lossless csb pruning with admm  input   un pruned rnn model w  lossless accuracy accu   block size in csb m   n   weight matrix size w   h  initial pruning rate init p r  initial step of pruning rate init p rst ep  output   maximally compressed model with csb pruning z     initialization.  u      z   w  w    w  flag  false  prunerate  init p r  stepprunerate  init p rst ep     progressive iteration.  repeat  foreach t               re train and pruning epoch.  do     solve eqn.   in admm   st subproblem   w   sgdtrain w   u  z      solve eqn.   in admm   nd subproblem      project weight matrix to csb pattern s  h  zi  j  partition w    u   i       w  m    j       n    foreach j       h  n   do     z   j  rowprune z   j           prunerate   foreach i       w  m   do     zi     columnprune zi              prunerate   u   u   w    z    update u     set progressive pruning rate.  if eval z   accu then  flag  true  stepprunerate  stepprunerate    prunerate  prunerate stepprunerate  else  if flag then  stepprunerate  stepprunerate    prunerate  prunerate  stepprunerate  until stepprunerate      init p rst ep   eval z   accu     with the admm based pruning technique in   . .   the weight  matrices can be pruned to an arbitrary sparse pattern by defining  the constraint s and applying the pattern specific projection in  eqn.  . to obtain the rnn model with csb pattern  we develop the  csb pruning algorithm following the admm principle. further  the  maximum pruning rate under lossless constraint is automatically      unified architecture for csb rnn   .  overview of acceleration framework  an overview of the csb rnn acceleration framework is illustrated  in fig.  . although the csb pruning  step   shrinks the model  size and therefore reduces the computation in inference  parallel     csb rnn  a faster than realtime rnn acceleration framework with compressed structured blocks    original dense model    step    csb pruning         csb pruned weight       pe    pe    pe    pe    pe    pe    pe    pe    pe    step   unified rnn  dataflow architecture     .     .      different rnn types  on one design                   lstm    sru    gru           high pe efficiency    super real time    imbalanced workload  low pe utilization   blk   blk   blk   blk     challenges in parallel acceleration    fpga prototype          control  instructions  step    compilation          figure    overview of csb rnn acceleration framework  including  i  csb pruning algorithm   ii  unified rnn dataflow  architecture   iii  workload compilation with csb pruned  model.  hardware acceleration is still in demand to achieve realtime performance. the challenges in accelerating csb rnn are two fold. first   the architecture should be adaptive to various rnn cell types  i.e.   lstm  gru  etc. second  the kernel matrix in fine grained blocks  may not provide enough inner block parallelism for large scale  hardware. to further improve the concurrency  inter block parallelism should be leveraged. however  the pruned blocks may have  different sparsities  leading to the workload imbalance issue for  inter block parallelism  which usually causes a low utilization of  processing element  pe . to address these challenges  csb rnn  proposes an architecture compilation co design. in the architecture  aspect  step    we propose a unified rnn dataflow architecture  that is programmable for different rnn cell types    .    in particular  a novel csb engine is designed with the support of workload  sharing and is equipped in csb rnn architecture to address the  workload imbalance issue    .  . in the compilation aspect  step     we define control instructions for the hardware and propose the  compilation algorithms to conduct the particular rnn type computation and balanced workload scheduling     .     .     programmable rnn dataflow architecture    to generalize the architecture for different rnn cell types  we investigated the existing rnn cells and extracted the arithmetic primitives  which compose the rnn computation in a dataflow fashion.  fig.   presents the hardware components in this architecture  where  each operation unit serves the corresponding arithmetic primitive.  in particular  the csb engine computes the main workload  mvm   with the weight matrices after csb pruning  csb mvm . the units       are the element wise multiplication and addition.   and    operate the activation functions sigmoid and tanh  respectively.  the datapaths  arrows on fig.    interconnect the operation units  and on chip buffers  which transmit the intermediate results and  compose the dataflow graph for rnn cell computation. importantly   rnn dataflow architecture provides the programmable datapath   red arrows on fig.   . thus  the proper operation units can be interconnected by programming control instructions for a particular  rnn cell type.     .     pe array    csb engine    the csb pruning scheme greatly shrinks the weight matrix size  and therefore reduces the main workload in inference. although    csb weight  bu er    csb engine    ics      june    july          barcelona  spain    bu era    load  unit    bu erb         input    programmable  datapath    bu erc    fixed  datapath  onchip memory       bu erbias    operation unit    bu erd    bu ere    store output  unit    figure    rnn dataflow architecture. operation units serve  the rnn arithmetic primitives  the programmable datapaths construct the proper dataflow for target rnn cell via  instructions.  the fine grained structure of csb contributes to the regularity and  makes efficient hardware acceleration possible  it is still challenging  to design a parallel architecture that can fully exploit the benefits  of csb pruning. the challenges in an efficient csb engine design  are two fold. first  both the inner block and inter block parallelism  should be fully exploited  as the regular inner block computation  provides very limited concurrency with small block size. second  the  inter block workload imbalance issue exists due to the sparsity  varies among blocks. the following   . .  and   . .  address these  two challenges  respectively.     . .  hierarchical design for inner  and inter block parallelism.  as illustrated in fig.    the csb engine design is in a two level  hierarchy  processing element  pe  level and pegroup level. the  hardware instances in each level are organized in a  d fashion that  the architecture is composed of k   l pegroups  and each pegroup  contains p   q pes. the parallel pes inside one pegroup process  inner block multiplication concurrently  while the pegroups computing different blocks in parallel  inter block parallelism .  inside each pegroup  because the size of csb kernel matrix  m n   might be larger than that of pe array  p   q   multi pass processing  is required to handle the entire block. thus  each pegroup contains  a neuronaccumbuffer  which stores the partial results and sums  up with the accumulation of horizontal pes in each pass. the input neurons required by the current block are preloaded to the  blockneuronbuffer and broadcasted to the pe array. each pe column shares the same input neuron as the unpruned weights are  vertically aligned in the structured block with csb pruning. importantly  the weightbuffer provides the csb formatted weight   fig.     including the weight values  kernel matrix  for pes  column  index for blockneuronbuffer to read the proper input neuron  row  index for neuronaccumbuffer to accumulate the multiplicationresults to proper address in neuronaccumbuffer  and the kernel  matrix size  m   n  for the pegroup control logic which conducts  proper pass count in both axes.  in the higher level of the design hierarchy  the pegroups process  blocks in the row major order. the pegroups in one column concurrently compute the vertical blocks. therefore  the pegroups in one  column share the same partition of input neuron vector  while multiports are provided on blockneuronbuffer for concurrent access.  similarly  the blocks in horizontal axis are mapped to pegroups in  the same row  with multi pass processing. after the computation     pe   p       pe   p q          pegroup  l       pegroup          blockneuronbu er         input  stream            pegroup  k       weight  bu er    reorderlogic              pe   p       pe array    pe     q     neuronaccum  bu er    pe           r. shi  p. dong  t. geng  y. ding  x. ma  h.k. h. so  m. herbordt  a. li and y. wang    neuronaccum  bu er    pe                csb weight  bu er    ics      june    july          barcelona  spain    pegroup  k  l          input  stream blockneuronbu er    ouput  stream    figure    two level hierarchical organization of csb engine  for the main workload  csb mvm  computation.  of each block row  the results in neuronaccumbuffers are accumulated in horizontal and output to reorderlogic to obtain the  output neuron vector.   . .  inter pegroup workload sharing.  workload imbalance challenge  the blocks in csb pruned  model may have different sized kernel matrices  and the resultant  inter block workload imbalance brings challenges to exploit the interblock parallelism on hardware. as fig.   b  demonstrates  with the  straightforward design  the workload imbalance issue results in  low utilization of pegroups. the presented mvm workloads are  allocated to       pegrounps that each contains   pes. during the  execution  pegroup    enter the idle state before the pegroup  accomplishes its workload  which results in a severe under utilization  of the overall hardware. in fact  the imbalanced sparsity naturally  exists in the rnn models. however  existing works         relieve  the hardware pain by pruning the model with a region balanced  sparsity compulsively. as a result  the neglect of natural sparsityimbalance significantly harms the pruning ratio and model accuracy.  by contrast  we handle this issue by improving the architecture  with the workload sharing technique.  inter pegroup workload sharing  the concept of workload  sharing is illustrated in fig.   c . each pegroup processes not only  the originally allocated block but also a partition of block from the  neighboring pegroup  which is arranged with a heavier workload.  in the hardware aspect  as fig.   c   dedicated workload sharing  paths  red arrows  are set for the inter pegroup data transmission   and the interconnection adopts the torus topology in both dimensions. with the hardware support of workload sharing  pegroup   migrates the extra workloads to pegroup  and pegroup   and  pegroup  migrates the block  workload partition to pegroup .  that significantly balances the workload and improves the utilization. considerations in the workload sharing design are two fold.   i  the input neurons should be sharable between the pegroups    ii  the output neuron accumulation should be performed interpegroups. we discuss these issues and our strategies within two  cases  in which the workload is shared between neighboring pegroups  in horizontal or in vertical  respectively. for the horizontal sharing  case  an extra data port is set on the blockneuronbuffer to solve  the issue  i   which enables the pegroup to access input neurons    from the neighboring pegroup in horizontal. the issue  ii  is naturally solved by the hierarchical csb engine design  as the pegroup  can store the partial results of the shared workload partition in its  local neuronaccumbuffer  which will be accumulated in horizontal  after processing the entire block row. for the vertical sharing case   the pegroup column shares the same blockneuronbuffer  thus  the issue  i  is naturally solved by hardware. about the issue  ii    the pegroup should accumulate the vertically shared workload to its  original pegroup  as the vertical pegroups compute different blockrows that cannot be accumulated in a mixed manner. however   concurrent accumulation to one address in neuronaccumbuffer  leads to the read after write  raw  data hazard. to address this  issue  an accumulation path is set between vertical pegroups and  connected to the adder  which accepts parallel results from neighboring pegroups  sums up and stores to the neuronaccumbuffer  for one shot. with the hardware support on workload sharing  we  propose the compilation scheme in next section that schedules the  partition and sharing by analyzing the csb pruned matrix and  generates the instruction to control the hardware sharing behavior.         compilation for csb pruned model    the proposed rnn dataflow architecture is controlled by the precompiled instructions. the instruction set includes the macroinstruction and micro instruction  where the former one conducts the operation units  in fig.    for the proper rnn dataflow   cell type   and the later one instructs the csb engine with interpegroup workload sharing behavior as described in   . . . correspondingly  the compilation is composed of two phases  rnn  dataflow compilation    .   and workload sharing scheduling    .  .     .     rnn cell to dataflow architecture     . .  macro instruction set. we define the macro instruction set  for our rnn dataflow architecture    .  . as fig.    the microinstruction is composed of multiple sections  that each section  provides control signals for corresponding rnn primitive hardware. all sections are concatenated to form a very long instruction  word  vliw  item. note that each section contains count operand  to indicate the size of workload for corresponding hardware primitive. thus  one vliw instruction is regarded as accomplished until  all hardware primitives finish the workload. the operands in each  instruction section are classified into two types  the count type  controls the hardware iteration count  and the other operands indicate the proper data source or destination for each primitive. for  the first type  the value of count in element wise operation units   only csb engine excluded  is measured by data element as these  units perform element wise operation. differently  the counth v in  csb engine section represents the horizontal vertical block iteration counts over the entire csb engine in processing the particular  weight matrix. for the second operand type  addr memory  and  addr buffer  give the access address of external memory  normally dram  and built in buffers in the architecture  respectively.  importantly  the programmable datapaths in the architecture  fig.     are indexed  and the dataflowidx is set in the operand to indicate  the proper data source or destination for hardware primitive. with     csb rnn  a faster than realtime rnn acceleration framework with compressed structured blocks     a  csb pruned weight matrix  block     ics      june    july          barcelona  spain     c     horizontal sharing     b     block     pegroup     pegroup     pegroup     pegroup     kernel  matrix    pegroup     vertical sharing    pegroup     workload  sharing path    pegroup     pegroup     horizontal sharing    block     block     pegroup   pegroup   pegroup   pegroup   clock  cycle  utilization    .      .      .            clock  cycle  utilization    pegroup     pegroup     pegroup     pegroup                                    figure    inter block workload imbalance issue occurs when mapping the csb pruned matrix  a  to the vanilla  basic  csbengine  b   which results in a low hardware utilization. we propose the workload sharing technique that significantly increases  the utilization and reduces the time consumption  as demonstrated in  c .  primitive    srcop     srcop     srcop      a  partition of  kernel matrix workload  m   mv vertical    dst    addr memory   count  addr buffera   loadunit  addr buffera   blocksizeh v counth v  addr bufferb   csb engine  addr bufferb   addr bufferbias   count  sum   count  addr buffere   sum   count dataflowidx  addr bufferc   sigmod      dataflowidx  addr bufferc   count  tanh      dataflowidx  addr buffer   streamidx  count dataflowidx  addr bufferd   mult   addr bufferc   addr buffere   count  mult   count  addr memory   storeunit dataflowidx  addr buffere     sharing    n      nv   nh   mh    horizontal  sharing     b  micro instruction  with partition scheduling for workload balance  sharing tripcount  no  local   horizontal  vertical    rowidx    colidx    m    n   rowidx   n   colidx   m    mh    nh  rowidx    nh colidx    mh   mv    nv  rowidx    nv colidx    mv  micro instruction for next blocks                                        block iteration    idx    figure    macro instruction set  vliw like  for rnn  dataflow architecture that constructs proper arithmetic  dataflow for different rnn cell types.    figure    micro instruction indicates the kernel matrix  workload and the scheduling of partition for workload balancing.    the above settings  rnn models with various cell types can be translated to several vliw instructions that are repetitively executed  during rnn inference.    three items   i  local workload that is originally allocated  excluding  the portion shared to other pegroups   ii  workload shared from  the neighboring pegroup in horizontal   iii  workload shared from  the neighboring pegroup in vertical. the micro instruction contains   operands  as fig.   b . the operand sharing gives a flag   local horizontal vertical  to indicate the data source  where local  means the input and output neurons are in local pegroup  horizontal  sharing  indicates the input neurons should be read from the  blockneuronbuffer of left pegroup  vertical  sharing  means the  output should be accumulated to the neuronaccumbuffer of upper  pegroup. the operand tripcount gives the size of workload. note  that  for each block  the kernel matrix is divided to tree regular  partitions as fig.   a   for local  no sharing   vertical  and horizontalsharing  respectively. the sizes of partitioned matrices are denoted  as m    n      mv    nv    mh    nh   which are turned to tripcount  values in the three micro instruction items. the operands rowidx  and colidx provide the non zero row and column indices of each  submatrix. note that each micro instruction item may contain multiple rowidx and colidx corresponding to the tripcount value.  further  these two operands are stored in individual instruction  memories that are read and reused periodically in the submatrix  computation.     . .  macro instruction compilation. the objective of compilation is to minimize the vliw instruction count that maximizes  the utilization of operation units. we invoke the list scheduling  method      that is usually applied in vliw compilation. the rnn  model with a particular cell type is translated to the directed acyclic  graph  dag   in which the nodes represent the arithmetic primitives and the edges are data dependencies. in the list scheduling   we adopt the as soon as possible  asap  strategy that the operation  nodes are mapped to the corresponding hardware resources once  the resource is available and the dependency edge is ready. with the  proper operation units and interconnection in the rnn dataflow  architecture  the macro instruction compilation can quickly achieve  an optimum point  in which the processing throughput is bounded  by the main workload  csb mvm  on csb engine.     .     workload scheduling on csb engine     . .  micro instruction set. the micro instructions are generated  for each pegroup individually  which control the csb mvm operations on csb engine. specifically  the micro instruction contains  the csb compression information  i.e.  kernel matrix size  row  and  column index in fig.   c   for the block workload allocated to the  certain pegroup. in particular  the kernel matrix workload is partitioned to three submatrices and shared to neighboring pegroups   as fig.   a    the micro instructions for one block iteration include     . .  micro instruction compilation. the compilation of microinstruction is essentially searching the workload partition scheme to  achieve the optimal balance  which facilitates a higher hardware utilization  efficiency . specifically  the compiler analyzes the weight     ics      june    july          barcelona  spain    r. shi  p. dong  t. geng  y. ding  x. ma  h.k. h. so  m. herbordt  a. li and y. wang    matrices and selects the proper partition variables  as fig.   a   for  each kernel matrix. every k   l blocks  one block iteration  are analyzed individually  which are executed on pegroups in concurrent.  within one block iteration  each pegroup is supposed to take the  equivalent workload after balancing.  we regard the search of optimal partition variable as a satisfiability modulo theories  smt  problem       which searches the feasible  solutions in the constrained region. the existing smt solver      takes the constraints with logic programming and gives the satisfiability  existence of solution  and feasible solutions. in the compilation for each block iteration  we declare the integer variables  including m    k  l   n    k  l    mh  k  l    nh  k  l    mv  k  l    nv  k  l    where k       k  and l       l . the constraints are represented  with the constraint logic programming  clp   in which each clause  gives a specific search limitation. the clp in compilation is listed in  eqn.    where   represents logic and and   represents or. clp     constraint the feasible search region  as the size of the partitioned  workload should   kernel matrix size  m   n . clp    guarantee  regular partitions as fig.   a . clp  determines the values of m    and n   . to improve the pegroup utilization  we set clp  constraint  that the size of partition workload should be integer multiple of the  pegroup size. thus  the pes are fully utilized on the shared workload  partition. also  it helps to shrink the search space and speed up the  compilation. within the idealized situation  each pegroup is scheduled with workload that is the average value over all pegroups in  the current block iteration. otherwise  the pegroup with maximum  workload determines the run time  clock cycle  for this iteration.  clp  gives the constraint on the maximum workload that  to all  pegroups  the exceeding part of scheduled workload to the average  value  av   should   mar in  which is given before search. the last  clp combines all above constraints to a conjunctive form  which is  subsequently sent to smt solver for a feasible solution.    algorithm    micro instruction compilation  input   csb pruned weight matrix wc sb    block size in csb m   n   weight matrix size w   h    size of each pegroup p   q   prgroup count k   l  output   micro instruction list micr oi nst     temporal block iterations in vertical.  for i     to  h  n  k   do     temporal block iterations in horizontal.  for j     to  w  m  l   do  mar  in        k       k     l       l .   m k  l    n k  l    av   analyze  wc sb  i j      search with multiple rounds.  repeat  clp  buildclp  m k  l    n k  l    av   mar  in      give solution if satisified.   satisfiability  partitionvar     smtsolver  clp   mar  in  p   q  until satisfiability  true   micr oi nst  append  micr oi nst   partitionvar     once the smt problem is satisfied  the search stops and the partition variables  m     n      mv    nv    mh    nh   for each pegroup are  assembled and appended to the micro instruction list  that conducts  the csb engine computation in a workload balanced fashion.         evaluation    in this section  we first brief the implementation of the csb rnn  framework    .    and then give deep evaluations from the performance of csb pruning algorithm    .   to the improvement  with the architecture compilation co design    .  . meanwhile      mainstream rnn models from multi domains are invoked as the  evaluation benchmarks and presented in table    in which we also  list the non structured pruning rates as the theoretical optimum.    clp             mh  k  l     m k  l            nh  k  l     n k  l       clp             mv  k  l      m k  l                nv  k  l     n k  l       clp         mh  k  l     m k  l        nv  k  l      nh  k  l     n k  l       clp         nv  k  l     n k  l        mh  k  l      mv  k  l     m k  l       clp        m    k  l     m k  l      mv  k  l       n    k  l     n k  l      nh  k  l       clp         mh  k  l   p   mv   k  l   p          nh  k  l   q   nv   k  l   q          .     clp          m    k  l     n    k  l      mh  k  l         nh  k  l         the csb pruning flow was implemented with pytorch       a framework for deep learning model development. the benchmark models  were first trained with the sgd and the accuracy is regarded as the  lossless target value in the subsequent csb pruning. these baseline  models were fed in the csb pruning flow and get compressed with  the lossless constraints. in regarding the architecture compilation  co design  the proposed rnn dataflow architecture was realized  with verilog rtl and implemented on an fpga vendor evaluation board  xilinx zcu      on which the fpga contains enough  resources for our architecture with different design scales. the  compiler was implemented in c   with the strategies in    and  z      as the smt solver. with the csb pruned model  the compiler dumps the macro instructions    .   to build the proper rnn  dataflow and micro instructions    .   for the workload balancing.  these instructions are loaded to the rnn dataflow architecture before processing sequence continuously. with regard to the detailed  hardware efficiency  i.e  csb engine utilization   cycle level rtl  simulation was performed to profile the inference behavior.       mv  k      l      nv  k      l      av      mar  in  clp      clp    clp     clp    clp      clp    clp    clp            based on the above formulation  we propose the compilation  scheme in algorithm   that seeks out the optimal scheduling solution. for a given csb formatted weight matrix wcsb   the compiler  partitions it to  w  m l     h  n  k  temporal block iterations and  schedules each iteration individually. before the multi round search   the compiler firstly analyzes the weight partition for current block  iteration that gives the kernel matrix size  m  n  for each block and  the average workload  av  . the mar in is initialized to   that targets to schedule an idealized average workload on each pegroup.  in the search round  buildclp constructs the constraints representation  which is input to smtsolver. in case the constraints cannot  be satisfied  satisfiability is false  over the feasible region  the  mar in value is supposed to increase by p   q in the next round.    implementation and experiments setup     csb rnn  a faster than realtime rnn acceleration framework with compressed structured blocks    ics      june    july          barcelona  spain    table    benchmark models in csb rnn evaluation  app.  abbr.  idx.    applications    dataset    layer  index     lstm            lstm            lstmp            gru           li gru          gru             lstm              lstm              lstm              lstm                 layer rnn cell         mt     machine translation    ptb                  mt     machine translation    ptb                  sr     speech recognition    timit                 sr     speech recognition    timit                 sr     speech recognition    timit                 sr     speech recognition    tdigit                  spp    stock price prediction    s p                     sc     sentiment classification imdb                  sc     sentiment classification    mr                   qa    question answering    babi              input  neuron                                                                                                    hidden  neuron                                                                                                              original model  non structued pruning   weight bias result prunerate  weight result  perplexity     k  k    .     . k     .       .     ppl  lower is better     k  k    .     . k    m  k    .    . m  perplexity  ppl     .      .      m  k    .    . m  phoneme error rate  .  m  k    .      . k    .       .       .      . k   per  lower is better   .  m  k   . m  k    .      . k  per    .       .      . m  k    .      . k     . k   .      . k  per    .       .      m   .       . k  accuracy     . k  . k   .      .    . k    .     normalized price dist.   k  . k   .      . k   .     .     lower is better      k  . k   .      k   .  m  k    .      . k  accuracy    .      .      . k   .      . m  k   higher is better    . m  k    .      . k  accuracy     . k  k    .      .      . k    .        . k  k   .      . k    .       .     accuracy     . k  k   .      . k     . k  k   .      . k  evaluation metric    non structured pruning  csb pruing  block      csb pruing  block      csb pruing  block      csb pruing  block         figure      a  shows the pruning rate comparison between non structured pruning  optimum  and csb pruning in different  block sizes.  b  shows the normalized index overhead  nio . comparing  a  and  b   we gain the insight that csb pruning  dramatically reduces the nio while maintaining a high pruning rate.     .     evaluation of csb pruning rate    the csb pruning is first evaluated in the aspect of pruning rate   which is a significant metric to score the model compression methods. because the parameterizable block size determines the structural granularity in pruning  we present the attainable maximum  pruning rate with various block sizes in   . . . further  comparison  with the prior art rnn compression schemes is given in   . . .   . .  selection of optimum structural granularity. csb pruning  provides the flexibility that improves the pruning rate and also  the hardware friendly regularity. importantly  a trade off exists between these two targets that motivate the following investigation.  reducing the block size facilitates a more fine grained pruning and  thus a higher pruning rate. however  more individual blocks require  extra storage for row and column index with the csb formatted  weight matrix  fig.   . therefore  we present both the attainable  pruning rate and the index overhead with different block sizes in  each benchmark model. the block is set to square with sizes of                   considering the weight matrix dimensions in different  models. note that for matrix with very small size  e.g.           in  sr    the short dimension      is partitioned to q blocks uniformly  after padding a zero column. multiple layers in one model adopt    the same pruning rate. the attainable pruning rate for each case is  presented in fig.    a   further  the index overheads are divided by  the corresponding weight count for normalization  and the values  of the normalized index overhead  nio  are presented in fig.    b .  notably  the results with non structured pruning are given for comparison  leftmost bar for each application   and its index overhead  is obtained by compressing the non structured weight matrices  with the compressed sparse row  csr  format.  as a result  the csb pruning rate ranges from  .   to      which  dramatically reduces the original model size by order of magnitude.  with the growth of block size  the pruning rate decreases as the  coarse granularity block reduces the pruning flexibility. we note  that  in all benchmarks  the csb pruning is capable of reaching a  maximum pruning rate with the block size of    or     which is  close to non structured pruning. in the aspect of nio  the index  overhead of non structured pruning exceeds       as at least one  index is required for a non zero element. nevertheless  for csb  pruning  the nio is below     in most cases due to index reusability in the structured blocks. the nio shows a significant decay  while enlarging the block size. with the block size of     the nio  declines to        which is     of that in non structured pruning.  interestingly  we gain the insight that with a block size of    and     ics      june    july          barcelona  spain    r. shi  p. dong  t. geng  y. ding  x. ma  h.k. h. so  m. herbordt  a. li and y. wang    table    pruning rate comparison    mt   mt     sr     sr   sr     compression  technique  column pruning       csb pruning  row column       bank balanced      csb pruning  block circulant       row balanced       bank balanced      csb pruning  block circulant       csb pruning  column pruning      csb pruning    prune  rate        .                      .                         .           weight  width     bit     bit  floating     bit     bit     bit     bit     bit     bit     bit     bit     bit     bit    metric  ppl  ppl    per    per  accu    result     .       .      .      .      .      .       .       .       .       .       .       .       .       improvement       .         .      .         .     .     .         .         .          most models achieve the close pruning rate. for instance       and     in mt   both are     in sr . therefore  the larger block  size      is preferable for its low index overhead.   . .  comparison with prior art compression schemes. the csb  pruning rate is further compared to the prior art rnn compression techniques in table  . the listed competitive techniques are  proposed to enable a faster  hardware friendly rnn inference with  the compressed model. note that these competitors quantized the  weight to    bit fixed point numbers  thus  we do the same quantization on csb pruned model and report the corresponding results  for a fair comparison. in table    row column      technique prunes  each weight matrix as an entire block. comparing to it  our finegrained csb pruning improves the compression rate to  .  . the  row balanced      or bank balanced     techniques compulsively  train the model to a balanced sparse pattern  however  csb pruning remains the natural unbalanced sparsity in rnn model and  achieves a higher   .    pruning rate. overall  the csb pruning  improves the pruning rate to  .    .   of the existing schemes   while maintaining an even better model accuracy.     .     evaluation of rnn dataflow architecture  with csb pruned model     . .  hardware resource consumption. the hardware resource  consumption  cost  of the rnn dataflow architecture is given in  fig.     with various csb engine configs  p q k l and max supported block size . notably  the csb engine with different workload  sharing configs  including no sharing  vertical sharing  horizontalsharing   d sharing  are synthesized individually to evaluate the  hardware overhead on workload sharing technique. the consumption of hardware logic and memory from the fpga vendor tool are  presented in fig.   . the configurable logic block  clb  left axis   is the fpga building block for logic  which is used as the logic  resource metric  the memory resource is given in megabit  mb  in the right axis . note that most memory resource on our fpga  device is configured as the weight buffer  although they may not be  fully used by small rnn models. the multiplier in each pe     bit  fixed point  is mapped to digital signal processor  dsp  on fpga   and the dsp count in design is   p   q   k   l that is omitted  here. as a result  the hardware support of workload sharing costs  an acceptable overhead  which is   .     .    and   .   for three  sharing cases  vertical horizontal  d sharing   respectively.    clb  architecture  without workload sharing   clb  workload sharing  hardware overhead      pxqxkxl      x x x   maxblock       x x x   maxblock      no   sh  v s arin  ha g  h s ring  h   d aring   sh  ari  ng    abbr.    memory size     x x x   maxblock        x x x   maxblock       x x x   maxblock       x x x   maxblock       figure     hardware resource consumption with multi csbengine configs.   . .  performance. due to the workload imbalance issue  the processing performance of rnn dataflow architecture  csb engine in  specific  is not deterministic. hardware efficiency  the ratio of effective computation on pes  is invoked to evaluate the improvement  of our workload sharing technique. we obtained the csb engine  efficiency by measuring the pe pipeline utilization using    benchmarks listed in table   with different design choices of workload  sharing. moreover  csb pruned models with different block sizes are  used to evaluate the impact of block size on efficiency. the efficiency  is measured layer by layer on hardware with       pegroups and  each contains       pes. the results are presented in fig.   . overall   for the csb engine without workload sharing  the efficiency is      on average  which results from the imbalanced workload  sparsity   of blocks. the single dimensional sharing  vertical or horizontal   improves the efficiency to an average of    . after the  d sharing  is adopted  the efficiency is further improved to     on average  i.e.   only    execution time of csb engine is invalid. this    pipeline  gap is inevitable  as a few extremely imbalanced sparsity exists  in some weight matrices. for instance  we found diagonal dense  matrix exists that the blocks on the matrix diagonal contain significant workload compared to other blocks. in this case  the workload  sharing path in the current design is not enough  while adding more  sharing paths brings extra hardware costs.  comparing the efficiency within the same layer but different  pruning block sizes  it is apparent that the smaller block size is  applied  the lower hardware efficiency csb engine can achieve   particularly in the no sharing csb engine cases. this is because  the small block includes less workload  with the same pruning  rate  but more temporal block iterations  which lead to pe idle  more easily. as mentioned in   . .   using smaller block sizes in  compression guarantees higher model pruning rates  which benefits are significantly encroached by the performance degradation  with small compression block in the no sharing cases. nevertheless   we gain the insight that our architecture compilation co design  for  d sharing cases significantly subdues the degradation. for  instance  in layer    l   of mt  case  the no sharing degradation  from block    to block    is      while it is reduced to    by the   d sharing. on average  the degradation is reduced from     to    . in summary  with the proposed workload sharing technique  a  smaller block size in csb pruning does not bring significant degradation on hardware efficiency anymore  only    on average   so  that the benefits from higher pruning rates can be more sufficiently  exploited.     csb rnn  a faster than realtime rnn acceleration framework with compressed structured blocks    no sharing    vertical sharing only    horizontal sharing only    ics      june    july          barcelona  spain     d sharing  with both vertical   horizontal    figure     the efficiency  utilization  of the proposed architecture with different sharing strategies. the novel workload sharing technique significantly improves the average efficiency from      no sharing  to       d sharing . this improvement fully  exploits the benefits of fine grained csb pruning.  table    latency and power efficiency comparison  abbr.    mt     sr     sr      pe freq.   mhz   bbs                csb rnn          c lstm                e rnn                ese                 csb rnn          e rnn                csb rnn          work    latency      s    .     .     .     .      .     .     .     .      power   watt        .                .        .     power eff.   k frames w     .       .     .     .     .      .     .      .      power eff.  improv.       .       .       .           .          .        . .  comparison with related works. the overall performance  of csb rnn  i.e.  csb pruned model inference speed on the proposed rnn dataflow architecture  is listed in table   and compared  with the prior art designs. we collected the statistics including the  pe count   pe   operating frequency  latency in processing one  input frame and the power of design. as table   shows  with the  same benchmark applications  the csb rnn reduces the latency  by         that speeds up the processing from  .    to   .     correspondingly  nevertheless  csb rnn only uses         pe  counts  hardware resource  of the competitors to attain this performance. the latency ranges from  .   s to  .   s with different  model sizes. for generic high precision speech recognition  at most         frames should be processed per second  which requires a  latency       s to meet the realtime performance. as the achieved  latency with benchmark models is much lower than this requirement  the csb rnn provides a faster than realtime performance  and facilitates the device processing more complex rnn models in  the future. besides the latency  we compare the power efficiency   k frames per watt  among these competitive designs. the results  show the csb rnn achieves significant improvements from  .     to   .    on power efficiency in processing the same model  which  makes the csb rnn quite suitable for embedded scenarios. further   while the existing works were designed for a particular rnn cell  type  csb rnn can be reprogrammed to adapt to different cells.         conclusion    this paper presents csb rnn  an optimized full stack rnn acceleration framework. the fine grained structured csb pruning significantly improves the pruning rate compared to existing hardwarefriendly pruning schemes. meanwhile  an architecture compilation  co design is proposed that sufficiently exploits the benefits of the  csb pruned model. the experiments show that the entire csb rnn    acceleration framework delivers a faster than realtime performance  on extensive rnn models  and dramatically reduces the latency and  improves the power efficiency compared with the existing works.  future work  we are extending the csb technique to other neural  network layers. in particular  the transformer models are composed  of more complex dataflow  however  the same mvm primitive as  rnn. with improvement on the dataflow abstraction  the proposed  csb pruning and csb engine will contribute to the realtime transformer inference.    acknowledgments  we would like to thank the anonymous reviewers for their valuable  comments. this research was supported in part by the croucher  foundation  croucher innovation award        the research grants  council of hong kong grant number crf c       g  grf         .  this research was supported in part by the u.s. doe office of science  office of advanced scientific computing research  under  award         cenate   center for advanced architecture evaluation . this research was supported  in part  by the nsf through  awards ccf          ccf          ccf          cns           and ccf          the nih through awards  r  gm       and  r  gm        and by a grant from red hat.    