introduction  temporal and sequential modeling are important subjects  in machine learning. rnns architecture has recently become  a popular choice for modeling temporal and sequential tasks.  although rnns have been researched for about two decades            their recent resurgence reflects improvements in  computer hardware and the growth of available datasets. many  state of the arts in speech recognition          and machine  translation         has been achieved by rnns.  however  most rnn models are computationally expensive  and have a huge number of parameters. since rnns are  constructed by multiple linear transformations followed by  nonlinear transformations  we need multiple high dimensional  dense matrices as parameters. in time steps  we need to apply  multiple linear transformations between our dense matrix with  high dimensional input and previous hidden states. especially  for state of the art models on speech recognition     and  machine translation      such huge models can only be implemented in high end cluster environments because they need  massive computation power and millions of parameters. this  limitation hinders the creation of efficient rnn models that are  fast enough for massive real time inference or small enough  to be implemented in low end devices like mobile phones      or embedded systems with limited memory.  to bridge the gap between high performance state of the art  model with efficient computational and memory costs  there  is a trade off between high accuracy model and fast efficient  model. a number of researchers have done notable work to  minimize the accuracy loss and maximize the model efficiency.  hinton et al.     and ba et al.      successfully compressed    a large deep neural network into a smaller neural network by  training the latter on the transformed softmax outputs from  the former. distilling knowledge from larger neural networks  has also been successfully applied to recurrent neural network  architecture by     . denil et al.      utilized low rank matrix  decomposition of the weight matrices. a recent study by  novikov et al.      replaced the dense weight matrices with  tensor train  tt  format      inside convolutional neural  network  cnn  model. with the tt format  they significantly  compress the number of parameters and kept the model  accuracy degradation to a minimum. however  to the best of  our knowledge  no study has focused on compressing more  complex neural networks such as rnns with tensor based  representation.  in this work  we propose tt rnn  which is an rnn  architecture based on tt format. we apply tt format to  reformulate two different rnns  a simple rnn and a gru  rnn. our proposed rnn architectures are evaluated using  two different tasks  sequence classification and sequence prediction. in section ii  we briefly review rnn. in section iii   we describe the details of our proposed tt rnn architecture.  in section iv  we describe the tasks and datasets  followed by  the experimental results. we present related works in section  v. finally  we conclude our result in section vi.  ii. recurrent neural network  a. simple recurrent neural network  an rnn is a kind of neural network architecture that models sequential and temporal dependencies     . typically  we  define input sequence x    x    ...  xt    hidden vector sequence  h    h    ...  ht   and output vector sequence y    y    ...  yt  .  as illustrated in fig.    a simple rnn at time t is can be  formulated as   ht         f  w xh xt   whh ht     bh             yt         g why ht   by  .           where w xh represents the weight parameters between the  input and hidden layer  whh represents the weight parameters  between the hidden and hidden layer  why represents the  weight parameters between the hidden and output layer  and bh  and by represent bias vectors for the hidden and output layers.  functions f     and g    are nonlinear activation functions  such  as sigmoid or tanh.     memory cell values that are useful for the current memory  cell and the forget gates retain the previous memory cell values  that are useful for the current memory cell. the output gates  retain the memory cell values that are useful for the output  and the next time step hidden layer computation.    fig.  . recurrent neural network    b. gated recurrent neural network  simple rnns cannot easily be used for modeling datasets  with long sequences and long term dependency because the  gradient can easily vanish or explode           . this problem is caused by the effect of bounded activation functions  and their derivatives. therefore  training a simple rnn is  more complicated than training a feedforward neural network.  some researches addressed the difficulties of training simple  rnns. for example  le et al.      replaced the activation  function that causes the vanishing gradient with a rectifier  linear  relu  function. with an unbounded activation function  and identity weight initialization  they optimized a simple  rnn for long term dependency modeling. martens et al.       used a second order hessian free  hf  optimization method  rather than the first order method such as gradient descent.  however  estimation of the second order gradient requires  extra computational steps. modifying the internal structure  from rnn by introducing gating mechanism also helps rnns  solve the vanishing and exploding gradient problems. the  additional gating layers control the information flow from the  previous states and the current input    . several versions of  gated rnns have been designed to overcome the weakness of  simple rnns by introducing gating units  such as long short  term memory  lstm  rnn and gru rnn. in the following  subsections  we explain both in more detail.     long short term memory rnn  the lstm rnn was  proposed by hochreiter et al.    . lstm is a gated rnn  with three gating layers and memory cells  utilizes the gating  layers to control the current memory states by retaining the  valuable information and forgetting the unneeded information.  the memory cells store the internal information across time  steps. as illustrated in fig.    the lstm hidden layer values  at time t are defined by the following equations        it           w xi xt   whi ht     wci ct     bi      ft           w x f xt   wh f ht     wc f ct     b f      ct         ft    ot           w xo xt   who ht     wco ct   bo      ht         ot    ct     it    tanh w xc xt   whc ht     bc      tanh ct      where      is sigmoid activation function and it   ft   ot and ct are  respectively the input gates  the forget gates  the output gates  and the memory cells. the input gates retain the candidate    fig.  . long short term memory unit.       gated recurrent unit rnn  the gru rnn was proposed by cho et al.      as an alternative to lstm. there  are several key differences between gru and lstm. first   a gru does not have memory cells     . second  instead of  three gating layers  it only has two  reset gates and update  gates. as illustrated in fig.    the gru hidden layer at time t  is defined by the following equations        rt           w xr xt   whr ht     br             zt           w xz xt   whz ht     bz             h t         f  w xh xt   whh  rt    ht              zt      ht     zt    ht       bh    h t                where      is a sigmoid activation function  f     is a tanh  activation function  rt   zt are the reset and update gates  h t is the  candidate hidden layer values  and ht is the hidden layer values  at time t. the reset gates control the previous hidden layer  values that are useful for the current candidate hidden layer.  the update gates decide whether to keep the previous hidden  layer values or replace the current hidden layer values with  the candidate hidden layer values. gru can match lstm s  performance and its convergence speed sometimes surpasses  lstm  despite having one fewer gating layer     .  in this section  we provided the formulation and the details  for several rnns. as we can see  most of the rnns consist  of many dense matrices that represents a large number of  weight parameters that are required to represent all of the rnn  models. in the next section  we present an alternative rnn  model that significantly reduces the number of parameters and  simultaneously preserves the performance.  iii. proposed tensor train based rnn  in this section  we describe our proposed approach to  compress rnn using tensor train  tt  format representation.  we start with the description of tensor train      and then  represent the linear transformation operation in the tt format     enumerating the index qk         ..  rk     and qk       ..  rk   in  matrix gk   jk   across all k       ..  d    w  j    j    ..  jd     jd      x  g    j    q    q   ..gd   jd   qd     qd  .      q   .. qd    fig.  . gated recurrent unit        . after that  we describe the details of our approach for ttrnn including a simple rnn and more sophisticated rnn  with gating units. applying the tt format to represent the  weight parameters in rnn presents more difficulties compared  to the standard feedforward nn. to tackle this problem  we  also propose a local initialization trick in the last subsection.  a. tensor train  tt  format  before defining tensor train  tt  format  we will explain  the notations which we borrow from            that will be  used in later sections. in general cases  one dimensional arrays  are called vectors  two dimensional arrays are called matrices   and all higher multidimensional arrays are commonly called  tensors.  we represent vectors with lower case letters  e.g.  b    matrices with upper case letters  e.g.  w  and tensors with  calligraphic upper case letters  e.g.  w . each element from  the vectors  matrices and tensors is represented explicitly using  indexing in every dimension. for example  b i  is the i th  element from vector b  w p  q  is the element of the p th  row and the q th column from matrix w  w  j    ..  jd   is the  element at index   j    ..  jd   of tensor w and d is the order of  tensor w. based on previous description       we assume that  d dimensional array  tensor  w is represented in tt format       if for each k       ..  d  and for each possible value of  the k th dimension index jk       ..  nk   there exists a matrix  gk   jk   such that all elements of w can be computed as the  following equation    w  j    j    ..  jd     jd      g    j      g    j   ...gd     jd       gd   jd  .      for all matrices gk   jk   related to the same dimension k  they  must be represented with size rk     rk   where r  and rd must  be equal to   to retain the final matrix multiplication result as  a scalar. in tt format  we define a sequence of rank  rk  dk    and we call them tt rank from tensor w. the set of matrices  gk    gk   jk   njkk   where the matrices are spanned in the same  index are called tt core. we can describe eq.  in detail by    fig.  . illustration for eq.   calculating an element w  j    ..  jk   using set of  tt cores  gk   jk   dk      by factoring the original tensor w into multiple ttcores  gk  dk     we can compress the number of elements  q  needed to represent the original tensor size from dk   nk to  pd  k   nk rk   rk .  b. representing linear transformation using tt format  almost all of the parts of neural networks are composed of  linear transformations   y   w x   b            m n    m    where w   r  is the weight matrix and b   r is  the bias vector. in most cases  matrix w has many more  parameters than bias b. therefore  we can utilize the tt format  for optimizing our neural networks by replacing weight matrix  w with tensor w in tt format     .  we represent the tt format for matrix w   r m n where  q  q  m   dk   mk and n   dk   nk as tensor w by defining  bijective functions fi   z    zd  and f j   z    zd  . function fi  maps each row p       ..  m  into fi  p     i   p   ..  id  p   and f j  map each column q       ..  n  into f j  q      j   q   ..  jd  q  .  after defining such bijective functions  we can access the  value from matrix w p  q  in tensor w with the index vectors  generated by fi  p  and f j  q . we transform eq.  to represent  matrix w in the tt format   w p  q       w fi  p   f j  q                        w i   p   ..  id  p    j   q   ..  jd  q                       g  i   p   j   q  ..gd id  p   jd  q           where for each k       ..  d    gk  ik  p   jk  q        rrk    rk    ik  p              ..  mk      jk  q              ..  nk  .    to represent the linear transformation in eq.  with eq.      we need to reshape the vector input x into tensor x and  bias vector b into tensor b with order d to match our tensor     where x is the tensor representation of input xt and ht   is  the tensor representation of previous hidden states ht   .    table i  fully connected vs tt layer running time and memory  operation  fc forward  tt forward  fc backward  tt backward    time  o mn   o dr  m max m  n    o mn   o d  r  m max m  n      memory  o mn   o r max m  n    o mn   o r  max m  n      d. compressing gru rnn with tt format    w. the following equation calculates a similar operation with  y p    w p    x   b p  where we map row index p to vector   i   p   ..  id  p   and enumerate all possible mappings for all  columns in matrix w   x  y  i   p   ..  id  p      g   i   p   j   ..gd  id  p   jd     j   ..  jd    x   j    ..  jd     b  i   p   ..  id  p         we can control the shape of tt cores  gk  di   by choosing  factor m as  mk  dk   and n as  nk  dk   as long as the number  of factors is equal between m and n. we can also define ttrank  rk  dk   and treat them as a hyper parameter. in general  if  we use a smaller tt rank  we will get more efficient models  but this action restricts our model to learn more complex  representation. if we use a larger tt rank  we get more  flexibility to express our weight parameters but we sacrifice  model efficiency. table i compares the forward and backward  propagation times and the memory complexity between the  fully connected layer and the tt layer in big o notation     .  we compare the fully connected layer with matrix w   r m n  versus the tt layer with tensor w with tt rank  rk  dk   . in the  table  m denotes max  mk  dk      and r denotes max  rk  dk    .  c. compressing simple rnn with tt format  we represent a simple rnn in tt format and call this  model tt srnn for the rest of this paper. from section  ii a  we focus our attention on two dense weight matrices    w xh   whh  . previously  we defined w xh   r m n as inputto hidden parameters and whh   r m m as hidden to hidden  parameters.  q  first  we factorize matrix shape m into dk   mk and n into  qd  d  k   nk . next  we determine tt rank  rk  k   for our model  and substitute w xh with tensor w xh and whh with tensor  whh . tensor w xh is represented by set of tt cores  gkxh  dk    where  k       ..  d   gkxh   rmk  nk  rk    rk   and tensor whh is  d  represented by set of tt cores  ghh  k  k   where  k       ..  d    hh  mk  mk  rk    rk  gk   r  . we define bijective functions fix and fih to  access row p from w xh and whh in the set of tt cores. we  rewrite our simple rnn formulation to calculate ht in eq.    x  atxh  p     w xh  fix  p     j    ..  jd      xt   j    ..  jd           in this section  we apply tt format to represent a gated  rnn. among several rnn architectures with gating mechanism  we choose gru to be reformulated in tt format because it has less complex formulation and similar performance  as lstm. we call this model tt gru for the rest of this  paper. in section ii b   we focus on the following six dense  weight matrices   w xr   whr   w xz   whz   w xh   and whh  . weight  matrices w xr   w xz   w xh   r m n are parameters for projecting  the input layer to the reset gate  the update gate  the candidate  hidden layer  and whr   whz   whh   r m m are respectively  parameters for projecting previous hidden layer into the reset  gate  the update gate and candidate hidden layer.  qd  qd  we factorize m    k   mk   n    k   nk and set ttrank as  rk  dk   . all weight matrices  w xr   whr   w xz   whz    w xh   whh   are substituted with tensors  w xr   whr   w xz    whz   w xh   whh   in tt format. tensors w xr   w xz   w xh are  represented by a set of tt cores   gkxr  dk      gkxz  dk      gkxh  dk      where  k       ..  d    gkxr   gkxz   gkxh   rmk  nk  rk    rk  . tensor  d  whr   whz   whh are represented by a set of tt cores   ghr  k  k      hz d  hz  hh d  hr  hh   gk  k      gk  k     where  k       ..  d    gk   gk   gk    rmk  mk  rk    rk  . we define bijective function fix to access row  p from w xr   w xz   w xh and function fih to access row p from  whr   whz   whh in the set of tt cores. we rewrite the gru  formulation to calculate rt in eq.    atxr  p            x    whr  fih  p     j    ..  jd      ht     j    ..  jd      j  ..  j    atxr  ahr  t            xr d     a      ..  atxr  m   h t  i  hr  ahr  t      ..  at  m     rt           atxr   ahr  t   br  .         atxz  p       x    w xz  fix  p     j    ..  jd      xt   j    ..  jd      j   ..  jd    ahz  t  p          x    whz  fih  p     j    ..  jd      ht     j    ..  jd      j  ..  j      ht     j    ..  jd                  i            zt           atxz   ahz  t   bz  .    i                       ahh  t         h    ht         f  atxh   ahh  t   bh       hh  ahh  t      ..  at  m             next  we rewrite the gru formulation to calculate zt in eq.         xz d     a      ..  atxz  m   h t  i  hz  ahz  t      ..  at  m     h    atxh      ..  atxh  m     x    atxz  ahz  t    whh  fih  p     j    ..  jd       j   ..  jd    atxh    w xr  fix  p     j    ..  jd      xt   j    ..  jd      j   ..  jd    ahr  t  p       j   ..  jd    ahh  t  p     x            finally  we rewrite the gru formulation to calculate h t in     eq.    atxh  p       x    w xh  fix  p     j    ..  jd      xt   j    ..  jd      j   ..  jd    ahh  t  p          x    whh  fih  p     j    ..  jd         j   ..  jd    atxh         ahh  t          rt   j    ..  jd     ht     j    ..  jd     h  i  atxh      ..  atxh  m   h  i  hh  ahh  t      ..  at  m     h t         f  atxh   ahh  t   bh  .            after rt   zt and h t are calculated  we calculate ht on eq.  with  standard operations like element wise sum and multiplication.  in practice  we could assign a different d for each weight  tensor as long as the input data dimension can also be factorized into the d values. we could also put different tt rank  for each tensor and treat them as our model hyper parameter.  however  to simplify our implementation we use the same  tt rank for both the input and hidden projection weight  q  tensors. we also use the same factorizations m   dk   mk  qd  and n   k   nk for all weight tensors in tt srnn and ttgru.  we do not substitute bias vector b into tensor b because  the number of bias parameters is insignificant compared to the  number of parameters in matrix w. in terms of performance   the element wise sum operation for bias vector b is also  insignificant compared to the matrix multiplication between  a weight matrix and the input layer or the previous hidden  layer.  e. initialization for tt cores parameters  weight initialization is one critical detail for training deep  neural networks. especially for our rnn with tt format that  has many mini tensors and several multiplications  the ttrnn will have a longer matrix multiplication chain than a  standard rnn  and the hidden layer value will quickly saturate      . therefore  we need to carefully choose the initialization  method to help our proposed model start in a stable condition.  in our implementation  we follow glorot initialization       to keep the same variance of weights gradients across layers  and time steps to avoid the vanishing gradient problem. we  initialize all the tt cores as follows    k       ..  d      gk         where     k         n     k     s      nk   rk      mk   rk        by choosing a good initialization  our neural network will  converge faster and obtain better local minima. based on our  preliminary experiments  we get better starting loss at the first  several epochs compared to the randomly initialized model  with the same  k on gaussian distribution for all tt cores.  iv. experiments  in this section  we evaluate our proposed rnn model with  tt formats  tt srnn and tt gru  and compare them to    baseline rnns  a simple rnn and gru . we conducted  the experiments on sequence classification tasks  where each  input sequence was assigned a single class  and sequence  prediction tasks  where we predicted the next time step based  on previous information     . we used mnist dataset for  the sequence classification task and polyphonic music datasets  for the sequence prediction task. for both tasks  we used  local glorot initialization trick from section iii e for all the  tt cores weight parameters on the tt srnn and tt gru  models. we used adam algorithm      to optimize our model  parameters.  for reports on both tasks  we simplified the model description as follows  rnn hf where f denotes the number of  hidden units  e.g.  rnn h    means rnn with     hidden  units  and tt srnn hf r  where   denotes the tt rank   e.g.  tt srnn h  x   r  means tt srnn with hidden  units   x   in tt format and tt rank   . we used a grid  search to determine the best number of hidden layer units  for both models and the shape of tt format based on the  validation set performance.  a. sequence classification on sequential mnist  we evaluated our proposed model tt srnn and tt gru  for classification task using the mnist dataset     . the  mnist dataset consists of    x    grayscale images from  ten classes  digits     . the mnist dataset has a training set  with       images  a development set with       images  and  a test set with       images. we have three different ways to  represent the mnist dataset in our experiments.  for the first experiment  we fed each row starting at the  top row and ending at the bottom row  which means we fed  a vector with    values at each time step and a total of     time steps to represent an image. we used the latest hidden  layer activation as our image representation and put a softmax  layer to classify the digits. this task s difficulty is medium for  a simple rnn and an easy task for gated rnn. our baseline  models consists of rnn and gru with     hidden units.  for our proposed model  we use tt srnn and tt gru with          shapes and ranks       . for all the models  we use a  projection layer with    hidden units before we feed the input  to our rnn. the projection layer is used to embed the pixel  representation into richer feature representation. we show the  result on table ii. we repeated all of the experiments five  times with different weight parameters initializations. both the  baseline and proposed models converged with good accuracy  in several epochs and we achieved similar accuracy with a  compression rate up to    times.  in our second experiment  we fed each pixel starting at  the top left corner and ending at the bottom right corner   which means we fed a pixel at one time step and in total  we needed     time steps to represent an image. this task  is very challenging even for an rnn with gating mechanism  because the rnn needs to model very long sequences     .  as in the first task  we fed the softmax layer using the latest  hidden layer values. for this very long dependency task  we  only benchmarked the gated rnn variants  gru and tt      table ii  compression rate and accuracy for mnist row  model  rnn h     tt srnn h  x   r   tt srnn h  x   r   gru h     tt gru h  x   r   tt gru h  x   r     rnn params                                           compr.       .      .         .     .      test acc    .     .      .     .      .     .     .     .      .     .      .     .      table iii  compression rate and accuracy for pixel mnist  model  gru h     tt gru h  x   r   tt gru h  x   r   tt gru h  x   r     rnn params                              compr.       .     .      .      test acc    .     .     .     .     gru . for our proposed model  we used tt gru with output  shapes          and three different tt ranks          . for all  the models  we use a projection layer with    hidden units  before we feed the input to our rnn. fig.   compares the  validation set cost for each epoch. we can observe that the  tt gru able to converge as fast as the baseline gru model.  in table iii  our proposed model matched the baseline model  with tt rank   and reduced the parameters    times smaller  compared to the baseline model.    fig.  . comparison between baseline gru with     hidden units  tt gru  with         output shape and tt rank         on the pixel mnist validation  set.    in the last experiment  we used the most difficult task       to push the limits of the gated rnn model. we shuffled the  mnist pixel by pixel and applied the same shuffled index to  all the samples  and fed them one by one in a similar way as in  the previous experiment. for the baseline and proposed model   we used the same configuration as in the previous experiment.  fig.   compares the validation set cost for each epoch. in table  iv  we show that our proposed models was able to match  the baseline models with tt rank   and reduced the rnn  parameters to    times smaller.    fig.  . comparison between baseline gru with     hidden units  tt gru  with         output shape and tt rank         on the p mnist validation set.  table iv  compression rate and accuracy for p mnist  model  gru h     tt gru h  x   r   tt gru h  x   r   tt gru h  x   r     rnn params                              compr.       .     .      .      test acc    .     .     .     .     b. sequence prediction on polyphonic music  for the sequential modeling tasks  we used four polyphonic  music datasets       piano midi.de  nottingham  musedata   and jsb chorales. all of these datasets have    binary values  per time step  and each consists of at least seven hours of  polyphonic music. our baseline models are a simple rnn with      hidden units and a gru rnn with     hidden units. our  proposed models are tt srnn and tt gru with                output shapes and tt ranks       . before we fed our input into  the rnn  we projected them using hidden layer with     hidden units. in the polyphonic modeling task  we measured two  different metrics  negative log likelihood  nll  and accuracy   acc . to calculate the accuracy  we followed the evaluation  metric proposed by      where acc   t p  t p   fp   fn .  we only used true positive  tp   false positive  fp   false  negative  fn  and ignored the true negative  tn  because most  of the notes were turned off in the dataset. table v lists all of  the results of our experiments on the baseline and proposed  models. we repeat all experiments five times with different  weight parameters initialization.  the table shows that all of these models have similar  performances based on the negative log likelihood and the  accuracy in the test set. our proposed models was able to  reduce the number of parameter with significant compression  ratio and preserved the performance at the same time.  v. related work  compressing parameters on neural network architecture has  become an interesting topic over the past several years due to  the increased complexity of neural networks. the number of  parameters and processing times has also grown tremendously     table v  compression rate  negative log likelihood and accuracy for all polyphonic music test set  model    params    compr.    rnn h     tt srnn h x x x  r   tt srnn h x x x  r   gru h     tt gru h x x x  r   tt gru h x x x  r                                                       .      .          .      .      nottingham  nll  acc   .    .     .   .    .    .     .   .    .    .     .   .    .    .     .   .    .    .     .   .    .    .     .   .     along with their performance. a number of researchers comes  up with many different ways to tackle this problem.  ba et al.      and hinton et al.      distilled  the knowledge  from a deep neural network into a shallow neural network.  first  they trained a state of the art model with a deep and  complex neural network using the original dataset and hard  label as the target. after that  they reused the trained deep  neural network by extracting output from the softmax layer  and used them as the output target for a shallow neural  network. by training the shallow network with a soft target   they achieved a better performance than the model trained  using hard target labels. recently  tang et al.      utilized  a similar approach for training rnn with a trained dnn.  however  they had to train two different neural networks and  built different structures to transfer the knowledge from bigger  models.  from the probabilistic perspective  graves et al.      proposed a variational inference method for learning the mean and  variance of gaussian distribution for each weight parameter.  they reformulated the variational inference as the optimization  of a minimum description length     . by modeling each  weight parameter  they learned the importance of each weight  in regard to the model. after the training process was finished   they pruned the parameters by removing the weight that has  a high probability to be zero. however  they still needed  large matrix multiplication and represented their model in  dense weight matrix  and thus the algorithmic and memory  complexity remained the same as in the original model.  another approach to tackle the compression problem by a  technical perspective is to limit the precision for weight parameters. gupta et al.      and courbariaux et al.      minimized  the performance loss while using fewer bits  e.g.     bits   to represent floating points. courbariaux et al.      proposed  binaryconnect to constrain the weight possible values to     or   . most of these ideas can be easily applied with our  proposed model since several deep learning frameworks have  built in low precision floating point options           .  model compression using low rank matrix has also been  reported           . both of these works showed that many  weight parameters are significantly redundant  and by representing them as low rank matrices  they reduced the number  of parameters with only a small drop in accuracy. recently  lu  et al.      used low rank matrix ideas to reduce the number of  parameters in an rnn. novikov et al.      utilized tt format  to represent weight matrices on feedforward neural networks.    pianomidi  nll  acc   .    .     .   .    .    .     .   .    .    .     .   .    .    .     .   .    .    .     .   .    .    .     .   .     musedata  nll  acc   .    .      .   .    .    .      .   .    .    .     .   .    .    .      .   .    .    .     .   .    .    .      .   .     jsb chorales  nll  acc   .    .      .   .    .    .      .   .    .    .      .   .    .    .      .   .    .    .      .   .    .    .      .   .     from their empirical evaluation on dnn based architecture   the feedforward layer represented by the tt format has a far  better compression ratio and smaller accuracy loss compared  to the low rank matrix approach.  to the best of our knowledge  there are only a few research  about compression on rnn models  and none of these works  have utilized tensor based format to represent the weight  matrices for rnn models. in this work  we presented an  rnn model by using tt format weight to re parameterize  the weight matrices. we also compared the performance  to standard uncompressed rnns with a greater number of  parameters. we expect our model could minimize the number  of parameters and preserved the performance simultaneously.  vi. conclusion  in this paper  we presented an efficient and compact rnn  model using tt format representation. using tt format  we  represented dense weight matrices inside the rnn layer with  multiple low rank tensors. our proposed tt srnn and ttgru significantly compressed the number of parameters while  simultaneously retaining the model performance and accuracy.  we evaluated our model with sequence classification and sequence prediction tasks. on sequence classification  with very  long dependency tasks  our proposed rnns model reduced  the rnn parameters up to    times smaller compared to the  original models without losing any accuracy. on the sequence  prediction task  we evaluated our model with multiple music  datasets  and our proposed rnns reduced the rnn parameters  up to    times smaller while preserving the performance.  acknowledgment  part of this work was supported by microsoft core     project as well as jsps kakenhi grant numbers           and         .  