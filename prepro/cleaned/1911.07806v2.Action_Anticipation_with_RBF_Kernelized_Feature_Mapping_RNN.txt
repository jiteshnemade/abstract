introduction    action anticipation  sometimes referred to as action prediction  is gaining a lot of  attention due to its many real world applications such as human computer interaction            sports analysis          and pedestrian movement prediction                 especially in the autonomous driving scenarios.  in contrast to most widely studied human action recognition methods  in action  anticipation  we aim to recognize human action as early as possible                 .  this is a challenging task due to the complex nature of video data. although a video  containing a human action consists of a large number of frames  many of them are not  representative of the action being performed  large amount of visual data also tend to  contain entangled information about variations in camera position  background  relative  movements and occlusions. this results in cluttered temporal information and makes  recognition of the human action a lot harder. the issue becomes even more significant  for action anticipation methods  as the algorithm has to make a decision using only a  fraction of the video at the very start. therefore  finding a good video representation that  extracts temporal information relevant to human action is crucial for the anticipation  model.  to over come some of these issues  we resort to use deep convolutional neural networks  cnns  and take the deep feature on the penultimate layer of cnn as video          y. shi et al.    fig.    overview of proposed feature mapping rnn   given a frame extracted from  video data  the algorithm first passes the rgb image i t  through a deep cnn to acquire high level features of the image xt . the vector is then split into smaller segments  xit of equal length. each scalar element in the segmented vector is used as input to a single lstm cell that produces the prediction of corresponding feature element in frame   t   k   where k    . after all segments are processed with lstms  all the prediction  segments x it k are concatenated back together to form x t k   which contains high level  features of i t   k .    representation. another motivation to use deep cnns stems from the difficulty of generating visual appearances for future. therefore  similar to vondrick et al.       we  propose a method to generate future features tailored for action anticipation task  given  an observed sequence of deep cnn features  a novel recurrent neural network  rnn   model is used to generate the most plausible future features and thereby predicting the  action depicted in video data. an overview of this model can be found in fig.  .  the objective of our rnn is to map the feature vector at time t denoted by xt to the  future feature vector at  t   k  denoted by xt k . because only a fraction of the frames  are observed during inference  the future feature generator should be highly regularized  to avoid over fitting. furthermore  feature generator needs to model complex dynamics  of future frame features.  this can be resolved by parameter sharing. parameter sharing is a strong machine  learning concept that is being used by many modern leaning methods. typically  cnns  share parameters in the spatial domain and rnns in the temporal dimension. in our  work  we propose to utilize parameter sharing in an unconventional way for rnn models by expanding it to the feature domain. this is based on the intuition that the cnn  feature activations are correlated to each other.  by utilizing parameter sharing across feature activations  our proposed rnn is  able to learn the temporal mapping from xt to xt k with significantly fewer parameters. this greatly boosts the computational efficiency of the prediction model and  correspondingly shortens the response time. we call our novel rnn architecture feature mapping rnn .  to model complex dynamic nature of video data  we make use of a novel mapping  layer inside our rnn. in principle  the hidden state of the rnn captures the temporal information of observed sequence data. in our method  hidden state of the rnn is  processed by a linear combination of gaussian radial basis function  rbf  kernels to     feature mapping rnn         produce the future feature vector. while a linear model defines a simple hyperplane as  mapping functions  the kernelized mapping with rbf kernels can model complex surfaces and therefore has the potential of improving the prediction accuracy. in our work   we also implement rbf kernels on the action classification multi layer perceptron to  improve the performance of classifiers.  ideally  we are interested in learning the probability distribution of future given  the past features. to learn this conditional distribution  inspired by the of generative  adversarial networks       an adversarial approach is used to evaluate the cost of the  feature mapping rnn. the rnn is trained with an adversarial loss and re constrictive  l  loss. in this way  the model is optimized not only with the intention of reducing the  euclidean distance between the prediction and ground truth  but also taking probability  distribution of the feature vector into consideration.  in a summary  our contributions are     we propose a novel rnn architecture that share parameters across temporal  domain as well as feature space.    we propose a novel rbf kernel to improve the prediction performance of  rnns.    we demonstrate the effectiveness of our method for action anticipation task  beating state of the art on standard benchmarks.         related work    the model proposed in this paper focuses on future video content generation for action prediction and action anticipation                                      . in contrast to the widely studied action recognition problem  the action anticipation literature  focuses on developing novel loss functions to reduce the predictive generalization error            or to improve the generalization capacity of future content such as future  appearance      and future features     . the method propose in this paper also focuses on future content generation and therefore could further benefit from novel loss  functions as proposed in           .  in the early days  yu et al.      make use of spatial temporal action matching to  tackle early action prediction. their method relies on spatial temporal implicit shape  models. by explicitly considering all history of observed features  temporal evolution of  human actions is used to predict the class label as early as possible by kong et al.     .  li et al.  s work      exploits sequence mining  where a series of actions and object  co occurrences are encoded as symbolic sequences. soomro et al.      propose to use  binary svms to localize and classify video snippets into sub action categories  and  obtain the final class label in an online manner using dynamic programming. in        action prediction is approached using still images with action scene correlations. different from the above mentioned methods  our work is focused on action anticipation  from videos. we rely on deep cnns along with a rnn that shares parameters across  both feature and time dimensions to generate future features. to model complex dynamics of video data  we are the first to make use of effective rbf kernel functions  inside rnns for the action anticipation task.  on the other hand  feature generation has been studied with the aim of learning  video representation  instead of specifically for action anticipation. inspired by natural          y. shi et al.    language processing technique      authors in      propose to predict the missing frame  or extrapolate future frames from an input video sequence. however  they demonstrate  this only for unsupervised video feature leaning. other popular models include the unsupervised encoder decoder scheme introduced by      for action classification  probabilistic distribution generation model by      as well as scene prediction learning using  object location and attribute information introduced by    . research in recent years on  applications of generative adversarial network on video generation have given rise to  models such as mocogan       tgan      and walker et al.  s work      on video  generation using pose as a conditional information. the mechanisms of these gan variations are all capable of exploiting both the spatial and temporal information in videos   and therefore have showed promising results in video generation.  moreover  trajectory prediction       optical flow prediction       path prediction          and motion planning          sports forecasting      activity forecasting of      are also  related to our work. all these methods generate future aspects of the data. our novel  rnn model  however  focuses on generating future features for action anticipation.        .     approach  overview    similar to methods adopted by other action anticipation algorithms  our algorithm makes  predictions of action by only observing a fraction of video frames at the beginning of  a long video. the overall pipeline of our method is shown in fig.  . first  we extract  some cnn feature vectors from frames and predict the future features based on the past  features. subsequently  a multilayer perceptron  mlp  is used to classify generated features. we aggregate predictions from observed and generated features to recognize the  action as early as possible.   .     motivation    denote observed sequence of feature vectors up to time t by x   hx    x    x          xt i  and future feature vector we aim to produce by x t k   where k     and xt   rd . we  are interested in modeling the conditional probability distribution of p  xt k   x    x     x          xt       where   denotes the parameters of the probabilistic model.  it is natural to use rnns or rnn variants such as long short term memory   lstm       to model the temporal evolution of the data. however  learning such a  mapping could lead to over fitting since these methods tend not to utilise the temporal  coherence and the evolutionary nature of video data     .  furthermore  a naive cnn feature mapping using a lstm from past to the future  is also prone to over fitting. a lstm with hidden state of dimensionality h and takes  feature vectors of dimensionality d as input uses parameters in the order of   dh   d   .  as an example  if we use the penultimate activations of inception v       as feature  vectors  d          a typical lstm  h        would require parameters in the order  of     . we believe that the effectiveness of such models can be largely improved by  utilising the correlation of high level activations of modern cnn architectures        .     feature mapping rnn         motivated by these arguments  we propose to train a lstm model where parameters  are not only shared in the time domain  but also across feature activations. by doing so   we aim to self regularize the feature generation of the algorithm. we name our novel  architecture feature mapping rnn . furthermore  to increase the functional capacity  of rnns  we make use of radial basis functions  rbf  to model temporal dynamics  of the conditional probability distribution p  xt k   x    x    x          xt     . these  mechanisms will be introduced in details in the following subsection.   .     feature mapping rnn with rbf kernel mapping    a traditional feature generation rnn architecture takes a sequence of vectors up to time  t as input and predicts the future feature vector x t k . typically  the following recurrent  formula is used to model the prediction   ht   f  xt   ht                  where ht is the hidden state  ht   rh   which captures the temporal information of the  sequence and   are the parameters of the recurrent formula. then we utilize this hidden  state to predict the future feature vector xt k using the following formula   x t k   ht   w           where w   rh d is the parameter that does the linear mapping to predict the future  feature vector.  as introduced previously  in our feature mapping rnn the parameters   are shared  across several groups of feature activations. this is achieved by segmenting the input  feature vector of dimensionality d into equal size sub vectors of dimensionality d   where d is referred to as feature step size.  now let us denote the ith sub feature vector of size d by xit . intuitively  if we  concatenate all such sub feature vectors in an end to end manner  we will be able to  reconstruct the original feature vector xt . the time sequence of data for the ith subfeature vector is now denoted by x i   h xi    xi    xi          xit i. if we process each  sequence x i in units of xit with the rnn model in equation   and equation    we will  be able to predict xit k and by concatenating them end to end  generate xt k . this  approach reduces the number of parameters used in the rnn model from   dh   d     to   dh   d     which results in a considerable boost in computational efficiency especially when d   d. however  the parameter complexity of the model would remain  polynomial and is relevant to multiple hyperparameters.  to further improve the efficiency of our model  we adopt an even bolder approach   we propose to convert the sequence of vectors of x i   h xi    xi    xi          xit i to a  i j   sequence of scalars. let us denote the j th dimension of sub vector xit by xt . now instead processing sequence of vectors x i   we convert the sequence x i to a new sequence     i     i d   i     i     i k   i d   i     i. size of the  of scalars x i   h x    x          x    x    x            xt         xt     d  i  sequence of scalars x is equal to t   d and we generate d number of such sequences  from each original sequence of feature vector x.          y. shi et al.    we then propose to process sequence of scalars using a rnn  lstm  model. the  computation complexity is now linear  with number of parameters used in the recurrent  model  lstm  reduced to   h      and depends only on the hidden state size.  again  given the current sequence of vectors x  we want to generate future feature  vector  xt k . in the  d  e our rnn model  this is translated to predicting sequence of scalars  i            i d     d  xt k         xt k from sequence x i for all sub feature vectors i     to d  . then we  merge all predicted scalars for time t   k to obtain xt k .  therefore  mathematically our new rnn model that share the parameter over feature activations can be denoted by the following formula   i l     ht         i l       f  xt   hi l  t                        where   is the new parameter set of the rnn  lstm  and the future l th scalar of i th  sub feature vector is given by   i l     i l     x t k   ht          w .           to further improve the functional capacity of our feature mapping rnn   we make  use of radial basis functions  rbf . instead of using a simple linear projection of the  hidden state to the future feature vector  we propose to exploit the more capable radial  basis functional mapping. we call this novel rnn architecture the rbf kernelized  feature mapping rnn   denoted by the following formula   i l     x t k      h  hi l     l    i  t  j   jl exp    l      j    j    n  x           where  lj    jl and  jl are parameters learned during training and n the number of rbf  kernels used. these parameters are shared across all sub feature vectors. the future feature vector x it k is calculated as the linear combination of rbf kernels outputs. since  the rbf kernels are better at modeling complex planes in the feature space  this functional mapping is able to accurately capture more complicated dynamics. implementing  the kernalised rbf on our feature mapping rnn enables the model to do so with fewer  parameters than classical rnns. illustration of our rbf kernelized mlp is shown in  fig  .  note that the method we have presented here only uses non overlapping featuresub vectors  i. e. no overlapping exists between   consecutive sub vectors. however   overlapping feature sub vectors can be used to improve the robustness of feature generation. therefore  instead of using a non overlapping feature stride of d  we use an  overlapping stride of size s. in this case  we take the average between all overlapping  i l   parts of   consecutive sub vectors to obtain x t k .   .     training of feature mapping rnn    data generation  especially visual data generation with raw images  has remained a  challenging problem for years mainly due to the absence of suitable loss function. the     feature mapping rnn         most commonly used function for this task is the l  loss. however  it works under the  assumption that data is drawn from a gaussian distribution  which makes the loss function ineffective when dealing with data that follows other distributions. as an example   if there exists only two equally possible value v  and v  for a pixel  the possibility for  vavg    v    v      to be the true value for that pixel is minimal. however  vavg will  be assigned to the output in a neural network that uses l  loss to evaluate the cost.  this property of the l  loss function causes a  blurry  effect on the generated output.  similar observations can be seen for feature vector generation.  recent developments in generative adversarial networks address this issue successfully     . traditional gan consists of   cnns  one of them is named generator  denote as g  and the other discriminator  denote as d . the gan effectively  learns the probabilistic distribution of the original data  and therefore eliminates the   blockiness  effect caused by l  loss function. here  we propose to train the feature mapping rnn algorithm using a combination of l  and adversarial loss  which  is realized by implementing the feature mapping rnn as the generator denoted by  g   xit   x it k . by doing so  we are able to produce prediction that is both accurate  and realistic.  l  loss  the l  loss is defined as the mean squared error between the generated  feature and the real feature vector of the future frame given as follows   lg   xt       xit k   x it k        xit k   g xit    .           adversarial loss  we use generator adversarial loss proposed by      where we train  g so that d believes g xit   comes from the dataset  at which point d g xit       . the  loss function is defined as   lgadv     log d g xit    .           by adding this loss to our objective function  the rnn is encouraged to generate feature  prediction with probabilistic distribution similar to the original data. finally  the loss  function of our rnn generator g is given by   lg      lg       lgadv .           the discriminator is trained to judge whether its inputs are real or synthetic. the objective is to output   when given input is the real data xit k and   when input is generated  data g xit  . therefore  the discriminator loss is defined as   ld    log d xit k      log     d g xit    .   .            action classifier and inference    to evaluate the authentication of predicted features generated by the feature matching  rnn  we again use the frame features to train a   layer mlp appended with a rbf  kernel layer  equation    to classify videos as early as possible. the classification loss  is evaluated using a cross entropy loss. feature mapping rnn and the action classification mlp is trained separately. one might consider training both mlp and the feature mapping rnn jointly. however  in terms of performance  we did not see that much  of advantage.          y. shi et al.    fig.    illustration of rbf keneralized  multilayer perceptron.    fig.    testing procedure  ture mapping rnn    of    fea     during inference  we take advantage of all observed and generated features to increase the robustness of the results. accuracy is calculated by performing temporal  average pooling on all predictions  see fig   .        .     experiments  datasets    three datasets are used to evaluate the performance of our model  namely ut interaction        jhmdb         and ucf           . we follow the standard protocols for each  of the datasets in our experiments. we select these datasets because they are the most  related to action anticipation task that has been used in prior work        .  ut interaction the ut interaction dataset  uti  is a popular human action recognition dataset with complicated dynamics. the dataset consists of   types of human  interactions executed under different backgrounds  zoom rates and interference. it has  a total of    video sequences split into   sets. each video is of approximately   minute  long  depicting   interactions on average. the available action classes include handshaking  pointing  hugging  pushing  kicking and punching. the performance evaluation methodology requires the recognition accuracy to be measured using a    fold  leave one out cross validation per set. the accuracy is evaluated for    times while  changing the test sequence repeatedly and final result is yielded by taking the average  of all measurements.  jhmdb    jhmdb    is another challenging dataset that contains     video clips of     types of human actions. quite different from the ut interaction where video clips of  different actions are scripted and shot in relatively noise free environments  all videos in  jhmdb    are collected from either movies or online sources  which makes the dataset  a lot more realistic. each video contains an execution of an action and the dataset is split  into   sets for training  validation and testing.     feature mapping rnn         ucf       ucf       is a subset of ucf   . the dataset consists of more than       videos from    action classes of ucf   . since all the videos are collected from  youtube  the diversity of data in terms of action types  backgrounds  camera motions   lighting conditions etc are guaranteed. in addition  each video depicts up to    actions  of the same category with different temporal and spatial features  which makes it one  of the most challenging dataset to date.   .     implementation details    feature mapping rnn the feature mapping rnn is trained with batch size of       using a hidden size  h  of   in all experiments unless otherwise specified. the default  dimensionality of feature sub vector referred to as feature step size d  is set to    . we  make use of six rbf kernels within the rbf kernelized feature mapping rnn . feature  stride is set to    and weight of the adversarial loss       is set to   and the weight for  l  loss is set to     i. e.     .  action classifier mlp the a simple two layer mlp classifier consists of two hidden  layers with     and     activation respectively. we also use rbf kernels along with  the mlp where number of kernels set to    . mlp is trained with batch size of    .  training and testing procedures we use pre trained inception v       penultimate  activation as the frame feature representation. the dimensions of each feature vector is        d        . the action classification mlp is trained on the feature vectors from  the training split of the datasets. these features are also used to train our feature mapping rnn to generate future features. both models are trained with learning rate  .     and exponential decay rate  . .  protocols following the experimental protocol          we used only the first r        for ut interaction and     for jhmdb     of the video frames to predict action class  for each video. to utilise our model  we generate extra p   referred to as prediction  percentage  of the video features using our rbf kernalized feature mapping rnn .  therefore  we make use of  r  p   feature vectors of the original video length to make  the final prediction. to generate the next future feature at test time  we recursively apply  our feature mapping rnn given all previous features  including the generated ones .  we then use our action classification mlp to predict the action label using max pooling  or simply average the predictions. this procedure is demonstrated more intuitively in  fig. .   .     comparison to state of the art    we compare our model to the state of the art algorithms for action anticipation task on  the jhmdb    dataset. results are shown in table  . our best algorithm  denoted as  fm rbf gan inception v  in the table  outperforms the state of the art by      and  we can clearly see that the implementation of kernel svm and adversarial training improves the accuracy by around   to   . in addition  to show the progression of how our  method is able to outperform the baseline by such a large margin  we also implemented  the feature mapping rnn on top of vgg   so that the deep cnn pre processing is  consistent with other methods in table  . the fm vgg   entry in the table shows an           y. shi et al.    table    comparison of our model against  state of the arts on jhmdb    dataset  for action anticipation. we follow the protocol of jhmdb    for action anticipation and predictions are made from using  only     of video sequence.  method  accuracy  elstm            within class loss            dp svm           others  s svm           where what            context fusion            fm vgg         fm ksvm gan vgg         ours  fm inception v        fm rbf gan inception v         table    comparison of our model against  state of the arts on ut interaction dataset  for action anticipation. following protocol of ut interaction  predictions are  made from using only     of video sequence.  method  accuracy  elstm            within class loss           context fusion            cuboid bayes            i bow            d bow            cuboid svm            bp svm            ours            improvement from baseline elstm  which is purely influenced by the implementation of feature mapping rnn .  experiments are also carried out on the two other mentioned datasets  where our best  method outperforms the state of the art by     on ut interaction and    on ucf       as shown in table   and table   respectively.  we believe these significant improvements suggests the effectiveness of two main  principles  the parameter sharing and expressive capacity of rbf functionals. to further  investigate the impact of each component  we perform a series of experiments in the  following sections.    table    comparison of our model against state of the arts on ucf       dataset for  action anticipation. again  predictions are made from using only     of video sequence.  method  accuracy  temporal fusion           road            road   broxflow           ours          .     analysis    in this section we compare the influence of different components of our rbf kernelized  feature mapping rnn . as shown in table    we compare following variants of our  rnn model  including    a  feature mapping rnn   use only l  loss to train the feature mapping rnn       feature mapping rnn           b  feature mapping rnn  rbf  our rnn with kernalised rbf  still only using  l  loss    c  feature mapping rnn   rbf   gan  rbf kernelized feature mapping rnn  with adversarial loss.  apart from the feature mapping rnn  based models  we also conduct experiments on  the following method as comparisons to our model    d  linear  a matrix of size d   d is used for feature generation  d is dimension  of input feature     e  vanilla lstm  generate future action features with traditional vanilla lstm. l   loss is used to train it    f  vanilla lstm   rbf  vanilla lstm with kernalised rbf  using only l  loss    g  vanilla lstm   rbf   gan  rbf kernalized vanilla lstm with added adversarial loss.  note that all the results are obtained using features extracted by inception v  network   and the accuracy are acquired using max pooling at prediction percentage p      .  table    comparison of different approach on jhmdb    dataset  method  linear  vanilla lstm  vanilla lstm   rbf  vanilla lstm   rbf   gan  feature mapping rnn  feature mapping rnn   rbf  feature mapping rnn   rbf   gan    accuracy    .      .      .      .      .      .      the results in table   shows the proposed scheme outperforms the linear model significantly while using fewer parameters. most interestingly  the feature mapping rnn outperforms vanilla lstm by almost    indicating the impact of parameter sharing in  the feature space. we can also conclude from table   that the application of adversarial loss as well as rbf kernel layers encourages the model to generate more realistic  future features  which is reflected by the improvement in accuracy with feature mapping rnn  rbf and feature mapping rnn  rbf gan. it is also shown in the table    that vanilla lstm trained with rbf kernel yields almost    higher accuracy than  plain vanilla lstm  which proves further that the rbf layer is something the baseline  can benefit from. regrettably  the vanilla lstm with adversarial training model failed  to stabilise due to large number of parameters needed in the lstm cells to reconstruct  the original feature distribution.  the influence of rbf kernalized feature mapping rnn is quite distinctive. if we  compare the red curve to the green one  we can see that the discrepency between them  becomes larger as the prediction percentage increases. this indicates that the rbf kernalized feature mapping rnn generate more accurate future features in the long term   and hence it is a more robust model than plain feature mapping rnn . comparing the  red and green curve to the orange and blue one  we can also conclude that the adversarial           y. shi et al.    fig.    prediction accuracy without pooling for jhmdb    dataset at different  video prediction percentages p. rbf kernalized feature mapping rnn is trained  using adversarial loss is able to achieve  the highest stable accuracy.    fig.    prediction accuracy evaluated at  different feature step sizes on jhmdb     dataset. the accuracy plotted in the image  is found by implementing feature step size  between d     to      with increment of    on the model and the rolling average is  taken among every    measurements. no  temporal pooling is used.    loss assist the rnn training in a similar way. even without the assistance of gan loss  and rbf kernel  the feature mapping rnn still performs better than liner projection  rnn.   .     influence of hyper parameters    feature step size the accuracy of the generated data indicates the existence of strong  correlations between the d dimensional segments of the feature vectors. by default  we  resort to feature step size of      d       . in order to further explore this property  we  experimented with different feature step sizes. in fig.   we plot the recognition accuracy  against feature step size. we observe that small feature step size guarantees effective  feature generation. specifically  the prediction remains above     when feature step  size is smaller than    . this phenomena can be explained by the intuition that when  feature step size is large  the model tries to generalize a large set of features with mixed  information at one time step  which results in degraded performance.  it is also interesting to note that the prediction accuracy oscillates drastically as the  feature step size exceeds    . this indicates that perhaps the feature vector summarizes  information of the original image in fixed size clusters  and when we attempt to break  these clusters by setting different feature step size  the information within each time  step lacks continuity and consistency  which subsequently compromises the prediction  performance.  although smaller feature step size builds a more robust model  the training time with  feature step size    takes only half the amount of time of training with step size    with  no compromise on prediction accuracy. therefore  it might be beneficial sometimes to  choose a larger feature step size to save computational time.     feature mapping rnn    table    prediction accuracy at different feature  stride size  s   interval size  s    s    s       s       s       s          accuracy    .      .      .      .      .      .      table    prediction accuracy using lstm cells  with different state size   h .  hidden state size  h    h    h    h       h       h         accuracy    .      .      .      .      .      .            table    prediction accuracy using different number of rbf kernels.  no. of kernels  k    k    k       k       k       k        k          accuracy    .      .      .      .      .      .      .      interval sizein this section we experiment the effect of overlapping sub feature vectors  on our rbf kernalized feature mapping rnn . recall that the feature mapping rnn is  denoted by g   xit   x i   t k . instead of incriminating i by the multiple of feature step  size d  in an attempt to improve the prediction accuracy  we define an feature stride s  that is smaller than d. the prediction accuracy of feature mapping rnn with several  different feature stride value is shown in table  .  lstm state size this section aims at investigating the influence of lstm cell s hidden state size  h  on the model s performance. since the hidden state stores essential  information of all the input sequence data  it is common to consider it as the  memory   of the rnn. it is intuitive to expect an improvement in performance when we increase  the size of the hidden state up to some extent.  however  the results in table   shows that increasing the lstm state size does not  have much effect on the prediction accuracy  especially when the state size becomes  larger than  . this is because in the proposed feature mapping rnn model  each lstm  cell takes only one scalar as input  as opposed to the traditional rnn cells that process  entire vectors. as the hidden state size is always greater than the input size  equal to      it is not surprising that very large h does not have much influence on the model  performance.  number of rbf kernels in this section we study the influence of number of gaussian surfaces used in feature mapping rnn . we calculate prediction accuracy while  increasing the number of gaussian kernels from    to    . results are as shown in table  . the results show a general trend of increasing prediction performance as we add  more number of kernels  with the highest accuracy achieved at when k      . however  result obtained when k       is worse than when k    . this phenomena could  be explained by over fitting  resulted from rbf kernel s strong capability of modeling  temporal dynamics of data with complex boundaries.  conclusions for hyper parameters tuning the conclusion from these experiments is  that the model is not too sensitive to the variation of these hyper parameters in general   which demonstrates its robustness. results further demonstrated the computational efficiency of our approach. since it is possible to effectively train the model with very few  parameters  it can be stored on mobile devices for fast future action anticipation.                y. shi et al.    conclusions    the proposed rnn which uses a very few parameters outperforms state of the art algorithms on action anticipation task. our extensive experiments indicates the model s  ability to produce accurate prediction of future features only observing a fraction of  the features. furthermore  our rnn model is fast and consumes fraction of the memory which makes it suitable for real time execution on mobile devices. proposed feature mapping rnn can be trained with and without lables to generate future features.  our feature generator does not use class level annotations of video data. therefore   in principle  we can increase the robustness of the model utilizing large amount of  available unlabelled data. the fact that the model is able to generate valid results using very few parameters provides strong proofs for the existence of inner correlation  between deep features  which is a characteristic that can have implications on many  related problems such as video tracking  image translation  and metric learning.  in addition  by appending a rbf layer to the rnn  we observe significant improvement in prediction accuracy. however  it was also noted that over fitting occurs when  the model is implemented with too many kernel rbfs. to fully explore functional capacity of rbf function  in future studies  we aim to implement kernel rbfs on fully  connected layer of popular deep cnn models such as resnet       alexnet      and  densenet     .  in conclusion  proposed rbf kernalized feature mapping rnn demonstrates the  power of parameter sharing and rbf functions in a challenging sequence learning task  of video action anticipation.    