introduction  in recent years  deep learning models have been particularly effective in dealing with data that have complex internal structures. for example  convolutional neural networks   cnn  are very effective in handling image data in which   d spatial relationships are critical among the set of raw  pixel values in an image  lecun et al.        krizhevsky  et al.       . another success of deep learning is handling sequence data  in which the sequential relationships  within a variable length input sequence is crucial. in sequence modeling  recurrent neural networks  rnn  have  been very successful in language translation  cho et al.         sutskever et al.        bahdanau et al.         speech  recognition  graves et al.         image captioning  i.e.     submitted to icml     .    summarizing the semantic meaning of an image into a sentence  xu et al.        karpathy   fei fei        lebret  et al.         recognizing actions in videos  donahue et al.         srivastava et al.         or short term precipitation  prediction  shi et al.       .  there is  however  one important difference between rnn  and cnn. the key building blocks of cnn  such as nonlinear activation function  convolution and pooling operations  etc.  have been extensively studied. the choices are  becoming convergent  e.g.  relu for nonlinear activation   small convolution kernels and max pooling. visualization  also help us understand the semantic functionalities of different layers  zeiler   fergus         e.g.  firing at edges   corners  combination of specific edge groups  object parts   and objects.  the community s understanding of rnn  however  is not  as thorough  and the opinions are much less convergent.  proposed by hochreiter and schmidhuber         the long  short term memory  lstm  model and its variants have  been the overall best performing rnn. typical lstm is  complex  having   gates and   hidden states. using lstm  as an representative example  we may find some obstacles  that prevent us from reaching a consensus on rnn     depth with unrolling. different from cnn which  has physical layers  rnn is considered a deep model  only when it is unrolled along the time axis. this  property helps in reducing parameter size  but at the  same time hinders visualization and understanding.  this difficulty comes from the complexity of sequence  data itself. we do not have a satisfactory solution for  solving this difficulty  but we hope this paper will help  in mitigating difficulties caused by the next one.    competing and complex structures. since its inception  the focus of lstm related research has been  altering its structure to achieve higher performance.  for example  a forget gate was added by gers et al.         to the original lstm  and a peephole connection made it even more complex  gers et al.       .     minimal gated unit for recurrent neural networks    it was not until recently that simplified models were  proposed and studied  such as the gated recurrent  unit  gru  by cho et al.       . gru  however   is still relatively complex because it has two gates.  very recently there have been empirical evaluations on  lstm  gru  and their variants  chung et al.         jozefowicz et al.        greff et al.       . unfortunately  no consensus has yet been reached on the best  lstm like rnn architecture.  theoretical analysis and empirical understanding of deep  learning techniques are fundamental. however  it is very  difficult if there are too many components in the structure.  simplifying the model structure is an important step to enable the learning theory analysis in the future.  complex rnn models not only hinder our understanding.  it also means that more parameters are to be learned and  more components to be tuned. as a natural consequence   more training sequences and  perhaps  more training time  are needed. however  evaluations in  chung et al.         and  jozefowicz et al.        both show that more gates do  not lead to better accuracy. on the contrary  the accuracy  of gru is usually higher than that of lstm  albeit the fact  that gru has one less hidden state and one less gate than  lstm.  in this paper  we propose a new variant of gru  which  is also a variant of lstm   which has minimal number of  gates only one gate  hence  the proposed method is named  as the minimal gated unit  mgu . evaluations in  chung  et al.        jozefowicz et al.        greff et al.         agreed that rnn with a gated unit works significantly better than a rnn with a simple tanh unit without any gate.  the proposed method has the smallest possible number of  gates in any gated unit  a fact giving rise to the name minimal gated unit.  with only one gate  we expect mgu will have significantly  fewer parameters to learn than gru or lstm  and also  fewer components or variations to tune. the learning process will be faster compared to them  which will be verified by our experiments on a diverse set of sequence data in  section  . what is more  our experiments also showed that  mgu has overall comparable accuracy with gru  which  once more concurs the observation that fewer gates reduces  complexity but not necessarily accuracy.  before we present the details of mgu  we want to add that  we are not proposing a  better  rnn model in this paper.   the purpose of mgu is two fold. first  with a simpler  model  we can reduce the requirement for training data  ar     we also believe that  without a carefully designed common  set of comprehensive benchmark datasets and evaluation criteria   it is not easy to get conclusive decisions as to which rnn model  is better.    chitecture tuning and cpu time  while at the same time  maintaining accuracy. this characteristic may make mgu  a good candidate in various applications. second  a minimally designed gated unit will  in principle  make our analyses of rnn easier  and help us understand rnn  but this  will be left as a future work.     . rnn  lstm  gru  and more  we start by introducing various rnn models  mainly  lstm and gru  and their evaluation results. these studies in the literature have guided us in how to minimize a  gated hidden unit in rnn.  a recurrent neural network uses an index t to indicate different positions in an input sequence  and assumes that  there is a hidden state ht to represent the system status at  time t.  rnn accepts input xt at time t  and the status is  updated by a nonlinear mapping f from time to time   ht   f  ht     xt   .           one usual way of defining the recurrent unit f is a linear  transformation plus a nonlinear activation  e.g.   ht   tanh  w  ht     xt     b              where we combined the parameters related to ht   and xt  into a matrix w   and b is a bias term. the activation  tanh   is applied to every element of its input. the task of rnn is  to learn the parameters w and b  and we call this architecture the simple rnn. an rnn may also have an optional  output vector y t .  lstm. rnn in the simple form suffers from the vanishing  or exploding gradient issue  which makes learning rnn using gradient descent very difficult in long sequences  bengio et al.        hochreiter   schmidhuber       . lstm  solved the gradient issue by introducing various gates to  control how information flows in rnn  which are summarized in table    from equation   a  to   f   in which    x              exp  x            is the logistic sigmoid function  applied to every component of the vector input  and is the component wise product between two vectors.  figure  a illustrates the data flow and operations in lstm.    there is one more hidden state ct in addition to ht    which helps maintain long term memories.    the forget gate f t decides the portion  between   and     of ct   to be remembered  determined by parameters wf and bf .       we use boldface letters to denote vectors.     minimal gated unit for recurrent neural networks  table  . summary of three gated units  lstm  gru  and the proposed mgu.   is the logistic sigmoid function  and  means  component wise product.    lstm  long short term memory              a  long short term memory  lstm     f t      wf  ht     xt     bf          a     it      wi  ht     xt     bi          b     ot      wo  ht     xt     bo          c     c t   tanh  wc  ht     xt     bc          d     ct   f t    ct     it      e     ht   ot    tanh ct   .    c t        f     gru  gated recurrent unit               z t      wz  ht     xt     bz          a     r t      wr  ht     xt     br          b     h t   tanh  wh  r t      c     ht        z t      ht     xt     bh        ht     z t    h t .      d     mgu  minimal gated unit  the proposed method    b  coupled lstm    f t      wf  ht     xt     bf          a     h t   tanh  wh  f t      b     ht        f t         ht     xt     bh        ht     f t    h t .      c            the input gate it determines which portion of time t s  new information is to be added to ct with parameter  wi and bi .     c  gated recurrent unit  gru                 the inputs are transformed as the update c t with parameters wc and bc . c t  weighted by it   and ct     weighted by f t   form the new cell state ct .    an output gate ot is determined by parameters wo and  bo   and controls which part of ct is to be output as the  hidden state ht .  lstm learning follows typical stochastic gradient descent  and back propagation  graves   schmidhuber       .     d  minimal gated unit  mgu  the proposed method   figure  . data flow and operations in various gated rnn models. the direction of data flow are indicated by arrows  and operations on data are shown in rectangles. five types of element  wise operations  logistic sigmoid  tanh  plus  product and one  minus  are involved. for operations with parameters  logistic sigmoid and tanh   we also include their parameters in the rectangle.  these figures are different from diagrams that illustrate gates as  switches  but match better to the equations in table  .    there are a lot of variants of lstm. the original lstm   hochreiter   schmidhuber        does not include the  forget gate  which was later introduced in  gers et al.        . gers et al.        makes lstm even more complicated by allowing the three gates  f t   it   ot   to take ct   or  ct as an additional input  called the peephole connections.  we choose the specific form of lstm in table   because  of two reasons. first  as will soon be discussed  the forget  gate is essential. second  the peephole connection does not  seem necessary  but it complicates the learning process.  however  recently the trend is reversed  researchers found  that simplifying the lstm architecture may improve its     minimal gated unit for recurrent neural networks    performance.  lstm with coupled gates. greff et al.        evaluated a  variant of lstm  which couples the forget and input gates  into one   it       f t    t         as illustrated in figure  b. the coupling removed one  gate and its parameters  wi and bi    which leads to reduced computational complexity and slightly higher accuracy. greff et al.        also observed that removing the  peephole connection has similar effects.  gru. the gated recurrent unit  gru  architecture further simplifies lstm like units  cho et al.       . gru  contains two gates  an update gate z  whose role is similar  to the forget gate  and a reset gate r  whose role loosely  matches the input gate . gru s update rules are shown  as equation   a  to   d   and the data flow and operations  are illustrated in figure  c. beyond removing the output  gate from lstm  gru also removed the hidden  slowlychanging  cell state c. note that gru has appeared in different forms. when cho et al.        originally proposed  gru  its form is different from equation  d  as  ht   z t    ht          z t      h t .           however  these two forms are mathematically equivalent.  we adopt equation  d because it is more popular in the  literature.  evaluations in  chung et al.        found that when lstm  and gru have the same amount of parameters  gru  slightly outperforms lstm. similar observations were also  corroborated in  jozefowicz et al.       .  scrn. instead of using gates to control information flow  in rnn  the structurally constrained recurrent network   scrn  added a hidden context vector st to simple rnn   which changes slowly over time if the parameter   is  large  mikolov et al.         as  st    st            bxt    ht     p st   axt   rht                       in which   was fixed to  .    and the hidden state ht hinges  on three factors  st   xt and ht   . scrn has still fewer  parameters than gru  and has shown similar performance  as lstm in  mikolov et al.       .  irnn. le et al.        showed that minor changes to the  simple rnn architecture can significantly improve its accuracy. the key is to initialize the simple rnn s weight  matrix to an identity matrix  and use relu  rectified linear unit  as the nonlinear activation function. this method   named as irnn  has achieved accuracy that is much  closer to lstm than that of simple rnn. especially in the  mnist dataset  lecun et al.         irnn significantly    outperforms lstm. similarly  jozefowicz et al.         also showed that proper initialization is also important for  lstm. they showed that the bias of the forget gate should  be set to a large value  e.g.    or   . the same initialization  trick was used in  le et al.        too.  lstm variants. greff et al.        proposed  in evaluation of the importance of lstm components    variants  of the lstm architecture. the evaluation results  however  showed that none of them can outperform the vanilla  lstm model. their vanilla lstm architecture has the  peephole connections. hence  it is slightly different from  the lstm architecture used in this paper  cf. table   .  gru variants. jozefowicz et al.        proposed three  variants of gru. in these variants  they add the tanh nonlinearity in generating the gates  removing dependency to  the hidden state in generating the gates  and make these  changes while generating h t   etc. these variants sometimes achieve higher accuracy than gru  but none of them  can consistently outperform gru.  overall  with all these recent results and proposals we feel  that among them gru has some advantage in learning recurrent neural networks. although it is not always the  model with the highest accuracy or fewest parameters  it  has stable performance  and it is usually one of the most  accurate models  and relatively small amount of parameters. we will use gru as the baseline method and compare  it with the proposed mgu  minimal gated unit  cf. next  section  in our experiments.     . minimal gated unit  as introduced in section    we prefer an rnn architecture that has the smallest number of gates without losing  lstm s accuracy benefits. however  the choice of which  gate to keep is no easy job. fortunately  several recent evaluations have helped us make this decision. now we briefly  summarize knowledge from these evaluations.  jozefowicz et al.         the forget gate is critical  and its  biases bf must be initialized to large values   the input  gate is important  but the output gate is unimportant   gru and lstm have similar performance.  greff et al.         the forget and output gates are critical  and many variants of lstm  mainly simplified  lstm variants  act similarly to lstm.  chung et al.         gated units work better than simple  units without any gate  gru and lstm has comparable accuracy with the same number of parameters.  one notable thing is that different evaluations may lead to  inconsistent conclusions  e.g.  on the importance of the output gate jozefowicz et al.        and greff et al.            minimal gated unit for recurrent neural networks    disagreed. this is inevitable because data with different  properties have been used in different evaluations. however  at least we find the following consensus among these  evaluations     having a gated unit is key to high performance of  rnn architectures     the forget gate is unanimously considered the most  important one  and     a simplified model may lower complexity and maintain comparable accuracy.  hence  we propose the minimal gated unit  mgu   which  has the smallest possible number of gates in any gated unit.  mgu only has   gate  which is the forget gate. mgu is  based on gru  and it further couples the input  reset  gate  to the forget  update  gate  by specifying that  r t   f t    t .            note that we use f  instead of z  to denote the only gate   because it is treated as the forget gate  which can be considered as a renaming of the update gate z in gru . the  equations that define mgu are listed in table   as equations   a  to   c   and the data flow and operations are illustrated in figure  d.  in mgu  the forget gate f t is first generated  and the  element wise product between     f t and ht   becomes  part of the new hidden state ht . the portion of ht   that  is  forgotten   f t ht     is combined with xt to produce  h t   the short term response. a portion of h t  determined  again by f t   form the second part of ht .  comparing the equation sets in table   and the parameterized operations in figure    it is obvious that mgu is more  simplified than lstm or gru. while lstm has four sets  of parameters that determine f   i  o and c   and gru has  three sets of parameters for z  r and h   mgu only has two  sets of parameters  one for calculating f   the other for h . in  other words  mgu has only roughly half the parameter size  of that of lstm  and     of that of gru  because wf  or  wz    wi  or wr    wo   wh have the same size. mgu also  has slightly more parameters than scrn  as will be shown  in the example in section  . . since the parameter size of  irnn is the same as that of simple rnn  it must have the  smallest number of parameters.  in short  mgu is a minimal design in any gated hidden unit  for rnn. as we will show in the next section by experiments on a variety of sequence data  mgu also learns rnn  for sequences without suffering from the gradient vanishing or gradient exploding problem  thanks to the forget gate  in mgu . because mgu only has few factors to tune  it is  easier to find the best practices for mgu than for other  gated units.     . experimental results  in this section  we will evaluate the effectiveness of mgu  using four datasets. the simple adding problem is used  as a sanity check in section  . . the imdb dataset and  the mnist dataset are sentiment and image classification  problems with sequence inputs  presented in section  .   and  .   respectively. finally  we evaluate mgu on the  penn treebank  ptb  language modeling dataset in section  . .  as was shown in the evaluations  chung et al.        jozefowicz et al.         gru has comparable accuracy with  lstm  and has fewer parameters than lstm. we will use  gru as a baseline architecture  and compare the proposed  mgu with gru. if not otherwise specified  we compare  these two algorithms with the same number of hidden units.  all rnns are implemented with the lasagne package in  the theano library.   the dropout technique is not used in either gru or mgu.  because the focus of this paper is not absolutely high accuracy  we did not evaluate model averaging  ensemble   either.  the metrics that are compared include accuracy  or error   or perplexity  computed from the test sets  the average running time per epoch  and the number of parameters in the  hidden units. we only count the parameters of the hidden  unit  i.e.  the preprocessing and fully connected regression  or classification layer s parameters are not counted.   . . the adding problem  the adding problem was originally proposed in  hochreiter   schmidhuber         and we use the variant in  le  et al.       . the input has two components  one random number in the range        and the other is a mask  in            . in the sequence  whose length ranges from     to      only   numbers are with mask     and the output  should be the sum of these two. both mgu and gru use      hidden units  batch size is      and the learning rate  is      . for this problem  we use a bidirectional network  structure  graves   schmidhuber         which scans the  sequence both from left to right and from right to left  the  overall hidden state is the concatenation of the hidden state  in both scans. on top of the last time step s hidden state   we add a fully connected layer to transform the last hidden  state vector to a regression prediction.  we generated a training set of        examples and a test  set of       examples. in figure    we show the mean  squared error of this simple regression task on the test set.  mgu is slightly worse than gru in the first     epochs.       http   deeplearning.net software theano    http   lasagne.readthedocs.org.     minimal gated unit for recurrent neural networks   .       .    gru  mgu     .      gru  mgu     .     .    .    accuracy        mean squared error     .    .     .       .     .     .       .     .    .          .                                              .           epochs                        epochs        figure  . test set mean squared error comparison  mgu vs.  gru  on the adding problem. lower is better.    figure  . test set classification accuracy comparison  mgu vs.  gru  on the imdb dataset. higher is better.    however  after     epochs these two algorithms have almost indistinguishable results. after       epochs  the  mean squared regression error of both methods are below   .      .     for gru and  .     for mgu.    though by a small margin. after convergence  the accuracy of gru is   .  . mgu achieves an accuracy of    .    obtaining a  .   relative improvement. the input  of imdb is longer than that of the adding problem. in this  larger and longer dataset  mgu verifies again it has comparable  or slightly better in this case  accuracy with gru.    this simple experiment shows that mgu can smoothly  deal with sequence input with a moderate length around    . and  mgu has fewer parameters than gru           mgu  vs.        gru .  mgu also trains faster  which takes on average  .   seconds per epoch  while gru requires  .   seconds.   . . imdb  the second problem we study is sentiment classification  in the imdb movie reviews  whose task is to separate the  reviews into positive and negative ones. this dataset was  generated in  maas et al.       .  there are        movie  reviews in the training set  another        for testing. we  use the provided bag of words format as our sequence input. the maximum sequence length is    . both mgu  and gru have     hidden units  batch size is     and the  learning rate is      with a  .   momentum. similar to  the adding problem  we use a fully connected layer on top  of the last hidden state to classify a movie review as either  positive or negative.  we show the accuracy of this binary classification problem  in figure    evaluated on the test set. in the x axis of figure    we show the epoch number divided by    . because  both curves show that they converge after        epochs   it is too dense to show the results of the model after every  training epoch.  mgu consistently outperforms gru in this example  al     available at http   ai.stanford.edu  amaas data sentiment .    again  mgu has two thirds of the number of parameters in  gru. mgu has        parameters  while gru has         parameters. however  mgu trains much faster than gru  in this problem. the average running time per epoch for  mgu is  .  seconds  only     of that of gru  which takes    .  seconds per epoch on average .   . . mnist  the mnist dataset by lecun et al.        contains images of handwritten digits          . all the images are of  size        .  there are        images in the training set   and        in the test set. the images are preprocessed  such that the center of mass of these digits are at the central  position of the         image.  mnist has been a very popular testbed for deep neural  network classification algorithms  but is not widely used in  evaluating rnn models yet. we use this dataset in two  ways. the first is to treat each row     pixels  as a single  input in the input sequence. hence  an image is a sequence  with length     corresponding to the    image rows  from  top to bottom . for this task  we use     hidden units and  the batch size is    . the learning rate is      with a  .    momentum. a fully connected layer transfer the last row s  hidden state into an output vector with    elements. accuracy of mgu and gru on the test set in this task are shown  in figure    where the x axis is the epoch number divided       available at http   yann.lecun.com exdb mnist      minimal gated unit for recurrent neural networks      gru  mgu                            perplexity        accuracy        gru  mgu                                                                                             epochs        figure  . test set classification accuracy comparison  mgu vs.  gru  on the mnist dataset. this is the first task  where the  sequence length is   . higher is better.    by    .  the performance on mnist is similar to that on the adding  problem  cf. figure     but the role of mgu and gru has  reversed. in the first       epochs  x axis value up to       gru is slightly better than our mgu. however  from then  on till both algorithms  convergence  mgu s accuracy is  higher than that of gru. in the end         epochs   gru  achieves a   .    accuracy  while mgu is slightly higher  at   .   . for this task  mgu has        parameters   while gru has        parameters. the average per epoch  training time of mgu is     seconds  faster than that of  gru      seconds .  the second task on mnist treats every pixel as one component in the input sequence. an image then becomes a  sequence of length      with pixels scanned from left to  right  and top to bottom. this task tests how mgu works  when the sequence length is long. in this task  gru has         parameters while mgu has       . other settings   learning rate etc.  are the same as those in the first task.  after        epochs  mgu s test set accuracy is   .      while with the same input length       and epoch number   irnn s accuracy was below      see le et al.        figure   . after        epochs  irnn s accuracy was roughly       which is still worse than mgu s   .    at         epochs. note that when the number of epochs continued  till          irnn reached a high accuracy of above    .  although we did not run mgu till this number of epochs   we have reasons to expect that mgu will similarly achieve  a high accuracy if much longer training time is given. the  accuracy of lstm on this task is only around     even  after         epochs  see le et al.        figure   .  the average training time per epoch of mgu is   .  sec                       epochs                     figure  . test set perplexity comparison  mgu vs. gru with      hidden units  on the ptb dataset. lower is better.    onds. however  gru is much slower in this long sequence  task  which takes    .  seconds per epoch.  if the same  training time budget is allocated  mgu has significantly  higher accuracy than gru.   . . penn treebank  the penn treebank  ptb  dataset provides data for language modeling  which is released by marcus et al.       .  for this dataset  we work on the word level prediction  task  which is the same as the version in  zaremba et al.        .  there are        words in the vocabulary. it has     k words in the training set    k in the validation set   and   k in the test set. as in  zaremba et al.          we use two layers and the sequence length is   . the  batch size is     and the learning rate is  .  . because  dropout is not used  we tested mgu and gru on small  networks  whose number of hidden nodes range in the set                                    . without dropout  both  units overfit when the number of hidden units exceed    .  a fully connected layer predicts one of the        words.  as a direct comparison  we show the perplexity of mgu  and gru in figure   when there are     hidden units. the  x axis is the epoch number divided by      .  gru has a small advantage over mgu in this task. in  the right end of figure    gru s perplexity on the test set  is    .    while mgu s is higher at    .  . we observe  the same behavior on the validation set. we can also compare these two algorithms when they have roughly the same     we did not have enough time to wait for gru to run to a high  epoch number on this task. the accuracy of gru is higher than  that of mgu in the early training stage. however  mgu takes  over after     epochs. this trend is similar to that of figure  .     available at https   github.com wojzaremba lstm   or from  http   www.fit.vutbr.cz imikolov rnnlm simple examples.tgz.     minimal gated unit for recurrent neural networks    number of parameters  as was done in  chung et al.       .  the comparison results are shown in table  .  mgu has one third less parameters than gru. thus  the  number of parameters are roughly the same for mgu with      hidden units and gru with     hidden units. when  we compare these two algorithms using these settings  the  gap between gru and mgu becomes smaller     .   vs.     .   on the test set  and    .   vs.    .   on the validation set .  if we compare these two algorithms with the same amount  of training time  mgu is faster than gru. mgu with      units is roughly as fast as gru with     units  and  mgu  with     units is similar to gru with     units. when  the numbers of hidden units are same  e.g.        the proposed mgu can run more epochs than gru given the same  amount of training time  which we expect will continue to  decrease the perplexity of mgu.  we can also compare mgu with the results in  jozefowicz et al.       . when there are     hidden units  the  total number of parameters  including all layers  of mgu  is  . m  which can be fairly compared with the   m tst   result in  jozefowicz et al.        table     so is gru  with     units. our gru implementation  with     hidden units  has a test set perplexity of    .    lower than  the gru  with  m parameters  result in  jozefowicz et al.          which is    .      exp  .     . the proposed  mgu  with     hidden units  achieves a test set perplexity  of    .    also lower than the gru result in  jozefowicz  et al.       .  the scrn method has still fewer parameters than the  proposed mgu. when there are     hidden units  mgu  has        parameters. a similar scrn architecture has      hidden units and    context units  see mikolov et al.         table     which has        parameters  amounting  to roughly     of that of mgu. on this dataset  however   scrn seems to saturate at test set perplexity      because  scrn with     and     hidden units arrived at this same  perplexity. mgu gets lower perplexity than scrn on this  dataset.   . . discussions  we have evaluated the proposed mgu on four different sequence data. the comparison is mainly against gru  while  results of irnn and scrn are also cited when appropriate. the input sequence length ranges short               moderate        and long      . the sequence data range  from artificial to real world  and the task domains are also  diverse.  the proposed method is on par with gru in terms of accuracy  or error  or perplexity . given its minimal design  of one gate  mgu has only two thirds of the parameters    of gru  and hence trains faster in all datasets. however   in some problems  e.g.  penn treebank   gru converges  faster than mgu. overall  through these experimental results we believe mgu has proven itself as an attractive alternative in building rnn.     . conclusions and future work  in this paper  we proposed a new hidden unit for recurrent neural networks. the proposed minimal gated unit   mgu  has the minimal design in any gated hidden unit  for rnn. it has only one gate  the forget gate  and does  not involve the peephole connection. hence  the number  of parameters in mgu is only half of that in the long  short term memory  lstm   or two thirds of that in the  gated recurrent unit  gru . we compared mgu with  gru on several tasks that deal with sequence data in various domains. mgu has achieved comparable accuracy  with gru  and  thanks to the minimal design  trains faster  than gru.  based on our evaluations  mgu could be readily used as  the hidden unit in an rnn  which may reduce memory  footprint and training time in some applications. more importantly  the minimal design will facilitate our theoretical  analysis or empirical observation  e.g.  through visualization  of rnn models  and enhance our understanding and  facilitate progresses in this domain. a minimal design also  means that there are fewer possibilities of producing variants  which will complicate the analysis .  ample ways are possible to further this line of research.  beyond analysis and understanding  we will also run mgu  with more epochs  in more diverse and complex tasks  and  regularize mgu to improve its accuracy.    