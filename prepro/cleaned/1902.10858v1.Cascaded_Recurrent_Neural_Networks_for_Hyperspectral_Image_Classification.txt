introduction  with the development of imaging technology  current hyperspectral sensors can fully portray the surface of the earth  using hundreds of continuous and narrow spectral bands   ranging from the visible spectrum to the short wave infrared  spectrum. the generated hyperspectral image  hsi  is often  considered as a three dimensional cube. the first two are  spatial dimensions  which record the locations of each object.  the third one is spectral dimension  which captures the  spectral signature  reflective or emissive properties  of each  material in different bands along the electromagnetic spectrum     . using such rich information  hsis have been widely  this work was supported in part by the natural science foundation  of china under grants          and           in part by the natural  science foundation of jiangsu province  china  under grant bk        .   corresponding author  qingshan liu.   r. hang and q. liu are with the jiangsu key laboratory of big data analysis technology  the school of automation  nanjing university of information  science and technology  nanjing         china  renlong hang    .com   qsliu nuist.edu.cn .  d. hong is with the remote sensing technology institute  imf   german  aerospace center  dlr         wessling  germany  and signal processing  in earth observation  sipeo   technical university of munich  tum          munich  germany e mail  danfeng.hong dlr.de .  p. ghamisi is with the helmholtz zentrum dresden rossendorf  hzdr    helmholtz institute freiberg for resource technology  hif   exploration  d      freiberg  germany  e mail  p.ghamisi gmail.com .    applied to various applications  such as land cover land use  classification  precision agriculture  and change detection. for  these applications  one basic but important procedure is hsi  classification  whose goal is to assign candidate class labels to  each pixel.  in order to acquire accurate classification results  numerous  methods have been proposed. for example  one can directly  consider the rich spectral signature as features and feed them  into advanced classifiers  such as support vector machine   svm       random forest     and extreme learning machine     . however  due to the dense spectral sampling of hsis   there may exist some redundant information among adjacent  spectral bands. this easily leads to the so called curse of  dimensionality  the hughes effect  which causes a sudden  drop in classification accuracy when there is no balance  between the high number of spectral channels and a limited  number of training samples. therefore  a large number of  works were proposed to mine discriminative features from  the high dimensional spectral signature    . popular models  include principle component analysis  pca   linear discriminant analysis  lda           and graph embedding         .  besides  representation based models have also been employed  to hsi classification in recent years. in      and       sparse  representation was proposed to learn discriminative features  from hsis. similarly  collaborative representation was also  widely explored           . in these models  an input spectral  signature is usually represented by a linear combination of  atoms from a dictionary  and the classification result can be  derived from the reconstructed residual without needing to  train extra classifiers  which often costs much time.  although the aforementioned models have demonstrated  their effectiveness in the field of hsi classification  there  still exist some drawbacks to address. for the traditional  feature extraction models  we need to pre define a mining  criterion  e.g.  maximizing the between class scatter matrix in  lda   which heavily depends on the domain knowledge and  experience of experts. for the representation based models   their goal is to reconstruct the input signal  leading to suboptimal representation for classification. additionally  all of  them can be considered as shallow layer models  which limit  their potentials to learn high level semantic features. recently   deep learning             a very hot research topic in machine  learning  has shown its huge superiority in most fields of  computer vision           and natural language processing            . the goal of deep learning is to learn nonlinear  highlevel semantic features from data in a hierarchical manner.  due to the effects of multi path scattering and the het           erogeneity of sub pixel constituents  hsi often lies in a  nonlinear and complex feature space. deep learning can be  naturally adopted to deal with this issue           . in the  past few years  many deep learning models were successfully  applied to hsi classification. for example  in            the  autoencoder model has been used to learn deep features  from high dimensional spectral signature directly. similar to  autoencoder  deep belief network was also explored to extract  spectral features          . however  both of them belong  to fully connected networks  which contain large numbers  of parameters to train. different from them  convolutional  neural networks  cnns  have local connection and weight  sharing properties  thus largely reducing the number of training  parameters          . in       hu et al. proposed to use onedimensional cnn to learn and represent the spectral information. this model is comprised of an input layer  a convolutional  layer  a pooling layer  a fully connected layer and an output  layer. the whole model is trained in an end to end manner   thus achieving satisfying results for hsi classification.  besides spectral information  hsis also have rich spatial  information. how to combine them together has been an active  research topic in the field of hsi classification           .  one potential method is to extend the spectral classification  model into its spectral spatial counterpart. for instance  in             a three dimensional cnn was employed to spectralspatial classification of hsis. however  due to the simultaneous convolution operators in both spectral domain and  spatial domain  the computational complexity is dramatically  increased. in addition  the number of trainable parameters in  three dimensional cnns is also a problem. in order to perform  three dimensional convolution  the dimensionality of the input  and the dimensionality of the kernel  filter  should be equal.  this heavily increases the number of parameters. another  candidate method for spectral spatial classification is the one  based on two branch networks. one branch is for spectral  classification and the other one for spatial classification. in             one dimensional cnn or autoencoder was used  to learn spectral features and two dimensional cnn was  designed to learn spatial features. these two features are  then integrated together via feature level fusion or decisionlevel fusion. for two dimensional cnns  only a few principal  components were extracted and used as inputs  thus reducing  the computational consuming compared to three dimensional  cnns.  most of existing models can be considered as vectorbased methodologies. recently  a few works attempted to  regard hsis as sequential data  so recurrent neural networks   rnns  were naturally used to learn features. in       wu et.al  proposed using rnn to extract spectral features from hsis. in       and       a variant of rnn using long short term memory   lstm  units was designed to learn spectral spatial features  from hsis. in       another variant of rnn using gated  recurrent units  grus  was employed. compared to the widely  explored cnn models  rnns have many superiorities. for  example  the key component of cnns is the convolutional operator. due to the kernel size limitations of it  one dimensional  cnns can only learn the local spectral dependency while  easily ignoring the effects of non adjacent spectral bands.    different from them  rnns  especially using gru or lstm   often input spectral bands one by one via recurrent operators   thus capturing the relationship from the whole spectral bands.  besides  rnns often have smaller numbers of parameters to  train than cnns  so they will be more efficient in the training  and inferring phases.  benefiting from its powerful learning ability from sequential  data  current rnn related models often simply input the whole  spectral bands to networks  which may not fully explore  the redundant and complementary properties of hsis. the  redundant information between adjacent spectral bands will  increase the computational burden of rnns without improving  the classification results. sometimes such redundancy may  reduce the classification accuracy since it increases withinclass variances and decreases between class variances in the  feature space. besides  it may also increase the difficulties in  learning complementary information. to address these issues   we propose a cascaded rnn model using gated recurrent  units  grus  in this paper. this model mainly consists of  two rnn layers. the first rnn layer focuses on reducing  the redundant information of adjacent spectral bands. these  reduced information are then fed into the second rnn layer  to learn their complementary features. besides  in order to  improve the discriminative ability of the learned features   we design two strategies for the proposed model. finally   we also extend the proposed model to its spectral spatial  version by incorporating some convolutional layers. the major  contributions of this paper are summarized as follows.     we propose a cascaded rnn model with grus for hsi  classification. compared to the existing rnn related  models  our model can sufficiently consider the redundant and complementary information of hsis via two  rnn layers. the first one is to reduce redundancy and  the second one is to learn complementarity. these two  layers are integrated together to generate an end to end  trainable model.     in order to learn more discriminative features  we design  two strategies to construct connections between the first  rnn layer and the output layer. the first strategy is  the weighted fusion of features from two layers  and  the second one is the weighted combination of different  loss functions from two layers. their weights can be  adaptively learned from data itself.     to capture the spectral and spatial features simultaneously  we further extend the proposed model to its  spectral spatial counterpart. a few convolutional layers  are integrated into the proposed model to learn spatial  features from each band  and these features are then  combined together via recurrent operators.  the rest of this paper is organized as follows. section ii  describes the details of the proposed models  including a brief  introduction of rnn  and the structure of the proposed model  as well as its modifications. the descriptions of data sets and  experimental results are given in section iii. finally  section  iv concludes this paper.          first layer rnn    cascaded  rnn model    rnn  rnn    second layer rnn    rnn  rnn  rnn  fig.    flowchart of the proposed model.  ii. m ethodology  as shown in fig.    the proposed cascaded rnn model  mainly consists of four steps. for a given pixel  we firstly  divide it into different spectral groups. then  for each group   we consider the spectral bands in it as a sequence  which is  fed into a rnn layer to learn features. after that  the learned  features from each group are again regraded as a sequence  and fed into another rnn layer to learn their complementary  information. finally  the output of the second rnn layer is  connected to a softmax layer to derive the classification result.  a. review of rnn    function  c is the number of classes to discriminate. all of  these weight parameters in equation     and     can be trained  using the following loss function  l      n    x   yi log y i          yi  log     y i     n i             where n is the number of training samples  yi and y i are  the true label and the predicted label of the i th training  sample  respectively. this function can be optimized using a  backpropagation through time  bptt  algorithm.  b. cascaded rnns    rnn has been widely used for sequential data analysis   such as speech recognition and machine translation           .  assume that we have a sequence data x    x    x            xt     where xt   t                  t   generally represents the information at the t th time step. when applying rnn to hsi  classification  xt will correspond to the spectral value at the  t th band. for rnn  the output of hidden layer at time t is  ht     whi xt   whh ht     bh             where   is a nonlinear activation function such as logistic  sigmoid or hyperbolic tangent functions  bh is a bias vector   ht   is the output of hidden layer at the previous time  whi  and whh denote weight matrices from the current input layer  to hidden layer and the previous hidden layer to current hidden  layer  respectively. from this equation  we can observe that via  a recurrent connection  the contextual relationships in the time  domain can be constructed. ideally  ht can capture most of  the time information for the sequence data.  for classification tasks  ht is often fed into an output layer   and the probability that the sequence belongs to i th class can  be derived by using a softmax function. these processes can  be formulated as  ot   woh ht   bo  e i ot  bi  p  y    i    b    pc   j ot  bj  j   e           where bo is a bias vector  woh is the weight matrix from  hidden layer to output layer    and b are parameters of softmax    hsis can be described as a three dimensional matrix x    rm n k   where m  n and k represent the width  height and  number of spectral bands  respectively. for a given pixel x    rk   we can consider it as a sequence whose length is k  so  rnn can be naturally employed to learn spectral features.  however  hsis often contain hundreds of bands  making x  a very long sequence. such long term sequence increases the  training difficulty since the gradients tend to either vanish or  explode     . to address this issue  one popularly used method  is to design a more sophisticated activation function by using  gating units such as the lstm unit and gru     . compared  to lstm unit  gru has a fewer number of parameters        which may be more suitable for hsi classification because it  usually has a limited number of training samples. therefore   we select gru as the basic unit of rnn in this paper.  the core components of gru are two gating units that  control the flow of information inside the unit. instead of using  equation      the activation of the hidden layer for band t is  now formulated as  ht        ut  ht     ut h t           where ut is the update gate  which can be derived by  ut     wu xt   vu ht               where   is a sigmoid function  wu is a weight value  and vu  is a weight vector. similarly  h t can be computed by  h t   tanh wxt   v rt    ht                      rnn    rnn    rnn    rnn  rnn    rnn  rnn    rnn    rnn  c    l    concatenation    fig.    the first improvement strategy.    l    l    l  l    loss function    sum operator    fig.    the second improvement strategy.    where denotes an element wise multiplication  and rt is the  reset gate  which can be derived by  rt     wr xt   vr ht        l    c    rnn    output layer    l           due to the dense spectral sampling of hyperspectral sensors   adjacent bands in hsis have some redundancy while nonadjacent bands have some complementarity. in order to take  account of such information comprehensively  we propose a  cascaded rnn model. specifically  we divide the spectral  sequence x into l sub sequences z    z    z            zl    each  of which consists of adjacent spectral bands. besides the  last sub sequence zl   the length of the other sub sequences  is d   floor k l   which denotes the nearest integers less  than or equal to k l. thus  for the i th sub sequence zi   i                   l   it is comprised of the following bands      x i    d             xi d    if i    l   zi          x i    d             xk     otherwise.  then  we feed all the sub sequences into the first layer  rnns respectively. these rnns have the same structure and  share parameters  thus reducing the number of parameters to  train. in the sub sequence zi   each band has an output from  gru. we use the output of the last band as the final feature       representation for zi   which can be denoted as fi   rh     where h  is the size of the hidden layer in the first layer rnn.       after that  we can combine fi   i                  l  together to                 generate another sequence f    f    f            fl   whose  length is l. this sequence is fed into the second layer rnn  to learn their complementary information. similar to the firstlayer rnns  we also use the output of gru at the last time l  as the learned feature f    . to get a classification result of x   we need to input f    into an output layer whose size equals  to the number of candidate classes c. both of these two layer  rnns have many weight parameters. we choose equation      as a loss function and use the bptt algorithm to optimize  them simultaneously.    better than the first layer rnns. however  the performance  of the first layer rnns will have effects on the second layer  rnn. in order to improve the discriminative ability of f      an  intuitive method is to construct relations between the first layer  rnns and the output layer. here  we propose two strategies  to achieve this goal. the first strategy is based on the featurelevel connection shown in fig.  . instead of feeding the output  of the second layer rnn into the output layer only  we attempt  to feed all the output features from the first  and the secondlayer rnns in a weighted concatenation manner. specifically   the input of the output layer is computed as follows         as described in subsection ii b  the second layer rnn is  directly connected to the output layer  so it may be optimized                                              where wi   r    i                  l  are fusion weights for  the first layer rnns  and w      r  is the fusion weight for  the second layer rnn. these weights can be integrated into  the whole network and their optimal values are automatically  learned from data. the same as the original two layer rnn  model  we also use equation     to construct the loss function  and use the bptt algorithm to optimize it.  different from the first improvement strategy  our second  strategy is based on the output level connection. as shown in  fig.    we feed the features extracted by the first layer rnns  into output layers  respectively  so that they can learn more discriminative features. combining these features together using  the second layer rnn will result in a better f    . in particular        for fi   i                  l   we can input it into an output       layer and construct a loss function li   i                  l .       meanwhile  we also input f  into an output layer and  construct another loss function l    . after that  a weighted  summation method can be used to combine them together   which can be formulated as  l      x          w l   w    l     l     l i   i i         c. improvement for cascaded rnns           f     w  f    w  f            wl fl   w    f                      where wi   r  and w      r  are fusion weights  li and  l    are derived from equation    . the final loss function  l  can be optimized by using the bptt algorithm. in the  prediction phase  we can delete the output layers of the first           spectral spatial  cascaded rnns    convolutional layers  first layer rnn    rnn  second layer rnn    rnn  rnn    fig.    flowchart of spectral spatial cascaded rnn model.  layer rnns and use the output from the second layer rnn  as the final classification result.  d. spectral spatial cascaded rnns  due to the effects of atmosphere  instrument noises  and  natural spectrum variations  materials from the same class  may have very different spectral responses  while those from  different classes may have similar spectral responses. if we  only use the spectral information  the resulting classification  maps will have many outliers  which is known as the  salt  and pepper  phenomenon. as a three dimensional cube  hsis  also have rich spatial information  which can be used as a  complement to address this issue. among numerous deep  learning models  cnns have demonstrated their superiority in  spatial feature extraction. in       a typical two dimensional  cnn is designed to extract spatial features from hsis. the  input of this model is the first principle component of hsis.  inspired from the two dimensional cnn model  we extend  the cascaded rnn model to its spectral spatial version by  adding some convolutional layers. fig.   shows the flowchart  of the proposed spectral spatial cascaded rnn model. for a  given pixel x   rk   we select a small cube x    r    k  centered at it. then  we split this cube into k matrices  x i   r      i                  k  across the spectral domain.  for each x i   we feed it into several convolutional layers to  learn spatial features. the same as       we also use three  convolutional layers  and the first two layers are followed by  pooling layers. the input size       is        . the sizes  of the three convolutional filters are                         and              respectively. after these convolutional  operators  each x i will generate a     dimensional spatial  feature si . similar to the cascaded rnn model  we can also  consider s    s    s            sk   as a sequence whose length is  k. this sequence is divided into l sub sequences  and they  are subsequently fed into the first layer rnns respectively  to reduce redundancy inside each sub sequence. the outputs  from the first layer rnns are combined again to generate  another sequence  which are fed into the second layer rnn  to learn complementary information.  compared to the cascaded rnn model  the spectral spatial  cascaded rnn model is deeper and more difficult to train.    therefore  we propose a transfer learning method to train it.  specifically  we firstly pre train the convolutional layers using  all of x i   i                  k . we replace two layer rnns by an  output layer whose size is the number of classes c. besides   we assume that the label of x i equals to the label of its  corresponding pixel x. then  we will have n   k samples.  these samples are used to train convolutional layers. after  that  the weights of these convolutional layers are fixed and  the n training samples are used again to train the two layer  rnns. finally  the whole network is fine tuned based on the  learned parameters.  iii. e xperiments  a. data description  our experiments are conducted on two hsis  which are  widely used to evaluate classification algorithms.  indian pines data  the first data set was acquired by the  aviris sensor over the indian pine test site in northwestern  indiana  usa  on june         . the original data set contains      spectral bands. we utilize     of them after removing  four bands containing zero values and    noisy bands affected  by water absorption. the spatial size of the image is            pixels  and the spatial resolution is    m. the number of  training and test pixels are reported in table i. fig.   shows  the false color image  as well as training and test maps of this  data set.  pavia university scene data  the second data set was  acquired by the rosis sensor during a flight campaign over  pavia  northern italy  on july        . the original image was     a      b      c     fig.    visualization of the indian pines data.  a  false color  image.  b  training data map.  c  test data map.          table i  numbers of training and test pixels used in the  indian pines data set.  class no.                                                              class name  corn notill  corn mintill  corn  grass pasture  grass trees  hay windrowed  soybean notill  soybean mintill  soybean clean  wheat  woods  building grass trees  stone steel towers  alfalfa  grass pasture mowed  oats  total    training                                                                         test                                                                                        table ii  numbers of training and test pixels used in the  pavia university data set.  class no.                                  class name  asphalt  meadows  gravel  trees  metal sheets  bare soil  bitumen  bricks  shadows  total    training                                                       test                                                                 recorded with     spectral channels ranging from  .    m to   .    m. after removing noisy bands      bands are used.  the image size is           pixels with a spatial resolution of   .  m. there are nine classes of land covers with more than       labeled pixels for each class. the number of pixels for  training and test are listed in table ii. their corresponding  distribution maps are demonstrated in fig.  .  b. experimental setup  in order to highlight the effectiveness of our proposed  models  we compare them with svm  one dimensional cnn    d cnn   two dimensional cnn   d cnn   and the original rnn using gru  rnn . for simplicity  the cascaded  rnn model using grus is abbreviated as casrnn  the  two improvement methods of casrnn based on feature level  and output level connections are abbreviated as casrnn f  and casrnn o  respectively  the spectral spatial casrnn is  abbreviated as sscasrnn. some of their explanations are  summarized as follows.     svm  the input of svm is the original spectrum  signature. we choose gaussian kernel as its kernel  function. the penalty parameter and the spread of  the gaussian kernel are selected from a candidate set     a      b      c     fig.    visualization of the pavia university data.  a  falsecolor image.  b  training data map.  c  test data map.                               using a fivefold cross validation  method.      d cnn  the structure of  d cnn is the same as that  in     . it contains an input layer  a convolutional layer  with    kernels whose size is         a max pooling  layer whose kernel size is        a fully connected layer  with     hidden nodes  and an output layer.      d cnn  the structure of  d cnn is the same as that  in       which consists of three convolutional layers and  two max pooling layers. please refer to table ix in       for the design details of it.     rnn  gru is used as the basic unit of rnn. the  number of hidden nodes is chosen from a candidate set                           via a fivefold cross validation method.  the deep learning models are constructed with a pytorch  framework. to optimize them  we use a mini batch stochastic  gradient descent algorithm. the batch size  the learning rate  and the number of training epochs are set to      .    and       respectively. for svm  we use a libsvm package in a  matlab framework. all of the experiments are implemented  on a personal computer with an intel core i         .  ghz  processor    gb ram  and a gtx titan x graphic card.  the classification performance of each model is evaluated  by the overall accuracy  oa   the average accuracy  aa   the  per class accuracy  and the kappa coefficient. oa defines the  ratio between the number of correctly classified pixels to the  total number of pixels in the test set  aa refers to the average  of accuracies in all classes  and kappa is the percentage of  agreement corrected by the number of agreements that would  be expected purely by chance.  c. parameter analysis  there exist three important hyperparameters in the proposed  models. they are sub sequence numbers l  as well as the  size of hidden layers in the first layer rnn and the secondlayer rnn. to test the effects of them on the classification  performance  we firstly fix l and select the size of hidden  layers from a candidate set                            . then   we fix the size of hidden layers and choose l from another  set                              . since the same hyperparameter                                  oa        oa                                                                                                                                                                fig.    performance of the casrnn model with different sizes  of hidden layers on the indian pines data  left  and the pavia  university data  right .                          oa        oa                                                                                                                                                                    fig.    performance of the sscasrnn model with different  sizes of hidden layers on the indian pines data  left  and the  pavia university data  right .            oa              casrnn  casrnn f  casrnn o  sscasrnn              d. performance comparison          in this section  we will report quantitative and qualitative  results of our proposed models and their comparisons with  the other state of the art models. table iii reports the detailed  classification results of different models on the indian pines  data  including oa  aa  kappa and class specific accuracy.  the bold fonts in each row denote the best results. several conclusions can be observed from this table. first  if we directly  input the whole spectral bands into rnn  its oa  aa and  kappa values are   .       .    and   .     respectively   which are all lower than those achieved by svm and  dcnn models. this indicates that rnn cannot fully explore  the long term spectral sequence of hsis. on the contrary   considering the redundant and complementary properties of  spectral signature  our proposed model casrnn can improve  the performance of rnn by   percents  thus outperforming  svm and  d cnn. second  compared to casrnn  casrnnf and casrnn o can obtain better results  which validates the  effectiveness of the two improvement strategies. in terms of  each class accuracy  casrnn f almost increases all of them  in comparison with casrnn  so it might be more powerful  than casrnn o on the indian pines data. third  compared to  spectral classification models   d cnn significantly improves  the classification results by about    percents. it means that                       number of subsequences          fig.    performance of different models on the indian pines  data with different sub sequence numbers l.            oa        values are used for casrnn and its two improvements  i.e.   casrnn f and casrnn o   we only demonstrate the performance of casrnn here  shown in fig.  . in this threedimensional diagram  the first two axes  named hidden  and  hidden   respectively correspond to the number of hidden  nodes in the first layer rnn and the second layer rnn   while the third axis represents the classification accuracy oa.  from this figure  we can observe that when hidden        and hidden         casrnn can achieve better oa than  the other values on the indian pines data. the best oa  appears when hidden        and hidden       . for  the pavia university data  oa changes a little larger than  the indian pines data  but we can still find the best value  when hidden        and hidden      . similarly  fig.    shows oa values achieved by sscasrnn using different  hidden sizes. we can see the optimal parameter values are  hidden         hidden        for the indian pines data   and hidden         hidden        for the pavia university  data  respectively.  fig.   and fig.    evaluate the effects of l on classifying the  indian pines and the pavia university data sets  respectively.  in these figures  different colors represent different models.  they are casrnn  casrnn f  casrnn o and sscasrnn.  as l increases  oas achieved by these models tend to increase  firstly and then decrease. given the same l  sscasrnn significantly outperforms the other three models. for the indian  pines data  the maximal oas of four models appear at the  same l  so their optimal l values are set as   . different from  the indian pines data  four models have different optimal l  values on the pavia university data. as shown in fig.     the  optimal l value is   for sscasrnn  and   for the other three  models.    casrnn  casrnn f  casrnn o  sscasrnn                                   number of subsequences          fig.     performance of different models on the pavia university data with different sub sequence numbers l.          table iii  classification results     of different models on the indian pines data.  class no.                                                         oa  aa  kappa    c     svm    .      .      .      .      .      .      .      .      .      .      .      .           .      .           .      .      .       d cnn    .      .      .      .      .      .      .      .      .      .      .      .      .      .      .           .      .      .      rnn    .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      casrnn    .      .     .      .      .      .      .      .      .      .      .      .      .      .      .           .      .      .      casrnn f    .      .     .     .      .      .      .      .      .      .      .      .      .      .      .           .      .      .      casrnn o    .      .      .      .      .      .      .      .      .      .      .      .      .      .      .          .      .      .       d cnn    .      .           .      .      .      .      .      .           .      .           .                .      .      .      sscasrnn    .      .           .      .           .      .      .           .      .                          .      .      .       a      b      c      d      e      f      g      h     c     c     c     c     c     c     c     c     c   c      c   c      c      c      c      fig.     classification maps of the indian pines data using different models.  a  svm.  b   d cnn.  c  rnn.  d  casrnn.   e  casrnn f.  f  casrnn o.  g   d cnn.  h  sscasrnn.          table iv  classification results     of different models on the pavia university data.  class no.                             oa  aa  kappa    svm    .      .      .      .      .      .      .      .      .      .      .      .       d cnn    .      .      .      .      .      .      .      .      .      .      .      .      rnn    .      .      .      .      .      .      .      .      .      .      .      .      casrnn    .      .      .      .      .      .      .      .      .      .      .      .      casrnn f    .      .      .      .      .      .      .      .      .      .      .      .      casrnn o    .      .      .      .      .      .      .      .      .      .      .      .       d cnn    .      .      .      .      .      .      .      .      .      .      .      .      sscasrnn    .      .      .      .           .      .      .      .      .      .      .      c   c   c   c      a      b      c      d     c   c   c   c   c      e      f      g      h     fig.     classification maps of the pavia university data using different models.  a  svm.  b   d cnn.  c  rnn.  d  casrnn.   e  casrnn f.  f  casrnn o.  g   d cnn.  h  sscasrnn.  the consideration of spatial information is very important  on the indian pines data  because there are many large and  homogeneous objects shown in fig.   c . by incorporating the  spatial information into casrnn model  our proposed model  sscasrnn can further increase the performance to above     percents. besides  it can obtain highest accuracies in     different classes  which sufficiently certifies the effectiveness  of sscasrnn.  in addition to the quantitative results  we also visualize  classification results of different models shown in fig.   .  different colors in this figure correspond to different classes.    compared to the groundtruth map in fig.   c   spectral  classification models  i.e.  svm   d cnn  rnn  casrnn   casrnn f and casrnn o  have many outliers in the classification map due to the spectral variability of materials.  this phenomenon can be alleviated by  d cnn  because it  makes use of the spatial contextual information instead of  the spectral information. for homogeneous regions  especially  large objects   d cnn performs very well. however  it will  easily result in an over smoothing problem especially for small  objects  as demonstrated in fig.    g . different from  d cnn  and spectral models  sscasrnn takes advantage of spectral           and spatial information simultaneously. as shown in fig.      h   it has significantly fewer outliers than spectral models   and retains more boundary details of objects than  d cnn.  table iv and fig.    are the classification results of different  models on the pavia university data. similar conclusions can  be observed from them. for spectral models  casrnn is better  than rnn  while casrnn f and casrnn o are superior to  casrnn. all of these models have the  salt and pepper   phenomenon in their classification maps. compared to the best  spectral model   d cnn can improve oa and kappa by more  than   percents. in addition  it generates fewer outliers and  leads to a more homogeneous classification map. nevertheless   without using the spectral information  its performance is not  very high  and the classification map is easily to be oversmoothed. combining the spectral and spatial information  together  our proposed model sscasrnn can alleviate these  issues. it improves oa from   .    to   .     and generates  more details in the classification map. however  in comparison  with the indian pines data  the classification results achieved  by sscasrnn are still not very high. one possible reason is  that there exist many small objects in the pavia university data   which increases the difficulty in exploring spatial features.  iv. c onclusions  in this paper  we proposed a cascaded rnn model for  hsi classification. compared to the original rnn model  our  proposed model can fully explore the redundant and complementary information of the high dimensional spectral signature. based on it  we designed two improvement strategies  by constructing connections between the first layer rnn and  the output layer  thus generating more discriminative spectral  features. additionally  considering the importance of spatial  information  we further extended the proposed model into its  spectral spatial version to learn spectral and spatial features  simultaneously. to test the effectiveness of the proposed models  we compared them with several state of the art models on  two widely used hsis. the experimental results demonstrate  that the cascaded rnn model can obtain higher performance  than rnn  and its modifications can further improve the  performance. besides  we also thoroughly evaluated the effects  of different hyperparameters on the classification performance  of the proposed models  including the hidden sizes and the  number of sub sequences. in the future  more experiments will  be conducted to validate the effectiveness of our proposed  models. in addition  more powerful spectral spatial models  will be explored. since the sizes and shapes of different objects  vary  using the patches or cubes with same sizes as inputs  easily leads to the loss of spatial information.  