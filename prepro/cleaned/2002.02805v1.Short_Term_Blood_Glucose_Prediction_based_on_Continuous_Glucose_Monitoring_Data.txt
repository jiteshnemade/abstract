introduction  people with insulin requiring diabetes are challenged to  achieve and maintain metabolic control. one key challenge  is that insulin is a drug with a very narrow therapeutic index   i.e. a short range between having too high blood glucose   bg  levels  hyperglycemia  and dangerously low bg levels   hypoglycemia . in this context  identifying the optimal dose  size and timing of insulin doses are critical. for patients who  make meal time injections  adjusting the individual meal dose  to the meal at hand  potentially making a correction bolus  or eating correction carbohydrates  is part of managing their  diabetes.  digital tools to support the management of diabetes are  becoming available      and often contain some predictive  capabilities. for instance  short term bg  stbg  predictions  can benefit patients on insulin treatment who are at risk  of hypoglycemia. due to the narrow therapeutic index of  insulin and the risk of prompting an adverse insulin treatment  behaviour  the predictive capabilities of current and future  digital tools must be as precise as possible given the available  data. obviously  predictions of future bg levels depend on  the amount and quality of the past bg level data. furthermore  the need for insulin is dynamically impacted by several  factors such as meal size composition and physical activity    department of applied mathematics and computer science  cognitive systems  technical university of denmark  dtu   denmark     xaim novonordisk.com   mmor dtu.dk             novo nordisk a s  device r d  hillerd  denmark  novo nordisk a s  medical   science  sborg  denmark  novo nordisk a s  data analytics  sborg  denmark    level. hence  predictions of future bg levels benefit from  having such data available as well    . however  although  access to more data sources  carbohydrates  exercise  dose  size  add further information and can improve the prediction  performance  the availability of such data suffer from it  typically having to be actively collected and therefore are  either completely lacking or in low quality. as a result  bg  predictions have to perform well even when only bg data  are available. another issue is that there may be situations  where the immediate historic data are not available  e.g.  when a patient is using the tool for the first time or if data  has not been generated or collected during a weekend or  vacation. therefore  bg predictions also have to perform  well or fail safe in situations where the immediate historic  data is limited.  the use of continuous glucose monitoring  cgm  is  increasing and unlocks new possible approaches to diabetes  therapy        . with cgm  an abundance of real time bg  measurements become available for patients with type    diabetes  t d  and progressed type   diabetes  t d . the  question is how the cgm data can be optimally leveraged to  prepare future decision support tools. for example  access to  large quantities of cgm data creates an opportunity to learn  structural properties of the cgm signal  which may be used  for predicting future bg values. thus  optimised predictive  modeling could be leveraged to improve the opportunities for  patients to make therapeutic decisions based on forecasted  bg levels.  several well established predictive methods exist in timeseries analysis  e.g. the autoregressive integrated moving  average  arima   which is a robust linear approach    . a  recent addition to predictive methods for time series is the  use of recurrent neural networks  rnns  in deep learning   dl . rnns have proven effective for a variety of sequence  modeling and forecasting tasks  most notably in language  and speech    . rnns are typically implemented with long  short term memory  lstm  units that provides a gating  mechanism enabling the rnns to preserve in memory events  further in the past         . in this context  previous studies  have indicated the benefits of rnns for stbg prediction                 . motivated by recent results in transfer learning   tl   we would like to investigate the ability of transferring  models learned from one task to another     . the question  is whether there would be benefits of deploying the more  advanced predictive methods for bg predictions compared  to traditional methods such as arima.     the specific research questions in the study are      does a population based lstm model generalize well  to unseen patients  i.e. if cgm trends acquired from a  population can enable stbg prediction on individual  patients      does increasingly added patient specific cgm data  to a pretrained population based lstm  i.e.  using  tl  improve predictive performance on individual patients      how does the lstm based solely on patient specific  data perform when trained on limited cgm data   in order to answer the specific research questions  this  study systematically investigates lstm based rnns for  stbg prediction up to    min horizon into the future in  comparison to arima. additionally  the study explores the  feasibility application of tl in this context. the models  outlined below are compared in this study      population based lstm  pretraining       finetuned patient specific lstm  with pretraining       patient specific lstm  without pretraining       patient specific arima.  ii. methods  in this section  a comprehensive description of the included cgm data  examined models  and experimental setup  is provided. an overview is supplied in fig.  .  a. cgm data  the cgm data used in this study is acquired from the  cornerstones care r platform powered by glooko which  is a diabetes management application capturing real world  cgm data.    consecutive days of cgm data  henceforth  referred to as a period  are acquired from    patients with  diabetes with readings every   minutes  i.e.     bg readings  per day . however  cgm data from real life patients often  have missing values  or gaps  caused either by sensor or  human error. inspired by the international cgm consensus   only cgm days and periods with more than     available  bg readings are included    . however  an average of      available bg readings is present for the included periods.  furthermore  no information exist about the diabetes type   demographics or treatment regimens of the patients based  on missing access.  b. arima  arima models are considered some of the most flexible  and popular autoregressive techniques for continuous timeseries forecasting    . arima is specified by   parameters   p  d  and q. here  p refers to the number of autoregressive  terms  d represents the number of nonseasonal differences   and q depicts the number of lagged forecast errors in the  prediction equation. when working with a data sequence  x    x    . . . xn   we can describe arima as        xt    c      xt                p xt p        t       t      q  t q          where xt  denotes the differenced time series  which has  been differenced d times  c is a constant    is the ar p   component    is the ma q  component  and   is the lagged  forecast errors.  c. long short term memory network  lstm   rnn is a type of dl architecture which repeatedly uses  the same weights along a sequence of data. the lstm cell  extends the recurrent computation of an rnn with a gating  mechanism. this gating mechanism allows the lstm to  represent relationships between data points far apart         .  the cgm data is modeled as a one dimensional time  series  i.e. the bg value at each time step  see eq.  . to  limit the complexity  we only allowed the lstm to observe  the k preceding bg values  denoted as the historical window   before predicting the next bg value.  x t   f  xt     xt   . . .   xt k             where xi is a single bg measurement  x i is the predicted bg  value  k corresponds to the historical window size  and f  is an lstm based function with a multilayered perceptron   mlp  resulting in one continuous value  x t . the network  architecture  f   is defined as follows   x t   f  xt     xt   . . .   xt k     w  gt           gt   a uht   b            ht   lstm xt     ht                   where a is the rectified linear unit  relu  activation  function      applied element wise  w  u  b  and the parameters   of the lstm are trainable with gradient descent  methods  and ht k is the zero vector. we optimize f using the  root mean square error  rmse  between the target xt   and  predicted value x t . when training f we regularize the model  by having a historical window size sequentially increased  from   to k as used in language modeling         .  d. cgm data partitions  as shown in fig.    cgm data from    patients and  their corresponding periods     consecutive cgm days   are partitioned into two separate sets  a training set of     randomly selected patients for training of the populationbased lstm and the remaining    patients as a test set for  patient specific models.  for each of the    patients  their period of    cgm days  is divided into two   day parts. the latter   days are used  for testing all developed models and the first   days are  used for training. the training data is then further restricted  into either      or   preceding days such that the test data  is contiguous with the preceding training days  dark blue  boxes in fig.   . these restrictions are made to examine  whether increasingly added cgm days will have an impact  and improve the performance of the patient specific models.  e. data processing   inclusion criteria  the cgm data is normalized to have zero mean and a  variance of one for all models  i.e. standardizing training  data and applying equivalent standardization with training     fig.    flowdiagram of the experimental setup including both the population based and patient specific models presented  by two different blocks. furthermore  the flow of information within between blocks and in which order are depicted by  the numbered squares. all model specifications for both lstms and arima are outlined with corresponding color coded  boxes within the two blocks. blue box  population based lstm  pretraining   orange box  finetuned patient specific lstm   with pretraining   green box  patient specific lstm  without pretraining   red box  patient specific arima.    mean and standard deviation on the test data. when training  the lstms  only historical windows with no missing values  are used. at test time  windows with missing values are  discarded if they fulfill at least one of two conditions      input window contains more than   missing values within  the last hour  and    most recent value is missing. missing  values of remaining windows are linearly interpolated. all  missing values are linearly interpolated when fitting arima.  only samples  xt   at which all models have made predictions  are considered when evaluating on the test set.  f. population based lstm  pretraining   taking the population block into account in fig.    the  population training set is fed to the population based lstm   blue box   also considered as pretraining for the patientspecific lstms. in this context  a sliding window  using     minute steps  of bg values are used as input in batches of     windows.  bayesian optimization  bo  implemented in sigopt       is used to efficiently explore a predefined bounded hyperparameter space for lstms  dropout level of the mlp layer   and historical window size.    distinct lstm networks   experiments  with the outlined parameters suggested by bo  are then trained on    patients and evaluated on a validation  set of   patients. the architecture and configuration corresponding to the best performing population based lstm are  found in terms of root mean square error  rmse  on the  validation set. networks are optimized by the rectified adam   radam  gradient descent method using default parameters  as proposed by     .    g. patient specific lstms  the best performing lstm and its configuration found by  bo are used to produce patient specific lstms based on the  patient specific cgm data. to evaluate whether any knowledge from the population based lstm can be transferred to  patient specific lstms  we evaluate two conditions.  in the first case  we use tl to finetune the populationbased lstm to individual patients  orange box   i.e. learnable parameters of the population based lstm are transferred and finetuned with patient specific training data        and   cgm days . given the limited data available on a  patient level  a sliding window with   minute steps is used  while     and     of the data is partitioned into training  and validation sets  respectively. networks are finetuned with  stochastic gradient descent  sgd   using a momentum of   .   and a learning rate of      to mitigate catastrophic  forgetting.  for the second case  patient specific lstms with the  same hyperparameters and patient specific data as in the  first condition are used  green box  but with their network  weights randomly initialized rather than transferred from the  population based lstm. networks in the second condition  are optimized using radam with default parameters  as for  the population based lstm .  h. scheduling   early stopping  all lstms use a factor    reduce on plateau learning  rate schedule  i.e. learning rate is reduced by a factor of     whenever no improvement is obtained within    epochs   hence reducing oscillations near local optima. similarly  in  order to prevent overfitting  early stopping is applied when     fig.    example of    minutes of predictions for a all the  examined models where historical data is followed by ground  truth bg values.    no improvement is observed on a held out validation set  within    epochs.    fig.    mae  mmol l  for the    test patients as function  of provided cgm training days  ph      min  for all the  models illustrated as boxplots.    i. patient specific arima  for the patient specific arima models  autoarima       is able to fit a suitable model for every case of cgm  training day s  available       or    for each specific patient   red box . thus  a total    arima models are fitted. the  parameter search ranges for d  p  and q in the autoarima  are presented in fig.   allowing larger arima models to  be examined  i.e. taking up to   hours of historical bg  values and their errors into account . autoarima conducts  differencing tests to determine d  fit multiple models within  a given range for p and q  followed by selecting the best  model based on the lowest akaike information criterion      . the best performing model parameters are evaluated  on the test data. in practice  autoarima is implemented  by the pdmarima python module and the models are fitted  using newton s method.  j. prediction horizon  prediction horizons  phs  of                 and    min  into the future are tested  i.e. all models predict    bg  values into the future     min  conditioned on the preceding  historical window.  the lstm models are trained using teacher forcing        in which during training the model receives the ground  truth output at time t as input at time t    . for testing   we sequentially predict the next bg value and update the  input sequence with the computed prediction in an autoregressive manner. to improve performance of the lstms   the ensemble bg prediction acquired from   different seeds  is computed as the final prediction  i.e. the average of    separately predicted bg values.  for the arima models  we similarly predict the next  bg sequentially and update the input sequence with the  computed prediction in an autoregressive manner during  testing. fig.   shows    minutes of predictions for all the  examined models where historical data is followed by ground  truth bg values.  as performance metrics  the mean absolute error  mae     fig.    mae  mmol l  for the    test patients as function  of different phs  cgm training days      illustrated as  boxplots.     eq.    and rmse  eq.    are used for model comparison.         n   s   s    s    s   mae x j   x  j     xi  j   x i  j       n i           s   s   rmse x j   x  j      s         n    s    s     xi  j   x i  j     n i             here  n represents the number of predictions  s the considered  subject and j is the index of ph ranging from   to   .  iii. results   discussion  the best performing lstm model found by bo is defined  by the following specifications    layers of bidirectional  lstm with a hidden state of size     no dropout on the mlp  layer     hidden units   and   hour historical window as input  sequence. autoarima generally found small models to  perform best  with the most commonly occurring complexity     table i  tables for all the model performances in terms of average mae and rmse with corresponding standard error of  the mean based on the   cgm test days of the    patients  cgm training days     .  model  population based lstm  pretraining   finetuned patient specific lstm  with pretraining   patient specific lstm  without pretraining   patient specific arima  last observation carried forward    mae  standard error   mmol l      min   .        .       .        .       .        .       .        .       .        .           min   .        .       .        .       .        .       .        .       .        .           min   .        .       .        .       .        .       .        .       .        .           min   .        .       .        .       .        .       .        .       .        .        model  population based lstm  pretraining   finetuned patient specific lstm  with pretraining   patient specific lstm  without pretraining   patient specific arima  last observation carried forward       min   .        .       .        .       .        .       .        .       .        .           min   .        .       .        .       .        .       .        .       .        .           min   .        .       .        .       .        .       .        .       .        .        rmse  standard error   mmol l     being d      p      and q    .  tab. i shows all the model performances in terms of  average mae and rmse with corresponding standard error  of the mean based on the   cgm test days of the     patients  cgm training days     . additionally  to ground  our results from other methods  last observation carried  forward  locf  is also included which applies the last  available bg observation as its future prediction for all times.  in tab. i  the overall best performing model is the  population based lstm with mae ranging between  .         .     to  .        .      mmol l  for ph      and     min  respectively. this is followed by the finetuned  patient specific lstm both in terms of mae and rmse.  patient specific arima performs slightly better than patientspecific lstm until ph      min whereas for ph      and     min patient specific lstm achieves a modestly better  performance. as expected  locf performs worse than the  other models as it does not benefit from the historical  trends other than the last bg value. furthermore  within the  investigated models exploiting cgm dynamics  we deal with  small performance differences. fig.   shows the mae for all  models  as boxplots  across the    patients as a function of  available training cgm days of       and   having ph fixed  to    min.  given the described setup in this study  we find that the  population based lstm  blue  performs most favorable for  stbg prediction as the model does not rely on patientspecific cgm training days available. this implies that  learning from a population can be beneficial on patient level.  in other words  generalizing well when applied on the unseen  patients considered for stbg prediction. from fig.   it is  further clear that performance is patient dependent given the  width of the boxplots.  the finetuned patient specific lstm  orange  performs  similarly to the population based lstm. in other words   the added patient specific cgm data    cgm days  to a  pretrained population based lstm  tl  do not improve  predictive performance on individual patients. in this context   there are two possible explanations of the given outcome      more patient specific cgm data with longer periods are       min   .        .       .        .       .        .       .        .       .        .           min   .        .       .        .       .        .       .        .       .        .           min   .        .       .        .       .        .       .        .       .        .        required for training purposes of the lstms in order to  improve performance on patient level  and    tl is known  for being difficult to deploy in practice e.g. sub optimal  local minima  slow convergence  and catastrophic forgetting  indicating undesired pitfalls during training.  on the other hand  considering the patient specific lstm   green   the number of available cgm days for each specific  patient have an impact. the model transcends locf with    training days  initially being the worst performing model  with only one training day  see fig.   . this suggests  that more patient specific historical data will benefit the  performance and enable learning individual trends.  the patient specific arima models  red  perform well  in general regardless of available cgm training days. nevertheless  it has a larger midspread than the population based  lstm  blue . hence  the linear models perform comparably  with the more complex lstm based models. simultaneously  if only limited cgm data is available  in this case  up to   cgm days   a patient specific arima would be a  promising alternative to the patient specific lstm.  fig.   visualizes the change of mae as a function of  different phs  with the number of cgm training days fixed  to  . mae is increased by increased ph for all the models.  at the same time  it also indicates for the locf approach  that a short ph of e.g.    minutes might still provide some  insight of the bg level depending on the task.  limitations of this study includes the relatively small  number of patients. considering a total sample of    patients  may show indications  but more patients and cgm data  are required to investigate consistency and robustness to the  developed models.  another limitation is unspecified patient information  i.e.  we do not know the diabetes type  demographics or treatment  regimens  all of which are known to impact bg. in particular   t d and t d bg dynamics are very different and their  dissimilar effects might distort the underlying signal in the  cgm data.  future studies should focus on a specific cohort of patients  with known treatment regimens  demographics and if possible other data sources  carbohydrates  exercise  dose size . in     particular  examining population  and patient tailored stbg  prediction models aimed towards specific groups of patients.  moreover  the problem could also be approached as a classification problem as opposed to regression. in this context   targeted towards hypoglycemic event detection including  e.g. expert dependent features acquired from the cgm data   a strategy successfully used in other tasks               .  ideally  future solutions could be based on different hybrid  models which are dynamically deployed depending on data  availability and quality.  iv. conclusion  in this study  we explore the effectiveness and potential of  using lstm based rnns for stbg prediction in contrast to  a linear model such as arima. we find that a populationbased lstm model generalizes well to individual patients   being the best performing model in all ph cases. adding  up to   cgm training days in the finetuned patient specific  lstm  tl  do not seem to suffice improve performance.  this indicates the need for further investigation and more  patient specific cgm data with longer periods. the patientspecific lstm does not perform well when only one day of  cgm data are available. on the other hand  it benefits by  added patient specific training days performing similar to the  patient specific arima with   days of cgm data available.  given the outcome of this study  advanced predictive  models could be deployed when designing decision support  tools with bg forecasting capabilities. however  in some  situations the conventional linear arima approach performs  as well or better  in particular when little cgm data are  available. a future solution may be a hybrid predictive  approach dynamically deploying different models depending  on data availability and quality.  