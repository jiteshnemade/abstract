introduction  recurrent neural networks  rnn  have successfully been used for sequential tasks like language  modeling       machine translation       and speech recognition    . they approximate complex   non linear temporal relationships by maintaining and updating an internal state for every input element. however  they face several challenges while modeling long term dependencies  motivating  work on variant architectures.  firstly  due to the long credit assignment paths in rnns  the gradients might vanish or explode     .  this has led to gated variants like the long short term memory  lstm       that can retain information over long timescales. secondly  it is well known that deeper networks can more efficiently  approximate a broader range of functions         . while rnns are deep in time  they are limited in  the number of non linearities applied to recent inputs. to increase depth  there has been extensive  work on stacking rnns into multiple layers        . in vanilla stacked rnns  each layer applies a  non linearity and passes information to the next layer  while also maintaining a recurrent connection  to itself. to effectively propagate gradients across the hierarchy  skip or shortcut connections can be  c the university of texas at austin and intel corporation. preprint. under review.     used         . alternatives like recurrent highway networks      introduce non linearities between  timesteps through  micro ticks      . pascanu et al. increase depth by adding feedforward layers  between state to state transitions     . gated feedback networks      allow for layer to layer interactions between adjacent timesteps. all these variants thus introduce topological modifications to  retain information over longer timescales and model hierarchical temporal dependencies.  another development is the bidirectional rnn  bi rnn          . while rnns are inherently  causal  bi rnns model acausal interactions by processing sequences in both forward and backward  directions. they achieve state of the art performance on tasks like parts of speech tagging       and sentiment analysis      suggesting that several natural language processing  nlp  tasks greatly  benefit from combining temporal information in both directions.  architectural variants of rnns are highly effective for different sequential processing tasks. however  the number of possibilities necessitates a combinatorial search over architectures and their  hyperparameters like number of layers or representation size  as well as painstaking hyperparameter  tuning. exploring all options is difficult and expensive. in this paper  we simplify this exploration by  firstly showing that any stacked rnn is  in fact  equivalent to a single layer rnn with specific constraints on the recurrent weights and outputs.this equivalence trivializes the search among stacked  rnn architectures  as equivalent solutions can be learned in a single layer. the second aim of this  paper is to show that a flattened rnn can approximate a bi rnn by removing constraints on the  recurrent weights. while this approximation carries some error  this method is capable of yielding  similar solutions at potentially much lower computational cost.      background  given a sequential input  xt  t  ...t   xt   rd   a single layer rnn is defined by         h t   f w x xt   w h h t     b h            y t   g w o h t   b o             where f     and g     are element wise activation function such as tanh and softmax  h t   rn is  the hidden state at timestep t with n units  and y t   rm is the network output. learned parameters  include input weights w x   recurrent weights w h   bias term b h   output weights w o   and bias term  b o . the initial hidden state is denoted h   .  stacked recurrent units are typically used to provide depth in rnns        . based on      a stacked  rnn with k layers is given by                         f wx    xt   wh ht     bh         i    i      i   i    i   ht   f wx i  ht    wh ht     bh   i      . . .   k         k   yt   g wo ht   bo           ht                     where the activation function and parameterization follow the single layer rnn. separate weights   i    i    i   and bias terms for each layer i are given by wx   wh   and bh . the hidden state for this layer at   k    i        timestep t is ht . the stacked rnn has initial hidden state vectors h  . . . h  corresponding to  the k layers. the hat operator is used for vectors and matrices in the single layer rnn  while those  without are for the stacked rnn.      a stacked rnn is equivalent to a single layer rnn  the mathematical structure of a stacked rnn is similar to a single layer rnn with the addition of  between layer connections that add depth. here we show that any stacked rnn can be flattened  into a single layer rnn that produces the exact same sequence of hidden states and outputs  albeit  with some time delay. to illustrate this  we rewrite the parameters of a single layer rnn using the        weights and bias terms of the stacked rnn from equations                     wh                                         wx wh                 wx  .  ..  ..  ..  bh     ..           .  .  .                   .      b h     .     w x     .    w h          ..  ..  .       ..   i    i     ..    .  . wx wh       .   k         bh           ..  ..  .  .           k    k             wx wh  w o              wo     b o   bo                    where w x   rkn d are the input weights  w h   rkn kn the recurrent weights  b h   rkn the  biases  w o   rm kn the output weights  and b o   rm the output biases.  intuitively  one can see from eq.     that each layer in the stacked rnn is converted into a group  of units in the flattened rnn. the block bidiagonal structure of the recurrent weight matrix w h  makes the hidden state act as a buffer  where each group of units only receives input from itself and  the previous group. information processed through this buffering mechanism eventually arrives at  the output after k     timesteps. it is important to note that the flattened rnn performs the same  computations as the stacked version by trading depth in layers for depth in time.  next  we define the following notation  for a vector v   rkn with k blocks  the subvector v i    rn  refers to its ith block following the partition from equation    . we now prove that a single layer  rnn parametrized by eq.         is exactly equivalent to the stacked rnn in eqs.        . the  proof easily extends to more complex recurrent cells such as lstms and grus.  theorem  . given an input sequence  xt  t  ...t and a stacked rnn with k layers defined by   i   equations         with activation functions f     and g      and initial states  h   i  ...k   the single i    i   layer rnn defined by equations         and initialized with h   such that h i     h     i     . . . k   produces the same output sequence but delayed by k     timesteps  i.e.  y t k     yt for all  t     . . . t . further  the sequence of hidden states at each layer i are equivalent with delay i        i    i   i.e.  h t i     ht for all     i   k and t    .  proof. the proof is included in section   of the supplementary material.         theorem   makes an assumption that h   in the single layer rnn can be initialized such that it   i    i   achieves h i     h  for all blocks. lemma   below implies that initialization for the flattened  rnn can always be computed from the stacked rnn. the intuition behind it is that we can compute   i    i    i   recursively from h i     h  to h   for block i  while  inverting  the activation function effect in  the process. all commonly used activation functions are surjective  thus it is enough to know the  right inverse of the activation function f      see proof of lemma . for example  when f     is the  relu  the right inverse is the identity function r  d    d.  lemma  . let f   r   d be a surjective activation function that maps elements in r to elements   i   in interval d. also  let h    dn for i     . . . k be the hidden state initialization for a stacked rnn  with k layers as defined in        . then  there exists an initial hidden state vector h     rkn for a   i    i   single layer network in equation     such that h i     h   i     . . . k.  proof. the proof is included in section   of the supplementary material.         from this theorem we see that there are two notable differences between a generic single layer  rnn and a flattened k layer stacked rnn. first  the output of the flattened rnn is delayed by k      timepoints. second  the flattened rnn has a specific sparsity structure in its weight matrices that  is not present in the generic rnn. this second difference is merely a matter of model parameters   which are learned from data in any case. in the following sections we will explore whether singlelayer rnns trained with delayed output can learn equivalently complex functions as stacked rnns.         a      b     figure    a stacked rnn is equivalent to a single layer rnn under the given weight constraints.  the flattened rnn produces the same representations as the stacked network  albeit with a time  delay.  a  stacked rnn with k     layers where connections show the different weight parameters.   b  weights of the flattened rnn that are equivalent to connections in the stacked rnn.   .  the weight matrices  suppose one takes a flattened rnn as in eq.     and adds non zero elements to regions not populated  by weights from the equivalent stacked rnn. these non zero weights do not correspond to existing  connections in the stacked rnn. so what do they correspond to   to explore this question we illustrate a   layer stacked rnn in figure    a . here solid arrows show  the standard stacked rnn connections. the flattened rnn weight matrices w h   w x   and w o are  shown in figure    b   where the color of each block matches the corresponding arrow in figure    a .  blocks on the main diagonal of w h connect groups of units to themselves recurrently  while blocks  on the subdiagonal correspond to connections between layers in the stacked rnn. more generally    j    i   block  i  j  in w h corresponds to a connection from ht to ht j i   in the stacked rnn. thus   blocks in the lower triangle  i.e. i   j      correspond to connections that point backwards in time   and from a lower layer to a higher layer. for example  the orange block        in figure    b   and  the dashed orange lines in figure    a   connects layer   at time t to layer   at time t    . in a  later section we will test whether this type of connection can be exploited to mimic some aspects  of a bi rnn. conversely  blocks in the upper triangle  i.e. j   i  point forward in time and from a  higher layer to a lower layer. for example  the red block        in figure    b   and the dashed red  lines in figure    a   connects layer   at time t to layer   at time t    .  which possible connections in a stacked rnn cannot be represented in the flattened rnn  first   the flattened rnn cannot emulate connections that go backwards in time from a higher layer to a  lower layer. however  such connections introduce loops and thus are also impossible in stacked  rnns. second  the flattened rnn cannot emulate connections that go forward in time from a lower  layer to a higher layer. however  such connections would merely serve as  shortcuts   and don a z t  facilitate additional non linear transformations to the input.  thus we see that adding weights to empty regions in the flattened rnn can mimic the behavior  of many stacked recurrent architectures that have previously been proposed. among others  it can  approximate the indrnn       td rnn       skip connections       and all to all layer networks      . simply removing the constraints on w h during training will enable a single layer rnn to  learn the necessary stacked architecture. however  unlike an ordinary rnn  this requires the output  to be delayed based on the desired stacking depth. further  while the single layer network has the  same total number of units as the corresponding stacked rnn  relaxing constraints on w h would  mean that the single layer would have many more parameters.   .  the delay in the output  flattening a stacked rnn introduces a delay of k   timesteps between the input at timestep t and its  respective output at timestep t   k    . this delay plays the role of the k non linear transformations        d rnn  d   layer number of  non linearities  stacked rnn   bi rnn  rnn  max    t      t   d    d    t         bi rnn has more  timestep relative to  current input   t  d rnn has more non linearities  non linearities    figure    number of non linearities that can be applied to past and future sequence elements with  respect to current input   t   . the d rnn only sees d steps into the future.    that the stacked rnn computes for each timepoint. that is  non linearities are achieved in the  temporal direction instead of across layers. the idea of increasing the number of temporal nonlinearities has been previously explored as micro steps           where additional timesteps are  inserted between each pair of elements in both the input and output sequences. for a sequence  of length t   the computational effort of micro step models grows with the delay d proportionally  to o  dt  . in contrast  temporal depth in a flattened rnn is obtained by applying the delay as  in theorem  . this allows the model to maintain k non linear transformations between input and  output  but to have a computational complexity that only grows proportionally to o  d   t  .     .  approximating bidirectional rnns  when sparsity constraints on the weight matrices are removed  a flattened rnn gains the ability to  peek at future inputs. a similar idea was used in the past as a baseline for bidirectional recurrent  neural networks  bi rnns          . these papers showed that bi rnns were superior to delayed  rnns for relatively simple problems  but it is not clear that this comparison holds true for problems  that require more non linear solutions. if a recurrent network can compute the output for time  t by exploiting future input elements  what conditions are necessary to approximate its bi rnn  counterpart  moreover  can the delayed output rnn obtain the same results  and  given these  conditions  is there a benefit to use the delayed output rnn instead of the bi rnn   to answer these questions we first focus on the functional richness and the influence of each element  in the sequence. figure   shows the number of non linear transformations that each network can  apply to any input element before computing the output at timestep t  . the generic rnn processes  only past inputs  t   t     and the number of non linearities decreases for inputs closer to timestep  t    reaching   at t   t  . the bi rnn has identical behavior for causal inputs but is augmented  symmetrically for acausal inputs by the inclusion of a backward rnn. in contrast  the delayedoutput rnn  d rnn  has a similar behavior for the causal inputs but with a higher number of  non linearities. this trend continues for the first d acausal inputs with a decreasing number of nonlinearities until the number reaches zero at t   t    d    .  in order for a d rnn to have at least as many non linearities as a bi rnn for every element in  a sequence  it needs a delay that is twice the sequence length. however  long delays have the  disadvantage of increasing the memory requirements on the network. on the other hand  a d rnn  can beat a bi rnn when the needed acausal information is limited to only a few future elements  or when the non linear influence of these nearby inputs on the learned function is higher than the  remaining ones in the sequence.  interestingly  using the d rnn with low delay values to approximate a bi rnn has additional benefits. one advantage is in computational cost  for a sequence of length t   the cost to compute a  forward pass for the d rnn is t   d  while for the bi rnn the cost is  t . thus  in practice it  is possible to trade off small performance degradations for computationally cheaper networks. beyond the computational costs  d rnns can also be used in applications where it is critical to output  values in  near  realtime        .         .   accuracy     .     d lstm val  d lstm test  max performance  lstm  bilstm     .    .    .                                                           delay d    figure    comparison of different delay values for a d lstm network for reversing a sequence.  lstm and bi lstm networks are shown for reference. the network is capable of achieving the  expected statistical bound. the d lstm with highest delay is capable of solving the task as good as  the bi lstm.      experiments  we test the capabilities of the d rnn in three experiments designed to shed more light on the  relationships between d rnns  rnns  and bi rnns. for this purpose  the rnn implementation  is switched to lstms  which avoid vanishing gradients and are better able to retain information  across delays. the delayed lstm networks are denoted as d lstms. to train each d lstm  the  input sequences are padded at the end with zero vectors and loss is computed by ignoring the first   delay  timesteps. all models are trained using the adam optimization algorithm      with learning  rate  e          .   and       .   . during training  the gradients are clipped      at  .  to avoid  explosions. experiments were implemented using pytorch  . .      .     .  experiment    sequence reversal  first  we propose a simple test to illustrate how the d lstm can interpolate between a regular lstm  and bi lstm. in this test we require the recurrent architectures to output a sequence in reverse order  while reading it  i.e. yt   xt  t   for t      ..  t . solving this task perfectly is only possible when  a network has acausal access to the sequence. moreover  depending on how many acausal elements  a network accesses  it is possible to analytically calculate the expected maximum performance that  the network can achieve. given a sequence of length t with elements from a vocabulary     ...  v     a causal network such as the regular lstm can output the second half of the elements correctly and  guess those in the first half with probability   v . when a network has access to d acausal elements it  can start outputting correct elements before  point  and can achieve an expected      reaching      the halfway        true positive rate  tpr  of        v    d       t     v . we generate data sequences of length  t      by uniformly sampling integer values between   and v    . the training set consists  of        sequences  the validation set        and test set      . output sequences are the input  sequences reversed. values in the input sequences are fed as one hot vector representations. all  networks output via a linear layer with a softmax function that converts to a vector of v probabilities  to which cross entropy loss is applied. the lstm and d lstm networks have     hidden units   while the bi lstm has    in each direction in order to keep the total number of parameters constant.  we use batches of     sequences and train for       epochs with early stopping after    epochs and      e  .  figure   shows accuracy on this task as a function of the applied delay. the lstm network does not  use acausal information and is unable to reverse more than half of the input sequence. conversely   the bi lstm has full access to every element in the sequence  and can perfectly solve the task.  for the d lstm network  performance increases as we increase the delay in the output  reaching  the same level as the bi lstm once the network has access to the entire sequence before being  required to produce any output  delay    . this experiment shows that  for a simple linear task  the  d lstm can  interpolate  between lstm and bi lstm by choosing a delay that ranges from zero  to the length of the input sequence.         .     .            .            .            .            .      .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .          log   mse    filter acausality a          scale       a  lstm     b  bi lstm     c  d lstm  d        d  d lstm  d        figure    error maps for the sine function experiment with different degrees of non linearity  horizontal axis  and amounts of acausality of the filter  vertical axis . tested architectures   a  lstm    b  bi lstm   c  d lstm with delay    and  d  d lstm with delay   . dark blue regions depict  perfect filtering  low error   transitioning to yellow regions with high error.     .  experiment    evaluating network capabilities  the first experiment showed how a d lstm with sufficient delay can mimic a bi lstm. in the next  experiment we aim at comparing how well d lstm  lstm  and bi lstm networks approximate  functions with varying degrees of non linearity and acausality.  drawing  pa inspiration from       we require each recurrent network to learn the function yt    sin   j  c   wj c xt j  . the parameter   scales the argument of the sine function and thus  controls the degree of non linearity in the function  for small   the function is roughly linear  while  for large   the function is highly non linear. scalars a      acausal  and c      causal  control the  length of the causal and acausal portions of the linear filter w that is applied to the input x.  we generate datasets with different combinations of       .   . . .    .   and a       . . .        choosing c such that a   c     . for each combination  we generate a filter w with    elements drawn  uniformly in   .    .    and random input sequences with t      elements drawn from a uniform  distribution   .    .  . in total  there are        generated sequences for training        for validation  and       for testing with each set of parameter values. the output is computed following the  previous formula and with zero padding for the borders. we generate   repetitions of each dataset  with different filters w and inputs x.  we train lstm  d lstm with delays   and     and bi lstm networks to minimize mean squared  error  mse . the lstm and d lstm have     hidden units and the bi lstm has    per network   matching the numbers of parameters. a linear layer after the recurrent layer outputs a single value  per timestep. model are trained in batches of     sequences for       epochs. training is stopped  if the validation mse falls below  e  . training is repeated five times for each     a  value.  figure   shows the average test mse for each model as a function of    degree of input non linearity   and a  acausality . lstm performance  fig.    a   is poor everywhere except where the filter is  purely causal. surprisingly  the network performs quite well even when the amount of non linearity      is quite high. the reason for this seems to be that temporal depth enables the lstm to approximate this function well. bi lstm performance  fig.    b   follows a similar trend for the causal  case  a      as the forward lstm  but also has good performance for acausal filters  a      when  the function is nearly linear    is small . as the non linearity of the function increases  however   bi lstm performance suffers. this occurs because the bi lstm needs to approximate a highly  non linear function with a linear combination of its forward and backward outputs  which cannot be  done with small error. improving performance would require stacked bi lstm layers.  in contrast  d lstm networks have excellent performance for both non linear and acausal functions.  the d lstm with delay    fig.    c   shows a clear switch in performance from acausality a      to  . this perfectly matches by the limit of acausal elements that the network has access to. for the  d lstm with delay     fig.    d    the network performs well for acausality values a up to   .  an interesting outcome of this experiment is the better performance observed for the d lstm over  the bi lstm. this shows that the d lstm can be a better fit than a bi lstm for the right task.  furthermore  the d lstm network seems to approximate the functionality of a stacked bi lstm by  approximating highly non linear functions. in practice  this could be a great benefit for applications        table    parts of speech performance for german  de   english  en   and french  fr  languages.  the models are composed of two subnetworks at character level and word level. best bidirectional  network and best forward only network are marked in bold for each language.  lang.  char level network  word level network  validation accuracy test accuracy  de    en    fr    lstm  d lstm delay    d lstm delay    bi lstm    lstm  d lstm delay    bi lstm  bi lstm      .      .      .      .      .      .      .      .        .      .      .      .      .      .      .      .      lstm  d lstm delay    d lstm delay    bi lstm    lstm  d lstm delay    bi lstm  bi lstm      .      .      .      .      .      .      .      .        .      .      .      .      .      .      .      .      lstm  d lstm with delay    d lstm with delay    bi lstm    lstm  d lstm with delay    bi lstm  bi lstm      .      .      .      .      .      .      .      .        .      .      .      .      .      .      .      .      where there is no need to treat the whole sequence. moreover  this could be impossible in other cases   such as streamed data. in such cases  the d lstm would shine over bidirectional architectures. on  the other hand  we expect the bi lstm to perform better when the acausality needs for the task are  longer than the delay  i.e.  a   d.   .  experiment    real world part of speech tagging  in the previous experiments  we show that d lstm is capable of approximating and even outperforming a bi lstm in some cases. in practice  however  the elements in a sequence may have  different forward and backward relations. this poses a challenge for delayed networks that are constrained to a specific delay. if the delay is too low  it may not be enough for some long dependencies  between elements. if it is too high  the network may forget information and require higher capacity   and maybe training data . this is prevalent in several nlp tasks. therefore  we test on the part ofspeech  pos  tagging task where bi lstms achieve state of the art performance            . the  task involves processing a variable length sequence to predict a pos tag  e.g. noun  verb  per word   using the universal dependencies  ud       dataset. more details can be found in the supplementary material.  the dual bi lstm architecture proposed by plank et al.      is followed to test the approximation  capacity of the d lstms. in this model  a word is encoded using a combination of word embeddings  and character level encoding. the encoded word is fed to a bi lstm followed by a linear layer  with softmax to produce pos tags. the character level encoding is produced by first computing  the embedding of each character and then feeding it to a bi lstm. the last hidden state in each  direction is concatenated with the word embedding to form the character level encoding.  the character level bi lstm has     units in each direction and the lstm d lstms have      units to generate encodings of the same size. for the word level subnetwork  the hidden state is of  size     for the bi lstm  and     units for the lstm d lstm to match the number of parameters.  the networks are trained for    epochs with cross entropy loss. we train combinations of networks  with delays    lstm         and   for the character level subnetwork  and delays   through   for the  word level. each network has   repeats with random initialization.  results are presented in table  . for brevity  we include a subset of the combinations for each  language  the complete table can be found in the supplementary material . for the character level  model  lstms without delay yield reduced performance. however  replacing only the characterlevel bi lstm with a lstm does not affect the performance  supplementary material . this suggests that only the word level subnetwork benefits from acausal elements in the sentence. interestingly  using a d lstm with delay   for the character level network achieves a small improvement  over the double bidirectional model in english and german. replacing the word level bi lstm        with an lstm decreases performance significantly. however  using a mere d lstm with delay    improves performance to within  .   of the original bi lstm model.      conclusions  in this paper we show that stacked rnns  which are frequently used to increase depth and representational complexity for sequence problems  can always be flattened into a single layer rnn with  delayed output. relaxing constraints imposed by the flattening process allows this delayed rnn  to look at future as well as past elements  making it possible to approximate bidirectional rnns  but at reduced computational cost. although delayed output rnns have been touched upon previously  this paper reinforces the idea that the simple action of introducing a delay can have significant  impact on the capabilities and performance of rnns.    