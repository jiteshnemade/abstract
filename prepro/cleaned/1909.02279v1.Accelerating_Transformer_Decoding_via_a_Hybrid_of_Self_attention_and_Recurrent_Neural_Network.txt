introduction    recently  the transformer  vaswani et al.         has achieved new state of the art performance for  several language pairs. instead of using recurrent architectures to extract and memorize the  history information in conventional rnn based  nmt model  bahdanau et al.         the transformer depends solely on attention mechanisms  and feed forward layers  in which  the context information in the sentence is modeled with a selfattention method  and the generation of next hidden state no longer sequentially depends on the  previous one. the decoupling of the sequential  hidden states brings in huge parallelization advantages in model training.  however  at inference time  due to its dependence on the whole history of previously generated words at each predicting step and a large  amount calculation of multi head attentions  the    sub module   encoding   decoding   attention   selfatt gru   ffn   softmax   others  total    rnmt    .      .     .     .     .     .      .     trans. base     .      .     .      .     .     .     .      .     trans.  layer     .      .     .     .     .     .     .      .     table    time breakdown of rnmt decoding  and transformer decoding  both with beam size  .  trans. base  refers to the base transformer with   layer  encoder and decoder and trans.  layer  refers to the  transformer with   layer encoder and decoder. the  time is measured on a single tesla k   gpu. the   others  item in decoding contains beam expansion   data transmission etc..    transformer is much slower than rnn based  models. this restricts its application in on line  service  where decoding speed is a crucial factor.  we conduct analysis to show time cost of different components of both the transformer and the  rnn based nmt model  rnmt  in table   on       pseudo sentences. all models decode the  same length target sentences for a fair comparison.  the rnmt is a standard single layer gru model  with     embedding size       hidden dimension   bahdanau et al.       . the transformer basic   model follows the basic setting in  vaswani et al.        .   k source  and target  vocabularies are  used for all models. as shown in table    though  the trans. base  has much deeper encoder  it is still  faster than a single layer rnn due to the high parallelizable multi head attention. however  the decoding cost of the transformer decoder is a significant issue which is over   times of that of rnn  decoder and occupies     of the total decoding  time. this is dominated by the high frequency of  computing target to source attention  target selfattention and feed forward network. we also analyze a single layer self attention decoder to com          .     our hybrid model  model architecture    as shown in figure    our hybrid model contains  a self attention based encoder and an rnn based  decoder. in this section  we describe the details of  our hybrid model.    figure    the hybrid architecture.    pare with the single layer rnn and find that even  with a big sacrifice of translation quality  selfattention still slower than rnn in decoding.  as for nmt inference speedup  numerous approaches have been proposed for rnn based  nmt models  devlin        zhang et al.         kim and rush        shi and knight       . for  transformer  gu et al.        proposed a nonautoregressive transformer where output can be  simultaneously computed. they achieved a big  improvement on decoding speed at the cost of  the drop in translation quality. recently  zhang  et al.        proposed an efficient average attention to replace the target self attention of decoder.  however  this mechanism cannot be applied to the  target to source multi head attention and cannot  reduce the calculation of ffn which are the bottleneck for further speedup as shown in table  .  in this paper  we propose a hybrid architecture where the self attention encoder and rnn decoder are integrated. both the two modules are  fast enough as shown in table  . by replacing  the original transformer decoder with an rnnbased module  we speed up the decoding process  by a factor of four. furthermore  by leveraging the  knowledge distillation hinton et al.        kim  and rush         where the original transformer  is regarded as the teacher and our hybrid model  as the student  our hybrid model can improve  the translation performance significantly  and get  comparable results with the transformer.    encoder for our hybrid model  the encoder  stays unchanged from the original transformer  network  which is entirely composed of two sublayers  self attention modules and feed forward  networks. the self attention is a multi head attention network which generates the current state  by integrating the whole source context. the following feed forward layer is composed of two  linear transformations with a relu activation in  between. the layer normalization and residual  connection are used after each sub layer. to  model position information  additive position embeddings are used.  this kind of encoder avoids the recurrence in  rnns. the self attention connects all positions  with a constant number of operations and each position has direct access to all positions  which ensures the model to learn more distant temporal dependencies. unlike the rnn encoder processing  the sentence word by word using sequential operations as long as the sentence  self attention network only depends on the output of the previous  layer  no need to wait for hidden states to propagate across time  which improves model parallelization and speeds up both the training and inference process. based on the above analysis  selfattention network is leveraged as the encoder of  our hybrid mode.  decoder the original transformer decoder contains three sub modules in each layer  an inner  self attention between the current state and target history states  an inter multi head attention between target state and source context  and a feed  forward network. this structure is highly parallelizable in training  however  during inference  it  is impossible to take full advantage of parallelism  because target words are unknown. it has to generate target words one after another as rnn does.  in addition  the inner self attention must access to  all history states  which increase the complexity   and the inter multi head attention is computed in  each layer with the same computational complexity with the inner self attention as shown in table  . as for the rnn decoder  it predicts each     target word just depending on the previously hidden state  the previous word and the source context  computed by the attention mechanism. from table    we can see that the transformer decoder is  the most computationally expensive part which is  almost   times slower than rnmt  so we leverage a single layer rnn decoder with gru  cho  et al.        as recurrent unit for our hybrid model.  it notes that the network structure of chen et al.         is a little similar to ours. but they focus on  finding an optimal structure to improve the translation quality and combine the encoder and decoder  from different model families. their way of combination keeps a larger amount of model parameters  while we aim at accelerating the decoding  speed with light network.  attention mechanism to find the most suitable  attention functions  we use three different attention functions  additive attention  bahdanau et al.          dot product attention and multi head attention. the multi head attention is as same as the  one in our encoder without ffn layer.   .     model training    we use a two stage training for our hybrid model   the pre training and the knowledge distillation  fine tuning. in the first stage  our model is generally trained to maximize the likelihood estimation  using the bilingual training data. in the second  stage  we apply sequence level knowledge distillation kd  hinton et al.        kim and rush         where the transformer is regarded as the  teacher model and our hybrid model is the student model. formally  given the bilingual corpus  d     x n    y  n    n  n   where x is the source sentences and y is the corresponding target ones  the  training objective of the second stage is   l  s        n  x    log p  y  n   x n     s      n    n  x          kl p  y x n     t    p  y x n     s       n           where   is a hyper parameter for regularization  terms which is set to   in all experiments   s and   t are model parameters of the student and teacher  models  and kl is the kullback leibler divergence terms.  t is pre trained and fixed. in equation    the first term guides the model to learn from  the training data and the second term guides it to    learn from the teacher network.         experiment     .     setup    our proposed model is evaluated on nist  openmt chinese english and wmt       chinese english tasks. all experimental results  are reported with ibm case insensitive bleu     papineni et al.        metric.  dataset  for nist openmt s chinese english  task  we use a subset of ldc corpora  which contains  . m sentence pairs. nist      is used  as development set and nist                   as test sets. we keep the top   k most frequent  words for both sides  and others are replaced with   unk  and post processed following luong et al.        . for wmt      chinese english task  we  use all available parallel data  which consists of    m sentence pairs  . the newsdev     is used as  development set and the newstest     as the test  set. all sentences are segmented using byte pair  encoding bpe   sennrich et al.       .   k and    k tokens are adopted as source and target vocabularies respectively.  baselines and implement details  we compare the decoding speed of our model with a standard   layer rnn based nmt model  rnmt    bahdanau et al.        and a base transformer  model  vaswani et al.       . both baselines  are implemented with pre computing and weight  combination techniques. specifically  for rnmt   we use precomputed attention  devlin        and  for transformer  we pre compute  k  v   of inter multi head attention and cache all previous  computed  k  v   in inner self attention for each  layer. linear operations in rnn or multi head attention are combined into one. for transformer   the model size is     and ffn filter size is     .  two different transformer systems are used  one  is   heads     encoder decoder layers  the other  is   heads     encoder decoder layers. rnmt is  a single layer gru model with     embedding  size       hidden dimension and additive attention. our hybrid model uses the same encoder parameters with transformer and same decoder parameters with rnmt. all the experiments are conducted on a single titan xp gpu. in inference   beam search is used with a size    and a length  penalty  . . the decoding batch size is set to   for  all the experiments.       http   www.statmt.org wmt   translation task.html     system  rnmt   kd  trans. teacher   hybrid additive attn   kd  hybrid dot attn   kd  hybrid multi attn   kd  trans. teacher   hybrid additive attn   kd  hybrid dot attn   kd  hybrid multi attn   kd    n                                                                                    time     s     s     s     s     s     s     s     s     s      s     s     s     s     s     s     s    speed  w s      .      .      .      .      .      .      .           .      .      .      .      .           .      .     nist        .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      nist        .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      nist        .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      avg.    .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      table    decoding time and case insensitive bleu scores     for chinese english nist datasets.  trans.  is  short for transformer model.  n  refers to the depth of encoder and decoder.  time  refers to the total decoding  time on all the testsets and  speed w s   denotes the decoding speed measured by word per second. the  avg.  is  short for the average bleu score.     .     system  rnmt   kd  trans. teacher   hybrid dot attn   kd  hybrid multi attn   kd  trans. teacher   hybrid dot attn   kd  hybrid multi attn   kd    results and discussions    table   shows the decoding speed and bleu  scores of different models on nist datasets. for  each hybrid model  we use transformer model  with corresponding layers as the teacher. from  the table    all our hybrid models are faster  than transformer and the   layer rnmt model.  specifically  our    and   layer hybrid models  achieve significant speedup with factors of  . x  and  . x compared with the   layer and   layer  transformer teachers. we can find that the time  cost of the three different attention models is very  close. this is mainly due to the pre computation  and weight combination. as for translation performance  both transformers and the hybrid models outperform the rnmt and rnmt kd. with  the help of sequence level knowledge distillation   all the hybrid models achieve significant improvements and even get comparable results with the  transformer.  we further verify the effectiveness of our approach on wmt      chinese english translation  tasks. results are shown in table  . similar with  the above results  our hybrid models can get  . x  and  . x speedup compared with   layer and  layer transformer  and with help of the knowledge  distillation  our models achieve comparable bleu  scores with the transformer.  our offline experiments show that the hybrid  model improves when the rnn decoder becomes  deeper          layers   but with slower decoding  speed. however  after applying kd  they get sim     n                                                                time     s     s     s     s    s     s     s     s     s     s     s     s    speed w s      .           .      .      .      .      .      .      .      .      .      .     test    .      .      .      .      .      .      .      .      .      .      .      .      table    decoding time and case sensitive bleu  scores     for wmt      chinese english task.    ilar bleus as the hybrid model with single layer  decoder.         conclusion    in this work  we propose a hybrid network to accelerate transformer decoding. specifically  we  use a self attention network as the encoder and  a single layer rnn as the decoder. our hybrid  models fully take advantage of the parallelization  capability of self attention and the fast decoding  ability of rnn based decoder. in addition  to improve the translation quality  we firstly pre train  our model using the mle based method  and then  the sequence level knowledge distillation is used  to fine tune it. experiments conducted on nist and  wmt   chinese english tasks show that our hybrid network gains significant decoding speedup   and achieves comparable translation results with     the strong transformer baseline.    