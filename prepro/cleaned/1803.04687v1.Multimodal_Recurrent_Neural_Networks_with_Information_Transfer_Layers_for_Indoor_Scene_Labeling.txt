introduction    d    ata comes from different sources and in different forms   images  videos  texts and audios. each of which may  complement the other in information content. thus  multiple  data modalities are usually more informative for a task than a  single data modality. with the enormous availability of various  electronic and digital multimedia devices  huge volumes of  multimodal data contents are being generated on the internet  daily. however  for real world applications  these modalities  should be first well integrated and appropriately fused to have  more comprehensive information contents. many methods  have been developed in multimodal learning to exploit both  the different characteristics as well as the shared relationships  between different data modalities in order to perform various  tasks                                  . one of the rigid milestones in developing many visual based data applications is  scene understanding. it is mainly applied to understand the  a. h. abdulnabi is working with both the rapid rich object search   rose  lab at nanyang technological university  singapore  and the advanced digital sciences center  adsc   illinois at singapore pt ltd  singapore  email  abrarham    ntu.edu.sg. b. shuai is with rose  z. zuo is  with rose  l. chau and g. wang are with the department of electrical  and electronic engineering  nanyang technological university  singapore   emails  elpchau ntu.edu.sg and wanggang ntu.edu.sg respectively. address  of adsc  advanced digital sciences center    fusionopolis way  illinois  at singapore  singapore       . address of rose  the rapid rich object  search lab  school of electrical and electronic engineering  nanyang technological university  singapore        .    fig.  . an example scene image that have both the rgb image and the  depth plane. the corresponding depth information can be utilized to better  differentiate ambiguous appearance in the rgb image.    contents of an image or video prior performing the target  task  e.g. large scale video retrieval      . imperatively  scene  labeling  i.e  semantic segmentation or image parsing  is a  crucial part of understanding an outdoor or indoor captured  scene image. the task here is to classify each pixel into its  semantic category  belonging to some object or stuff  in an  input scene image.  in our paper  we tackle the problem of rgb d indoor  scene labeling where we process two different data modalities   rgb color channels and depth planes. indoor rgb d scene  labeling is one of the most challenging visual classification problems           . many applications are built on  understanding the surrounding scenes  e.g. social behavior  understanding      and objects detection and recognition     .  this problem is usually addressed as a multimodal learning  problem where the task is to exploit and fuse both rgb and  depth modalities to better label each pixel. depth planes provide informative representation where the rgb representations  are ambiguous. for example  figure   show how the depth  information can help to distinguish some similar appearance  locations in the rgb image.  scene labeling in general is a challenging classification  problem since the scene image tends to contain multiple messy  objects. these objects may also have variations due to factors  affecting their appearance and geometry in the image. one  key useful strategy is to leverage the neighborhood contextual  information for each pixel within each modality                      . typically  the feature representation of a pixel is extracted  from a local patch  cropped from the scene image  containing  that target pixel and used for classification. long range global  contextual information  distant image patches  are important  as well for local pixel classification. however  both local and  global contextual information should be utilized adequately to          maintain a good balance between the discriminative features  and the abstract top level features of that pixel feature representation.  recently  recurrent neural networks  rnn  has been  shown to be very successful on encoding contextual information into local feature representations                . recurrent  models have feedback connections so that the current state  is engaged in the calculation of the future state. thus  rnn  is effectively used in tasks which require modeling the long  and short range dependency within the input sequence  e.g.  speech recognition and natural language processing                        . we use rnns to model the contextual information  within each modality. however  traditionally  rnn is only  used for a single modality signals. in this paper  we introduce  a new multimodal rnns method which models contextual  information into local representations from multimodal rgbd data simultaneously. in our work  we first train convolution  neural networks       cnns  to extract features from local  rgb d image patches  from both the rgb images and the  depth planes . these convolutional local features form the  input to our multimodal rnns to further contextualize them  and select informative patterns across the modalities. our  model can be easily extended to perform prediction tasks  considering more modalities      .  our new multimodal rnns method is built based on the  basic quad directional  d rnns structures           . the  quad directional  d rnn contains four hidden states where  each is dedicated to traverse the image in specific  d direction  out of four possible directions  top left  top right  bottom left  and bottom right . to process two modalities which are the  rgb image and the depth plane  our model has two rnns   one of which is assigned to learn the representations of a single  input modality. to connect both rnn modalities and allow  information fusion  we develop information transfer layers  that crossly connecting the rnns. the transfer layers are  responsible about learning to select and transfer the relevant  patterns from one modality to the other modality  and vice  versa. concretely  for every patch in the input image  and  during the process of learning the rnn hidden representations  for one modality  our method does not only encode the  contextual information within its own modality  but also learns  to encode relevant contextual patterns from the other modality.  as a result  our method can learn powerful context aware and  multimodal features for local pixel representations.  our method is different from existing deep multimodal  learning methods                       . they usually concatenate inputs at the beginning or concatenate learned middle level features to extract high level common features as  the representations of the multimodal data. these methods  mainly focus on discovering common patterns between different modalities. although common patterns are important to  extract  however these methods are prone to miss important  modality specific information that are highly discriminative  within a single modality  e.g.  some texture patterns inside  the rgb channels. concretely  our model retains modalityspecific information by assigning an rnn model to learn  features from each modality. our method also allows sharing information between modalities by using the information    transfer layers to adaptively transfer relevant cross modality  patterns.  our model is trained end to end and the transfer layers  are learned to extract relevant across modality information for  each patch of the image. we perform experiments on two  popular rgb d benchmarks  the nyu v  and v  and achieve  comparable performance with other state of the art methods  on them. additionally  the proposed method significantly outperforms its counterparts baselines  e.g. concatenation of the  rgb d data as the input of rnn models .  the remaining parts of our paper are summarized as follows  we first discuss the related work in section ii. our proposed model alongside the framework details is presented in  section iii. experiments on popular rgb d benchmarks and  results are demonstrated in section iv. finally  we conclude  the paper in section v.  ii. r elated w ork  because this work is mainly related to rgb d scene  labeling and rnn  we briefly review the most recent  literature.  indoor rgb d scene labeling  many papers propose  different methods to solve rgb d scene labeling                                   . the most popular indoor scene datasets  are the nyu depth datasets v       and v      . the initial  results      are generated by extracting sift features on  the rgb color images in addition to depth maps. their  results prove that depth information can refine the prediction  performance.  further improvements are made to the nyu v  by the  work       where they adapt a framework of kernel descriptors which converts the local similarities  kernels  to patch  descriptors. they further use a superpixel markov random  field  mrf  and a segmentation tree for contextual modeling.  other works           explore depth information through  a feature learning approach      learns their features using  convolution neural network on four channels  three from the  rgb image and the fourth is the depth image. wang et  al.      adapt an unsupervised feature learning approach by  performing the learning and encoding simultaneously to boost  the performance. another interesting work done by tang et  al.      proposes new feature vectors called histogram of  oriented normal vectors  honv   designed specifically to  capture local geometric properties for object recognition with  a depth sensor.  meanwhile some works in rgb image parsing utilize the label dependency modeling approaches  such as graphical models  conditional random fields crfs        and other works  perform feature learning to generate hierarchical multiscale  features that are capable of capturing the input context  which  is successfully applied through the aid of deep convolutional  neural networks           . in our multimodal recurrent neural  networks  we can capture short and long range context within  and between input modalities.  multi modal learning  in contrast to single view approaches  most multimodal learning methods introduce a separate function to model one data modality and jointly optimize          fig.  . illustration of the one dimensional   d  rnn  right side  and  multidimensional  e.g. two directional   d rnn  left side      . the key  idea is to replace the single recurrent connection in the  d rnns with as  many recurrent connections as there are dimensions in the input data.    fig.  . the top part illustrates the concept behind the multi modal multiview learning method where usually different data modalities are combine  by various techniques to exploit all information sources. multi modal is also  efficient to combine and fuse the rgb color channels with depth planes   bottom part     them together                      . deep networks have been  applied to learn features over multiple views                        . multimodal learning improves the classification  performance through exploiting the sharable knowledge of the  data across modalities.  figure  shows the general method of how multimodal  learning is applied. notice that the key idea here is to allow  fusion sharing between the modalities at some point so that  the join space is able to capture the relationships between  the input modalities. typical methods in the literature can  be categorized based on whether they combine on feature  level                  or classifier level           . some  works perform preprocessing  module level  steps on various  modalities to generate helpful cues before the learning process   as in                 . fusion can be done by combining  all features into one high dimensional vector  or by jointly  training multiple classifiers to maximize the mutual agreement    on distinct modalities of the input data           . these  methods employ typical regularizations which are applied to  explore shared visual knowledge  such as group structured  sparsity     .  we can also group multimodal methods according to their  training procedures into three main categories  co training              multiple kernel learning             and subspace  learning                      . co training algorithms tend to  train alternatingly to maximize the mutual agreement on two  distinct views of data. multiple kernel learning approaches  improve the performance by exploiting different types of  kernels that correspond naturally to different views  and combine these kernels either linearly or non linearly. meanwhile   subspace learning methods are mainly similar in obtaining a  latent subspace that is shared by multiple modalities. another  interesting line of work proposed by gupta et al.       where  they propose to transfer supervision between images from different modalities. their model is able to learn representations  for unlabeled modalities and can be used as a pre training  procedure for new modalities with limited labeled data. in our  work and different from all previous methods  we introduce  information transfer layers between two rnn modalities to  perform the multimodal learning task simultaneously.  recurrent neural networks  rnns   a recurrent model  refers to a model which has connections between its units to  form a directed cycle  for example  when a feedback connection from the current state is engaged in the calculation of the  future state. this structure creates a sort of complementary  internal state for the network  which then allows it to exhibit  dynamic temporal behavior. rnn is effectively used in tasks  which require sequence modeling  like speech recognition   handwriting recognition  and other natural language processing  tasks                       .  one major drawback in the standard rnn is the vanishing  gradient problem     . this drawback limits the context range  of the input data  because the capacity of the model is  limited to capture enough long dependencies. to address this  problem  hochreiter and schmidhuber      propose the long  short term memory  lstm   where they treat the hidden  layer as multiple recurrently connected subnets  known as  memory blocks  thus allowing the network to store and access  information over long periods of time. graves et al. extend  the idea of unidirectional lstm network into bidirectional  networks which have shown good improvements over the  unidirectional networks           .  graves et al. also extend the one dimensional rnn into  multidimensional one            as shown in figure . the          key idea is to replace the single recurrent connection in the   d rnns with as many recurrent connections as there are  dimensions in the input data. another interesting work from  graves et al. investigates deep structures of rnns       which  is also successfully applied in opinion mining by irsoy et al.      . in our work we evaluate both deep and lstm structures  alongside the basic quad directional  d rnns. we found  that there is no significant difference in the label prediction  performance before and after stacking multiple layers of the  network or engaging the lstm units with the basic quaddirectional  d rnns. thus  we only show the results of our  multimodal rnns model using the basic quad directional  drnns.  iii. m odel f ramework  we first extract convolutional features from local rgb d  patches using our trained cnn models. then  our multimodalrnns are developed to further learn context aware and multimodal features based on the convolutional features. afterwards  a softmax classifier is trained to classify each patch  into its semantic category. different from traditional single  modality rnn  our multimodal rnns also have transfer layers to learn to extract relevant contextual information across  both modalities at each time step. below  we first introduce  the traditional rnns.   d rnn and  d rnn  the popular elman type  d rnn       and its  d version are designed to capture the dynamic  behavior of the signal over time  so that the hidden representations can capture the contextual information from the first time  step until the current time step. its forward pass is formulated  as the following   h t    f  u x t    w h t      b   y  t    g v h t    c     y  i j    g v h i j    c     fig.  . a conceptual illustration to show our principal design in our proposed  multimodal  d rnns. this illustration is based on simple  d rnns and  intended to clarify the design structure of two crossly connected  d rnns  through the introduced transfer layers. in our model  since we process  d  input signals  images  we use  d rnns instead of  d rnn to retain the  spacial dependencies within each scene image.           where x t    y  t  and h t  are the input  output and hidden  neurons at the time t respectively. the functions f  .  and g .   are element wise non linear functions with bias terms b and c   and the matrices u   w and v are the input to hidden  hidden  to hidden and hidden to output weights  respectively. the  drnn                  is generalized from the  d rnn so that  the data propagation will come from two dimensional neurons  instead of one  thus the formulation becomes   h i j    f  u x i j    w h i   j    w h i j      b     fig.  .  d rnn structure is a generalization of  d rnn. a visualization  of processing patches in  d image is shown. the scan is only presented in  one direction  top left . here  the previous hidden states from the top and  the left sided are encoded into the current hidden state  i.e. the previous top  and left neighbored hidden state information are engaged in calculation of  the current network state alongside the current patch at the same location. an  image is approximated through four directions  top left  bottom left  top right  and bottom right accordingly. thus  we use the quad directional  d design  to accumulate information from all sides.           we can notice now the propagation is in a  d plane from  top left regions and continues to flow until the end of the  d  sequence  in our case the image patch sequence   where  i  j   denotes the location of pixels or patches.  we show first a conceptual illustration of our proposed  model using  d rnns as shown in figure  . concretely  in  this paper  we adapt  d rnn to learn hidden representations  for local rgb d image patches. figure   shows one scan direction using  d rnn  which scans only the top left sequence.  scanning the image in only one direction leaves some patches  in the top left sequence without being informed of the contextual information from the bottom right patches during the    forward pass of the testing images. we approximate images  using four directional  d sequences of patches following the  work     . the other three directions are top right  bottom left  and bottom right sequences. we combine the features that are  learned from these four directions to obtain the final features  of the image patches.  a. multimodal rnns via information transfer layers  traditional rnns are developed to model single modality  signals. in this work  we extend rnns to represent rgbd signal by our multimodal rnns  where we have a pair  of single rnns and each of them is assigned to process  one modality  either rgb or depth . besides  we propose a  transfer layer to connect the hidden planes in one rnn model  and the other rnn model s hidden planes and vise versa. this  transfer layers will learn to adaptively extract relevant patterns  from one modality to enhance the feature representations of  the other modality. if the depth rnn is processing the depth  patch in the sequence at location  i  j   the other rnn  which  is rgb rnn  will be processing the corresponding rgb  patch at the same location. the depth rnn is also fed with  the processed hidden state values obtained from the rgbrnn and vice versa. the rgb d data flows concurrently          in both models where their internal processing clocks are  synchronized  so that at each time step we process a pair of  rgb and depth local patches simultaneously.  the architecture of our model is summarized in figure  .  it is an end to end learning framework. recurrent layers and  transfer layers are automatically learned to maximize the  labeling performance on the training rgb d data. compared  to the baseline which concatenates rgb d data and thus mixes  the multimodality information  both relevant and irrelevant    our method retains modality specific information and only  shares relevant cross modality information.  given one  d direction in the rnn  the first hidden plane  from the quad directional hidden planes   as shown in figure    the current state of the network that is being processed  at  i  j  depends basically on four main previous states. two  are obtained from the network itself and the others are obtained  through the transfer layers from the previous states processed  in the other modality network  in addition to the input patch  features from either the rgb or the depth images at location   i  j . both networks are synchronized and process the input  modalities simultaneously.  given an rgb image ic where c refers to  color  and  is processed by rgb rnn and a depth image id where d  refers to  depth  and is processed by depth rnn  in this  paper  we first extract multiple patches from the images and  generate their corresponding convolutional feature vectors to  form the input to our multimodal rnns model. we denote  the corresponding convolutional feature maps as xc or xd for  each patch in ic or id . concretely  the forward propagation  formulation to process one hidden plane out of the four  quad   directional hidden planes is as the following     fig.  . an abstract overview of our multimodal rnns model which consists  mainly of an rgb rnn and a depth rnn. the rgb rnn will process a  patch from the rgb input plane in addition to its own previous hidden states  and the selections made by the transfer layer from the previous hidden states  of the depth rnn. the depth rnn will also process the corresponding patch  from the depth input plane in addition to its own previous hidden states and  the selections made through the transfer layer from previous hidden states of  rgb rnn. for the simplicity in this figure  we only show one hidden plane  from the quad directional rnn hidden planes. every hidden plane is crossly  connected to its peer in the multimodal rnns through information transfer  layers.    h i j     f  uc x i j     wc h i   j     wc h i j     c  c  c  c   i   j       sc hd   i j     hd     i j       f  ud xd     i j         sc hd      bc       i   j       wd hd     i j         wd hd      sd h i   j     sd h i j       bd    c  c           zc i j    g vc hc i j    cc     i j     zd     i j      i j       g vd hd      cd      where xc  is a feature vector of a certain patch at location   i j    i  j  in the rgb image and xd  is a feature vector of   i j   a certain patch at location  i  j  in the depth image. hc   i j   and hd  are the hidden sates inside each rgb rnn and  depth rnn respectively. the weight matrices uc and ud  are responsible to for input hidden mapping in rgb and  depth modalities respectively. in the other hand  here we have  two types of hidden hidden transformation matrices  withinmodality hidden hidden transformation and across modality  hidden hidden transformation. wc and wd are within modality  hidden hidden mapping inside the rgb rnn and the depthrnn respectively. meanwhile  sc is a transformation weight  matrix to transform features from the depth to rgb hidden  states  from the depth rnn modality hidden state to the  rgb rnn modality hidden state . sd is a transformation  weight matrix to transform features from the rgb to depth  hidden states  from the rgb rnn modality hidden state    to the depth rnn modality hidden state   i.e. these weight  matrices act as the information transfer layers that crossly  connect the rgb and depth hidden states. ultimately  the  vc and vd are the hidden output transformation matrices in  each corresponding modality.  sc and sd are learnt to extract shared patterns between the  modalities. notice that the weight matrix that transforms from   i   j   the top side hidden state in the rgb rnn hc  and the  weight matrix that transforms from the left side hidden state   i j     hc  are shared which is wc . similarly  wd in the depth i   j    i j     rnn is shared  transforms from hd  and from hd   .  also the case in the transfer layers  sc and sd are shared. in   i   j    i j     detail  sc transforms from hd  and from hd  . while   i   j    i j     sd transforms from hc  and from hc  .the nonlinear  function f  .  is relu max x     in our implementation. the  function g .  is the typical softmax and cc and cd are biases.  since we adopt the quad directions  the forward pass is  similar to equation   and in addition to the remaining directions beside the top left sequence  i.e. top right  bottom left  and bottom right. to facilitate readability of the equation and  to easily distinguish between all quad four directions  we will  use arrow notations to represent different directions.   refers  to the top left processing sequence  . indicates the top right  sequence    is the bottom left and finally   is the bottom           right sequence. now  the full model forward propagation pass  in the rgb rnn becomes   h i j       f  uc  xc i j    wc  hc i   j     wc  hc i j       c    sc  hd     i   j        sc  hd     i j          b   c      h i j   .   f  uc. xc i j    wc. hc i   j .   wc. h i j     .  c  c    sc. hd     i   j     .  sc. hd     i j       .  b.  c      h i j       f  uc  xc i j    wc  hc i   j     wc  h i j        c  c    sc  hd     i   j        sc  hd     i j          b   c             h i j       f  uc  xc i j    wc  hc i   j     wc  h i j     c  c    sc  hd  zc i j      i   j    i j        sc  hd     bc       i j   .  i j      i j     g vc hc     vc hc  .  vc hc    vc  h i j    c     c  c    sequences . by this method  the processing of a specific  patch will rely on both previous hidden neighbors from its  own modality in addition to the other modalities  thus learns  more contextually aware hidden representations of the rgb d  image patches from both modalities.  our labeling task is a typical supervised classification  problem. we aggregate the cross entropy losses from both  modalities and calculate the loss for every patch. the error  signal of an image is averaged across all the valid patches   those that are semantically labeled   which is mathematically  formulated as the following   l           as mentioned  we use four quad directional  d rnn to  approximate each image. the arrows    .       indicate the  quad directions  top left  top right  bottom left and bottom i j    i j    i j    i j   right   and the hc      hc  .   hc    and hc  are the corresponding four hidden planes. each hidden plane  has its own weight matrices besides the introduced transfer  layers sc   e.g. uc   input hidden mapping   wc   hiddenhidden mapping   sc   the transfer layer from depth rnn  hidden plane to rgb rnn hidden plane   vc   hidden output  mapping  and the bias term which is denoted by b   c . note that  the connection between the two hidden planes hd   and the  corresponding hc   is weighted by the corresponding weight  matrix sc    which is leaned to extract shared patterns between  the modalities.  simultaneously  the forward pass in the depth rnn model  is as follows    i j     hd        f  ud  xd     i j       wd  hd     i   j        wd  hd     i j         sd  h i   j      sd  hc i j       b   c  d     i j      i   j    i j   .  i j   .  wd. hd  hd .   f  ud xd   wd. hd  .  sd. hc i j    .  b.    sd. h i   j   c  d     i j      i j        i   j      i j      wd hd  hd     f  ud xd   wd hd     i   j      i j       s d hc     sd hc     b   d     i j    i j    i   j    i j     hd     f  ud  xd   wd  hd     wd  hd    sd  h i   j      sd  hc i j       bc  d     i j      i j   .  i j      i j   zd    g vd hd    vd hd .  vd hd     i j     vd  hd    cd     i j        .                   where xd is the feature vector of a certain patch at location   i j    i j    i j    i  j  in the depth image. the hd     hd     hd  .   i j   and hd   are the quad hidden planes in the depth rnn.  each hidden plane accompanies its own weight matrices ud    input hidden mapping   wd   hidden hidden mapping   sd    transfer layer from rnn rgb hidden plane to depth rnn  hidden plane   vd   hidden output mapping  and its own bias  term b   d . the remaining terms in the quad planes are similar  to the case of the first hidden plane. the function f  .  is  a nonlinear relu unit and the function g .  is the typical  softmax and cd is a bias term.  we can notice that the cross connections through the transfer layers are applied in each processing direction  four    n  b    xx   log zcn  b     y n   b    log zdn  b     y n   b    n n    b           where      is the indicator function  n is the total number  of patches and b is the number of semantic classes  y n is  the ground truth label for the rgb d patch representation   which is the same as that of the center pixel  zcn and zdn  are the class likelihood generated from both the rgb rnn  and the depth rnn for the patch representations xnc and xnd  respectively  where they are b dimensional vectors. note that  we ignore the contribution of unlabeled  invalid  patches in  the loss calculation.  optimization of the multimodal rnns model  to learn  the multimodal rnns parameters in equations   and    we  optimize the objective function in equation   with a stochastic  gradient based method. both rgb rnn and depth rnn are  optimized simultaneously using back propagation through  time  bptt      . we unfold both networks in time and  calculate their gradients which are back propagated at each  time step throughout both networks. this is similar to the  typical multilayer feed forward neural network propagation   but with the difference that the weights are shared across time  steps defined by the architecture of the recurrent network.  the whole model is differentiable and trained end to end.  the propagation of the gradients through the rgb rnn and  depth rnn is simultaneous.  in more detail  we provide the first plane  top left sequence   backward pass in the rgb rnn. the backward pass derivations of the remaining sequences  the remaining three planes   can be easily inferred by following similar derivation strategy  as we will explain next in the backward pass formulations  for the first plane  but with considering the change of the  sequence directions  top right  bottom left and bottom right  sequences . similarity  the total derivations for the depth rnn  are straightforward and exactly the same as for the rgb rnn  but with swapping the notations of c and d and vise versa.  in the rgb rnn top left sequence  we generate the gradients of the loss function in equation   by deriving it with  respect to the model internal parameters  i.e. top left sequence  weight matrices uc    wc    sc    vc    b   c and cc .  notice that since the weight matrix wc  is shared between   i   j     i j     hc  and hc    and the transfer layer weight matrix   i   j    i j        sc is shared between hd    and hd     hence  we will rewrite the forward pass as the following to further  facilitate the understanding of the weight sharing concept and  the backward pass           h c     h i   j      h i j        c  c   i   j     i j     h d     hd    hd      i j      i j        hc     f  uc xc   wc hc    sc  h d    b   c    zc i j    g vc  h i j      cc    c         notice that the other remaining terms that present orig i j    i j   inally in equation    i.e. vc. hc  .  vc  hc     and     i j   vc hc     do not engage any of the internal parameters  of the top left sequence  hence we omit them.  before we formulate the backward pass  figure   show an  illustration of the forward and backward passes in the topleft plane sequence. notice that the derivatives which are  computed in the backward pass at each hidden state at some  specific location  i  j  are processed in the reverse order of  forward propagation sequence.  for better readability  we ignore the south east arrow  sign   in the following derivations for the top left sequence.  notice that now there are two types of error signals  direct   i j    zc  one which is reachable directly through the loss    i j      hc  and indirect ones  i.e. the error signals that are coming from  neighboring future states at  i      j  and  i  j      locations    i   j    i j      i j      i   j    hc   zc   hc   zc      .     i   j    i j    i j      i j    hc   hc   hc   hc  concretely  and given that the derivative of loss function  l with respect to the softmax output function g is g    .      i j    i j    l  zc  .    hc        similarity  f   .        hence at loca i j    g   f   zc  .   tion  i  j  the backward pass of the rgb rnn  derivations  w.r.t. all internal parameters  is formulated as the following    vc   g    zc i j    h i j    t  c  dh i j     vct g    zc i j      wct dh i   j     f    h i   j      c  c  c    wct dhc i j      f    h i j        c   uc   dh i j     f    h i j     x i j    t  c  c  c   wc   dh i   j     f    hc i   j    h i j    t  c  c    dh i j       f    h i j       h i j    t  c  c  c            i j  t     sc   dh i   j     f    hc i   j    hd  c          i j     dh i j       f    h i j       hd  t  c  c   bc   dh i j     f    h i j      c  c     i j    cc   g  zc     i j   dhc    the term  is defined to allow the model to propagate  local information internally. the   sign is the hadamard prod i j   uct  element wise . we can notice through the term dhc that  there are two sources of gradients  one is generated directly  from the current hidden state and the other generated indirectly  from the bottom and right locations  corresponding to the three   i j   terms  direct error  vct g    zc    indirect error from neighbor   i   j    i   j   bottom location  wct dhc    f    hc    and indirect from   i j      i j     right neighbor location  wct dhc    f    hc    . notice    fig.  . an illustration of the forward and backward passes in the top left  plane sequence in both rgb rnn and depth rnn.  best viewed in color     that these two types of error signals  direct and indirect   are due to the weight sharing effect. it is also important to  mention that in our derivations we show the error signals  coming from the direct bottom and right neighbors  however   in real implementations the equations are recurrently applied to  allow propagation of the errors recursively  recurrently  from  all potential future neighbors until we reach out the current  referenced patch at specific location.  similarly  the backward passes for the remaining three  planes in the rgb rnn are straightforward to be derived following equation   but with paying attention to the directions  of the processing sequence. additionally  the backward pass  in the depth rnn follows exactly similar derivations but with  swapping the notations of c with d and vise versa. notice that  the derivation w.r.t cc is the same in the other remaining three  planes sequences.  to recap  we adopt bptt to train both basic quaddirectional  d rnn  rgb rnn and depth rnn . we use  the cross entropy loss in our implementation and the errors  are calculated by chain rule as we described previously in  detail.  iv. e xperiments and r esults  a. datasets  we evaluate our model on the benchmark dataset nyu  versions   and             . the nyu v  dataset contains       rgb d indoor scene images labeled with around     categories. the nyu v  is also comprised of video sequences  from indoor scenes as recorded by both the rgb and depth  cameras from the microsoft kinect. it contains      densely  labeled pairs of aligned rgb and depth images. we follow  the settings in       where the first task is to predict the pixel          label out of four semantic classes  ground  furniture  props  and structure. the second task is to predict the label out of     categories. the accuracy is calculated in terms of total pixelwise  average class wise and intersection over union  average  iou  among the semantic classes for comparison.                 b. cnns and rnns training  we train our convolutional neural networks  cnns  on both  nyu version   and   images. we follow the network structure  proposed by      without considering any spatial information  channels  rgb locations . we also didn t perform any hybrid  sampling as they mention in their implementation details.  the cnn model consists mainly of three convolutional layers   where in between there are max pooling and relu layers   and two last fully connected layers followed by an b way  softmax loss layer. in more details  conv            or           max pool         conv                    max pool          conv                    max pool         fc              and fc       b . the cnn models are trained on          patches associated with their centering pixel labels. each  image contains      patches. each cnn model produces a     dimensional vector per patch for each modality. we use  these cnn features as the input of our rnn models.  in cnn training  we start by a learning rate of  .     and decrease it every   epochs by     divided by    .  the momentum is initialized as  .  and remains the same  throughout the training. we perform the typical normalization  as a preprocessing step of images  we subtract the mean image  and divide by the standard deviation. the cnn models nearly  converge after    epochs  around   hours on nvidia tk    gpu   however  we iterate the models in most cases until      epochs.  we train our multimodal rnns using bptt  back propagation through time . we adapt stochastic gradient descent   sgd  throughout our training. we initialize the learning rate  as  .      and decrease it by  .   after each epoch. the  momentum is set to  . . the internal parameter dimension  of the networks is set to   . we apply gradient clipping            and set the threshold value to           however  even if  the relu can potentially cause gradient explosion  it plays  a critical role in rnn to mitigate the gradient vanishing  problem. thus  we mainly use relu in all of our rnn  models. the other rnn model parameters are initialized either  by randomly or zeros. notice that we didn t consider any  pixels that have zero labels in our training  we held them  out. our model converges much faster compared to the other  competitive baselines. it converged at almost the similar speed  of a single rnn model  with the benefit of processing multiple  input modalities simultaneously.  c. baselines  to show the effectiveness of our proposed model  we  develop the following baselines for comparison alongside our  proposed model     cnn rgb  in this baseline  we train cnn based on  rgb images  input is three rgb channels  for label  prediction.                                       cnn depth  we train cnn as in  cnn rgb  but using  depth images only  input is one depth channel .  cnn rgbd  we train cnn as in  cnn rgb  with  extra depth images  input is four rgb d channels   similar to the work presented in    .  rnn rgb  in this baseline  we follow the structure of  the quad directional  d rnn proposed by     . we use  the model in  cnn rgb  to extract rgb features. we  only input the rgb features to train the rnn for label  prediction.  rnn depth  we extract depth features using the trained  model in  cnn depth  and use these features to train  quad  d rnn as in  rnn rgb . we only input the  depth features to train the rnn for label prediction.  rnn rgbd  we use the trained model in  cnnrgbd  for feature extraction. we use these rgb d  features to train quad  d rnn similar to  rnn rgb   baseline to perform label prediction.  rnn classifiers combined  or as we call it  postfusion   here we train rgb rnn and depth rnn for  label prediction and combine their classification scores on  the classifier level. we use the trained models in  cnnrgb  and  cnn depth  for feature extraction.  rnn features combined  or  pre fusion   here we use  both trained models in  cnn rgb  and  cnn depth  for  feature extraction. we concatenate both rgb and depth  features  to form higher dimensional feature vectors  and  train one quad  d rnn similar to  rnn rgb  baseline  to perform label prediction.  rnn hiddens combined  or  middle fusion   we fuse  the hidden representations of both rnns just before  classification and train them jointly.  multimodal rnns ours in this sitting we implement  our proposed multimodal rnns structure. here  we have  two internal rnn models one is responsible for processing the rgb features and the other is responsible for  processing the depth features. both models are optimized  simultaneously using bptt where we combine their  classification scores to finally obtain the label map per  rgb d image.  multimodal rnns ours multiscale  similar to our   multimodal rnns ours   but in this setting we applied  our proposed structure on multiscale convolutional features as proposed by     . we follow similar training  sittings to train multiscale cnn models on different  image sizes  then we use them to extract multiscale  features. we concatenate these features together to form  our final input patch representation to our multimodalrnns.  we also compare our results with other state of the art  methods.    d. results  given the input rgb images and their corresponding depth  images  we divide each image into non overlapping patches  of size        . the extracted local cnn features are used  as the input of our multimodal rnns. the rgb rnn and          algorithm  cnn rgb  cnn depth  cnn rgbd  rnn rgb  rnn depth  rnn rgbd  rnn features combined  rnn hiddens combined  rnn classifiers combined  multimodal rnns ours  multimodal rnns ours multiscale  wang et al.       gradient kdes       color kdes       spin surface normal kdes       depth gradient kdes       silberman et al       pei et al.       ren et al.       eigen et al.         pixel acc    .       .       .       .       .       .       .       .       .       .       .       .       class acc    .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       iou    .       .       .       .       .       .       .       .       .       .       .          table i  r esults on nyu v  dataset     . t he performance of all  baselines and other state   of   the   art methods is measured on  total pixel   wise   average class   wise accuracy and average  iou   intersection over union   among    semantic classes . t he  higher the better .    the depth rnn models process rgb and depth local patch  features respectively.  results on nyu v       categories  in this dataset the  task is to predict pixel labels out of    categories plus an  unknown category. table i shows the results of our baselines  alongside the multimodal rnns model and other state ofthe art methods. rnn models outperform cnn baselines by  a large margin. and our multimodal rnns further improve  the accuracy over the rnn baselines. our model achieves  comparable results with the other state of the art methods. the  accuracy comparison with all baselines shows the effectiveness  of our method with the proposed transfer layers. figure   show  some qualitative results generated by our method and the most  competitive baseline method  rnn features combined . our  multi modal rnns model can correctly classify many misclassifications results compared to the baseline in most cases.  results on nyu v      categories  for example  eigen  et al.      achieve higher results than our model on this  task  while we outperform their model on nyu v  task.  their multi scale cnn structure is also well designed to  address the problem. however  their network appears to be  much more complex than ours as they have higher number  of computational operations  way more convolutions . but  we still believe our work can be potentially orthogonal and  complementary to their method if it is trained end to end with  their method.  likewise  couprie et al.       use many types of features   including sift features  histograms of surface normals   d  and  d bounding box dimensions  color histograms  relative  depth and their support features. in our model  we only use the  transfer layers alongside the basic quad  d rnn structures   and can achieve much better performance. figure   also show  some qualitative results generated by our method and the most  competitive baseline method  rnn features combined . our    algorithm  cnn rgb  cnn depth  cnn rgbd  rnn rgb  rnn depth  rnn rgbd  rnn features combined  rnn hiddens combined  rnn classifiers combined  multimodal rnns ours  multimodal rnns ours multiscale  wang et al.       couprie et al.      stuckler et al.       khan et al.       mueller et al.       gupta et al.       cadena and kosecka      eigen et al.         pixel acc    .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       class acc    .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       iou    .       .       .       .       .       .       .       .       .       .       .       .          table ii  r esults on nyu v  dataset     . t he performance of all  baseline and other state   of   the   art methods is measured on  total pixel   wise   average class   wise accuracy and average  iou   intersection over union   among   semantic classes    g round   f urniture   p rops and s tructure . t he higher the  better .    multi modal rnns model can correctly classify many misclassifications results compared to the baseline in most cases.  results on nyu v       categories  on the nyu v    we evaluate our model to label image pixels with one of     categories plus an unknown category. we show the comparison  in this task with various baselines and state of the art methods  as shown in table iii. this is the most competitive benchmark  presented on this dataset. notice that the work of cadena  and kosecka     used many rgb image and  d features and  formulate the problem in the crf framework. meanwhile the  work of wang et al.      adapted an existing unsupervised  feature learning technique to directly learn features. they stack  their basic learning structure to learn hierarchical features.  they combined their higher level features with low level features and train linear svm classifiers to perform labeling.  compared to these methods  our model is much simpler and  achieve better performance.  figure    also shows some qualitative results generated  by our method and the most competitive baseline method   rnn features combined . our multimodal rnns model can  correctly classify many mis classifications results compared  to the baseline in most cases. we also show our per class  accuracy on this sitting  as shown in figure   . we can notice  that the improvement gain of our multimodal rnns over  cnn rgbd and rnn rgbd models is significant. this is an  evidence that our model can effectively learn powerful contextaware and multimodel features.  we also study the effect of increasing the dimensionality  of the internal hidden layer on single rnn performance. we  notice that rnn performs almost the same but slightly better  when the hidden layer dimension increases  while it becomes  extremely slow and takes a lot of time to converge. thus   we choose the dimension of the hidden layer to be   . all           fig.  .   example results from nyu v      semantic categories . columns  st and  nd  rgb color images and their corresponding depth images. column   rd  the prediction results of our most competitive baseline  rnn features combined  on the nyu v  four semantic categories. column  th  our multimodalrnns prediction results. column  th ground truth. our model is able to correctly classify many miss classified pixels compared with our baseline.  best  viewed in color     rnn models in our baselines can achieve good performance  and can converge in reasonable amount of time. figure     shows the relationship between a single rnn hidden layer  dimensionality versus the accuracy  in terms of global pixelwise  trained on the nyu v  to predict   semantic categories.  examining our multimodal rnns with other cnn  features   vgg features on nyu v       we also examine  our proposed multimodal rnns while replacing the input  cnn features by the extracted features from the vgg    pretrained model  extracted from conv    layer      . we focused  mainly on the most competitive task among our addressed    tasks  i.e. classifying the    classes in nyu v . the purpose  of these experiments is to validate whether our fusion structure  is a network independent model  and concretely orthogonal  to other cnn networks. in other words  replacing the cnn  models with more powerful network like vgg       resnet        fcns       dilated networks      and others can boost  the overall performance while the relative improvement of our  proposed cross connectivity fusion is maintained. throughout  all of our experiments  we observe that replacing our cnn  features with vgg features result in a constant overall  increase in the accuracies in all of our rnn models           fig.  .   example results. columns  st and  nd  rgb color images and their corresponding depth images. column  rd  the prediction results of our most  competitive baseline  rnn features combined  on the nyu v  four semantic categories. column  th  our multimodal rnns prediction results. column   th ground truth. our model is able to correctly classify many miss classified pixels compared with our baseline.  best viewed in color      including the baselines  of around    higher in terms  of iou  most competitive metric .  notice that in this paper  we didn t perform joint training  between the cnn layers and our multimodal rnn layers. in  contrast  we perform stage training  we first train the cnn  for feature extraction and then train our rnn model for final  local classification. this makes the performance comparisons  on the nyu v  tasks between our model and other stateof the art models not fair  many works perform end to end  joint training of various cnn or multi scale cnn and crfbased methods and even with rnn lstm . notice also that  end to end training with cnn can allow training of efficient  deconvolution layers to upsample the output feature maps    in order to restore their original resolution  while we use  simple bilinear interpolation to upsample our final output map  produced by the rnns. thus  in order to fairly examine the  full performance of our multimodal rnn model combined  with other types of recent cnn models like resnets        fcns      and dilated networks       a joint end to end  training is required. in this paper  we didn t perform this joint  training between the cnn and the rnn models as it is not  our main contribution  but we consider it as a very good future  work.  e. observations  from table i  iii and ii we have the following observations            fig.   .   example results from nyu v      semantic categories . columns  st and  nd  rgb color images and their corresponding depth images. column   rd  the prediction results of our most competitive baseline  rnn features combined  on the nyu v  four semantic categories. column  th  our multimodalrnns prediction results. column  th ground truth. our model is able to correctly classify many miss classified pixels compared with our baseline.  best  viewed in color            fig.   . per class accuracy comparison between our multimodal rnns  baselines cnn rgbd and rnn rgbd on nyu v  dataset         semantic  categories .    algorithm  cnn rgb  cnn depth  cnn rgbd  rnn rgb  rnn depth  rnn rgbd  rnn features combined  rnn hiddens combined  rnn classifiers combined  multimodal rnns ours  multimodal rnns ours multiscale  wang et al.       couprie et al.      hermans et al.       khan et al.         pixel acc    .       .       .       .       .       .       .       .       .       .       .       .       .       .       class acc    .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       iou    .       .       .       .       .       .       .       .       .       .       .          table iii  r esults on nyu v  dataset     . t he performance of all  baseline and other state   of   the   art methods is measured on  total pixel   wise   average class   wise and average iou    intersection over union   among    semantic classes . t he  higher the better .    modeling contextual dependencies between patches using rnns helps  the improvement gain achieved by our rnn  models over the cnn models is significant. cnn features  are locally learned when performing the convolutions and  thus fail to encode long range contextual information. rnns  are powerful on modeling short and long range dependencies  between patches within the image and can learn context aware  features effectively.  sharing information between rnns helps  we design  the baseline  rnn classifiers combined  that combines two  rnn models on the classifier level. our multimodal rnns  model outperforms this baseline as it benefits from the shared  contextual information extracted through the transfer layers.  learning transfer layers to connect rnns helps  the  baselines  rnn rgbd  and  rnn features combined  are  designed to mix rgb and depth data modalities together  before learning the features. our model outperforms these  baselines because it has an assigned single rnn for each  modality to retain the modality specific information. plus   the transfer layers are learned to adaptively extracts only the  relevant multimodal shared information.  v. c onclusion  this paper presents a new method for rgb d scene semantic segmentation. we introduce information transfer layers between two quad directional  d rnns. transfer layers extract  relevant contextual information across the modalities and help  each modality to learn context aware features that can capture  shared information. in our future work  we will evaluate the  effectiveness and the scalability of the transfer layers on more  modalities  modalities     .  copyright  c       ieee. personal use of this material is  permitted. however  permission to use this material for any  other purposes must be obtained from the ieee by sending a  request to pubs permissions ieee.org.    fig.   . the relationship between the hidden layer dimensions and the pixelwise accuracy in a single rnn model trained on the nyu v  dataset         acknowledgment  the authors would like to thank nvidia corporation for  their donation of tesla k   gpus used in this research at the           rapid rich object search lab. this research was carried out  at both the advanced digital sciences center  adsc   illinois  at singapore pt ltd  singapore  and at the rapid rich object  search  rose  lab at the nanyang technological university   singapore. this work is supported by the research grant for  adsc from a star. the rose lab is supported by the  national research foundation  singapore  under its interactive    digital media  idm  strategic research programme.  