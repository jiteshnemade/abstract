introduction    recurrent neural networks  rnns  have been widely applied in natural language processing tasks and have shown a proven ability to  process text sequences such as sentences. the effectiveness of rnns  is attributed to their exquisite network structure and the reuse of hidden layers  which enable rnns to process text sequences of arbitrary  length in theory  and memorize the semantics of text sequences encoded in the hidden layers. however  in practice the memory ability  of an rnn is limited by factors that are external or internal to the  network structure.    external factor  the length of input sequences is the external  factor that affects the memory ability of an rnn. it is not related  to the rnn s particular setting  but relies solely on the input data.  for example  in order to solve the gradient vanishing and exploding problem      that occurs on excessively long sequences  we  can manually shorten the length of input sequences to t by various means  such as removing stop words  or using a backpropagation through time  bptt  strategy     to limit the distance of  the gradient back propagation to t . consequently  the length of  sequences that can be memorized by an rnn would be no greater  than t .                tianjin key laboratory of cognitive computing and application  tianjin  university  tianjin  china  email  zccode gmail.com  department of information engineering  university of padua  padua  italy   email  qiuchili dei.unipd.it  school of computer science  beijing institute of technology  beijing   china  email  hualingyu hotmail.com  school of computer science  beijing institute of technology  beijing   china  email  dwsong bit.edu.cn  corresponding author       internal factor  the particular type of recurrent unit employed  by an rnn is the internal factor that affects the rnn s memory  ability. for instance  it was difficult to capture long term dependencies in a sequence with the early vanilla recurrent neural  network  vrnn        whereas the long short term memory   lstm  approach      and gated recurrent unit  gru      have  improved the memory ability through well designed gate structures.  therefore  the external and internal factors determine the upper  and lower bounds of an rnn s memory ability respectively. if a recurrent unit with limited memory ability  internal factor  is used to  process a long sequence  the rnn s hidden layer will be internally  constrained from effectively representing the semantics of the original sequence. it may also result in a reduced training speed. likewise   a recurrent unit with a powerful memory ability will not be able to  take its full advantage when it is used to process a short sequence   external factor .  currently  both factors are empirically adjusted as hyperparameters with respect to a specific task  but they may not always  reflect the actual memory ability of the chosen recurrent unit. the  lack of theoretical insights and effective ways to quantify the difference in memory ability between different types of recurrent units  largely limits the rational use of them and the development of new  recurrent units with a stronger memory ability. this paper aims to  fill the gap by developing a principled approach to measuring and  comparing the memory abilities of different recurrent units.  specifically  an rnn continually remembers the semantics of an  input sequence in the hidden layer through a recurrent unit  and then  performs a particular task based on the hidden layer. the hidden layer  is vital  as it not only memorizes the semantics of the original sequence  but also further adjusts the semantics to suit the target task.  therefore  the internal memory ability of an rnn is reflected by  the semantic difference between the original input sequence and the  hidden layer sequence at the processing time. in this paper  section      we define a range of evaluation indicators of memory ability to  quantitatively measure such semantic difference. further  we analyze  the relationship between the internal and external factors that affect  the memory ability  by observing how the values of these indicators  change with respect to varying input sequence length  as external  indicator .  the theoretical foundation of the above analysis is how to represent the semantics of a text sequence. semantics is concerned with  the relationship between signifiers such as words  signs and symbols   and what they stand for in reality     . words are semantic units that  convey meanings      and typically an n dimensional word vector  can be used to represent the semantics of a word. a sentence consists  of a sequence of words. the semantics of a word sequence should     contain not only the meanings of the individual words  but also the  meanings emerging from different sub sequences  word combinations  within the sequence. to this end  we propose  in section    an  n dimensional semantic euclidean space  denoted as srn    which  is extended from an n dimensional euclidean space of word embeddings  denoted as rn    to clarify the mapping relationship between  sequence semantics in srn and each point  word vector  in rn . furthermore  we propose a novel representation of sequence semantics  with a convex hull in srn   which provides a powerful mathematical  formulation and solid theoretical basis to assess the memory ability  of rnns. finally  extensive experiments are carried out  for an indepth theoretical and empirical analysis on the memory abilities of  different types of recurrent units. the results would provide a practical guidance for setting the most appropriate length of input sequence  that matches the memory ability of the recurrent unit used in an rnn.         related work    the research on the internal mechanism of rnns and comparison  between different types of recurrent units has drawn wide attention.  earlier work analyzed various abilities of rnns from a theoretical  perspective  such as the universal approximation ability      and the  generalization ability       but neglected analyzing the memory ability of rnns in natural language processing tasks. an empirical comparison of different types of recurrent units revealed that lstm and  gru is more effective than vrnn on specific tasks       but the  study failed to present the approximate sequence length that different types of recurrent units can memorize. karpathy et al.        explored the internal mechanism of lstm on a microscopic level  and  established a mapping between the neurons of hidden layers and the  content represented     . for example  one neuron can represent the  beginning and end of a sentence  while another may indicate a punctuation mark. on a higher level  the internal mechanism of rnns  was analyzed by hou and zhou             by modeling an rnn  as a finite state automaton  fsa . nonetheless  the problem of how  much information an rnn can memorize and how this could be used  to guide the setting of input sequence length  is yet to be solved.  there have also been works on evaluating the different abilities of  lstm  such as the ability of lstm to learn syntax sensitive dependencies         and context free grammars     . however  there is  still a lack of horizontal comparison among different recurrent units  in their memory ability.         measuring sequence semantics    the widely used word embeddings are commonly assumed to be an  n dimensional euclidean space  rn    in which the meaning of each  word in a pre defined vocabulary is represented by an n dimensional  vector. however  there is a difference between  semantic meaningfulness  and  correspondence to an actual word . for example  implicit or new meaning may emerge from a sequence of words. a vector representing such meaning may not necessarily associated to an  exact word. therefore  word embeddings only construct a small part  of the relationship between word vectors and semantics  limited to  a vocabulary of words. as a result  with rn it is difficult to capture the implicit semantics that are not corresponding to any existing  words in the vocabulary. for example  when dealing with the analogy problem a   b c   d  for which xd   xb   xa   xc needs to be  computed under a certain distance metric       there may not exist an  appropriate word d that corresponds to the derived vector representation xd . similarly  the vectors in the hidden layer of rnns that are    figure  . the sequence  european conference on artificial intelligence   in semantic euclidean space.    used to memorize the original sequence may hardly find their exact  correspondence words. thus using the word embedding space only  would seem not enough to measure the hidden semantics of words  and sequences.  therefore  we propose an n dimensional semantic euclidean  space  denoted as srn    which contains the semantic counterparts  of all points in rn and also allows the implicit semantics to be represented. a vector in the semantic euclidean space is essentially a  basic unit to express a latent semantics  and is therefore assigned  with a semantic meaning. it may or may not be associated to the  meaning of an actual word. moreover  we posit that the semantics of  a sequence can be represented as a convex hull in srn   and design  convex hull based metrics as evaluation indicators for the memory  ability of rnns.     .     definition of semantic euclidean space    the proposed n dimensional semantic euclidean space  srn   is extended from the n dimensional euclidean space  rn  . rn contains a  set of totally ordered arrays composed of n real numbers  n is a positive integer       rn    x    x    x    ...  xn   xi   r  i      ...  n            in the n dimensional semantic euclidean space  each n dimensional  vector in rn adopts a semantic correspondence   srn     x    x    x    ...  xn     rn  x   semantics            each point in srn is an n dimensional vector and has a semantic  meaning  hence called a semantic point. assuming that the size of a  vocabulary is v   each word in the vocabulary is represented by an  n dimensional word vector in rn . if a semantic point explicitly corresponds with a word in the vocabulary  it is called a specific semantic  point. the difference set srn  v n denotes the set of semantic points  that cannot be described by words in current vocabulary  called abstract semantic points. thus a set of pre trained n dimensional word  vectors is actually a subset of srn containing only specific semantic  points.  as is shown in figure    the meanings represented by the specific  semantic points can be described by words such as good and excellent  but we may not find any words in the vocabulary that correspond  to the semantics carried by abstract semantic points. for example   the vector  abstract semantic point  on the dotted line connecting the  vectors of good and excellent means good but not good enough to  be excellent  it is difficult to find the right word in the vocabulary to  express this semantics. to represent the semantics behind an abstract  semantic point  we need to use its nearest specific semantic point or  invent a new word.      .     representing sequence semantics as convex  hull    a sentence consists of sequence of words. the semantics of a sentence  in addition to the semantics expressed by the individual words  in the sentence  should also include the implicit semantics produced  by possible combinations of words. this is consistent with the concept of convex hull in rn . therefore  the semantics of a sequence  x    x    x    ...  xn  xi   srn   can be expressed by the convex  hull of all semantic points in x .         x     x     x     x  conv x       i xi  i         i                 i      i      the convex hull of a finite point set x is the set of all convex  combinations of its points     . in a convex combination  each point  xi in x is assigned with a weight or coefficient  i in such a way that  the coefficients are all non negative and summed up to one. these  weights are used in a weighted average of the points. for each choice  of coefficients  the resulting convex combination is a unique point  within the convex hull  and the whole convex hull can be formed  by all possible choices of coefficients. thus we can use convex hull  to express the semantics of a sequence  which includes not only the  meanings of all specific semantic points in the sequence  but also the  implicit meanings  abstract semantic points  emerging from possible  combinations of the specific semantic points.  it is interesting to show that the attention mechanism     is equivalent to utilizing partial semantics of a sequence  which is a subset  of the convex hull for a sequence. the core of the attention mechanism is to calculate a context vector c from a set of sequences  h    h    ...  hn .  c     n  x  i       i hi    n  x     i       i               i      based on the discussions above  the context vector c can be seen  as a point in the convex hull of the sequence. a particular attention  mechanism is potentially able to produce a subset of possible choices  of  i and hence give rise to a subset of convex hull. in a target task   the optimal coefficients  i s are learned  and the most characteristic  semantic point c within the subset of convex hull is therefore identified. different attention mechanisms or different weighting schemes  may give rise to different convex combination weights and different  points in the convex hull. in this way  the proposed convex hull delineates the set of sequence representations that could possibly be  produced by an attention mechanism based on the word representations. therefore  the convex hull is a reasonable choice for evaluating  the representation capacity of the word representations in a sequence.  the semantic representation of a sequence corresponds to the  meaning of the sequence  denoted as me. for any sequence of sen  mantic points  such as km     km   km     ...  kn  ki   srn    the  n  meaning of km  can be modeled as the following formula   n  n  me km     conv km              for a visual illustration  the red shaded area in figure   is the convex  hull that represents the meaning of a phrase  european conference  on artificial intelligence .     .     central idea of a sequence    the meaning of a sequence should be centered around a central idea.  for example  the central idea of  european conference on artificial    figure  . the semantic difference between an origin sequence  x    x    x    x    x  and the hidden layer sequence h    h    h    h    h   learned by rnns. the star represents the central idea of the origin sequence   and the central idea of hidden layer sequence is represented by diamond.    intelligence  is  ecai . as introduced above  me scopes the meaning of a sequence as an area in srn . intuitively  the central idea of  the sequence should be at the center of the me area. thus it is natural to use the centroid of convex hull to reflect the central idea of the  sequence.  in rn   a centroid is the mean position of all the points in all of the  coordinate directions. the centroid of a subset x of rn is computed  as follows   r  xg x dx  centroid x     r       g x dx  where the integrals are taken over the whole space rn   and g is  the characteristic function of the subset  which is   inside x and    outside it     .  similarly  for a sequence x   srn   centroid x   refers to  a specific or abstract semantic point in srn that expresses the  n  central idea of the sequence. for a specific sequence km     n  n   km   km     ...  kn  ki   sr    the meaning of km can be calcun  n  lated by me km      conv km     so the central idea  denoted as  n  ci  of km  is formulated as below   n  n  ci km     centroid conv km               note that the central idea of a sequence needs to be calculated as  n  n  centroid conv km      instead of centroid km    directly. this definition guarantees that the central idea of a sequence lies within the  convex hull  meaning  of the sequence. in contrast  even though the  geometric centroid of a convex object always lies within the area representing its meaning  a non convex object might have a centroid that  is outside the area  which is undesirable.  as shown in figure    the central idea of  european conference  on artificial intelligence  is represented by the purple star standing  for  ecai . when the vocabulary does not contain  ecai   it is an  abstract semantic point and we can express it with its nearest specific  semantic point. alternatively  the acronym  ecai  can be invented  to turn this abstract point into a specific semantic point.         assessing the memory ability of rnns in  semantic euclidean space    let a set of sequences be denoted as x    x    x    ...  xs    where  the size of x is s  the length of each sequence in x is t   and xi    xi    xi    ...  xit represents a sequence in x   . a sub sequence of xi  is represented by  xi  n  m    xi  m    xi  m     ...   xi  n   s.t. m   n.       in this paper  the notation  xi  t is equivalent to xit . both of them denote  the t  th element in xi     rnns process and memorize a sequence of information by continuously converting the original sequence xi into the hidden layer  sequence hi through recurrent units. the process is formulated as  below. f represents any recurrent unit  such as vrnn  lstm and  gru.  hij   f  xij   hi j         indicator  . h            ...    t  i reflect the change of memory  ability of rnns with the increase of sequence length under  .  given an input sequence xi of length t   let y denote the set of  sub sequences of length w in each xi .      y     xi  p w   xi   x    p           specifically  an input sequence xi is composed of n dimensional  word embedding vectors  while the points in xi are mapped onto  srn as specific semantic points. the hidden sequence hi is produced by recurrent units usually through some nonlinear functions  such as tanh or sigmoid. therefore  it is difficult to guarantee that  hi and xi are in the same space. to tackle this problem  the tied  model              is applied here. it performs a dot product between  each point in hi and all specific semantic points in srn where xi  belongs. this ensures hi and xi are in the same space. the vectors  in hi are usually abstract semantic points in srn .  it is common that the hidden layer sequence  instead of the original input sequence  is used to perform natural language processing  tasks. the hidden layer information learned by rnns carries the  semantic information of the original input sequence and meets the  needs of specific tasks. the sequence order information is implicitly  encoded by the recurrent structure  where a hidden unit is not a word  representation by itself  but the words contextual representation including the neighboring words and sequence information. therefore   the comparison between the set of input word embeddings and the  set of hidden units is a spontaneous choice that reflects the capacity  of memorizing the sequence information. we can assess the memory  ability of rnns by measuring the semantic relationship between the  original input sequence and the hidden layer sequence. in order for a  more comprehensive assessment  we propose six evaluation indicators of memory ability based on different ways of using the hidden  layers of rnns.    p         ...  t   w           w   is then the average of the memory ability over all subsequences in y under the indicator  .  based on shlr and mhlr  six evaluation indicators are defined  to measure the memory ability of rnns with different types of recurrent units from a spatial perspective. they are detailed below.     .     semantic coverage recall ratio    from the mhlr point of view  the hidden layers of rnns should  always construct its expression around the meaning of its input sequence  xi  qp . the meaning of the hidden layer sequence  hi  qp  should intersect with the meaning of  xi  qp as much as possible.  such intersection captures the valid part of the sequence information  learned by rnns  which is called semantic coverage and denoted as  sc   sc  xi  qp    hi  qp     me  xi  qp     me  hi  qp      different ways of rnns memorizing sequence  information    the hidden layers of rnns can be used in various ways  which can  be classified into two categories.  single hidden layer representation  shlr  only the last hidden layer vector hit is used to represent the central idea of the original input sequence xi . the typical encoder decoder model     and  many language models          are examples of using this method.  multiple hidden layer representation  mhlr  all hidden  layer vectors hi are used to represent the meaning of the original  input sequence xi . in other words  the meaning of xi is represented  by the meaning of hi . the multiway attention network  mwan        and hierarchical attention networks  han       are examples of  using this method.  both representations are based on the assumption that the meaning of the original input sequence is preserved in the hidden layers learned by rnns. since srn provides a basis for measuring the  meaning and central idea of a sequence of semantic points  in order  to assess the memory ability of rnns  we can measure the semantic difference between the original sequence and the hidden layer  sequence as the spatial difference between their convex hull representations of meaning and central idea.  the memory ability of rnns is also influenced by the input sequence length. suppose that an evaluation indicator   is used to measure the memory ability of rnns.   w   denotes the memory ability  of rnns for a sequence which is of length w   under the evaluation            in figure    the shaded pink portion represents a semantic coverage. the percentage of sc in the meaning of  xi  qp is called semantic coverage recall ratio  scrr . in order to observe the variation  of scrr with the increase of sequence length  the calculation process of scrr w   is as follows   s t  w  p  p       .            scrr w        i      p           hi  p w  sc  xi  p w  p  p  p w    me  xi  p                 s t   w         the normalization factor  s t   w       is the total number of  subsequences in the processed sequence  so that the final metric is the  average of the indicators for all subsequences processed by rnns.     .     semantic coverage precision ratio    like scrr  the percentage of sc in the meaning of hidden layer sequence  hi  qp is called semantic coverage precision ratio  scpr    s t  w  p    p    scpr w         .     i      p      sc  xi  p w    hi  p w     p  p  p w    me  hi  p         s t   w                 semantic coverage f measure    scpr and scrr sometimes give contradictory indications. as a  trade off  the f  measure takes into account both scpr and scrr  can be used   scfm w            scrr w     scpr w    scrr w     scpr w                .     explicit central idea offset ratio    after  xi  qp is processed by rnns  from the shlr point of view   hiq can be seen as an approximation of the central idea of  xi  qp .  therefore  the offset between the central idea of the original sequence  xi  qp and hiq can reflect the memory ability of rnns. since  hiq is directly used as the central idea. this evaluation indicator is  called explicit central idea offset  denoted by ecio and formulated  as follows   ecio  xi  qp   hiq     ci  xi  qp    hiq    s t  w  p  p    i      max    ecio  xi  p w     hi  p w    p    p       ecio  xi  p w     hi  p w     p            p   ... t  w     i     ... s     .     implicit central idea offset ratio    the meaning of the original sequence  xi  qp can be expressed by the  meaning of the corresponding hidden sequence  hi  qp when using  mhlr. in this case  the central idea of  hi  qp needs to be calculated  first when considering the offset distance between the central idea  of  hi  qp and the central idea of  xi  qp . this evaluation indicator is  called implicit central idea offset  denoted as icio and formulated  as below   icio  xi  qp     hi  qp      ci  xi  qp    ci  hi  qp              in figure    the solid yellow line represents an implicit central  idea offset. similar to ecior  in order to observe the variation of  implicit central idea offset ratio  icior  with the increase of sequence length  the calculation process of icior w   is as follows      s t  w        s t  w  p  p    i      max     icio  xi  p w     hi  p w      p  p            p   ... t  w     i     ... s    the normalization factor in ecior and icior is the maximum  possible value of ecir and icio in the process of processing sequence by rnns. it makes the ecior and icior values fall between   and    facilitating horizontal comparisons between different  indicators.     .     s t  w  p  p      cihr w             central idea hit ratio    from the shlr perspective  hiq is used to represent the central idea  of  xi  qp   thus it should be included in the meaning of  xi  qp . here   the inclusion of hiq in the meaning of  xi  qp is called the  hit of  central idea   and the exclusion of hiq in the meaning of  xi  qp is    i      cih  xi  pp w    hi  p w      p      s t   w                 experiments    using the proposed six indicators  we assess the memory ability of  three types of recurrent units  including vrnn  tanh is used as the  activation function   lstm and gru. each of them is incorporated  in a tied nnlm      with a single layer rnn to perform the language modeling task on penn treebank       ptb  and wiki        datasets. perplexity  ppl  is used as the performance metric. for the  tied nnlm  the number of neurons in the hidden layer is set to     and bptt is set to   . the latter means that the long text sequences  in the dataset are split into short ones with a maximum length     and  the short sequences will further generate sub sequences at lengths no  greater than   . these sub sequences will constitute the set y defined  in section  . . moreover  the tied nnlm uses a    dimensional  word embedding  based on which a semantic euclidean space sr    is constructed. the hidden layer vectors also belong to sr   in order  to use the evaluation indicators defined in section   to analyze the  memory ability of rnns.  the training of tied nnlm uses the stochastic gradient descent algorithm. the learning rate is initially set to be         and is halved  if no significant improvement is observed on the log likelihood of  validation data.  it should be pointed out that for calculating the indicators related  to semantic coverage  scrr  scpr  scfm   we can naturally consider using the area to complete the calculation of these indicators.  however  it is difficult to calculate the area in high dimensional space  at present. to do so  we reduce the high dimensional vector to  dimensions through t sne       which largely preserves the topological relationship of points in high dimensional spaces.     .     icio  xi  p w     hi  p w     p  p    p      then the last evaluation indicator  namely central idea hit ratio   cihr   is given as below             in figure    the green dotted line represents an explicit central  idea offset. when an rnn processes the sequence set x   we can use  the maximum value of the sub sequence offsets as the normalization  factor  and then calculate the ratio of the average offset distance in  the sub sequence set to the maximum distance to obtain an explicit  central idea offset ratio  ecior .  in order to observe the variation of ecior with the increase of  sequence length  the calculation process of ecior w   is as follows      s t  w        called the  miss of central idea . accordingly  we define central idea  hit  cih  as follows         hiq   me  xi  qp    q        cih  xi  p   hiq         hiq      me  xi  qp      experimental results and analysis    the results for the six indicators and the variation of results with  respect to the increase of sequence length  on the validation sets of  ptb and wiki    are shown in figure  .    scrr both lstm and gru well preserve the meaning of the  original sequence. the scrr of lstm and gru do not increase  much after the sequence length reaches   . in contrast  a trend of  non converging growth is observed for vrnn on ptb.    scpr when the sequence length is less than     the lstm and  gru have a rapid increase in the value of scpr  along the increase of sequence length  but then the growth rate significantly  drops. however  no matter how long the sequence is processed by  vrnn  its scpr value is always less than   . combined with the  scrr of vrnns  it can be seen that although the hidden layer  of vrnn can express the semantics of the original sequence to  some extent  this coverage of the original sequence semantics is  obtained at the cost of reducing scpr  which is not desirable.      a  ptb     b  wiki    figure  . the variation in memory ability  with respect to sequence length. the horizontal coordinates represent different settings of sequence length  and  the longitudinal coordinates represent different evaluation indicators. the ordinate scale of the lstm and rnn is shown on the left side of each picture. the  ordinate scale of the gru is on the right side of each picture.     a  wiki        b  wiki    figure  . comparison of memory abilities between different types of recurrent units. each indicator takes the optimal value across different sequence lengths.  different from scrr  scpr  scfm and cihr for which the higher value indicates a better memory ability  for icior and ecior  the lower value the better.  thus the value corresponding to icior and ecior shown in each picture is   minus the minimum of original value.    some extent reflects the process of semantic cognition. with the continuous input of words  the meaning of a sentence gradually becomes  clear. the hidden layer sequence of rnns gradually remembers the  meaning of the original sequence. when the sentence reaches a certain length  the memory ability of recurrent units reaches its maximum capacity  so the information carried by the new coming words  cannot be effectively memorized by the hidden layer.  cihr does not show a converging trend with the increase of sequence length. we speculate that it is because the increase in the  length of the sentence expands the area surrounded by the original  sequence in srn   which increases the probability of hits.  icior and ecior are distance based indicators  for which the  lower value indicates a better memory ability. the trends of these two  indicators are also consistent with the process of semantic cognition.     .   figure  . the influence of bptt on ppl. the horizontal coordinates represent the bptt  and the longitudinal coordinates represent ppl.      scfm scfm has almost the same trend as scrp. this shows that  in term of semantic coverage  lstm is slightly better than gru   and vrnn has a relatively lower capability of semantic coverage.    cihr although the central idea learned by lstm and gru under shlr can hit the meaning of the original sequence  a rather  low hit rate is observed. the central idea learned by vrnn almost  fails to hit the meaning of the original sequence.    icior the central idea learned by gru and lstm under mhlr  can well restore the central idea of the original sequence  and the  icior of lstm is almost  . . the central idea that vrnn has  learned is far from the central idea of the original sequence.    ecior the central idea of sequence that lstm learns under  shlr effectively restores the central idea of the original sequence. gru is less effective than lstm when learning short sequences  but when the sequence length is around     gru has  achieved similar or even better results compared with lstm.  vrnn always maintains a large deviation from the central idea  of the original sequence.  for scrr  scpr  scfm and cihr  a higher value indicates a  better memory ability. as is shown in figure    the values of scrr   scpr and scfm increase rapidly with the increase of the sequence  length at first  and then stabilize after reaching a certain level. this to    summary and verification    the proposed evaluation indicators for memory ability of rnns turn  out to be robust on both datasets. they can reliably reflect the difference in memory abilities caused by the internal factor from different  perspectives.  in order for a more intuitive understanding  we plot a radar map  of memory abilities of vrnn  lstm and gru on ptb and wiki    in figure  . it turns out that the memory abilities of gru and lstm  are stronger than vrnn in all aspects. lstm achieves better results  when using single hidden layer representation  shlr  to represent the central idea of the original sequence. when using multiple hidden layer representation  mhlr  to represent the meaning of the original sequence  lstm shows a memory ability similar  to gru s. moreover  all three recurrent units exhibit low values in  cihr  restraining the memory ability in this regard. this suggests  that more attention could be paid to cihr when designing new recurrent units for improved memory ability.  finally  it is worth noting that the indicator values for both lstm  and gru remain stable after the sequence length reaches   . we  conjecture that setting the value of bptt to     as external factor   may be unnecessary. when the value of bptt is around     the ppl  should be able to achieve a good performance.  in order to verify the above conjecture  we train the language  model under different bptt settings  and compute the ppl on the  validation set and the test set. in figure    we find that the lstm and  gru have achieved good results when the value of bptt is around      while the ppl value of vrnn keeps fluctuating. at this point   our conjecture has been verified.          conclusions and future work    in this paper  we have analyzed the internal and external factors affecting the memory ability of rnns  and constructed a semantic  euclidean space  srn   in which the meaning and central idea expressed by a text sequence can be measured. further  we define six  evaluation indicators on srn according to different ways of using  the hidden layer of rnns. these indicators are used for analyzing  the memory ability of rnns. through the analysis  we are able to  reveal the advantages and disadvantages of three widely used types  of rnns  vrnn  lstm and gru. the results give insights on the  choice of recurrent unit in different scenarios  and also provide strategies for selecting the hyper parameters of sequence length.  future research will be focused on defining more evaluation indicators in srn to investigate the internal mechanism of rnns  as well  as applying our indicators to more complicated tasks  such as reading  comprehension and machine translation. at the same time  we will  explore the design of new recurrent units based on the evaluation  indicators  to improve the memory ability of rnns.    acknowledgements  this work is funded in part by the national key research and development program of china  grant no.     yfc         and natural  science foundation of china  grant no. u         u        .    