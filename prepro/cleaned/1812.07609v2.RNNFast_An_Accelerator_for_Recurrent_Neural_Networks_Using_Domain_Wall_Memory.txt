introduction    deep learning is transforming the way we approach everyday computing. from speech recognition that empowers  today s digital assistants to business intelligence applications fueled by the analysis of social media postings   processing information in a way that preserves the correct context is crucial. for instance  the sentences  white  blood cells destroying an infection  and  an infection destroying white blood cells  have very different meanings  even though they contain the same words. traditional machine learning designs such as convolutional neural  networks  cnns  do not consider context and are therefore not well suited for solving such problems. recurrent  neural networks  rnns  are a powerful class of networks designed to consider context by retaining and using  authors  addresses  mohammad hossein samavatian  the ohio state university    columbus  ohio  usa  samavatian.  osu.edu  anys  bacha  university of michigan    dearborn  michigan  usa  bacha umich.edu  li zhou  the ohio state university    columbus  ohio  usa   zhou.    osu.edu  radu teodorescu  the ohio state university    columbus  ohio  usa  teodorescu.  osu.edu.  permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for  components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise  or republish  to  post on servers or to redistribute to lists  requires prior specific permission and or a fee. request permissions from permissions acm.org.         association for computing machinery.                   art     .    https   doi.org   .              acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .              mohammad hossein samavatian  anys bacha  li zhou  and radu teodorescu    information from previously processed inputs. rnns are used across a wide range of applications that include  speech recognition for digital assistants such as siri and google now  sentiment analysis for classifying social media  postings  and language translation. the popularity of rnn networks in production applications was highlighted  by google in a recent paper       which reports that rnn workloads represent almost     of the workloads on  google s tpu datacenters. this is in contrast to only    for cnn workloads.  however  rnn workloads are data intensive because they store a partial history of the output sequence and  perform computations on that history along with the current input. as a result  rnns require both vast amounts of  storage and increased processing power. for example  the rnn neuron requires    the number of weights and  multiply accumulate  mac  operations of a typical cnn cell. rnn networks are also generally quite large. for  instance  amodei et al.     developed a network for performing speech recognition that utilized seven recurrent  layers and a total of    million parameters. at this scale  rnns with large input sets are susceptible to memory  bottlenecks when running on existing accelerators such as gpus      or fpgas                                . in  addition  the fundamentally different design of the rnn cell makes previously proposed custom cnn accelerators                                                           not directly applicable to rnn workloads.  this paper presents rnnfast  a hardware accelerator for rnn networks. rnnfast leverages domain wall  memory  dwm   an emerging non volatile memory technology  to provide high density on chip storage as well  as energy efficient computation. dwm                              is a magnetic spin based memory technology   which stores information by setting the spin orientation of so called magnetic domains in a ferromagnetic wire.  multiple magnetic domains can occupy a single wire  referred to as  racetrack   allowing up to    bits to be  represented.  dwm has many attractive characteristics. it has read write latencies that are close to sram and write performance and energy that are substantially lower than stt ram and other non volatile memories     . perhaps  more importantly  dwm is expected to have     higher density than sram and     higher than dram or  stt ram. the technology would therefore allow dramatically higher storage capacity in the same chip area.  while the technology is still in the early stages of development  prototypes have yielded encouraging results    .  we show that dwm is very well suited for rnn acceleration due to its very high density  linear access pattern  and  low read write energy.  the rnnfast architecture is modular and highly scalable forgoing the need for long communication buses  despite the high output fanout of typical rnn networks. rnnfast allows flexible mapping of logic neurons to rnn  hardware blocks. the accelerator is designed to minimize data movement by closely interleaving dwm storage  and computation. the basic hardware primitive  the rnn processing element  pe  includes custom dwm based  multiplication and custom nonlinear functional units for high performance and low energy. rnnfast also includes  an error mitigation mechanism for position errors  expected to be relatively common in dwm. the error mitigation  is tailored to the rnnfast data access pattern to minimize overhead. we compare rnnfast with a state of the art  nvidia p    gpgpu and find rnnfast improves performance by   .   while reducing energy    .  we also compare with two alternative rnnfast designs.    a cmos based rnnfast design in which both  memories and logic use traditional cmos. we find the rnnfast design to be up to    more energy efficient than  the cmos version  in a much smaller chip area.    a memristor based implementation that uses an analog dotproduct engine  a state of the art design that has been shown to be very efficient for cnns        . rnnfast shows  better performance  energy and area than the memristor based design. qualitative comparisons with fpga based  rnn accelerators  google s tpu and microsoft s brainwave      also indicate rnnfast has better performance  and lower energy for similar workloads.  this paper makes the following main contributions     presents rnnfast  the first dwm based custom accelerator for lstms and other rnn variants.  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .     rnnfast  an accelerator for recurrent neural networks using domain wall memory    ht   xt    input vector  x       x       x       c       c       c       h       h       h       c       c       c       ht   xt              input  i  gate t    xt  ht               cell    x    output ot  gate    ct         x    ht    x    forget  gate    ft       h        h        h        c       c       c       xt  x     x   c  h        h        output vector   a     h        ht       b     h     h     c    xt  h     h     ...    ht      c  ht  time     c     fig.  .  a    layer rnn with   lstm cells layer   b  lstm cell   c  an lstm cell unrolled over time      introduces novel dwm based designs for efficient neural network hardware including sigmoid  and tanh  units.    implements an efficient error mitigation solution for dwm overshift errors.    presents a new efficient and scalable interconnection mechanism based on racetrack chains.    demonstrates that dwm is very well suited for efficient acceleration of recurrent neural networks.  the rest of this paper is organized as follows  section   provides background information. section   details the  design and implementation of rnnfast. section   presents the error mitigation aspects of the design. sections    and   describe the evaluation. section   discusses related work and section   concludes.         background    recurrent neural networks  rnn  are a powerful class of networks that have the ability to learn sequences. they  are applicable to anything with a sense of order that needs to be remembered. rnns are used across a wide range  of applications that includes speech recognition for enabling today s digital assistants  sentiment analysis for  analyzing posts  text and video  and classifying them as positive or negative  and machine translation for sequence  to sequence translation between languages.     .     the long short term memory cell    most recurrent neural networks make use of special  neurons  called long short term memory  lstm  cells          . lstms are designed to process and remember prior inputs and factor them into their outputs over time.  figure   shows an example of a very simple   layer rnn with   lstm cells layer. the output of each layer is a  vector that is supplied as the input to the following layer. in addition to those inputs  a feedback loop takes the  output vector of each layer and feeds it back as an additional input to each lstm neuron. an illustration of the  inputs and outputs of a single lstm cell c unrolled over time is shown in figure   c . an input x  into neuron c at  time step t      will generate an output h  that is propagated downstream to the next layer. in addition  h  is saved  within the neuron s memory cell for use in the next time step. at time step t      the same neuron c will process  input x    but also use the previously stored output h  to generate the new output h  .  a detailed look inside the lstm neuron  figure   b   reveals a significantly more complex operation compared  to cnn neurons. the strength of the lstm lies in the way it regulates the fraction of information it recalls from its  embedded memory and the fraction of input it processes for generating outputs over time. in other words  the lstm  cell progressively memorizes and forgets contextual information as it processes more inputs. this is achieved  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .              mohammad hossein samavatian  anys bacha  li zhou  and radu teodorescu    read port    read write port  read  current    write  current        write port    read  current    fixed  layer    shift  current    write  current        shift  current  domain domain free  wall  layer  write  current        write  current        ferromagnetic wire    fig.  . dwm device structure.    through special gates that are controlled through a set of mathematical functions      governed by equations           .  it    wxi xt  whi ht     bi       ft    wx f xt  wh f ht     b f           ot    wxo xt  who ht     bo           ct   ft   ct     it   tanhwxc xt  whc ht     bc           ht   ot   tanhct           the input gate it receives the input to be written into a neuron s memory cell at time step t. the forget gate ft  controls what information should be erased from a neuron s memory cell at time step t. the cell ct represents the  content of the neuron s memory cell. the output gate ot controls the amount of information read from the neuron s  cell and how much of it contributes to the output. the output ht represents the output of the cell to the next layer at  time step t. this output is also fed back into the input gate it   of the same lstm cell at time step t    . the w s  and bs represent the weights and biases  respectively.  note that   used in equations     and     represents the dot product operator. in addition  equations            represent neurons for an entire layer within a network. therefore  it   ft   ot   ct   ht   ht     and xt are vectors and all  w s are matrices. as such  if we augment a given matrix w to include the weights for both x and h such that its  dimensions are n   m  then each row in w l for hidden layer l would be mapped to neuron j where j      n. the  value m is the size of input vector.    l  w       w l     ...  l  wn     ...  ..  .  ...    l    w m  ..    .             l  wnm    the tanh and   activation functions are also outlined in equations     and     for clarity. these functions are  applied as elementwise operations on the resulting vectors.          z             e z        tanh z       z           acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .     rnnfast  an accelerator for recurrent neural networks using domain wall memory             because of the complex design  lstm cells require substantially more storage and computation relative to  their cnn counterparts. moreover  rnn networks are also generally fully connected  further increasing the data  movement overhead.     .     domain wall memory    domain wall  a.k.a. racetrack  memory was first proposed by parkin et al.      from ibm in     . in        annunziata et al.    demonstrated the first    mm dwm wafer  fabricated with ibm   nm cmos technology.  each die contained     racetrack cells  proving the feasibility of dwm fabrication. a large body of research has  since sought to improve and optimize the technology at device and circuit levels                              and  find solutions to improve its reliability     .  domain wall  racetrack  memory represents information using the spin orientation of magnetic domains in  a ferromagnetic wire  as shown in figure  . each of these domains can be independently set to an up spin or  down spin to represent the value of a single bit. since multiple magnetic domains can reside on a single wire   multiple bits         of data can be packed in a single dwm device  resulting in a very high density. three basic  operations can be performed on a dwm device  read  write and shift. a magnetic tunnel junction  mtj            structure is used to read data from the dwm cell  read port in figure   .  in a dwm device  all the magnetic domains share a single read mtj  generally referred to as a read head or  port . the bit to be read needs to be aligned with the mtj before it can be accessed. this is accomplished using a  property that is unique to dwm  called domain wall motion  which refers to the shifting of magnetic domains down  the ferromagnetic wire. when a current pulse of a suitable magnitude is applied through the ferromagnetic wire   the magnetic spins of all domains  move  across the wire in a direction opposite to the direction of the current. the  number of bit positions in a shift motion is controlled by the duration of the shift current. additional blank domains  are included at the ends of each racetrack to allow all data domains to be shifted to the read head without data loss  at the ends of the wire     .  writing into dwm is also fast and energy efficient due to recently developed       shift based writes  as  demonstrated in fig.    write port . the design of the write head consists of a ferromagnetic wire with two fixed  domains that straddle a free domain at an arbitrary location on the racetrack. one of the fixed domains is hardwired  to up spin and the other to down spin at fabrication. the spin of either of the fixed domains can be shifted into  the free domain through the domain motion process by applying a current pulse in the appropriate direction. the  latency and energy of shift based writes are equivalent to those of simple shifts.  the main challenge of racetrack memory is the access latency to data stored in a dwm tape which is variable  depending upon the number of shifts required to align the accessed bit with the read or write heads. rnnfast  mitigates this disadvantage by optimizing data placement for sequential access such that most accesses only require  a single shift.   . .  reliability issues. dwm technology also presents reliability challenges including possible misalignment of  the data domains leading to erroneous reads and or writes         . prior work      has classified dwm errors into  two main types   stop in the middle  and  out of step  errors. the first class of errors is caused when data domains  are not aligned with the read write heads  leading to invalid accesses. the second class of errors is caused when  the incorrect domain is aligned with the read write head which causes the wrong bit in the track to be accessed.  the errors are generally caused by variability in the magnitude or duration of the current pulse applied during the  domain shift operation. zhang et al.     has developed a technique for eliminating  stop in the middle  errors that  relies on the application of a short subthreshold shift current to nudge the misaligned domain back into alignment.  they also demonstrate that the subthreshold pulse is small enough that it cannot misalign a correctly aligned  domain. as a result  sub threshold shifts can virtually eliminate  stop in the middle  errors  at the cost of increasing  the number of  out of step  errors.  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .              mohammad hossein samavatian  anys bacha  li zhou  and radu teodorescu    tile group    tile group  tile group    tile group  tile group  tile group  tile group    interconnection network    conf  mem    tile group    interconnection network    comp. mem  interconnection network    i o interface    on chip dw memory    rnnfast chip  fig.  . rnnfast architecture overview at chip level.    while subthreshold shifts can be applied in both directions  we choose to apply them in the shift direction. as a  result  all  out of step  errors will be converted into overshift errors by   or more positions in the shift direction.  for a single position shift  which represents virtually all shifts in rnnfast  the probability of single bit overshift is  on the order of            which is quite high. however  the probability of multibit overshift is about         which  is negligible. as a result  rnnfast implements mitigation for single bit overshift errors.         rnnfast architecture    at a high level the rnnfast chip consists of global memory  a computational memory array  configuration  memory and i o interface as shown in figure  . the global memory is a dense memory block implemented using  dwm. this is the main memory of the accelerator and is used to store inputs and results. the computational  memory is the compute engine and is implemented primarily using dwm elements augmented with cmos logic  where appropriate. the compute array is organized as a pool of highly reconfigurable and tightly interconnected  tile groups.  one or more multi layer rnn networks can be mapped to multiple tile groups  in a weight stationary design   weights are stored locally in the computational memory . the configuration memory holds the runtime configuration settings for the chip. rnnfast is optimized to deliver low latency without batching  and it is also efficient for  batch workloads.     .     compute tiles    a compute tile consists of multiple lstm hardware units that share a single input and a single output racetrack.  they are interconnected with their nearest horizontal and vertical neighbors through racetrack memories. figure    shows the tile design and layout. the results of the computation within each tile are written directly onto the input  track of the tile belonging to the next layer in the network. tiles are organized in tile groups  which are connected  to each other through traditional wired interconnection networks.   . .  inter tile communication. rnns are typically fully connected networks requiring all inputs to be delivered  to all the neurons in a given layer. the high degree of connectivity that has to be supported by the hardware can lead  to substantial energy and area overheads when traditional wired interconnects are used. to address this challenge  we leverage the shifting mechanism of dwm racetracks for communication both within and across tiles.  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .     rnnfast  an accelerator for recurrent neural networks using domain wall memory      ...    ...    t    t    t    t    t    t    ...    t  ...    t  ...    t    t    t    t    ...        shift direction    ...    tile group        shift direction            ...  ...  ...            ...    tile      tile              ...    lstm  unit      lstm  unit              ...    lstm  unit       lstm  unit      lstm  unit              ...    lstm  unit       ...    tile    lstm  unit      lstm  unit              ...    lstm  unit       lstm  unit      lstm  unit              ...    ...  ...    lstm  unit               timestep t    i  i     i  i  i  i     ...    i      ...    lstm  unit       lstm  unit      ...    lstm  unit       tile n      lstm  unit      i  i     i  i  i  i     timestep t   ...    lstm  unit       lstm  unit      tile n      lstm  unit      ...  ...    tile n    ...  ...  ...    i   i      ...    lstm  unit      ...    tile n      tile n    ...    from  adjacent  tile group    from  adjacent  tile group     a   ...    from  adjacent  tile group    ...  ...  ...            tile      ...           i      lstm  unit       i   i      ...    lstm  unit      ...    tile n       b   fig.  .  a  compute tile layout  internal design and interconnection through racetrack chains.  b  reading inputs into tiles in  two consecutive timesteps.    within a tile  inputs are read sequentially from the tile s input racetrack and broadcast to all lstm units across a  locally shared bus. each read is followed by a shift of the input track to align the next input element with the read  head. figure    b  illustrates two timesteps in this process. in addition to the tile local broadcast  each input is also  sent to the neighboring tile on the left for addition to its input track. we call this process  chaining . chains are  essentially circular buffers that circulate all inputs to all tiles that are mapped to the same layer of the nn. chains  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .              mohammad hossein samavatian  anys bacha  li zhou  and radu teodorescu    of different lengths can be configured depending on the number of neurons in each layer of the network. racetracks  are connected through muxs  figure    a   that enable different chain lengths. a variable number of tracks can be  included in a chain by simply setting the right most track mux to   and the rest to  .     .     lstm units    each tile consists of multiple lstm compute units     in our design . rnnfast is a weight stationary design   with fixed capacity for weight storage in each lstm unit. a logical neuron can be mapped to one or more lstm  compute units depending on the number of weights it requires. we expect a   to   mapping between logical neurons  and hardware lstm units for most networks. however  when a logical neuron requires more weights than a single  lstm units can store  it is mapped to multiple lstm units. figure    a  shows three mapping examples for a  single logical lstm cell    lstm unit  top     lstm units  middle  and   lstm units  bottom .   . .  processing elements. the architecture of an lstm cell is shown in figure    b . each cell is subdivided  into four processing elements  pes    . per equations            each input xt is multiplied with four different  sets of weights. a single pe can be assigned to any one of the weight sets  known as gates   e.g. ig   fg   og  or cg . however  an lstm cell gate can be mapped to one or more pes across lstm units depending on its  storage requirements and input output fanout. allocating four hardware pes to each lstm unit allows rnnfast to  accommodate different rnn variants  see section  .  .  pes have racetrack based storage for weights and racetrack based compute units  including multiply accumulator   mac  engines for matrix multiplication. the mac engine is composed of        dwm based full adders. the  mac unit is deeply pipelined into    stages. in order to increase parallelism  each pe uses two mac engines  one  for the main input xt and one for the feedback input ht   .  each pe unit holds a set of weights and performs the dot product on the corresponding subset of inputs. each pe  only consumes inputs corresponding to the weights it stores. each input to a pe is multiplied by its weight and  accumulated with the result of the previous multiplication   . each pe stores the result of the accumulation in its  own output racetrack.   . .  input and weight mapping. the inputs and weights assignment to racetracks is a trade off between access  latency and hardware overhead. in rnnfast  inputs are spread across multiple racetracks with   bit per track. this  allows an entire input word to be read in a single cycle  as the top half of figure   illustrates. error detection bits  are also included in the tracks and their role will be detailed in section  . note that the input tracks do not require  dummy domains  figure   b . values at the end of the track are read and sent to the neighboring track.  unlike inputs  which move from track to track along the chain  weights are stationary at pe level and are reused  multiple times. this means that after scanning all weights  the tracks need to be returned to the initial weight. to  minimize the number of shifts  weight values are distributed both within and across multiple racetracks. weight  racetracks are provisioned with multiple read write heads    in our design  which divide the racetrack into      bit  segments. the left most segment domains are used as dummy domains and the rest of the segments are used to  store weight values. data layout is such that all read heads across all tracks can access all the bits of a single weight  simultaneously. racetracks are grouped in sets of    with each set storing    weights. the bottom of figure    illustrates this layout. weight w   red  is currently aligned with the read heads. a single position shift to the left  will align the next weight w   blue  with all the read heads. access to each set of weight racetracks is pipelined.  when all    weights are read from the current set of racetracks  the next set of weights will be read from next set.  while the new weights are accessed  the weights in previous set are shifted back to their initials positions. this  takes place when the racetrack set is not being accessed and is therefore off the timing critical path.   . .  result aggregation. if more than one lstm unit is mapped to a neuron the partial results of the individual  lstms have to be combined to form the neuron s output. aggregation units   in each lstm are used to sum up  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .     rnnfast  an accelerator for recurrent neural networks using domain wall memory        lstm unit   lstm cell  network size       one step  aggregation    lstm units   lstm cell  network size         two steps  aggregation    lstm unit   lstm cell  network size         three steps  aggregation    lstm cell      lstm cell       lstm cell      lstm cell       lstm unit    pe pe  pe pe    lstm unit    pe pe  pe pe    lstm unit    pe pe  pe pe    lstm unit    pe pe  pe pe    aggregation  unit    aggregation  unit    aggregation  unit    aggregation  unit    lstm cell    lstm unit    pe pe  pe    lstm cell      lstm unit    pe pe    pe    pe    lstm unit    pe pe    pe    aggregation  unit    aggregation  unit    lstm unit    pe pe    lstm unit    pe pe    pe    lstm unit    pe pe    pe    pe    aggregation  unit    pe    aggregation  unit    lstm cell      pe    pe    pe    aggregation  unit    lstm unit    pe pe    pe    aggregation  unit    pe         pe    pe    pe    aggregation  unit       delay           lstm unit    lstm unit    pe pe    aggregation  unit     a     pe           pe  weights set    pe     pe     pe     ig    fg    og c g    pe     aggregation unit    multipliers set  mac  add      aggregation  unit    from pes  accumulator    from  lstm unit    to lstm  bypass line  unit    pes or acc.  output activation  function    to next layer    mac  ig  input gate  fg  forget gate  og  output gate  cg  wc  x h      b   fig.  .  a  three mapping examples of logical lstm cells to lstm units.  b  lstm unit design.    partial results in that lstm block. in addition  the aggregation units apply the sigmoid and tanh functions and  perform the multiplication and accumulation operations in order to generate the final output of the cell.  for cases in which neurons span multiple lstm blocks  aggregation units in those blocks are linked to produce  the final result. this is achieved by collecting all the partial results computed by each lstm unit  mapped to the  same neuron  to a single aggregation unit. aggregation units are also chained through adjacent lstm units. each  aggregation unit sends out its final result to the adjacent aggregation unit to its left. the adjacent unit will use the  incoming result to either accumulate or bypass it to the next unit  figure       . even indexed aggregation units  consume and odd indexed aggregation units forward the incoming result. the leftmost lstm in a neuron will  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .     i       i            i      i      i      ...    i            i      i      i      ...    i                           w   w b                  edc    blank cells  r w         w      w           w      w      ...         w      w      ...         w    w       ...    ...    ...    input bits    r w    edc    weight bits    r w    i        r w    r w    ...    w      w   ...    w      w      ...    w      w   ...    w      w      ...    w      w      ...    w       w       w    w       ...    w    w       r w    ...    w      w      ...         ...    w      w      ...         ...    w       w       ...         ...    w    w       ...         ...  ...  ...  ...    weight layout    b          ...    i            ...      bit index  in bweight  index  b  bit index  wn  weight index    input layout    mohammad hossein samavatian  anys bacha  li zhou  and radu teodorescu    ...         ...            fig.  . mapping of inputs and weights to racetracks.    be responsible for the final aggregation and will apply the sigmoid and tanh. aggregation time is a logarithmic  function in the number of lstm cells mapped to a single neuron. this is also done by setting multiplexers in the  aggregation unit and power gating the inactive units in output generators at odd indexed lstm units.  the design tradeoff for lstm units is driven by the need to support networks both large and small. if lstm  units and pes are too large  storage space will be wasted when small networks are mapped. if they are too small   large networks will require several lstm units per neuron  increasing the aggregation time.     .     nonlinear functions    the nonlinear functions are an important component of the rnn cells and are used for output activation. rnnfast  uses hardware acceleration for the sigmoid and tanh nonlinear functions. the hardware is included in each  aggregation unit  figure   . we propose an area efficient approximate logic function based unit implemented  using dwm for the nonlinear functions.  the approximation has been proposed by prior work      as an alternative to the standard sigmoid follows  equation           z               z  i f z        z                         z i f z      this approximation has the advantage of being easier to implement in hardware. as equation   shows  the hardware  has to support division by  n numbers. this can be implemented using shift operations which are a feature of  racetrack memories. the tanh approximation function can be computed from the sigmoid function through two  multiplications and a subtraction. note that z    z    z    where z is the integer part of z.  figure   shows our dwm based implementation of the sigmoid approximation. sigmoid for a negative value  will be computed as follows  a  the output integer part is initialized with binary      b  two right shifts are performed  to compute z     c       is applied to the result  d  final result is shifted right   z   times. for a positive number two  subtraction steps are added in the beginning and end of above steps. to compute the tanh approximation  a right  shift      z  and a subtraction will be applied in the first and last steps respectively. this design is very area and  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .     rnnfast  an accelerator for recurrent neural networks using domain wall memory      msb            lsb  integer part    sign  bit  subtractor    counter     a  initialization           shift  logic   b    d      c           fractional part    fig.  . dw based implementation of sigmoid tanh.    energy efficient utilizing only a    bit racetrack memory  along with some simple subtraction and counting logic.  section   evaluates the relative merits of the approximate designs regarding luts.     .     rnnfast mapping and configuration    the rnnfast hardware can be configured to implement different network sizes and topologies. moreover  multiple  distinct neural networks can be mapped to the same chip.  outputs from one network can be delivered directly to the following network or stored in the on chip memory  for further processing  if needed. figure   illustrates an example of four networks a  b  c and d mapped to two tile  groups. tile groups are connected through a wired interconnect. the racetrack chains for each row of tiles have  additional read write heads to provide access to the inter tile network.  multilayer networks span multiple rows with different layers mapped to consecutive rows. tile groups are  designed with wide rows to accommodate most network sizes  e.g. nets a and c . however  when a network layer  cannot fit in a single row  rnnfast supports splitting it across tile groups  e.g. nets b and d . this is achieved by  extending the input output racetrack chains to neighboring tile groups using the inter group wire interconnect. we  chose to split layers across tile groups  as opposed to within a tile group  in order to allow consecutive network  layers to continue to be mapped to adjacent rows  preserving inter layer communication.  one important design constraint was to enable the extension of the racetrack chains across tile groups without  adding to the track chain shift latency. this is accomplished by implementing a look ahead read port at the end of  the track that reads inputs several cycles ahead of the end of the track  as illustrated for net d in figure  . this  allows the input to reach the destination row in the neighboring tile through the higher latency interconnect by the  time the same input reaches the end of the source track.   . .  other lstm variants. rnnfast is designed for the more demanding lstm design. however it is also  compatible with lstm variants like gated recurrent unit  gru  and vanilla rnn  which require fewer compute  resources. unlike lstm  the gru unit does not use a memory element to control the flow of information and are  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .                  mohammad hossein samavatian  anys bacha  li zhou  and radu teodorescu  net a    net b    l   l     tile    net c    l     l     l     l     interconnection  network    net d    connection    l     ...  ...  ...    ...    ...  ...    ...  ...    ...  ...    ...  ...    ...    ...    ...  ...    tile group      tile group    look ahead  read port                    fig.  . mapping multiple lstm networks to rnnfast. interconnection network helps extend racetrack chains beyond tile  groups for large networks.    gru    lstm  pe pe pe pe         c h    x       t     b    accumulator        b             x              t    output generator         t    aggregation  unit     b    output generator     b    aggregation  unit    accumulator  b    pe pe pe pe   b       x    c h    x       t    fig.  . lstm vs gru cell configuration on rnnfast    useful when input sequences are not very long. figure   shows how a gru cell can be mapped to a rnnfast  lstm unit. the shaded areas represent unutilized components. gru utilizes     of the mac resources.  simpler rnns like vanilla rnn  only utilize a single pe per neuron and do not need the aggregation unit. as a  result  rnnfast can map four vanilla rnn neurons in each lstm unit.  moreover  rnnfast allows the mapping of other network types such as bidirectional rnns  birnn . a birnn  consists essentially of two rnns stacked on top of each other. the output is computed based on the hidden state  of both networks. in our design the two networks are mapped on the hardware in an interleaved fashion. the  aggregation hardware is used to link the two networks. the input data is also duplicated and interleaved in reverse  order  x    xn   x    xn     x    xn     ...  xn   x   .  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .     rnnfast  an accelerator for recurrent neural networks using domain wall memory               . .  rnnfast configuration. the rnnfast configuration is programmed through configuration registers that  control input assignment at pe level  input track chaining  result aggregation setup  etc. a configuration file with the  lstm network s  specifications is loaded into the device driver of the accelerator and propagated to the appropriate  registers.      error mitigation design   .  dwm position errors  as detailed in section  .    out of step  shift errors  in which the wrong bit is aligned with the read write heads   are a significant reliability challenge for dwm. since rnnfast accesses data sequentially  that means virtually all  accesses require only single position bit shifts  we focus on single bit overshift errors which are expected to occur  with a probability of            which is quite high. we used pytorch      to inject error in weights for both im txt  and seq seq models.  while prior work      has shown that neural networks are quite resilient to errors  we find that error rates on the  order of dwm overshift errors can degrade output accuracy substantially. figure    shows the accuracy of the  output for two benchmarks  measured by the bleu  bilingual evaluation understudy  metric       relative to an  error free baseline. bleu is an algorithm for evaluating the quality of text which has been machine translated  from one natural language to another. quality is considered to be the correspondence between a machine s output  and that of a human. the models that we used have reported very close bleu scores to the state of the art models      . we inject single bit overshift errors in different dwm components of rnnfast  the racetrack chains used  to hold inputs and outputs for each nn layer  the weights associated with all pes  the dwm components of the  logic functions  mac units and the nonlinear functions . shift errors are modeled as a uniform distribution with an  overshift probability of  .              .   .       .      no err.  err. in logic  err. in inputs  err. in weights  combined     .      .    .     .    .     .      .    .     .    .     .      .          .    output bleu score    output bleu score     .      no err.  weight  frac  input  frac  weight int  input  int     .      .    im txt    seq seq    fig.   . output accuracy  bleu score  for logic  inputs and  weights components.         im txt    seq seq    fig.   . output accuracy  bleu score  for integer and fraction  components.    figure    shows that when errors are injected only in the logic  the drop in output accuracy is very low       for im txt and    for seq seq  two of the benchmarks we run. this is because overshift off by one errors in the  mac and nonlinear functions tend to produce results that are relatively close to the correct value. as a result   the accuracy of the output is very high. however  when errors are injected into the input chains and the weight  arrays  the output accuracy drops dramatically to between     and     of the original. when errors are injected  uniformly in all dwm tracks  the output accuracy drops below    for im txt and below     for seq seq  meaning  that the results are essentially useless. this data highlights that mitigation solutions for errors in the inputs as well  as weights are essential.  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .                  mohammad hossein samavatian  anys bacha  li zhou  and radu teodorescu    to better understand which errors have the worst effect on output quality  we selectively inject errors into  different bits of data words. rnnfast uses   s complement fixed point representation for both inputs and weights.  we inject errors separately into the integer and the fraction portions of the word. figure    shows the results of this  experiment. when errors are injected only in the fraction  the drop in accuracy is less than    for both inputs and  weights in im txt. for seq seq the accuracy degradation is worse when errors are injected in the weights compared  to inputs  but overall output quality is still reasonably high.  injecting errors with the same probability in the integer portion of the data words has a much more dramatic  effect  leading to a drop in output accuracy of between     and    . the large effect is due to the fact that in  these workloads both inputs and weights are represented with small fractional numbers. a single bit flip of the  integer fraction can turn a small number into a much larger value  which has a disproportionate effect on the rest of  the network.  the large effect on output accuracy is caused by the fact that due to   s complement representation  a single shift  error in a data word storing a small value can cause that value to be interpreted as a large value with the opposite  sign. for example the binary          .            .        in decimal  would flip into          .              .         or          .               .        when a non sign or sign bit in integer part inverted  respectively. this is also true for a negative number           .             .         turns into          .               .         after a sign bit flip.     .     rnnfast error mitigation    rnnfast addresses overshit errors by implementing an efficient error mitigation mechanism that considers the  sensitivity of rnn workloads to errors that result in very large values. we implement different error detection and  mitigation mechanisms for input output racetrack chains and for weight arrays. we take advantage of their design  characteristics to implement a more efficient sedsec design that has lower area overhead  requires fewer extra  domains and access ports compared to prior dwm edc solutions such as     .   . .  input errors. in order to detect overshit errors in the input tracks  we append a   bit pattern to the left side  of each track  as shown in the example in figure   . the figure shows a single track that stores bit n for multiple  inputs i    i  . in the initial state the error detection code  edc        is stored in the leftmost bits of the track.  input i  is read in the current cycle. at time t  the track is shifted left by   to access the next input. if the shift is  correct  the leading  check  bit should be a    . input i  is read and sent to the lstm units. a new edc code is  written at cycle t  in the first three bits of the track using three parallel write ports. note that updating the edc  does not introduce any time overhead since a write cycle already exists following each read to allow data to be  written into the next track in the chain.  at cycle t  we show an overshift error. the track has incorrectly shifted left   positions instead of  . this means  that i   instead of i    is now aligned with the read head. the check bit is now     indicating a shift error. to recover  from this error we use an additional read head to also read i  . the outputs of the two read heads are connected  to a multiplexer. the check bit value selects the multiplexer output  shown in blue in figure    . a     selects  the error free output and a     selects the overshifted output. a similar mechanism selects the correct location for  writing the input coming from the previous track in the chain. if an overshift error occurs  the write location is also  shifted to the left  as the right hand side of figure    shows.  at t  the edc code is again updated. following an overshift error the shift controller will not issue a shift  command for the following cycle  t    since the track is already properly aligned to access the next input  i    during  that cycle. note that  since individual words are stored across multiple tracks to enable single cycle access  an  overshift error will affect all inputs that share that track  up to    in our design . it is therefore important to detect  and correct these errors.  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .     rnnfast  an accelerator for recurrent neural networks using domain wall memory               . .  errors in weight arrays. a similar mechanism is deployed to detect and mitigate errors in weight arrays  associated with each pe. however  because the access timing to the weights array is more critical and weights are  stored in a more compact representation  the detection and mitigation steps are implemented differently. unlike  inputs  which move from track to track along the chain  weights are stationary at pe level and are reused multiple  times. this means that after scanning all weights  the tracks need to be returned to the initial weight. to minimize  the number of shifts  weight values are distributed both within and across multiple racetracks. weight racetracks  are provisioned with multiple read write heads    in our design . data layout is such that all read heads across all  tracks can access all the bits of a single weight simultaneously.  also  unlike the input racetrack chain  access to the weight arrays does not require a write cycle  so an update to  edc code is not feasible. we instead store a fixed edc pattern of         at the rightmost edge of the weight  tracks as shown in figure   . error detection logic detects an overshift error when the current edc bit does not  match the expected value. for instance  in the initial state  the read heads are aligned with bits from weight w  and  the error detection logic expects to read     from the edc.  at time t  a correct shift takes place and w  can be read. at time t  an overshift error occurs and weight w  is  read instead of w  . a recovery mechanism similar to the one for inputs could be employed. this would require  doubling the number of read heads in each track and extra logic. since weight storage in rnnfast is substantial   the overhead would be nontrivial. we can  however  avoid this extra overhead by leveraging the observation that  replacing the incorrect weight with  zero  yields very little loss in output accuracy compared to error free execution.  this is in contrast with using the erroneous weight  which can be a large value. the following cycle at t    the shift  controller will not shift because the track is already aligned for accessing the next weight.      evaluation methodology   .  rnnfast modeling infrastructure  we implemented a detailed behavioral model to evaluate performance  chip area and energy consumption of the  rnnfast design. a cycle level model that accounts for the latency of each component in the design is used for  the timing simulation. the simulated hardware is configured for each neural network in our benchmark set  by  enabling the appropriate number of hardware tiles  lstms and pes. since all lstm units execute independently  and in parallel  only a single lstm per tile is simulated to speed up simulation time. for the energy evaluation  the  number of reads  writes  shifts as well as decoder  adder multiplier and lut accesses are counted for all the units  in the design.  to understand the energy consumption  shift and write latency of the domain wall memory  dwm   an electrical  model is necessary. a verilog a based spice model for dwm from              was simulated on cadence  virtuoso. the dwm model estimates the effective resistance as a function of the length of the track and uses width  and thickness of the strip to calculate current density and position shift. a cadence component was created for the  dwm model and a test bench was setup to stimulate the device. a sensitivity analysis was conducted to study the  effect of track length on shift latency and energy.  table   shows the characteristics of the dwm we model and also lists the architectural parameters for rnnfast  and power area breakdown for different components. as weight values are in    bits precision  each four set of  racetracks stores    weights. for storing     weights each pe needs     racetracks  table   . we performed energy  analysis on the number of lstms per tile and chose the number of lstm per tile as   . more details are in section   . . the number of accumulator  multiplier  sigmoid and tanh units in the aggregation unit  figures   and    is  optimized for energy and performance. the smallest number of units that allows the lstm to operate without stall  cycles is chosen.    acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .                  mohammad hossein samavatian  anys bacha  li zhou  and radu teodorescu    write    from previous track    edc    i   i   ...          read  initial  state           i  i  i  i     ...                 to the next track      i  i  i  i  i     ...  i              i             t   write           i  i  i  i     ...    t   shift      i  i  i  i  i  i     ...  i              i     t   write                i  i  i  i     t                  i  i  i  i     no shift    i              t   read         over shift err.    t   read         correct shift    t   shift    to lstms    ...  ...    i    i      fig.   . mitigation mechanism for overshift errors in the input track chains.    delay shift sig.    b  bit index    wn   weight index  initial state    error  detection logic    r    r    ...    shift err. sig.    shift  controller    adder  clock  gate sig.    edc    w   w      w      ...    w   w      w      ... w                     ...    w   w      w      ...    w   w      w      ...                        ...    t      ... w   w   w   w   w      ...    w   w      w      ...                        ...    t   no shift    ... w   w   w   w   w      ...    w   w      w      ...                        ...    t  correct  shift  over shift  err.    ...    w      fig.   . mitigation mechanism for overshift errors in the weight track chains.     . .  rnnfast design variations. we compare our design with two alternative rnnfast architectures that use  cmos and memristor technologies. we call them rnnfast cmos and isaac rnn respectively. for rnnfastcmos  we used sram buffers for both lstm inputs and weight storage within pes. mac units are also  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .     rnnfast  an accelerator for recurrent neural networks using domain wall memory              dwm properties  racetrack width length thickness  number of bits per track  read shift write latency  read shift write energy     f     f    nm       ns    . ns    . ns   .  pj    .  pj    . fj    domain length  effective cell size  technology node     f   .  f      nm    power mw    .      area  m      .       .                      .   mm      .              .              tile properties  component  input buffer  lstm unit    configuration    track tile  with edc     per tile    total tile    specification     stripes track     cell stripe    pes lstm    aggre. lstm      pes     aggre. unit    pe properties  mac  weight array      pe    track pe  with edc    accumulator  multiplier  sigmoid  tanh      lstm    lstm    lstm    lstm        adder      stripes track     cell stripe    aggregation unit properties  approx. nonlinear func. design  approx. nonlinear func. design    on chip dw memory  size     mb   r w ports  area   . mm    acc. eng.   .  nj  acc. lat.   .  ns  leakage   . mw    table  . racetrack memory and rnnfast design parameters with associated power and area overheads.    implemented with cmos logic. we used sram based luts for the nonlinear functions. input sram buffers are  also chained like racetrack memories in order to deliver all inputs to all lstm units.  isaac rnn is an isaac      like design for rnn that stores inputs in edram and is entirely cmos and  memristor based. isaac rnn uses    x      bit memristor crossbars  similar to what was used in isaac  for  the dot product engine. we kept the input buffer and aggregation unit designs same as rnnfast in order to only  see the effect of memristor in the design and have a more fair comparison since edram and cmos logic has  higher energy consumption than dwm. each memristor dot product engine is capable of          multiplications  in parallel      inputs by    weights . in an lstm neuron each input is multiplied by   different weight sets. thus   each memristor dot product engine can handle   neurons  making each crossbar in isaac rnn computationally  equivalent to   lstms in rnnfast. thus there are    lstm units per tile for isaac rnn instead of    per tile  in rnnfast. inputs go bit by bit to the memristor crossbars. however  a chuck of     inputs needs to be supplied in  a single cycle. we changed the input layout to maximize the performance of isaac rnn  for a fair comparison.   . .  gpu baseline. we choose as a baseline system for evaluation a gpgpu optimized for machine learning   the nvidia tesla p     pascal architecture  with   gb of cowos hbm  memory. all our benchmarks use the  dnn optimized cudnn nvidia libraries version        which delivers roughly    performance improvement  relative to a standard gpu implementation for lstm on torch    . we measure runtime of the forward passes  through the lstm layers using instrumentation in deepbench. we measure power consumption using the nvidia  smi profiler. since the smi provides total board power  in order to isolate the active power of the gpu we subtract  power measured at gpu idle. since the board components are less energy proportional with activity compared to  the gpu  they will account for most of the idle power.   . .  puma. we also compared our design with puma     a recently proposed dnn accelerator built with  reram. the authors of puma released a simulator and toolchain that we use to compile and run our benchmarks.  we used the puma compiler to find the number of tiles required for each benchmark. we then set the simulator  configuration file to inference mode and used the puma simulator to measure runtime and energy consumption.  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .                   .     mohammad hossein samavatian  anys bacha  li zhou  and radu teodorescu    benchmarks    we used lstm based rnn workloads from the deepbench     open source benchmark suite for dnns  released  by baidu. for our experiments we used   bench.    platform    precision    im txt  seq seq    deepbench  deepbench       bit     bit    mach tran    deepbench       bit    lang mod  d speech    deepbench  deepbench       bit     bit    layers   neurons                                                          timestep            image caption  language translation          machine translation                language modeling  deep speech    description    table  . summary of the benchmarks evaluated.    image caption generator  this benchmark is based on the  show and tell  model       which is an encoderdecoder type neural network. the decoder is an lstm rnn that generates captions from a fixed length vector  input.  sequence to sequence model  this benchmark is based on the rnn encoder decoder model by cho et al.        which performs language translation. the encoder and decoder are   layer lstm networks. machine translation   also based on the rnn encoder decoder model by cho et al.     .  language modeling  a probability distribution over sequences of words. it is used in speech recognition   sentiment analysis  information retrieval and other applications     .  deep speech  a speech to text engine that uses a model trained by machine learning techniques  based on  baidu s deep speech research     .  all benchmarks are run using    bit precision arithmetic on both rnnfast and the p    gpu.         evaluation    we evaluate the rnnfast performance and energy consumption compared to the nvidia gpu  the cmos based  and the memristor based rnnfast design. we evaluate the reliability of the rnnfast error mitigation. we show  an area utilization estimate for different benchmarks. we also include a high level comparison to other rnn  accelerators.  gpu p     puma    isaac rnn  rnnfast cmos    rnnfast                       .        normalized energy consumption  log scale     speedup relative to gpu  log scale             im   tx    t    seq    ma     se    q    ma  ma  lan  d s  g m  g m  chchchpee  ean  tran  tran  tran  od  ch                              gpu p     puma    isaac rnn  rnnfast cmos    rnnfast     .      .       .       im     txt    seq     se    ma  q    ch     tran    ma  ma  lan  d s  g  m  g m  chchpee  ean  tran  tran  od  ch                              fig.   . rnnfast  rnnfast cmos  isaac rnn and puma fig.   . energy consumption for rnnfast  rnnfast cmos   runtime relative to the gpu p    execution.  isaac rnn and puma relative to the gpu p   .    acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .     rnnfast  an accelerator for recurrent neural networks using domain wall memory       .             performance improvement and energy saving    figure    shows the execution time speedup for rnnfast  rnnfast cmos and isaac rnn relative to the p     gpu for the seven benchmarks we run. rnnfast speedup relative to the gpu varies between     for im txt  and   .   for d speech  with an average speedup of   .  . rnnfast speedups increase with the network size   demonstrating the excellent scalability of the design. for instance  in mach trans we test three different network  sizes ranging from     to       we observe speedups increases from   .   to   .  . this is because the large  number of threads required to handle the larger network becomes a bottleneck even for the gpu  whereas rnnfast  scales much better.  isaac rnn also brings a substantial speedup relative to the gpu ranging between  .    for im txt and   .   for d speech. although this is significant  isaac rnn is more than  .   slower than the dwm rnnfast  implementation. this is primarily due to the higher latency of the lstm unit in isaac rnn  which is  .    higher than a rnnfast lstm unit. the higher latency is due to the memristor array read latency     ns  and  overheads that stem from the adc dac components. even though a single memristor array can handle up to    neurons  which increases throughput  isaac rnn is still fundamentally slower than rnnfast. rnnfast cmos  shows  .   speedup compared to rnnfast. this is due to faster cmos adders and random memory access instead  of the shift based access in rnnfast.  the puma reram based design is more general that issac and rnnfast  supporting both cnns and dnns.  however  its performance is lower than both isaac rnn and rnnfast. in general  puma tends to have better  performance than the gpu for larger networks  especially for multi layer networks  seq seq  where puma benefits  from its pipelined architecture.  figure    shows the energy consumption for rnnfast  rnnfast cmos and isaac rnn relative to the gpu  in log scale. rnnfast reduces energy consumption on average by    . this is due to a much faster execution  time achieved with about     the power of a gpu. the rnnfast cmos design has     higher energy compared  to rnnfast. this is reaches a      increase for d speech due to higher resource demand  which increases the  leakage energy for both compute and memory logic in cmos. this causes the cmos design to reach its maximum  tdp for smaller networks. isaac rnn also has higher energy usage than rnnfast due to its adc dac and  cmos logic.puma energy consumption is much lower than the gpu  however  as expected is not lower than  isaac rnn. rnnfast is much more energy efficient  using about     the energy of puma.  rnnfast offers a much more scalable design relative to a gpu due to its modular design and very high storage  density of dwm. figure    shows the log scale of execution time for the mach tran benchmark as a function of  problem  neural network  size ranging from     nodes to   k nodes per layer in a single layer configuration. for  problem sizes larger then   k  the gpu runs fail because the device runs out of memory. the gpu execution  time exhibits a super linear increase in execution time with problem size due to memory pressure. rnnfast is  consistently faster than the gpu in the range of   .     . k  to         k  and also scales better to very large  problem sizes of   k nodes and beyond. isaac rnn scales similarly to rnnfast but it is also  .   slower that  rnnfast on average for mach tran. rnnfast cmos shows almost    speedup over rnnfast  at the cost of  much higher energy.  figure    shows a similar trend for im txt. the gpu shows good performance up to  . k  but run time increases  exponentially beyond that.     .     error mitigation    we also evaluate rnnfast resilience to position errors. figure    shows the accuracy of the output as evaluated  by the bleu metric       as a function of the probability of position errors. we can see that for a relatively low  probability of errors of  .         the output accuracy is virtually unaffected. this is primarily due to the inherent  robustness of the rnn to errors. however  without error mitigation  the output accuracy degrades substantially at  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .          execution time  log scale                      mohammad hossein samavatian  anys bacha  li zhou  and radu teodorescu             gpu p     rnnfast cmos  isaac rnn  rnnfast    execution time  log scale                                .    .   k     .  k     . k     k     k     k     k      k            gpu p     rnnfast cmos  isaac rnn  rnnfast                       .    .   k     .  k     . k    number of lstm cells     k     k     k     k      k    number of lstm cells    fig.   . rnnfast  isaac rnn and gpu execution times vs. fig.   . rnnfast  isaac rnn and gpu execution times vs.  net size for mach tran  normalized to rnnfast  .   k.  network size for im txt  normalized to rnnfast  .   k.    higher errors rates. in the region around  .          highlighted region   which is the expected rate for single bit  position errors  the output accuracy drops to     for im txt and     for seq seq  an unacceptable performance for  most applications. when rnnfast error mitigation is enabled the drop in output accuracy is negligible at less than    .  the rnnfast error mitigation produces outputs with less than    accuracy loss even for much higher error rates  of      or around     accuracy loss for      . this shows that rnnfast edc is robust to much higher error rates  than what is expected for dwm technology.  it is also worth highlighting the fact that error mitigation incurs no performance penalty even when errors are  detected. correction or mitigation are performed without stalling the execution pipeline. this is an important design  consideration because of the highly synchronized nature of the design. a single stall to correct an error would  result in lost cycles for thousands of functional units.     .     nonlinear function hardware    we evaluate two designs for the nonlinear function hardware  a lut based implementation  and an approximate  logic function based unit. the function based implementation is area efficient since it does not require as much  storage as the lut based design. however the computation required  albeit simple  is slower than the simple  lookup of the lut version. the activation functions are not a significant latency bottleneck. however  at this scale  we have thousands of such units on chip and reducing their area adds up to real savings. figure    shows the  storage savings and performance degradation of the function based sigmoid tanh relative to the lut design for  multiple network sizes. the storage savings diminish as the network size increases because the storage space for  the weights dominates. for large networks the storage savings are about     which represents   gb of dwm for  a   k network. as for the performance cost  it starts at about     but falls below    for larger networks. the  approximated nonlinear function does not result in loss of accuracy as measured by the bleu score.     .     rnnfast parameter tuning    we also conduct a sensitivity analysis on number of lstm units per tile. figure    illustrates the tile input buffer  energy versus different number of lstms per tile for different network size. as the number of lstms per tile  increases  the power area overhead for the within tile bus increases super linearly. the minimum energy point is  different depending on the size of the network. the    lstm units per tile represents a reasonable compromise for  medium to large networks.  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .     rnnfast  an accelerator for recurrent neural networks using domain wall memory      im txt w o err det  im txt rnnfast    seq seq w o err det  seq seq rnnfast                  storage saving  performance degradation     .           .     percentage    output blue score     .           .    .             .    .            .    .               .  e        .  e      .  e      .  e     position error probability     .  e       fig.   . output accuracy for benchmarks    im txt and seq seq with and without rnnfast edc.    tile input energy  log scale                        .   k     .  k     . k     .  e        k     k     k     k      k    number of lstm cells    fig.   . storage saving and performance degradation for different network sizes for approx. function based sigmoid design relative to lut.                  .           power     .            .     .       .      .     .       .       .      bus power          .     .        .     .       .                                                            lstm per tile for di erent network size    fig.   . sensitivity analysis for the number of lstms per tile.     .     comparison to other rnn accelerators    several recent papers have proposed fpga based accelerators for rnns                                     . we  provide a qualitative comparison with some of the more recent ones  for which runtime and energy numbers were  available and similar applications were evaluated. table   summarizes the energy and runtime for fpga based  designs from                  as well as the energy and runtime of rnnfast while running networks of equivalent  size.  the networks used in              vary from vary small to large. rnnfast shows from  .   to     speedup.  compared to      rnnfast has     less energy consumption.  recently fowers et al.     introduced brainwave  an fpga based accelerator for rnn with no batching for real  time ai. while a very efficient design  brainwave has        higher energy energy than rnnfast. brainwave  also shows poorer performance for smaller networks  but slightly better performance for large ones  compared to  rnnfast. note that this is not a quantitative apples to apples comparison to our design given that brainwave uses    bit precision  vs    bit for rnnfast  and a   nm techology node  vs.   nm for rnnfast .  the google tpu is also capable of running rnn workloads efficiently. in      they report up to    better  performance for lstm workloads compared to nvidia k  . rnnfast is up to      faster than the newer  nvidia p    for workloads of similar size.  acm j. emerg. technol. comput. syst.  vol.    no.    article  . publication date  january     .                  mohammad hossein samavatian  anys bacha  li zhou  and radu teodorescu  fpga  design                            net size    timesteps    run time  s     energy   j                         k  k                              .       .  e     .                 .   na  na  est.                   rnnfast  run time   s    .      .  e    .              .     rnnfast  energy   j    .       .  e     .                   table  . energy and run time for fpga based rnns.         other related work    many customized accelerators for machines learning algorithms and dnns have been proposed recently                                                . the majority of this work focuses on improving the performance of  cnns  exploring the potential for resources sharing  leveraging emerging memory technologies  optimizing basic  operations  and developing domain specific methods.       used compression of the network model to reduce the memory footprint and accelerate real time networks in  which batching cannot be employed to improve data re use. eyeriss      explored local data reuse of filter weights  and activations in high dimensional convolutions in order to minimize the energy of data movement.  emerging memory technologies and in memory processing have been leveraged for cnn designs to address  memory latency limitations and to implement custom logic. prime      combined processor in memory architecture and reram based neural network computation. the crossbar array structure in reram can be used to  perform matrix vector multiplication as well as regular memory to increase memory space. puma      a recently  proposed general purpose and isa programmable accelerator built with reram. it has a spatial architecture  organized in cores  tiles  and nodes. puma features a microarchitecture  isa  and compiler co designed to optimize  data movement and maximize energy and area efficiency. the puma design is more general than isaac        and  as a result  it generally performs a bit worse in terms of throughput and energy efficiency. reram based  dnn accelerators benefit from the speed and efficiency of the memristor crossbar  however the need for additional peripheral circuits such as adcs and dacs  and other components  reduce the benefits of crossbar based  computation.  neurocube      proposed a programmable and scalable digital neuromorphic architecture based on  d highdensity memory integrated with a logic tier for efficient neural computing. the design in      also used reram  cross bar for rnn acceleration for a case of human activity detection with small network size of     and simple  vanilla rnn. cambricon      propose a novel domain specific instruction set architecture  isa  for neural  network accelerators. pudiannao      focuses on a range of popular machine learning algorithms. however all  these optimizations are cnns dnns specific. chung et. al      used dwm for cnn computations as well. they  proposed a new design that replaces the reram cross bar with a dwm based cnn layer for dot product. however   they still use costly adc dac circuits and also did not address dwm shift errors in their design.         conclusion    the unprecedented growth of available data is accelerating the adoption of deep learning across a wide range of  applications including speech recognition  machine translation  and language modeling. in this study  we propose  rnnfast  a novel accelerator designed for recurrent neural networks. our approach demonstrates that using domain  wall memory is not only feasible  but also very efficient. we compare our design with a state of the art p     nvidia gpu and find   .   better performance with     lower energy.    