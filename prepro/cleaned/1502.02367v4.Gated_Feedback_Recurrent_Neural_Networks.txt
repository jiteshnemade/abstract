introduction  recurrent neural networks  rnns  have been widely studied and used for various machine learning tasks which involve sequence modeling  especially when the input and  output have variable lengths. recent studies have revealed  that rnns using gating units can achieve promising results in both classification and generation tasks  see  e.g.   graves        bahdanau et al.        sutskever et al.        .  although rnns can theoretically capture any long term  dependency in an input sequence  it is well known to be  difficult to train an rnn to actually do so  hochreiter     junyoung . chung   umontreal . ca  caglar . gulcehre   umontreal . ca  kyunghyun . cho   umontreal . ca  find   me   the . web          bengio et al.        hochreiter       . one of the  most successful and promising approaches to solve this issue is by modifying the rnn architecture e.g.  by using a  gated activation function  instead of the usual state to state  transition function composing an affine transformation and  a point wise nonlinearity. a gated activation function   such as the long short term memory  lstm  hochreiter    schmidhuber        and the gated recurrent unit  gru   cho et al.         is designed to have more persistent memory so that it can capture long term dependencies more easily.  sequences modeled by an rnn can contain both fast  changing and slow changing components  and these underlying components are often structured in a hierarchical  manner  which  as first pointed out by el hihi   bengio         can help to extend the ability of the rnn to learn  to model longer term dependencies. a conventional way to  encode this hierarchy in an rnn has been to stack multiple levels of recurrent layers  schmidhuber        el hihi    bengio        graves        hermans   schrauwen        . more recently  koutn  k et al.        proposed a  more explicit approach to partition the hidden units in an  rnn into groups such that each group receives the signal from the input and the other groups at a separate  predefined rate  which allows feedback information between  these partitions to be propagated at multiple timescales.  stollenga et al.        recently showed the importance of  feedback information across multiple levels of feature hierarchy  however  with feedforward neural networks.  in this paper  we propose a novel design for rnns  called a  gated feedback rnn  gf rnn   to deal with the issue of  learning multiple adaptive timescales. the proposed rnn  has multiple levels of recurrent layers like stacked rnns  do. however  it uses gated feedback connections from upper recurrent layers to the lower ones. this makes the hidden states across a pair of consecutive timesteps fully connected. to encourage each recurrent layer to work at different timescales  the proposed gf rnn controls the strength  of the temporal  recurrent  connection adaptively. this ef      gated feedback recurrent neural networks    fectively lets the model to adapt its structure based on the  input sequence.  we empirically evaluated the proposed model against the  conventional stacked rnn and the usual  single layer rnn  on the task of language modeling and python program evaluation  zaremba   sutskever       . our experiments reveal that the proposed model significantly outperforms the  conventional approaches on two different datasets.    long short term memory  lstm  was proposed by  hochreiter   schmidhuber        to specifically address  this issue of learning long term dependencies. the lstm  maintains a separate memory cell inside it that updates and  exposes its content only when deemed necessary. more recently  cho et al.        proposed a gated recurrent unit   gru  which adaptively remembers and forgets its state  based on the input signal to the unit. both of these units are  central to our proposed model  and we will describe them  in more details in the remainder of this section.     . recurrent neural network  an rnn is able to process a sequence of arbitrary length  by recursively applying a transition function to its internal  hidden states for each symbol of the input sequence. the  activation of the hidden states at timestep t is computed as a  function f of the current input symbol xt and the previous  hidden states ht      ht  f  xt   ht     .  it is common to use the state to state transition function f  as the composition of an element wise nonlinearity with an  affine transformation of both xt and ht      ht     w xt   u ht                 where w is the input to hidden weight matrix  u is the  state to state recurrent weight matrix  and   is usually a  logistic sigmoid function or a hyperbolic tangent function.  we can factorize the probability of a sequence of arbitrary  length into  p x            xt     p x   p x    x          p xt   x            xt     .  then  we can train an rnn to model this distribution by  letting it predict the probability of the next symbol xt    given hidden states ht which is a function of all the previous symbols x            xt   and current symbol xt    p xt     x            xt     g  ht   .  this approach of using a neural network to model a probability distribution over sequences is widely used  for instance  in language modeling  see  e.g.  bengio et al.         mikolov       .   . . gated recurrent neural network  the difficulty of training an rnn to capture long term dependencies has been known for long  hochreiter         bengio et al.        hochreiter       . a previously successful approaches to this fundamental challenge has been  to modify the state to state transition function to encourage  some hidden units to adaptively maintain long term memory  creating paths in the time unfolded rnn  such that  gradients can flow over many timesteps.     . . . l ong s hort t erm m emory  since the initial      proposal  several variants of the  lstm have been introduced  gers et al.        zaremba  et al.       . here we follow the implementation provided  by zaremba et al.       .  such an lstm unit consists of a memory cell ct   an input  gate it   a forget gate ft   and an output gate ot . the memory  cell carries the memory content of an lstm unit  while  the gates control the amount of changes to and exposure  of the memory content. the content of the memory cell  cjt of the j th lstm unit at timestep t is updated similar  to the form of a gated leaky neuron  i.e.  as the weighted  sum of the new content c jt and the previous memory content  cjt   modulated by the input and forget gates  ijt and ftj    respectively   cjt   ftj cjt     ijt c jt             c t   tanh  wc xt   uc ht     .           where    the input and forget gates control how much new content  should be memorized and how much old content should be  forgotten  respectively. these gates are computed from the  previous hidden states and the current input   it     wi xt   ui ht                 ft     wf xt   uf ht                p     p  where it   ikt k   and ft   ftk k   are respectively the  vectors of the input and forget gates in a recurrent layer  composed of p lstm units.      is an element wise logistic sigmoid function. xt and ht   are the input vector and  previous hidden states of the lstm units  respectively.  once the memory content of the lstm unit is updated  the  hidden state hjt of the j th lstm unit is computed as        hjt   ojt tanh cjt .  the output gate ojt controls to which degree the memory  content is exposed. similarly to the other gates  the output gate also depends on the current input and the previous     gated feedback recurrent neural networks    hidden states such that  ot      wo xt   uo ht     .           in other words  these gates and the memory cell allow an  lstm unit to adaptively forget  memorize and expose the  memory content. if the detected feature  i.e.  the memory  content  is deemed important  the forget gate will be closed  and carry the memory content across many timesteps   which is equivalent to capturing a long term dependency.  on the other hand  the unit may decide to reset the memory  content by opening the forget gate. since these two modes  of operations can happen simultaneously across different  lstm units  an rnn with multiple lstm units may capture both fast moving and slow moving components.   . . . g ated r ecurrent u nit  the gru was recently proposed by cho et al.       . like  the lstm  it was designed to adaptively reset or update  its memory content. each gru thus has a reset gate rtj  and an update gate ztj which are reminiscent of the forget  and input gates of the lstm. however  unlike the lstm   the gru fully exposes its memory content each timestep  and balances between the previous memory content and the  new memory content strictly using leaky integration  albeit  with its adaptive time constant controlled by update gate  ztj .  at timestep t  the state hjt of the j th gru is computed by  hjt        ztj  hjt     ztj h jt             where hjt   and h jt respectively correspond to the previous memory content and the new candidate memory content. the update gate ztj controls how much of the previous  memory content is to be forgotten and how much of the  new memory content is to be added. the update gate is  computed based on the previous hidden states ht   and the  current input xt    zt     wz xt   uz ht                 the new memory content h jt is computed similarly to the  conventional transition function in eq.       h t   tanh  w xt   rt  where    u ht                 is an element wise multiplication.    one major difference from the traditional transition function  eq.      is that the states of the previous step ht    is modulated by the reset gates rt . this behavior allows  a gru to ignore the previous hidden states whenever it is  deemed necessary considering the previous hidden states  and the current input   rt     wr xt   ur ht     .            the update mechanism helps the gru to capture longterm dependencies. whenever a previously detected feature  or the memory content is considered to be important  for later use  the update gate will be closed to carry the current memory content across multiple timesteps. the reset  mechanism helps the gru to use the model capacity efficiently by allowing it to reset whenever the detected feature  is not necessary anymore.     . gated feedback recurrent neural  network  although capturing long term dependencies in a sequence  is an important and difficult goal of rnns  it is worthwhile to notice that a sequence often consists of both slowmoving and fast moving components  of which only the  former corresponds to long term dependencies. ideally  an  rnn needs to capture both long term and short term dependencies.  el hihi   bengio        first showed that an rnn can capture these dependencies of different timescales more easily  and efficiently when the hidden units of the rnn is explicitly partitioned into groups that correspond to different timescales. the clockwork rnn  cw rnn   koutn  k  et al.        implemented this by allowing the i th module to operate at the rate of  i     where i is a positive  integer  meaning that the module is updated only when  t mod  i      . this makes each module to operate at different rates. in addition  they precisely defined the connectivity pattern between modules by allowing the i th module  to be affected by j th module when j   i.  here  we propose to generalize the cw rnn by allowing  the model to adaptively adjust the connectivity pattern between the hidden layers in the consecutive timesteps. similar to the cw rnn  we partition the hidden units into multiple modules in which each module corresponds to a different layer in a stack of recurrent layers.  unlike the cw rnn  however  we do not set an explicit  rate for each module. instead  we let each module operate at different timescales by hierarchically stacking them.  each module is fully connected to all the other modules  across the stack and itself. in other words  we do not define the connectivity pattern across a pair of consecutive  timesteps. this is contrary to the design of cw rnn and  the conventional stacked rnn. the recurrent connection  between two modules  instead  is gated by a logistic unit           which is computed based on the current input and  the previous states of the hidden layers. we call this gating  unit a global reset gate  as opposed to a unit wise reset gate  which applies only to a single unit  see eqs.     and     .     gated feedback recurrent neural networks     a  conventional stacked rnn     b  gated feedback rnn    figure  . illustrations of  a  conventional stacking approach and  b  gated feedback approach to form a deep rnn architecture. bullets  in  b  correspond to global reset gates. skip connections are omitted to simplify the visualization of networks.    layer is computed by    the global reset gate is computed as     g    i j               wgi j    hj    t         ui j  g    h t           hjt      tanh w    j   j    hj    t              where h t   is the concatenation of all the hidden states  from the previous timestep t    . the superscript i j is  an index of associated set of parameters for the transition  from layer i in timestep t     to layer j in timestep t. wgi j  are respectively the weight vectors for the current  and ui j  g  is  input and the previous hidden states. when j      hj    t  xt .  in other words  the signal from hit   to hjt is controlled by  a single scalar g i j which depends on the input xt and all  the previous hidden states h t   .  we call this rnn with a fully connected recurrent transitions and global reset gates  a gated feedback rnn  gfrnn . fig.   illustrates the difference between the conventional stacked rnn and our proposed gf rnn. in both  models  information flows from lower recurrent layers to  upper recurrent layers. the gf rnn  however  further  allows information from the upper recurrent layer  corresponding to coarser timescale  flows back into the lower  recurrent layers  corresponding to finer timescales.  in the remainder of this section  we describe how to use  the previously described lstm unit  gru  and more traditional tanh unit in the gf rnn.    l  x       g    i j    u    i j    hit           i      where l is the number of hidden layers  w j   j and  u i j are the weight matrices of the current input and  the previous hidden states of the i th module  respectively.  compared to eq.      the only difference is that the previous hidden states are from multiple layers and controlled  by the global reset gates.  long short term memory and gated recurrent unit.  in the cases of lstm and gru  we do not use the global  reset gates when computing the unit wise gates. in other  words  eqs.         for lstm  and eqs.     and      for  gru are not modified. we only use the global reset gates  when computing the new state  see eq.     for lstm  and  eq.     for gru .  the new memory content of an lstm at the j th layer is  computed by     l  x  j  j   j j    i j i j i  c t   tanh wc  ht    g  uc ht   .  i      in the case of a gru  similarly   h jt      tanh w    j   j    hj    t         rjt    l  x       g    i j    u    i j    hit      .    i       . experiment settings     . . practical implementation of gf rnn     . . tasks    tanh unit. for a stacked tanh rnn  the signal from the  previous timestep is gated. the hidden state of the j th    we evaluated the proposed gf rnn on character level language modeling and python program evaluation. both     gated feedback recurrent neural networks    tasks are representative examples of discrete sequence  modeling  where a model is trained to minimize the negative log likelihood of training sequences     min       n tn       xx    log p xnt   xn    . . .   xnt          n n   t      where   is a set of model parameters.    table  . the sizes of the models used in character level language  modeling. gated feedback l is a gf rnn with a same number  of hidden units as a stacked rnn  but more parameters . the  number of units is shown as  number of hidden layers      number of hidden units per layer .    unit    architecture      of units    tanh    single  stacked  gated feedback                                  gru    single  stacked  gated feedback  gated feedback l                                          lstm    single  stacked  gated feedback  gated feedback l                                           . . . l anguage m odeling  we used the dataset made available as a part of the human  knowledge compression contest  hutter       . we refer  to this dataset as the hutter dataset. the dataset  which  was built from english wikipedia  contains     mbytes of  characters which include latin alphabets  non latin alphabets  xml markups and special characters. closely following the protocols in  mikolov et al.        graves          we used the first    mbytes of characters to train a model   the next   mbytes as a validation set  and the remaining  as a test set  with the vocabulary of     characters including a token for an unknown character. we used the average  number of bits per character  bpc  e   log  p  xt    ht      to measure the performance of each model on the hutter  dataset.   . . . p ython p rogram e valuation     . . models  we compared three different rnn architectures  a singlelayer rnn  a stacked rnn and the proposed gf rnn. for  each architecture  we evaluated three different transition  functions  tanh   affine  long short term memory  lstm   and gated recurrent unit  gru . for fair comparison  we  constrained the number of parameters of each model to be  roughly similar to each other.    zaremba   sutskever        recently showed that an rnn   more specifically a stacked lstm  is able to execute a short  python script. here  we compared the proposed architecture against the conventional stacking approach model on  this task  to which refer as python program evaluation.    for each task  in addition to these capacity controlled experiments  we conducted a few extra experiments to further  test and better understand the properties of the gf rnn.    scripts used in this task include addition  multiplication   subtraction  for loop  variable assignment  logical comparison and if else statement. the goal is to generate  or predict  a correct return value of a given python script. the  input is a program while the output is the result of a print  statement  every input script ends with a print statement.  both the input script and the output are sequences of characters  where the input and output vocabularies respectively  consist of    and    symbols.    for the task of character level language modeling  we constrained the number of parameters of each model to correspond to that of a single layer rnn with      tanh units   see table   for more details . each model is trained for at  most     epochs.    the advantage of evaluating the models with this task is  that we can artificially control the difficulty of each sample  input output pair . the difficulty is determined by  the number of nesting levels in the input sequence and the  length of the target sequence. we can do a finer grained  analysis of each model by observing its behavior on examples of different difficulty levels.  in python program evaluation  we closely follow  zaremba    sutskever        and compute the test accuracy as the  next step symbol prediction given a sequence of correct  preceding symbols.     . . . l anguage m odeling    we used rmsprop  hinton        and momentum to tune  the model parameters  graves       . according to the  preliminary experiments and their results on the validation  set  we used a learning rate of  .    and momentum coefficient of  .  when training the models having either gru  or lstm units. it was necessary to choose a much smaller  learning rate of          in the case of tanh units to ensure  the stability of learning. whenever the norm of the gradient  explodes  we halve the learning rate.  each update is done using a minibatch of     subsequences  of length     each  to avoid memory overflow problems  when unfolding in time for backprop. we approximate full  back propagation by carrying the hidden states computed  at the previous update to initialize the hidden units in the  next update. after every     th update  the hidden states     gated feedback recurrent neural networks     a  gru     b  lstm    figure  . validation learning curves of three different rnn architectures  stacked rnn  gf rnn with the same number of model  parameters and gf rnn with the same number of hidden units. the curves represent training up to     epochs. best viewed in colors.    were reset to all zeros.  table  . test set bpc  lower is better  of models trained on the  hutter dataset for a     epochs.     the gated feedback rnn  with the global reset gates fixed to    see sec.  .  for details .  bold indicates statistically significant winner over the column   same type of units  different overall architecture .    single layer  stacked  gated feedback  gated feedback l  feedback     tanh   .      .      .             gru   .      .      .      .          lstm   .      .      .      .      .        . . . p ython p rogram e valuation  for the task of python program evaluation  we used an  rnn encoder decoder based approach to learn the mapping from python scripts to the corresponding outputs as  done by cho et al.         sutskever et al.        for machine translation. when training the models  python scripts  are fed into the encoder rnn  and the hidden state of the  encoder rnn is unfolded for    timesteps. prediction is  performed by the decoder rnn whose initial hidden state  is initialized with the last hidden state of the encoder rnn.  the first hidden state of encoder rnn h  is always initialized to a zero vector.  for this task  we used gru and lstm units either with  or without the gated feedback connections. each encoder  or decoder rnn has three hidden layers. for gru  each  hidden layer contains     units  and for lstm each hidden  layer contains     units.  following zaremba   sutskever         we used the mixed    curriculum strategy for training each model  where each  training example has a random difficulty sampled uniformly. we generated          examples using the script  provided by zaremba   sutskever         with the nesting  randomly       sampled from        and the target length from          .  we used adam  kingma   ba        to train our models   and each update was using a minibatch with     sequences.  we used a learning rate of  .    and    and    were both  set to  .  . we trained each model for    epochs  with  early stopping based on the validation set performance to  prevent over fitting.  at test time  we evaluated each model on multiple sets of  test examples where each set is generated using a fixed target length and number of nesting levels. each test set contains        examples which are ensured not to overlap with  the training set.     . results and analysis   . . language modeling  it is clear from table   that the proposed gated feedback architecture outperforms the other baseline architectures that  we have tried when used together with widely used gated  units such as lstm and gru. however  the proposed architecture failed to improve the performance of a vanillarnn with tanh units. in addition to the final modeling  performance  in fig.    we plotted the learning curves of  some models against wall clock time  measured in seconds . rnns that are trained with the proposed gatedfeedback architecture tends to make much faster progress  over time. this behavior is observed both when the number  of parameters is constrained and when the number of hid      gated feedback recurrent neural networks  table  . generated texts with our trained models. given the seed at the left most column  bold faced font   the models predict next            characters. tabs  spaces and new line characters are also generated by the models.    seed    stacked lstm      pl icon      pt icon      ru icon      sv programspraket icon    text     revision     page    page    title iconology  title    id        id    revi     revision    id           id    timestamp             t        z    timestamp    contributor    username the courseichi  userrand  vehicles in   enguit  .    the inhibitors and alphabetsy and moral   hande in   in four   communications   and     title inherence relation  title    id        id    revision    id           id    timestamp             t        z    timestamp    contributor    username ro     username robert      su    aves      vi    februari      bi    agostoferos  n      pt darenetische      eo hebrew selsowen      hr   febber      io    februari      it    de februari      den units is constrained. this suggests that the proposed  gf rnn significantly facilitates optimization learning.  effect of global reset gates  after observing the superiority of the proposed gatedfeedback architecture over the single layer or conventional  stacked ones  we further trained another gf rnn with  lstm units  but this time  after fixing the global reset  gates to   to validate the need for the global reset gates.  without the global reset gates  feedback signals from the  upper recurrent layers influence the lower recurrent layer  fully without any control. the test set bpc of gf lstm  without global reset gates was  .    which is in between  the results of conventional stacked lstm and gf lstm  with global reset gates  see the last row of table    which  confirms the importance of adaptively gating the feedback  connections.  qualitative analysis  text generation  here we qualitatively evaluate the stacked lstm and gflstm trained earlier by generating text. we choose a subsequence of characters from the test set and use it as an  initial seed. once the model finishes reading the seed text   we let the model generate the following characters by sampling a symbol from softmax probabilities of a timestep and  then provide the symbol as next input.  given two seed snippets selected randomly from the test  set  we generated the sequence of characters ten times for  each model  stacked lstm and gf lstm . we show one  of those ten generated samples per model and per seed snippet in table  . we observe that the stacked lstm failed to  close the tags with   username  and   contributor   in both trials. however  the gf lstm succeeded to close    gf lstm   revision    id           id    timestamp             t        z    timestamp    contributor    username navisb  username    id        id     contributor    comment the increase from the time     username roma  username    id     id     contributor    comment vly    and when one hand  is angels and   ghost   borted and    mask r centrions      afghanistan       glencoddic tetrahedron      adjudan       dghacn    for example  in which materials  dangerous  carriers  can only use with one    both of them  which shows that it learned about the structure of xml tags. this type of behavior could be seen  throughout all ten random generations.  table  . test set bpc of neural language models trained  on the hutter dataset  mrnn   multiplicative rnn results from sutskever et al.        and stacked lstm results  from graves       .    mrnn   .      stacked lstm   .      gf lstm   .      large gf rnn  we trained a larger gf rnn that has five recurrent layers   each of which has     lstm units. this makes it possible  for us to compare the performance of the proposed architecture against the previously reported results using other  types of rnns. in table    we present the test set bpc  by a multiplicative rnn  sutskever et al.         a stacked  lstm  graves        and the gf rnn with lstm units.  the performance of the proposed gf rnn is comparable  to  or better than  the previously reported best results. note  that sutskever et al.        used the vocabulary of    characters  removed xml tags and the wikipedia markups    and their result is not directly comparable with ours. in this  experiment  we used adam instead of rmsprop to optimize the rnn. we used learning rate of  .    and    and     were set to  .  and  .    respectively.   . . python program evaluation  fig.   presents the test results of each model represented in  heatmaps. the accuracy tends to decrease by the growth     gated feedback recurrent neural networks     a  stacked rnn     b  gated feedback rnn     c  gaps between  a  and  b     figure  . heatmaps of  a  stacked rnn   b  gf rnn  and  c  difference obtained by substracting  a  from  b . the top row is the  heatmaps of models using grus  and the bottom row represents the heatmaps of the models using lstm units. best viewed in colors.    of the length of target sequences or the number of nesting  levels  where the difficulty or complexity of the python program increases. we observed that in most of the test sets   gf rnns are outperforming stacked rnns  regardless of  the type of units. fig.    c  represents the gaps between the  test accuracies of stacked rnns and gf rnns which are  computed by subtracting  a  from  b . in fig.    c   the red  and yellow colors  indicating large gains  are concentrated  on top or right regions  either the number of nesting levels or the length of target sequences increases . from this  we can more easily see that the gf rnn outperforms the  stacked rnn  especially as the number of nesting levels  grows or the length of target sequences increases.    stacked rnn with a same amount of capacity. large gflstm was able to outperform the previously reported best  results on character level language modeling. this suggests that gf rnns are also scalable. gf rnns were able  to outperform standard stacked rnns and the best previous records on python program evaluation task with varying difficulties.  we noticed a deterioration in performance when the proposed gated feedback architecture was used together with  a tanh activation function  unlike when it was used with  more sophisticated gated activation functions. more thorough investigation into the interaction between the gatedfeedback connections and the role of recurrent activation  function is required in the future.     . conclusion  we proposed a novel architecture for deep stacked rnns  which uses gated feedback connections between different layers. our experiments focused on challenging sequence modeling tasks of character level language modeling and python program evaluation. the results were  consistent over different datasets  and clearly demonstrated  that gated feedback architecture is helpful when the models are trained on complicated sequences that involve longterm dependencies. we also showed that gated feedback  architecture was faster in wall clock time over the training and achieved better performance compared to standard    acknowledgments  the authors would like to thank the developers of  theano  bastien et al.        and pylearn   goodfellow  et al.       . also  the authors thank yann n. dauphin  and laurent dinh for insightful comments and discussion.  we acknowledge the support of the following agencies for  research funding and computing support  nserc  samsung  calcul que bec  compute canada  the canada research chairs and cifar.     gated feedback recurrent neural networks    