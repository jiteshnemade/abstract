introduction  unlike feedforward neural networks  ffnn  such as deep neural  networks  dnns   the architecture of recurrent neural networks   rnns  have cycles feeding the activations from previous time steps  as input to the network to make a decision for the current input.  the activations from the previous time step are stored in the internal  state of the network and they provide indefinite temporal contextual  information in contrast to the fixed contextual windows used as  inputs in ffnns. therefore  rnns use a dynamically changing  contextual window of all sequence history rather than a static fixed  size window over the sequence. this capability makes rnns better  suited for sequence modeling tasks such as sequence prediction and  sequence labeling tasks.  however  training conventional rnns with the gradient based  back propagation through time  bptt  technique is difficult due to  the vanishing gradient and exploding gradient problems    . in addition  these problems limit the capability of rnns to model the long  range context dependencies to      discrete time steps between relevant input signals and output.  to address these problems  an elegant rnn architecture   long  short term memory  lstm    has been designed    . the original    the original manuscript has been submitted to icassp      conference  on november         and it has been rejected due to having content on the  reference only  th page. this version has been slightly edited to reflect the  latest experimental results.    architecture of lstms contained special units called memory blocks  in the recurrent hidden layer. the memory blocks contain memory  cells with self connections storing  remembering  the temporal state  of the network in addition to special multiplicative units called gates  to control the flow of information. each memory block contains an  input gate which controls the flow of input activations into the memory cell and an output gate which controls the output flow of cell  activations into the rest of the network. later  to address a weakness  of lstm models preventing them from processing continuous input  streams that are not segmented into subsequences   which would allow resetting the cell states at the begining of subsequences   a forget  gate was added to the memory block    . a forget gate scales the internal state of the cell before adding it as input to the cell through  self recurrent connection of the cell  therefore adaptively forgetting  or resetting cell s memory. besides  the modern lstm architecture  contains peephole connections from its internal cells to the gates in  the same cell to learn precise timing of the outputs    .  lstms and conventional rnns have been successfully applied  to sequence prediction and sequence labeling tasks. lstm models  have been shown to perform better than rnns on learning contextfree and context sensitive languages    . bidirectional lstm networks similar to bidirectional rnns     operating on the input sequence in both direction to make a decision for the current input has  been proposed for phonetic labeling of acoustic frames on the timit  speech database    . for online and offline handwriting recognition   bidirectional lstm networks with a connectionist temporal classification  ctc  output layer using a forward backward type of algorithm which allows the network to be trained on unsegmented  sequence data  have been shown to outperform a state of the art  hmm based system    . recently  following the success of dnns  for acoustic modeling              a deep lstm rnn   a stack of  multiple lstm layers   combined with a ctc output layer and an  rnn transducer predicting phone sequences   has been shown to  get the state of the art results in phone recognition on the timit  database     . in language modeling  a conventional rnn has obtained very significant reduction of perplexity over standard n gram  models     .  while dnns have shown state of the art performance in both  phone recognition and large vocabulary speech recognition               the application of lstm networks has been limited to phone  recognition on the timit database  and it has required using additional techniques and models such as ctc and rnn transducer to  obtain better results than dnns.  in this paper  we show that lstm based rnn architectures  can obtain state of the art performance in a large vocabulary speech  recognition system with thousands of context dependent  cd  states.  the proposed architectures modify the standard architecture of the  lstm networks to make better use of the model parameters while  addressing the computational efficiency problems of large networks.     rt       . lstm architectures    it  g         it     wix xt   wim mt     wic ct     bi    ft     wf x xt   wmf mt     wcf ct     bf    ct   ft ct     it g wcx xt   wcm mt     bc    ot     wox xt   wom mt     woc ct   bo    mt   ot h ct    yt   wym mt   by                                    where the w terms denote weight matrices  e.g. wix is the matrix  of weights from the input gate to the input   the b terms denote bias  vectors  bi is the input gate bias vector     is the logistic sigmoid  function  and i  f   o and c are respectively the input gate  forget gate   output gate and cell activation vectors  all of which are the same size  as the cell output activation vector m  is the element wise product    ct      ct    h         mt  projection    xt    ot         rt  output    ft    w   nc   nc       ni   nc       nc   no   nc      where nc is the number of memory cells  and number of memory  blocks in this case   ni is the number of input units  and no is the  number of output units. the computational complexity of learning  lstm models per weight and time step with the stochastic gradient  descent  sgd  optimization technique is o   . therefore  the learning computational complexity per time step is o w  . the learning time for a network with a relatively small number of inputs is  dominated by the nc    nc   no   factor. for the tasks requiring a  large number of output units and a large number of memory cells to  store temporal contextual information  learning lstm models become computationally expensive.  as an alternative to the standard architecture  we propose two  novel architectures to address the computational complexity of  learning lstm models. the two architectures are shown in the  same figure  . in one of them  we connect the cell output units to  a recurrent projection layer which connects to the cell input units  and gates for recurrency in addition to network output units for the  prediction of the outputs. hence  the number of parameters in this  model is nc   nr       ni   nc       nr   no   nc   nr   nc       where nr is the number of units in the recurrent projection layer. in  the other one  in addition to the recurrent projection layer  we add  another non recurrent projection layer which is directly connected to  the output layer. this model has nc   nr       ni   nc        nr    np     no   nc    nr   np     nc     parameters  where np is the  number of units in the non recurrent projection layer and it allows  us to increase the number of units in the projection layers without  increasing the number of parameters in the recurrent connections   nc   nr     . note that having two projection layers with regard  to output units is effectively equivalent to having a single projection  layer with nr   np units.  an lstm network computes a mapping from an input sequence  x    x    ...  xt   to an output sequence y    y    ...  yt   by calculating the network unit activations using the following equations  iteratively from t     to t      recurrent    memory blocks    input    in the standard architecture of lstm networks  there are an input  layer  a recurrent lstm layer and an output layer. the input layer  is connected to the lstm layer. the recurrent connections in the  lstm layer are directly from the cell output units to the cell input  units  input gates  output gates and forget gates. the cell output units  are connected to the output layer of the network. the total number  of parameters w in a standard lstm network with one cell in each  memory block  ignoring the biases  can be calculated as follows     yt    pt    fig.  . lstm based rnn architectures with a recurrent projection  layer and an optional non recurrent projection layer. a single memory block is shown for clarity.    of the vectors and g and h are the cell input and cell output activation  functions  generally tanh.  with the proposed lstm architecture with both recurrent and  non recurrent projection layer  the equations are as follows   it     wix xt   wir rt     wic ct     bi    ft     wf x xt   wrf rt     wcf ct     bf    ct   ft ct     it g wcx xt   wcr rt     bc    ot     wox xt   wor rt     woc ct   bo    mt   ot h ct    rt   wrm mt  pt   wpm mt  yt   wyr rt   wyp pt   by                                                         where the r and p denote the recurrent and optional non recurrent  unit activations.   . . implementation  we choose to implement the proposed lstm architectures on multicore cpu on a single machine rather than on gpu. the decision  was based on cpu s relatively simpler implementation complexity  and ease of debugging. cpu implementation also allows easier distributed implementation on a large cluster of machines if the learning time of large networks becomes a major bottleneck on a single  machine     . for matrix operations  we use the eigen matrix library     . this templated c   library provides efficient implementations for matrix operations on cpu using vectorized instructions   simd   single instruction multiple data . we implemented activation functions and gradient calculations on matrices using simd  instructions to benefit from parallelization.  we use the asynchronous stochastic gradient descent  asgd   optimization technique. the update of the parameters with the gradients is done asynchronously from multiple threads on a multi core  machine. each thread operates on a batch of sequences in parallel  for computational efficiency   for instance  we can do matrix matrix  multiplications rather than vector matrix multiplications   and for  more stochasticity since model parameters can be updated from multiple input sequence at the same time. in addition to batching of sequences in a single thread  training with multiple threads effectively     results in much larger batch of sequences  number of threads times  batch size  to be processed in parallel.  we use the truncated backpropagation through time  bptt   learning algorithm to update the model parameters     . we use a  fixed time step tbptt  e.g.     to forward propagate the activations  and backward propagate the gradients. in the learning process  we  split an input sequence into a vector of subsequences of size tbptt .  the subsequences of an utterance are processed in their original  order. first  we calculate and forward propagate the activations iteratively using the network input and the activations from the previous  time step for tbptt time steps starting from the first frame and calculate the network errors using network cost function at each time step.  then  we calculate and back propagate the gradients from a crossentropy criterion  using the errors at each time step and the gradients  from the next time step starting from the time tbptt . finally  the  gradients for the network parameters  weights  are accumulated for  tbptt time steps and the weights are updated. the state of memory  cells after processing each subsequence is saved for the next subsequence. note that when processing multiple subsequences from  different input sequences  some subsequences can be shorter than  tbptt since we could reach the end of those sequences. in the next  batch of subsequences  we replace them with subsequences from a  new input sequence  and reset the state of the cells for them.    chronously processing one partition of data  with each thread computing a gradient step on   or   subsequences from different utterances. a time step of     tbptt   is used to forward propagate  and the activations and backward propagate the gradients using the  truncated bptt learning algorithm. the units in the hidden layer  of rnns use the logistic sigmoid activation function. the rnns  with the recurrent projection layer architecture use linear activation  units in the projection layer. the lstms use hyperbolic tangent  activation  tanh  for the cell input units and cell output units  and  logistic sigmoid for the input  output and forget gate units. the  recurrent projection and optional non recurrent projection layers in  the lstms use linear activation units. the input to the lstms  and rnns is   ms frame of    dimensional log filterbank energy  features  no window of frames . since the information from the  future frames helps making better decisions for the current frame   consistent with the dnns  we delay the output state label by    frames.   . . results               . . systems   evaluation  all the networks are trained on a   million utterance  about       hours  dataset consisting of anonymized and hand transcribed  google voice search and dictation traffic. the dataset is represented with   ms frames of    dimensional log filterbank energy  features computed every   ms. the utterances are aligned with a     million parameter ffnn with       cd states. we train networks  for three different output states inventories            and     .  these are obtained by mapping       states down to these smaller  state inventories through equivalence classes. the     state set are  the context independent  ci  states    x    . the weights in all the  networks before training are randomly initialized. we try to set the  learning rate specific to a network architecture and its configuration  to the largest value that results in a stable convergence. the learning  rates are exponentially decayed during training.  during training  we evaluate frame accuracies  i.e. phone state  labeling accuracy of acoustic frames  on a held out development set  of         frames. the trained models are evaluated in a speech  recognition system on a test set of        hand transcribed utterances and the word error rates  wers  are reported. the vocabulary  size of the language model used in the decoding is  .  million.  the dnns are trained with sgd with a minibatch size of      frames on a graphics processing unit  gpu . each network is fully  connected with logistic sigmoid hidden layers and with a softmax  output layer representing phone hmm states. for consistency with  the lstm architectures  some of the networks have a low rank projection layer     . the dnns inputs consist of stacked frames from  an asymmetrical window  with   frames on the right and either    or     frames on the left  denoted   w  and   w  respectively   the lstm and conventional rnn architectures of various  configurations are trained with asgd with    threads  each asyn                                                             lstm c     r      . m   lstm c     r    p      . m   lstm c     r      m   lstm c     r      . m   lstm c      . m   dnn   w            m   dnn   w           m   dnn   w          m   rnn c     r      . m   rnn c     r        k   rnn c        k               number of training frames             e     fig.  .     context independent phone hmm states.          lstm c     r      . m   lstm c     r      . m   lstm c     r    p      . m   lstm c     r      m   lstm c      . m   dnn   w         lr      . m   dnn   w           m   dnn   w        lr      m   dnn   w        lr       m           frame accuracy        we evaluate and compare the performance of dnn  rnn and lstm  neural network architectures on a large vocabulary speech recognition task   google english voice search task.    frame accuracy         . experiments                                                                      number of training frames             e     fig.  .      context dependent phone hmm states.     lstm c     r    p      . m   lstm c     r      . m   lstm c     r      m   lstm c     r      . m   lstm c      . m   dnn   w         lr      . m   dnn   w        lr      . m   dnn   w         lr      . m   dnn   w          . m   dnn   w        lr      . m     frame accuracy                              rnn c        k   rnn c     r        k   rnn c     r      . m   dnn   w          m   dnn   w           m   dnn   w            m   lstm c      . m   lstm c     r      . m   lstm c     r      m   lstm c     r    p      . m   lstm c     r      . m               wer                                                                                number of training frames                  e     fig.  .      context dependent phone hmm states.    figure       and   show the wers for the same models for            and      state outputs  respectively. note that some of  the lstm networks have not converged yet  we will update the results when the models converge in the final revision of the paper. the  speech recognition experiments show that the lstm networks give  improved speech recognition accuracy for the context independent      output state model  context dependent      output state embedded size model  constrained to run on a mobile phone processor  and  relatively large      output state model. as can be seen from figure    the proposed architectures  compare lstm c     r    with  lstm c     are essential for obtaining better recognition accuracies than dnns. we also did an experiment to show that depth is  very important for dnns   compare dnn   w        lr    with  dnn   w        lr    in figure  .                                       number of training frames  x billion           fig.  .     context independent phone hmm states.          dnn   w        lr       m   dnn   w        lr      m   dnn   w           m   dnn   w         lr      . m   lstm c      . m   lstm c     r      m   lstm c     r    p      . m   lstm c     r      . m   lstm c     r      . m               wer        figure       and   show the frame accuracy results for            and      state outputs  respectively. in the figures  the name  of the network configuration contains the information about the network size and architecture. cn states the number  n   of memory  cells in the lstms and the number of units in the hidden layer in  the rnns. rn states the number of recurrent projection units in the  lstms and rnns. pn states the number of non recurrent projection units in the lstms. the dnn configuration names state the  left context and right context size  e.g.   w    the number of hidden layers  e.g.     the number of units in each of the hidden layers   e.g.       and optional low rank projection layer size  e.g.     .  the number of parameters in each model is given in parenthesis. we  evaluated the rnns only for     state output configuration  since  they performed significantly worse than the dnns and lstms. as  can be seen from figure    the rnns were also very unstable at the  beginning of the training and  to achieve convergence  we had to  limit the activations and the gradients due to the exploding gradient  problem. the lstm networks give much better frame accuracy than  the rnns and dnns while converging faster. the proposed lstm  projected rnn architectures give significantly better accuracy than  the standard lstm rnn architecture with the same number of parameters   compare lstm     with lstm          in figure  .  the lstm network with both recurrent and non recurrent projection layers generally performs better than the lstm network with  only recurrent projection layer except for the      state experiment  where we have set the learning rate too small.                                                                    number of training frames  x billion           fig.  .      context dependent phone hmm states.     . conclusion  as far as we know  this paper presents the first application of lstm  networks in a large vocabulary speech recognition task. to address  the scalability issue of the lstms to large networks with large number of output units  we introduce two architecutures that make more  effective use of model parameters than the standard lstm architecture. one of the proposed architectures introduces a recurrent projection layer between the lstm layer  which itself has no recursion   and the output layer. the other introduces another non recurrent  projection layer to increase the projection layer size without adding  more recurrent connections and this decoupling provides more flexibility. we show that the proposed architectures improve the performance of the lstm networks significantly over the standard lstm.  we also show that the proposed lstm architectures give better performance than dnns on a large vocabulary speech recognition task  with a large number of output states. training lstm networks on a  single multi core machine does not scale well to larger networks. we  will investigate gpu  and distributed cpu implementations similar  to      to address that.       .     dnn   w        lr      . m   dnn   w        lr      . m   dnn   w          . m   dnn   w         lr      . m   dnn   w         lr      . m   lstm c      . m   lstm c     r      . m   lstm c     r      m   lstm c     r      . m   lstm c     r    p      . m       .     wer          .     .     .     .     .     .     .                                              number of training frames  x billion           fig.  .      context dependent phone hmm states.     . 