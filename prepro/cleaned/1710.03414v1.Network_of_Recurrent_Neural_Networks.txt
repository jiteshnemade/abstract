introduction  in recent years  recurrent neural networks  rnns   elman        have been widely used in natural language processing  nlp . traditionally  rnns are directly used to build the  final models. in this paper  we propose a novel idea called   network of recurrent neural networks   nor  which utilizes existing basic rnn layers to make the structure design  of the higher level layers. from a standpoint of systems theory  von bertalanffy       von bertalanffy        a recurrent neural network is a group or an organization made up of  a number of interacting parts  and it actually is viewed as a  complex system  or a complexity. dialectically  every system  is relative  it is not only the system of its parts  but also the  part of a larger system. in nor structures  rnn is viewed  as the high level neuron  and several high level neurons are  used to build the high level layers rather than directly used  to construct the whole models.  conventionally  there are three levels of structure in deep  neural networks  dnns   neurons  layers and whole nets   or called models . from a perspective of systems theory   at each level of such increasing complexity  novel features  that do not exist at lower levels emerge  lehn      . for  example  at the neurons level  single neuron is simple and  its generalization capability is very poor. but when a certain number of such neurons are accumulated into a certain  elaborate structure by certain ingenious combinations  the  layers at the higher level begin to get the unprecedented ability of classification and feature learning. more importantly   copyright c       association for the advancement of artificial  intelligence  www.aaai.org . all rights reserved.    such new gained capability or property is deducible from  but not reducible to constituent neurons of lower levels. it s  not a property of the simple superposition of all constituent  neurons  and the whole is greater than the sum of the parts.  in systems theory  such kind of phenomenon is known as  whole emergence  wierzbicki      . whole emergence often comes from the evolution of the system  arthur and others        in which a system develops from the lower level  to the higher level  from simplicity to complexity. in this  paper  the motivation of nor structures is to introduce a  new structure level to rnn related networks by transferring  traditional rnn from the system to the agent and from the  outer dimension to the inner dimension  fromm      .  in       w. brian arthur  arthur and others       has  identified three mechanisms by which complexity tends to  grow as systems evolve     mechanism    increase in co evolutionary diversity. the  agent in the system seem to be a new instance of agent  class  type or species. as a result  the system seems to  have new external agent types or capabilities.    mechanism    increase in structural sophistication. the  individual system steadily accumulates increasing numbers of new systems or parts. thus  newly formed system  seems to have new internal subsystems or capabilities.    mechanism    increase by  capturing software . the  system capture simpler elements and learns to  program   these as  software  to be used as its own ends.  in this paper  with the guidance of first two mechanisms   we introduce two methodologies to nor structures design   which are named as aggregation and specialization. aggregation and specialization are natural operations for increasing complexity in complex systems  fromm      . the former is related to arthur s second mechanism  in which traditional rnns are aggregated and accumulated into a highlevel layer in accordance with a specific structure  and the  latter is related to arthur s first mechanism  in which the  rnn agent in a high level layer is specialized as the rnn  agent that performs a specific function.  we make several implementations and carry out experiments on three different tasks  including sentiment classification  question type classification and named entity recognition. experimental results show that our models outperform constitute simple rnn remarkably with the same num      ber of parameters and achieve even better results than gru  and lstm sometimes.    background  systems theory systems theory was originally proposed  by biologist ludwig von bertalanffy  von bertalanffy        von bertalanffy       for biological phenomena. in biology systems  there are several different levels which begin  with the smallest units of life and reach to the largest and  most extensive category  molecule  cell  tissue  organ  organ system  organization etc. traditionally  a system could  be decomposed into its individual components so that each  component could be analyzed as an independent entity  and  components could be added in a linear fashion to describe  the totality of the system  walonick      . however  von  bertalanffy argued that we cannot fully comprehend a phenomenon by simply breaking it down into elementary parts  and then reforming it. we instead need to apply a global and  systematic perspective to underline its functionality  mele   pels  and polese        because a system is characterized  by the interactions of its components and the nonlinearity of  those interactions  walonick      .  whole emergence in systems theory  the phenomenon   i.e.  the whole is irreducible to its parts  is known as emergence or whole emergence  wierzbicki      . emergence  can be qualitatively described as  the whole is greater than  the sum of the parts  upton  janeka  and ferraro      . or  it can also be quantitatively expressed as   n  x  w    pi         arbitrary sequences of inputs. formally  given a sequence  of vectors  xt  t     t   the equation of simple rnn  elman        is   ht   f  w xt   u ht               where w and u are parameter matrices  and f denotes a  nonlinearity function such as tanh or relu. for simplicity   the neuron biases are omitted from the equation.  actually  rnns can behave chaotically. there have  been some works analysing rnns theoretically or experimentally from the perspective of systems theory.  sontag        provided an exposition of research regarding systemtheoretic aspects of rnns with sigmoid activation functions.   bertschinger and natschla ger       analyzed the computation at the edge of chaos in rnns and calculated the critical  boundary in parameter space where the transition from ordered to chaotic dynamics takes place.  pascanu  mikolov   and bengio       employed a dynamical systems perspective to understand the exploding gradients and vanishing gradients problems in rnns. in this paper  we obtain methodologies from systems theory to conduct structure designs of  rnn related models.    network of recurrent neural networks  overall architecture    i    where w is the whole of the system and consists of n parts   and  pi  i     n is the i th part. in       philip w. anderson  highlighted the idea of emergence in has article  more is  different   anderson       in which he stated that a change  of scale very often causes a qualitative change in the behavior of the system. for example  in human brains  when one  examines a single neuron  there is nothing that suggests conscious. but a collection of millions of neurons is clearly able  to produce wonderful consciousness.  the mechanisms behind the emergence of complexity  can be used to design neural network structures. one  of the widely accepted reasons is the repeated application  and combination of two complementary forces or operations  stretching and folding  in physics term  thompson  and stewart         splitting and merging  in computer science term  hannebauer         or specialization and cooperation  in sociology term . merging or aggregating of agents  means generally a number of  sub  agents is aggregated or  conglomerated into a single agent. splitting or specializing  means the agents are clearly separated from each other and  each agent is constrained to a certain class or role  fromm       .  recurrent neural networks at the edge of chaos recurrent neural networks  rnns   werbos       elman        are a class of deep neural networks that possess internal short term memory due to recurrent feed back connections between units  which makes them be able to process    figure    overview of nor structure.  as the high level illustration shown in figure    nor architecture is a three dimensional spatial temporal structure.  we summarize nor architecture as four components  i  m   s and o  in which component i  input  and o  output  control the head and tail of nor layer  component s  subnetworks  is in charge of the spatial extension and component  m  memories  is responsible for the temporal extension of  the whole structure. we describe each component as follows   component i  component i controls the head of nor architecture. it does data preprocessing tasks and distributes  processed input data to subnetworks. at each time step  t  the form of upcoming input data xt may be various   such as one single vector  or several vectors with the multigranularity information  even the feature vectors with noise.  one single vector may be the simplest situation  and the  common solution is copying this vector into n duplicates and  feed each of them into one single subnetwork in the component s. in this paper  the copying method meets our needs      a  ma nor layer.     b  another ma nor layer.     c  ms nor layer.     d  ss nor layer.    figure    the sectional views of nor layers at one time step.  i  means the component i   o  means the component o  and   r  means rnn neuron.  and we formalize it as    xit  i     n      c xt             in which c means copy function  and xit will be fed into i th  subnetwork.  component m  component m manages all memories  over the whole layer  not only internal but also external  memories  weston  chopra  and bordes      . but in this  paper  component m only considers internal memory and do  not apply any extra processing to the individual memory of  each rnn neuron. that is   mjt   i ojt               where i means identity function  the superscript j is the  identifier of j th rnn neuron     mjt is the memory of jth rnn neuron at time step t and ojt   is the transformation  output of j th rnn neuron at time step t      .  component s  component s is made up of several different or same subnetworks. interaction may exist in these  subnetworks. the responsibility of component s is to manage the logic of each subnetwork and handle the interaction  between them. suppose component s has n in degree and  m out degree  i.e.  component s receives n inputs and produces m outputs  the k th output is generated by necessary  inputs and memories   skt   f   x  m              where skt is the k th output at time step t  x and m are  needed inputs and memories  and f is the nonlinear function  which can be one layer rnn or two layer rnn etc.  component o  to form a layer we need a certain amount  of neurons. so one of the nor properties is multiple rnns.  a natural approach to integrate multiple rnn neurons  signals is collecting all outputs first and then using a mlp layer  to measure the weights of each outputs. traditional neuron  outputs a single real value  so the collection method is directly arranging them into a vector. but rnn neurons is different  for each of them outputs a vector not a value. a simple method is concatenating all vectors and then connecting     the notation of the  subnetwork  is different from the  neuron   for one subnetwork may be composed of several neurons. we  use superscript i as the identifier of the subnetwork. so  the input  data of i th subnetwork at time step t is denoted as xit .     in this paper  we just use simple rnn  elman       applied  with relu activation as our basic rnn neuron. thus the memory  at this time step is just the output at last time step.    the concatenated vector to the next mlp. another is pooling  each rnn output vector into a real value  then arranging all  these real values into a vector  which seems same as traditional neurons. in this paper  the former solution is used and  formalized as   st    s t   s t           sm  t    ot   r wm lp   st                  where st is the concatenated vector  wm lp is the weight of  mlp and r means the relu activation function of mlp.    methodology i  aggregation  any operation with changing a boundary can cause a emergence of complexity. the natural boundary is the agent itself  and sudden emergence of complexity is possible at this  boundary if complexity is transfered from the agent to the  system or vice versa from the system to the agent. there  are two basic operations  aggregation and specialization  that  can be used to transfer complexity between different dimensions  fromm      .  according to arthur s second mechanism  internal complexity can be increased by aggregation and composition of  sub agents  which means a number of rnn agents is conglomerated into a single big system. in this way  aggregation  and composition transfer traditional rnn from the outer to  the inner dimension  from the system to the agent  for the  selected rnns are accumulated to become a part of a larger  group.  for a concrete nor layer  suppose it is composed of n  subnetworks  and i th subnetwork is made up of k i rnn  neurons. then  at the time step t  given the input xt   the  operation flow is as follows    . component i  copy xt into n duplications using equation       then we get x t   x t           xnt .   . component m  deliver the memory of each rnn neuron from the last time step to the current time step using    k   equation      then we get memories m     in  t           mt         k   first subnetwork and memories mt           mt  in second subnetwork  etc.   . component s  for each subnetwork i  take advantage of  i ki  the input xit and memories mi    to get the nont           mt  linear transformation output   i    i k  sit   f   xit   mi    t           mt       then  we get s t   s t           snt .             . component o  concatenate all outputs by equation      and use a mlp function to determine how much signals  in each subnetwork to flow through the component o by  equation    .  obviously  the number  the type and the interaction of the  aggregated rnns determine the internal structure or inner  complexity of the newly formed layer system. thus  we propose three kinds of topologies of nor aggregation method.  multi agent in systems theory  the natural description of  complex system is the multi agent system created by replication and adaptation.  replication  means to copy and reproduce a new rnn agent  and  adaptation  means they are  not totally same and some changes on weights or somewhere  else by variation can increase the diversity of the system.  as shown in figure   a   there is a nor layer  called manor  composed of four parallel rnns. figure   shows this  layer being unrolled into a full network. each subnetwork  of ma nor layer is a one tier rnn  thus at time step t  the  i th subnetwork of component s in ma nor is calculated  as   sit   oit   r wi xit   ui mit             where r means relu activation function  wi and ui are parameters of corresponding rnn neuron  oit is the nonlinear  transformation output and will be delivered to next time step  to be used as mit     and sit is the output of i th subnetwork  which is equal to oit .    figure    the unfolding of ms nor in three time steps.  introduce new agent type to the system and can learn sequence dependencies in different timescales.  figure   c  shows a nor layer made up of four subnetworks  in which two of them are one tier rnns and the others are two tier rnns. two kinds of timescale dependencies are learned in component s  which are formalized as  follows   s t  o t   r wt  x t   ut  m t              s t            s t  s t     o t   r wt  x t   ut  m t    ot      r wt    x t   ut    m     t                            ot   r wt ot   ut mt    ot      r wt    x t   ut    m     t                            ot   r wt ot   ut mt                                self similarity the above mentioned aggregation and  composition operation lead to big rnn agent groups. while  in turn  they can also be combined to form even bigger  group. such repeated aggregation and high accumulation  makes the fractal and self similar structure come into being.    figure    the unfolding of ma nor in three time steps.  the nonlinear function in equation     of each subnetwork  may be more complex. for example  figure   b  shows a  nor layer made up of three two tier rnns. at time step t   the i th subnetwork in component s is calculated as  i   i  i   i    oi    t   r wt xt   ut mt      sit     oi    t         r wti   xit         uti   mi    t                    multi scale the combination of multiple rnns in a  multi agent nor layer makes it somewhat like an ensemble. and empirically  diversity among the members of a  group of agents is deemed to be a key issue in ensemble  structure  kuncheva and whitaker      . one way to increase the diversity is to use the multi scale topology which    figure    the unfolding of ss nor in three time steps.  as shown in figure   d   we also use three paths. but  after each path first learns its own intermediate represen      tation  the second layers gather all intermediate representations of three paths to learn high level abstract features.  in this way  different paths do not learn and train independently. the connections among each other helps the model  easy to share informations. thus  it becomes possible that  the whole model learns and trains to be an organic rather  than parallel independent structure. we formalize the cooperation of component s as follows   o       r wt    x t   ut    m     t  t      s t  s t  s t    o     t  o     t        ot   o     t   o     t                     r wt    x t   ut    m     t                    r wt xt   ut mt             r wt     o     t   ot   ot               r wt     o     t   ot   ot               r wt     o     t   ot   ot                          ut    m     t             ut mt    ut    m     t              figure    gate specialization.                  methodology ii  specialization  we have mentioned that the emergence of complexity is usually connected to a transfer of complexity  a transfer at the  boundary of the system. aggregation and composition transfer complexity from the system to the agent  and from the  outer dimension to the inner dimension. another way to be  used to cross the agent boundary is the specialization or inheritance  which transfer complexity from the agent to the  system  and from the inner dimension to the outer dimension  fromm      . specialization is related to arthur s first  mechanism. it increases structural sophistication outside of  the agent by adding new agent forms. through inheritance  and specialization objects become objects of a certain class  and agents become agents of a certain type  and the more  such an agent becomes a particular class or type  the more it  needs to delegate special tasks that it can not handle alone to  other agents  fromm      .  the effect of specialization is the emergence of delegation  and division of labor in the newly formed groups. thus  the  formalization of k th output in component s can be rewritten as the following   skt   g f    x  m     f    x  m             fl   x  m           where fl is the l th specialized agent function  g means the  cooperation of all specialized agents  and l is the number  of specialized agents. equation      denotes the function f  in equation     is implemented by the separated operations  f    f            fl and g.  gate specialization we see gate mechanism is one of the  specialization methods. as shown in figure    a general  rnn agent is separated into two specialized rnn agents   one is for gate duty and the other is for generalization duty.  a concrete gate nor is shown in figure . in the original  multi agent nor layer  each rnn agent is specialized as  one generalization specific rnn and one gate specific rnn.    figure    the sectional views of gate nor layer at one  time step.  we formalize it as   o t     wt  x t   ut  m t              o t  s t  o t  o t  s t                             r wt  x t   ut  m t    o t o t    wt  x t   ut  m t    r wt  x t   ut  m t    o t o t    where   denotes the sigmoid activation and  element wise multiplication.                            denotes    relationship with lstm and gru we see long shortterm memory  lstm   hochreiter and schmidhuber        and gated recurrent unit  gru   chung et al.       as two  special cases of network of recurrent neural networks. take  lstm for example  at time step t  given input xt and previous memory cell ct   and hidden state ht     the transition  equations of standard lstm can be expressed as the following   i     wi xt   ui ht      f     wi xt   uf ht      o     wo xt   uo ht      g   tanh wg xt   ug ht      ct   ct   f   g i  st   tanh ct   o                                          from the perspective of nor  network of recurrent neural  networks   lstm is made up of four rnns  in which three     task  sentiment classification  question classification  named entity recognition      of params.      k      k      k      k      k      k      k      k      k    irnn                                                 gru                                               lstm                                              ma nor                                              ms nor                                            ss nor                                           gate nor                                           table    number of hidden neurons for rnn  gru  lstm ma nor  ms nor  ss nor and gate nor for each network  size specified in terms of the number of parameters  weights .  of four  i.e.  i  f   o rnns  are specialized for gate tasks to  control how much of informations let through in different  parts. moreover  there is only a shared memory ht   which  can be accessed by each rnn cell in lstm.  while in turn  lstm and gru can also be combined to  form even bigger group.    experiments  in order to evaluate the performance of the presented model  structures  we design experiments on the following tasks   sentiment classification  question type classification and  named entity recognition. we compare all models under the  comparable parameter numbers to validate the capacity of  better utilizing the parametric space. in order to verify the  effectiveness and universality of the experiments  we conduct three comparative tests under total parameters of different orders of magnitude  see table  . every experiment  is repeated    times with different random initializations and  then we report the mean results. it s worthy noting that our  aim here is to compare the model performance under the  same hyper parameter settings  not to achieve best performance for one single model.   le  jaitly  and hinton       showed that when initializing the recurrent weight matrix to be the identity matrix  and biases to be zero  simple rnn composed of relu activation function  named as irnn  can be comparable with  even outperform lstm. in our experiments  all basic rnn  neurons are simple rnns applied with relu function. we  also keep the number of the hidden units same over all rnn  neurons in a nor model. obviously  our baseline model is a  single giant simple rnn  elman       applied with relu  activation. at the same time  two improved rnns  gru   chung et al.       and lstm  hochreiter and schmidhuber        have been widely and successfully used in nlp  in recent years  so we also choose them as our baseline models.  the pre trained     d glove  b vectors  and     d  google news vectors  were obtained for the word embeddings. during training we fix all word embeddings and  learn only the other parameters in all models. the embeddings for out of vocabulary words are set to zero vectors.  we pad or crop the input sentences to a fixed length. the          https   nlp.stanford.edu projects glove   https   code.google.com archive p word vec     trainings are done through stochastic gradient optimizer descent over shuffled mini batches with the optimizer adam   kingma and ba      . all models are regularized by using  dropout  srivastava et al.       method. at the same time   in order to avoid overfitting  early stopping is applied to  prevent unnecessary computation when training. more details on hyper parameters setting can be found in our codes   which are publicly available at  .    sentiment classification  we evaluate our models on the task of sentiment classification on the popular stanford sentiment treebank  sst   benchmark  socher et al.        which consists of        movie reviews and is split into train         dev        and  test       . sst provides detailed phrase level annotation  and all sentences along with the phrases are annotated with    labels  very positive  positive  neural  negative  and very  negative. in our experiments  we only use the sentence level  annotation. one of our goals is to avoid expensive phraselevel annotation  like  qian  huang  and zhu      . another is  in practice  phrase level annotation is hard to provide.  all models use the same architecture  embedding layer    dropout layer   rnn nor layer   rnn nor layer    max pooling layer   dropout layer   softmax layer. the  first layer is the word embedding layer  next are two layer  rnn nor layers as the non linear feature transformation  layer. then a max pooling layer max pools all transformed  feature vectors by selecting the max value in each position  to get sentence representation. finally  a softmax layer is  used as output layer to get the final result. to benefit from  the regularization  two dropout layers with rate of  .  are  added after embedding layer and before softmax layer. the  initial learning rates of all models are set to  .    . we  use public available     d glove    b vectors to initialize  word embeddings. three different network sizes are tested  for each architecture  such that the number of parameters  are roughly     k      k and     k  see table   . we set  the minibatch size as   . finally  we use the cross entropy  criterion as loss function.  the results of the experiments are shown in table  . it is  obvious that nor models get superior performances compared with irnn baseline  especially when the network size  is big enough. all models improve with network size grows.  among all nor models  gate nor gets the best results.     model  irnn  gru  lstm  ma nor  ms nor  ss nor  gate nor       k params    .      .      .      .      .      .      .         k params    .      .      .      .      .      .      .         k params    .      .      .      .      .      .      .      table    accuracy     comparison over different experiments on sst corpus.  however  we find that lstm and gru get much better results in three comparative tests.          which consists of       sentences in the training set        sentences in the validation set and      sentences in  the test set.  model  irnn  gru  lstm  ma nor  ms nor  ss nor  gate nor       k params    .      .      .      .      .      .      .         k params    .      .      .      .      .      .      .         k params    .      .      .      .      .      .      .      table    f      comparison over different experiments on  conll      corpus.    question type classification  question classification is an important step in a question answering system which classifies a question into a specific  type. for this task  we use trec  li and roth       benchmark  which divides all questions into   categories  location   human  entity  abbreviation  description and numeric. terc  provides      labeled questions in the training set and      questions in the test. we randomly select     of the training  data as the validation set.  model  irnn  gru  lstm  ma nor  ms nor  ss nor  gate nor       k params    .      .      .      .      .      .      .         k params    .      .      .      .      .      .      .         k params    .      .      .      .      .      .      .      table    accuracy     comparison over different experiments on trec corpus.  all network types use the same architecture  embedding  layer   dropout layer   rnn nor layer   max pooling  layer   dropout layer   softmax layer. dropout rates are  set to  . . three hidden layer sizes are chosen such that the  total number of parameters for the whole model is roughly      k      k      k  see table  . all networks use a learning  rate of  .     and are trained to minimize the cross entropy  error.  table   shows the accuracy of the different networks on  the question type classification task. here again  nor models get better results than baseline irnn model. among  all nor models  ss nor also gets the best result. in this  dataset  we find the performances of lstm and gru are  even not comparable with irnn  which proves the validity  of results in  le  jaitly  and hinton      .    named entity recognition  named entity recognition  ner  is a classic nlp task which  tries to identity the proper names of persons  organizations   locations  or other entities in the given text. we experiment  on conll      dataset  tjong kim sang and de meulder    recently  popular ner models are based on bidirectional  lstm  bi lstm  combined with conditional random fields   crf   named as bi lstm crf  lample et al.      . the  bi lstm crf networks can effectively use past and future features via a bi lstm layer and sentence level tag  information via a crf layer. in our experiments  we also  adapt this architecture by replacing lstm with nors or  other variation of rnns. so the universal architecture of all  tested models is  embedding layer   dropout layer   birnn bi nor layer   crf layer. three hidden layer sizes  are chosen such that the total number of parameters for the  whole network is roughly     k      k and     k  see table  . we apply     dropout after embedding layer. initial  learning rate is set to  .    and every epoch it is reduced  by factor  .  . the size of each minibatch is   . we train  all networks for    epochs and early stop the training when  there are   epochs no improvement on validation set.  our results are summarized in the table  . not surprisingly  all nors perform much better than giant single rnnrelu model. as we can see  gru performs the worst  followed by irnn. compared to gru and irnn  lstm performs very well  especially when network size grows up. at  the same time  all nor models get superior performances  than irnn  gru and lstm. among them  ss nor model  get best results.    conclusion  in conclusion  we introduced a novel kind of systems theory  based neural networks called  network of recurrent neural network   nor  which views existing rnns  for example  simple rnn  gru  lstm  as high level neurons  and then utilizes rnn neurons to design higher level layers.  then we proposed several methodologies to design different nor topologies according to the evolution of systems  theory  arthur and others      . we conducted experiments  on three kinds of tasks  including sentiment classification   question type classification and named entity recognition  to  evaluate our proposed models. experimental results demonstrated that nor models get superior performances compared with single giant rnn models  and sometimes their  performances even exceed gru and lstm.     