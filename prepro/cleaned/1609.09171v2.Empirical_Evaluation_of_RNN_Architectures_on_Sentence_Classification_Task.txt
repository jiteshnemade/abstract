introduction    recurrent neural networks  rnns   especially those long short term memories   lstms       are good at modeling varying length sequential data and have achieved  state of the art results for many problems in natural language processing  such as  neural machine translation  question answering and text classification                   . rnn is now the most popular method in sentence classification  which is also a typical nlp task.  there are two widely used rnn structures in nlp tasks and we call them  tail  model  and  pooling model  separately. however  there is no work focusing on  comparing the performance of these various network architectures.  this paper presents the first empirical study using lstms to evaluate various rnn  structures on sentence classification task. we also present a hybrid architecture that  combines  tail model  with  pooling model  in this paper. experimental results  show that the  max pooling model  or  hybrid max pooling model  achieves the  best performance on most datasets  while  mean pooling model  constantly has  worse performance. and  tail model  does not outperform other models.          related work    a recurrent neural network     is introduced to process arbitrary length sequence and  widely used in nowadays nlp tasks. since simple recurrent network is difficult to  train because the gradients will either vanish or explode      various improvements to  the basic architecture were proposed and the long short term memory network is  perhaps the most successful one. lstm was originally introduced by hochreiter and  schmidhuber      and in recent years some kinds of simplified lstm were also introduced such as gated recurrent units    . lstm is used in our comparison since  lstm is comparable to gru and the two both outperform basic rnn    . we use  the lstm structure that is introduced by gers et al.     for comparison  since greff et  al.     evaluate the lstm variants and find the model by gers et al. performs comparable with other variants.  rnn has been successfully applied in nlp tasks and various rnn structures have  been proposed. however  to our best knowledge  we didn t notice any work that evaluates the performance of the various rnn structures. so the following part of this  paper will present the model comparison on sentence classification task.         model    there exist two commonly used rnn structures in nlp tasks and we call them  tail  model  and  pooling model  separately           . they both use rnn to provide  feature layer for the fully connected layer in classification tasks. on the other hand   we regard the two different structures may be complementary for each other. so the  third rnn structure is proposed in this paper and it will be called  hybrid model  in  the following part of paper.     fully connected  layer    last hidden state  of backward layer    last hidden state  of forward layer    blstm    blstm    x     x           blstm    xn    fig.  . tail blstm model    figure   demonstrates the network structure of the  tail blstm model  in which the  hidden layer is the concatenated hidden states of last node in the forward rnn and  backward rnn. so the hidden layer is represented as following      h                     mean the hidden state of the last node in forward rnn and backward  where    and    rnn respectively. it s obvious that    is the semantic mapping of sentence in forward      contains the semantic information of sentence in reverse direction.    direction and    can be regarded as the sentence feature and is fed into the next fully connected layer  for classification             here           is a softmax function in all of our experiments.     fully connected  layer    max mean pooling    blstm    blstm    x     x           blstm    xn    fig.  . pooling blstm model    the other widely used rnn structure is  pooling model   fig.  . the pooling  blstm model can be regarded as each blstm node s voting for the feature layer  and  mean pooling  is a common model for voting.  mean pooling  calculates the  value of the i th position in vector   by averaging the corresponding value of each  hidden state vector from blstm nodes as following                         here we suppose the length of blstm sequence is m and   is the hidden state vector of the j th blstm node.  the alternative for the  mean pooling  is  max pooling   which selects the max  value of each position in all hidden state vectors. that implies the value of the i th  position in vector   is calculated as          max                     we didn t find any previous work that compares the performance of these two pooling methods mean vs. max   so we also design some experiments to compare that.  the fully connected layer of  pooling model  also can be represented by eq.    .     fully connected  layer    last hidden state  of backward layer    last hidden state  of forward layer    max mean pooling    blstm    blstm    x     x           blstm    xn    fig.  . hybrid blstm model    it s natural to speculate that the  tail model  and  pooling model  can be complementary for each other as there would be feature fusion. so we propose the third  model called  hybrid model   fig.   which concatenates two different feature layers  as following                            the fully connected layer of  hybrid model  is also represented by eq.    .  the tail  pooling and hybrid models based on lstm are similar to blstm.         datasets and experimental setup     .     dataset    in order to evaluate the performance of the aforementioned three rnn structures  we  design experiments on the following datasets for sentence classification task   mr  the dataset of movie reviews with one sentence per review. the task is to detect positive negative reviews     .     sst    stanford sentiment treebank an extension of movie review but with fine grained labels  very positive  positive  neutral  negative  very negative .the dataset  is split into train dev test set and re labeled by socher et al.     .  sst    same data as sst   but only with binary labels. all neutral reviews are  removed. the dataset is also split into train dev test set.  subj  subjectivity dataset. the task is to classify sentences into two categories   subjective and objective     .  trec  trec question classification dataset  which classifies a question into    question types  person  location  numeric information  etc. . the dataset is split into  train dev test set     .  cr  customer reviews of various products  cameras  mp s etc. . the task is to  predict positive negative reviews     .  the statistics of the datasets are in table  .  table  . statistics of datasets. cv means the dataset is not split into train dev test set  thus we  use    fold cross validation.    dataset  mr  sst    sst    trec  subj  cr   .     class                      dataset size                                           test set size  cv                   cv  cv    experiment design    we design two groups of experiments  one is based on the blstm and the other is  based on lstm model. and the  pooling model  and the  hybrid model  each contains two experiments in order to compare the performance of max pooling and mean  pooling strategy. thus there are five experiments in each group for each dataset to  evaluate the performance of these rnn architectures.   .     parameters and setup    we use word vec vectors which were trained on     billion words from google  news as word embedding which can be accessed publicly. the vectors  dimensionality is     and those words not in the vectors are set randomly. we make the embedding static through all experiments to avoid the influence of the word embedding  parameters in comparison. the initial weights of network parameters are drawn from  a uniform distribution with standard deviation of  .   and the dropout is set to be  . .  instead of achieving best performance for one single model  our focus is to compare the model s performance on sentence classification task under comparable condition. in order to make the parameters of the rnn models comparable  we set unidirectional lstm s unit size of hidden layer to be     and bidirectional lstm s unit     size to be    . thus parameter size will be  .         for both unidirectional and  bidirectional lstm in all experiments.  the cross entropy criterion is used as loss function and stochastic gradient descent  with adagrad     is used for optimization. gradients are computed through full bptt  for lstm    . for those datasets that have no standard dev set  we randomly choose      of the training set as the dev set.         results and analysis    the experimental results are listed in the table   lstm based models  and table    blstm based models .  table  . lstm experiment results on datasets    model  lstm tail  lstm meanpool  lstm maxpool  lstm hybridmeanpool  lstm hybridmaxpool    mr    .      .      .      .      .      sst      .      .      .      .      .      sst      .      .      .      .      .      trec    .      .      .      .      .      subj    .      .      .      .      .      cr    .      .      .      .      .      table  . blstm experiment results on datasets    model  mr  sst   sst    trec  subj  cr  blstm tail    .     .     .     .     .     .    blstm meanpool    .     .     .      .      .     .    blstm maxpool    .      .     .      .     .     .    blstm hybridmeanpool   .     .     .      .      .     .    blstm hybridmaxpool    .     .     .      .      .     .    first of all  we compare the performance of above mentioned three kinds of models.  one obvious conclusion is that the  mean pooling model  constantly has worse performance than the others with large margin on most datasets  while  max pooling  model  or  hybrid max pooling model  achieves best performance on most datasets.  the possible reason may be the max hidden state value which the max pooling method chooses contains the most meaningful information of the sentence.  tail model   does not outperform other models.  obviously for the pooling strategy  max pooling method performs better than mean  pooling method.  if we select lstm meanpool model as a baseline lstm benchmark to compare  with the best run result  we can see the performance boost ranging from  .    to   .    on different datasets  table   . similar performance improvements can also be  observed in blstm models on all datasets  table   .     table  . performance improvement of lstm based model  best run vs. lstm meanpool     model  lstm meanpool  best run  performance  improvement    mr    .      .      .       sst      .      .      .       sst      .      .      .       trec    .      .      .       subj    .      .      .       cr    .      .      .       table  . performance improvement of blstm based model  best run vs.  blstm meanpool     model  mr  sst    sst    trec  subj  cr  blstm meanpool   .      .      .      .      .      .    best run    .      .      .      .      .      .    performance    .      .      .      .      .      .     improvement  out of our expectation   hybrid pooling model  does not constantly perform better  than  pooling model  or  tail model .  hybrid pooling model  only achieves best  performance in   out of    results. this indicates that the two different rnn structures   tail model  and  pooling model   didn t show the feature complementation  function and the model fusion can t offer the performance boost.  the above experiment results analysis suggests we should give priority to the   max pooling model  or  hybrid max pooling model  in sentence classification  tasks given the above optional model choices. as  tail model  is widely used in nlp  classification tasks  our experimental results provide another choice and the possibility to improve state of the art result.  table  . blstm vs.lstm model    model  mr  sst   sst    trec  subj  cr  tail  l  l  b  b  b  b  meanpool  b  b  b  b  b  b  maxpool  b  b  l  b  b  b  hybridmeanpool  l  l  b  b  b  l  hybridmaxpool  l  b  l  l  b  b  secondly  we compare the performance difference of lstm and blstm models.  there are    different experiments    model   datasets  for both lstm and blstm  models. we list the winner of each experiment in table   while  b  means blstm  outperforms lstm model and  l  means the vice versa. we can see from the comparison results that blstm outperforms lstm model in majority of experiments.  though the performance boosts of several experiments are not large enough to show  an overwhelming advantage  we believe that the blstm model has advantage because of the diversity of datasets and models in experiments. these experiment results probably indicate the blstm model can have better performance over lstm  model if both models use similar parameter settings and parameter number. we speculate this minor advantage may come from the blstm model s ability of capturing     more features though these extra features don t have much effect on the model performance.         conclusion    in this paper  we present the first empirical study using lstms to evaluate  performance of various rnn structures on sentence classification task. we also  present a hybrid architecture that combines  tail model  with  pooling model  in  this paper. experimental results show that the  max pooling model  or  hybrid max  pooling model  achieves the best performance on most datasets  while  mean  pooling model  constantly has worse performance than the others. and  tail model   does not outperform other models. as  tail model  is widely used in nlp classification tasks  our experimental results provide another choice and the possibility to improve state of the art result.    