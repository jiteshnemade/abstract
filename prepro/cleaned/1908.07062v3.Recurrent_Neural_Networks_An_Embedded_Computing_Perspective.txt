introduction    recurrent neural networks  rnns  are a class of neural  networks  nns  dealing with applications that have sequential data inputs or outputs. rnns capture the temporal  relationship between input output sequences by introducing  feedback to feedforward  ff  neural networks. thus  many  applications with sequential data such as speech recognition       language translation      and human activity recognition      can benefit from rnns.  in contrast to cloud computing  edge computing can guarantee better response time and enhance security for the  running application. augmenting edge devices with rnns  grant them the intelligence to process and respond to sequential problems. realization on embedded platforms in edge  volume           devices imposes some optimizations to rnn applications.  embedded platforms are time constrained systems that suffer  from limited memory and power resources. to run rnn  applications efficiently on embedded platforms  rnn applications need to overcome these restrictions.  a. scope of the article    in this article  we study rnn models and specifically focus  on rnn optimizations and implementations on embedded  platforms. the article compares recent implementations of  rnn models on embedded systems found in the literature.  for a research paper to be included in the comparison  it  should satisfy the following conditions        it discusses the implementation of an rnn model or the        rezk et al.  recurrent neural networks  an embedded computing perspective    figure    structure of the survey article. rnn models should run on an embedded platform in an edge device. section  ii discusses the objectives of such an implementation and the challenges facing it. section iii describes the rnn models in  detail. there follows a discussion of how algorithmic optimizations  section iv a  may be applied to rnn models and how  platform specific optimizations  section iv b  are applied to embedded platforms. the resulting implementations are discussed  in section v and compared to the objectives in section vi.         recurrent layer of an rnn model.  the target platform is an embedded platform such as  fpga  asic  etc.    to provide a complete study  the survey also addresses the  methods used for optimizing the rnn models and realizing  them on embedded systems.  this survey distinguishes itself from related works because  no existing article includes the components of the rnn  models with optimizations and implementations in a single  analysis  as may be seen in table  . the other surveys focus  on one or two aspects compared to those covered in this  article. some articles study rnns from an algorithmic point  of view         . while another group of survey articles looks  at the hardware implementations. for example  one survey  on neural networks efficient processing     studied cnns   cnn optimizations  and cnn implementations  while another cnn survey     studied cnn mappings on fpgas.  some articles were specialized in algorithmic optimizations  such as compression    . all algorithmic optimizations for  both cnns and rnns were surveyed in one article that  also discussed their implementations    . however  the main  scope of the article was optimizations  and so rnn models and their components were not studied. furthermore   the rnn implementations included were limited to speech  recognition applications on the tidigits dataset.  b. contributions    this survey article provides the following             a detailed comparison of rnn models  components  from a computer architecture perspective that addresses  computational and memory requirements.               a study of the optimizations applied to rnns to execute  them on embedded platforms.  an application independent comparison of recent implementations of rnns on embedded platforms.  identification of possible opportunities for future research.    c. survey structure    this survey article is organized as shown in figure  . section ii defines the objectives of realizing rnn models on  embedded platforms and the challenges faced in achieving  them. we then define a general model for rnn applications  and discuss different variations for the recurrent layers in  rnn models in section iii. however  it is difficult to run  rnn models in their original form efficiently on embedded  platforms. therefore  researchers have applied optimizations  to both the rnn model and the target platform. the optimizations applied to the rnn model are called algorithmic optimizations and are discussed in section iv a  the  optimizations applied to the hardware platform are called  platform specific optimizations and are discussed in section iv b. then  in section v  we present an analysis of  the hardware implementations of rnns suggested in the  literature. the implementations are compared against the  applied optimizations and their achieved performance. in  section vi  we compare the implementations analyzed in  section v with the objectives defined in section ii to define  the gap between them and propose research opportunities  to fill this gap. finally  in section vii  we summarize our  survey.    volume            rezk et al.  recurrent neural networks  an embedded computing perspective    table    comparison with related survey articles.  article  analysis of rnn models components  analysis of cnn models components  algorithmic optimizations  platform specific optimizations  rnn implementations  cnn implementations                                                                             speech recognition only       ii. objectives and challenges    implementation efficiency is the primary objective in implementing rnn applications on embedded systems. implementation efficiency requires the implementation to have  high throughput  low energy consumption  and to meet realtime requirements. a secondary objective for the implementation would be flexibility. flexibility requires the implementation to support variations in the rnn model  to allow for  online training  and to meet different applications requirements. in meeting these objectives  there exist challenges in  mapping these applications onto embedded systems  such as  the large number of computations to be performed within the  limited available memory. these objectives and challenges  are discussed in detail below.  a. objectives of realizing rnns on embedded  platforms    to realize rnn models on embedded platforms  we define  some objectives that will influence the solution. these objectives are divided into implementation efficiency objectives  and flexibility objectives.     implementation efficiency    since we target embedded platforms  we consider the online  execution of the application. to satisfy the implementation  efficiency objective  the implementation should have a high  throughput  low energy consumption  and meet the realtime requirements of the application. the real time requirements of the application pose additional demands for the  throughput  energy consumption and the accuracy of the  implementation. accuracy indicates how correct the model  is in performing recognition  classification  translation  etc.    high throughput throughput is a measure of performance. it measures the number of processed input output samples per second. application level inputs  and outputs are diverse. for image processing applications  the input can be frames and the throughput can  be the number of consumed frames per second  which  may also depend on the frame size. for speech text  applications  it can be the number of predicted words  per second. thus for different sizes and types of input  and outputs  throughput can have different units and the  throughput value may be interpreted in various ways.  to compare different applications  we use the number  of operations per second as a measure of throughput.    low energy consumption for an implementation to  be considered efficient  the energy consumption of the  volume                                                             fpgas only    this article                      implementation should meet embedded platforms  energy constraints. to compare the energy consumption  of different implementations  we use the number of  operations per second per watt as a measure of energy  efficiency.  real time requirements in real time implementations   a response cannot be delayed beyond a predefined deadline  and energy consumption cannot exceed a predefined limit. the deadline is defined by the application  and is affected by the frequency of sensor inputs and  the system response time. normally  the rnn execution  should meet the predefined deadline.       flexibility    the flexibility of the solution in this context is the ability of the solution to run different models under different  constraints without being restricted to one model or one  configuration. for an implementation to be flexible  we define  the following requirements that should be satisfied     supporting variations in rnn layer the recurrent  layers of rnn models can vary in the type of the layer   different types of the recurrent layer are discussed in  section iii b   the number of hidden cells  and the  number of recurrent layers.    supporting other nn layers rnn models have other  types of nn layers as well. a solution that supports  more nn layers is considered a complete solution for  rnn models  and not just a flexible solution. convolution layers  fully connected layers  and pooling layers  might be required in an rnn model.    supporting algorithmic optimization variations different algorithmic optimizations are applied to rnn  models to implement them efficiently on embedded systems  section iv . supporting at least one algorithmic  optimization for the hardware solution is in many cases  mandatory for a feasible execution of rnn models on  an embedded system. combinations of optimizations  will lead to higher efficiency and flexibility as this gives  the algorithmic designer more choices while optimizing  the model for embedded execution.    online training training is a process that sets parameter values within the neural network. in embedded  platforms  training is performed offline  and only inference is run on the platform at run time. for real life  problems  it is often not enough to run only inference  on the embedded platforms   some level of training  is required at run time as well. online training allows        rezk et al.  recurrent neural networks  an embedded computing perspective         the neural network to adapt to new data that was not  encountered within the training data  and to adapt to  changes in the environment. for example  online training is required for object recognition in autonomous cars  to achieve lifelong learning  by continuously receiving  new training data from fleets of robots and updating  the model parameters     . another example is in automated visual monitoring systems that continuously  receive new labeled data     .  meeting the requirements of different application  domains one aspect of flexibility is to support the requirements of different application domains. this makes  the implementation attractive because the solution can  support a wider range of applications. however  different application domains can have different performance  criteria. some application domains  such as autonomous  vehicles       might require very high throughput with  moderate power consumption  while others  such as  mobile applications             require extremely low  power consumption but have less stringent constraints  on throughput.    b. challenges in mapping rnns on embedded  platforms    we shall now take a look at the challenges faced by hardware  solutions to meet the objectives discussed above.     computation challenges    the main computation bottleneck in rnns is the matrix to  vector multiplications. the lstm layer  explained in detail  in section iii b  has four computation blocks  each of which  has one matrix to vector multiplication. for example  if the  size of the vector is      and the size of the matrices is               each matrix to vector multiplication requires              mac  multiply and accumulate  operations. the total  number of mac operations in the lstm would be                  .   mega mac  which is approximately equivalent  to   .  mop. the high number of computations negatively  affects both the throughput of the implementation and energy  consumption.  one other problem in rnns is the recurrent structure of  the rnn. in rnns  the output is fed back as an input in  such a way that each time step computation needs to wait  for the previous time step computation to complete. this  temporal dependency makes it difficult to parallelize the  implementation over time steps.     memory challenges    the memory required for the matrix to vector multiplications can be very large. the size and the access time of  these matrices become a memory bottleneck. the previous  example of the lstm layer  requires four matrices  each of  size            . consider    bit floating point operations   the size of the required memory for the weights would be                           m b. also  the high number       of memory accesses affects the throughput and energy consumption of the implementation     .     accuracy challenges    to overcome the previous two issues  computation and memory challenges   optimizations can be applied to rnn models  as discussed in section iv. these optimizations may affect  accuracy. the acceptable decrease in accuracy varies with  the application domain. for instance  in aircraft anomaly  detection  the accepted range of data fluctuation is only         .  iii. recurrent neural networks    the intelligence of humans  as well as most animals  depends on having a memory of the past. this can be shortterm  as when combining sounds to make words  and longterm  for example where the word  she  can refer back to   anne  mentioned hundreds of words earlier. this is exactly  what rnn provides in neural networks. it adds feedback  that enables using the outputs of previous time step while  processing the current time step input. it aims to add memory  cells that function similarly to human long term and shortterm memories.  rnns add recurrent layers to the nn  neural network   model. figure   presents a generic model for rnns that  consists of three sets of layers  input  recurrent  and output .  input layers take the sensor output and convert it into a vector  that conveys the features of the input. these are followed  by the recurrent layers  which provide feedback. in most  recent recurrent layer models  memory cells exist as well.  subsequently  the model completes similarly to most nn  models with fully connected  fc  layers and an output  layer that can be a softmax layer. fc layers and the output  layer are grouped into the set of output layers in figure  .  in this section  we discuss the input layers  different types  of recurrent layer  output layers  rnn modes of operation   deep rnn  and rnn applications and their corresponding  datasets.    figure    generic model of rnns with diverse recurrent  layers.    a. input layers  features extractor  and  corresponding applications and datasets    input layers are needed by many implementations to prepare the sensor output for processing  these may also called  feature extraction layers . often  the raw sensor data  e.g.   volume            rezk et al.  recurrent neural networks  an embedded computing perspective    the audio samples or video frames  are in a form that is  unsuitable for direct processing in the recurrent layer. also   the rnn performance  in learning rate and accuracy  can be  significantly improved if suitable features are extracted in the  input layer.  as sensor types  and numbers  change with the application  rnn models show a large variation with application  types as well. thus it is important to study which applications  an rnn model is used for and their corresponding datasets.  datasets are used by researchers to demonstrate success  in applying their methods and the modifications to them.  datasets differ in the size of the data samples  the values of  data samples  and the total size of the dataset. the success  of nn models is measured by accuracy. accuracy indicates  how correct the model is when carrying out recognition   classification  translation  etc.  in this section  we discuss examples from three application  domains where input layer pre processing is used  audio   video  and text. in table    we summarize these application domains and their corresponding datasets. for different  datasets  different metrics are used to assess accuracy.     audio inputs    audio feature extractors translate sound signals into feature  vectors. in speech processing  we often want to extract a  frequency content from the audio signal  in a similar way  to the human ear      . there are many ways to do this  for  example  by using short time fourier transform  stft   mel  frequency cepstral coefficients  mfcc  and linear predictive  coding  lpc  coefficients     .  applications  speech recognition  speech recognition applications receive audio as input   understand it  and translate it into words. speech recognition  can be used for phonetic recognition  voice search  conversational speech recognition  and speech to text processing     .     video inputs    when the input is a video signal  that is  a sequence of images  or frames  it is natural to use a convolutional neural network   cnn  as an input layer. cnn layers then extract image  features from each video frame and feed the resulting feature  vector to the recurrent layer. this use of a cnn as an input  layer before a recurrent layer has been employed for many  applications with video inputs  such as activity recognition   image description            or video description     .  the use of cnn as an input layer can also be found for  audio signals     . in this case  a short segment of audio  samples is transformed into a frequency domain vector using   for example  stft or mfcc. by combining a number of  these segments into a spectrogram  we can show information  about the source s frequency and amplitude against time.  this visual representation is then fed into a cnn as an image.  the cnn then extracts speech or audio features suitable for  the recurrent layer.  applications  image video applications  volume           image video applications cover any application that takes  images as input  for example  image captioning  activity  recognition  and video description.     text inputs    when the input is in the form of text  we often want to  represent words as vectors  and word embedding is one  common way to do this     . the word embedding layer  extracts the features of each word in relation to the rest of the  vocabulary. the output of the word embedding is a vector.  for two words with similar contexts  the distance between  their two vectors is short  while it is large for two words that  have different contexts.  following word embedding in an input layer  deeper text  analysis or natural language processing is performed in the  recurrent layers.  applications     text generation  rnn models can be used for language related applications such as text generation. rnn models can predict  the next words in a phrase  using the previous words as  inputs.    sentiment analysis  sentiment analysis is the task of understanding the  underlying opinion expressed by words           . since  the input words comprise a sequence  rnn methods are  well suited to performing sentiment analysis.  b. recurrent layers    in this section  we cover the various types of recurrent layers.  for each layer  we discuss the structure of the layer and  the gate equations. the most popular recurrent layer is the  long short term memory  lstm      . changes have been  proposed to the lstm to enhance algorithmic efficiency or  improve computational complexity. enhancing algorithmic  efficiency means improving the accuracy achieved by the  rnn model  which includes lstm with peepholes and  convlstm  as discussed in sections iii b  and iii b .  improving computational complexity means reducing the  number of computations and the amount of memory required by an lstm to run efficiently on a hardware platform. techniques include lstm with projection  gru  and  qrnn sru  which are discussed in sections iii b   iii b    and iii b   respectively. these changes can be applied to  the gate equations  interconnections  or even the number of  gates. finally  we compare all the different layers against  the number of operations and the number of parameters in  table  .     lstm    first  we explain the lstm  long short term memory   layer. looking at lstm as a black box  the input to lstm is  a vector combination of the input vector xt and the previous  time step output vector ht     where the output vector at time  t is denoted as ht . looking at the structure of an lstm         rezk et al.  recurrent neural networks  an embedded computing perspective    table    rnn input layer types and their corresponding application domains and datasets.  input type    applications    audio input    speech recognition    video input    image video applications    text input    text generation  sentiment analysis    dataset  tidigits       an        timit       wall street journal wsj        librispeech asr corpus       coco       moving mnist       comma.ai driving dataset       penn treebank  ptb        wikitext       text        wmt          imdb         it has a memory cell state ct and three gates. these gates  control what is to be forgotten and what is to be updated  by the memory state  forget and input gates . they also  control the part of the memory state that will be used as  an output  output gate . our description of the lstm unit  is based on its relationship with hardware implementations.  thus  in figure  a  we show the lstm as four blocks instead  of three gates because lstm is composed of four similar  computation blocks.  the computation block is the matrix to vector multiplication of the combination of xt and ht   with one of the weight  matrices  wf   wi   wc   wo  . this is considered the dominant computational task in lstms. each block is composed  of a matrix to vector multiplication followed by the addition  of a bias vector  bf   bi   bc   bo    and then the application of  a nonlinear function. each block might have element wise  multiplication operations as well. the nonlinear functions  used in the lstm are tanh and sigmoid functions. the four  computation blocks are as follow     forget gate the role of the forget gate is to decide  which information should be forgotten. the forget gate  output ft is calculated as  ft     wf  ht     xt     bf            where xt is the input vector  ht   is the hidden state  output vector  wf is the weight matrix  bf is the bias  vector  and   is the sigmoid function.  input gate the role of the input gate is to decide which  information is to be renewed. the input gate output it is  computed similarly to the forget gate output as  it     wi  ht     xt     bi                          using the weight matrix wi and the bias vector bi .  state computation the role of this computation is to  compute the new memory state ct of the lstm cell.  first  it computes the possible values for the new state  et   tanh wc  ht     xt     bc     c           where xt is the input vector  ht   is the hidden state  output vector  wc is the weight matrix  and bc is the bias  vector. then  the new state vector  ct is calculated by       accuracy measure metric  word error rate  wer   lower is better     phone error rate  per   lower is better   bleu  higher is better   cross entropy loss  lower is better   rms prediction error  lower is better   perplexity per word  ppw    lower is better     bilingual evaluation understudy  bleu    higher is better   testing accuracy  higher is better     the addition of the previous state vector ct   elementwise multiplied with the forget gate output vector ft  et element wise  and the new state candidate vector c  multiplied with the input gate output vector it as  ct   ft         ct     it    et    c           where is used to denote the element wise multiplication.  output gate the role of the output gate is to compute  the lstm output. first  the output gate vector ot is  computed as  ot     wo  ht     xt     bo              where xt is the input vector  ht   is the hidden state  output vector  wo is the weight matrix  bo is the bias  vector  and   is the sigmoid function. then  the hidden  state output ht is computed by applying the elementwise multiplication of the output gate vector ot  that  holds the decision of which part of the state is the  output  to the tanh of the state vector ct as  ht   ot    tanh ct  .           the number of computations and parameters for lstm are  shown in table  . matrix to vector multiplications dominate  the number of computations and parameters. for each matrix  to vector multiplication  the input vector xt of size m and  the hidden state output vector ht   of size n are multiplied  with weight matrices of size  m   n    n. that requires  n m   n  mac operations  which is equivalent to nm   n   multiplications and nm   n  additions. the number of parameters in the weight matrices is nm   n  as well. since  this computation is repeated four times within the lstm  computation  these numbers are multiplied by four in the total  number of operations and parameters for an lstm. for the  models in the studied papers  n is larger than m. thus  n has  a dominating effect on the computational complexity of the  lstm.     lstm with peepholes    peephole connections were added to lstms to make them  able to count and measure the time between events     . as  seen in figure  b  the output from the state computation is  volume            rezk et al.  recurrent neural networks  an embedded computing perspective     a  long short term memory  lstm .     b  lstm with peepholes.     c  lstm with projection layer.     d  gated recurrent unit  gru .     e  quasi rnn  qrnn .     f  simple recurrent unit  sru .    figure    different variations of an rnn layer.  volume                 rezk et al.  recurrent neural networks  an embedded computing perspective    used as input for the three gates. the lstm gate equations  are changed to   ft     wf  ht     xt   ct       bf              it     wi  ht     xt   ct       bi              ot     wo  ht     xt   ct     bo  .           and    where xt is the input vector  ht   is the hidden state output  vector  ct   is the state vector at time t      wf   wi   wo are  the weight matrices  and bf   bi and bo are the bias vectors.  the number of operations and computations for an lstm  with peepholes are shown in table  . there exist two rows  for an lstm with peepholes. the first one considers the  multiplication with the cell state in the three gates as a matrix to vector multiplication. the number of multiplications   additions  and weights increases by     n  . however  the  weight matrices multiplied with the cell state can be diagonal  matrices     . thus  the matrix to vector multiplication can  be considered as element wise vector multiplication  which  has become widely used for lstm with peepholes. in this  case  the number of multiplications  additions  and weights  will increase by  n only.     convlstm    convlstm is an lstm with all matrix to vector multiplications replaced with  d convolutions     . the idea is that  if the input to the lstm is data that holds spatial relations  such as visual frames  it is better to apply  d convolutions  than matrix to vector multiplications. convolution is capable  of extracting spatial information from the data. the vectors  xt   ht   and ct are replaced with   d tensors. one can  think of each element in the lstm vectors as a  d frame  in the convlstm vectors. convolution weights need less  memory than to vector matrices weights. however  using  them involves more computation.  the number of operations and parameters required for a  convlstm are shown in table  . the calculated numbers are  for a convlstm without peepholes. if peepholes are added   the number of multiplications  additions  and weights will  increase by  n. since the main change from an lstm is  the replacement of the matrix to vector multiplications with  convolutions  the change in the number of operations and  parameters would be via the nm   n  factor that appears  in multiplications  additions  and the number of weight equations. the number of multiplications and additions  macs   in convolutions of input vector xt and hidden state output  vector ht   is rcnmki     rcn    ks     where r is the number  of rows and c is the number of columns in the frames  n is the  number of frames in input xt   m is the number of frames in  output ht  or the number of hidden cells   ki is the size of the  filter used with xt   and ks is the size of the filter used with  ht   . the number of weights is the size of the filters used for  convolutions.          lstm with projection layer    the lstm is changed by adding one extra step after the  last gate     . this step is called a projection layer. the  output of the projection layer is the output of the lstm and  the feedback input to the lstm in the next time step  as  shown in figure  c. simply  a projection layer is like an fc  layer. the purpose of this layer is to allow an increase in the  number of hidden cells while controlling the total number of  parameters. this is performed by using a projection layer that  has a number of units p less than the number of hidden cells.  the dominating factor in the number of computations and  the number of weights will be  pn instead of  n    where n is  the number of hidden cells and p is the size of the projection  layer. since p   n  n can increase with a smaller effect on  the size of the model and the number of computations.  in table    we show the number of operations and parameters required for an lstm with a projection layer. in  the original paper proposing the projection layer  the authors  considered the output layer of the rnn as a part of the  lstm     . the output layer was an fc layer that changes  the size of the output vector to o  where o is the output size.  thus  there is an extra po term in the number of multiplications  additions  and weights. we put the extra terms between  curly brackets to show that they are optional terms. the  projection layer can be applied to an lstm with peepholes  as well. in table    we show the number of operations and  parameters for an lstm with peepholes and a projection  layer.     gru    the gated recurrent unit  gru  was proposed in          .  the main purpose was to make the recurrent layer able  to capture the dependencies at different time scales in an  adaptive manner     . however  the fact that gru has  only two gates  three computational blocks  instead of three   four computational blocks  as with the lstm makes it  more computationally efficient and more promising for highperformance hardware implementations. the three computational blocks are as follows     reset gate the reset gate is used to decide whether to  use the previously computed output or treat the input  as the first symbol in a sequence. the reset gate output  vector rt is computed as  rt     wr  ht     xt                     where xt is the input vector  ht   is the hidden state  output vector  wr is the weight matrix  and   is the  sigmoid function.  update gate the update gate decides how much of the  output is updated. the output of the update gate zt is  computed as the reset gate output rt using the weight  matrix wz as  zt     wz  ht     xt   .            volume            rnn layer    lstm  lstm   peepholes  lstm   peepholes  diagonalized   lstm   projection  lstm   peepholes  diagonalized    projection    multiplications   n     nm    n    lst mmul   n     nm    n    lst mmul    n    n     nm    n    lst mmul    n   np    nm    n   np    po     lst m p rojmul   np    nm    n   np    po     lst m p rojmul    n    number of operations  additions   n     nm    n    lst madd   n     nm    n    lst madd    n    n     nm    n    lst madd    n   np    nm    n   np     po     lst m p rojadd   np    nm    n   np     po     lst m p rojadd    n    convlstm     rcnmki     rcn  ks      n    gru     n     nm    n     .  lst mmul     rcnmki     rcn  ks       n   n     nm    n     .  lst madd    qrnn  sru     knm    n   nm    n     knm    n   nm    n    nonlinear   n    lst mnonlinear   n    lst mnonlinear   n    lst mnonlinear   n    lst mnonlinear   n    number of parameters  weights  biases   n     nm   n    lst mweights    lst mbiases   n     nm   n    lst mweights    n     lst mbiases   n     nm    n   n    lst mweights    n    lst mbiases   np    nm   np    po    n    lst m p rojweights   np    nm    n   np     po     lst m p rojweights     n      lst mbiases   n     n     nmki      n  ks       n     n      . lst mnonlinear   n   n     n     nm     .  lst mweights          knm   nm    n     n      lst mnonlinear      lst mbiases    in the table we use the following symbols  m is the size of input vector xt   n is the number of hidden cells in ht   p is the size of the projection layer  o is the size of the output  layer  r is the number of rows in a frame  c is the number of columns in a frame  ki is size of the  d filter applied to xt   ks is the size of the  d filter applied to ht     and k is  the size of  d convolution filter. the term  po  is an optional term as discussed in section iii b .    rezk et al.  recurrent neural networks  an embedded computing perspective    volume           table    comparing lstm and its variations.          rezk et al.  recurrent neural networks  an embedded computing perspective         output computation the role of this block is to compute the hidden state vector ht . first  it computes the  possible values for the hidden state vector e  ht  e  ht   tanh w  rt    ht     xt                where xt is the input vector  ht   is the hidden state  output vector  and w is the weight matrix. then  the  hidden state vector ht is computed from the old output  ht   and the new possible output e  ht as  ht        zt      ht     zt    e  ht .            as with lstm  we visualize a gru in figure  d as three  blocks  not two gates  as it has three blocks of matrix to vector  multiplications. in table    we show the number of operations and parameters required for a gru. the number of  operations and parameters is approximately  .   the number  of operations and parameters in the lstm.     qrnn and sru    the purpose of quasi rnn  qrnn       and simple recurrent unit  sru       is to make the recurrent unit friendlier  for computation and parallelization. the bottleneck in an  lstm gru is the matrix to vector multiplications. it is  difficult to parallelize this part because it depends on the  previous time step output ht   and previous time step state  ct   . in qrnn sru  ht   and ct   are removed from all  matrix to vector multiplications and appear only in elementwise operations. qrnn has two gates and a memory state.  it has three heavy computational blocks. in these blocks   only the input vector xt is used as input. it replaces the  matrix to vector multiplications with  d convolutions with  inputs along the time step dimension. for instance  if the  filter dimension is two  convolution is applied on xt and xt   .  the three computation blocks compute the forget gate vector  et   and the output gate  ft   candidate for new state vector c  vector ot as  ft     wf   xt               et   tanh wc   xt     c            ot     wo   xt               and    ft     wf xt   vf    ct     bf              rt     wr xt   vr    ct     br              and  respectively. in both gate calculations  ct   is used but only  for element wise multiplications. the parameter vectors vf  and vr are learned with weight matrices and biases during  training.  the third computational block is the state computation ct  ct   ft    ct          ft       w xt               where ct   is the old state vector and xt is the input  vector. the computation is controlled by the forget gate  output vector ft that decides what is to be forgotten and what  is to be treated as new.  finally  the sru output ht is computed from the new  state ct and the input vector xt checked by the update gate   which decides the parts of the output that are taken from the  new state and the parts that are taken from input  using the  equation  ht   rt ct        rt   xt .        figure  f visualizes the sru. the output computation is  performed in the same block with the update gate. it is worth  observing that in neither qrnn nor sru  ht   are used in  the equations   only the old state ct   is used. the number  of operations and parameters for an sru is shown in table  .  in table    we compare the lstm and all of its variations  against the memory requirements for the weights and the  number of computations per single time step. this comparison helps to understand the required hardware platform for  each of them. to make it easier for the reader to understand  the difference between the lstm and the other variants  we  show the equations for operations and parameters in terms of  lstm operations and parameters if they are comparable.  c. output layers    the output layers in the rnn model are the fc layers and  the output function.     fc  fully connected  layers    where wf and wc   wo are the convolution filter banks and      is to denote the convolution operation.  the state vector ct is computed as  ct   ft    ct          ft      et  c            and the hidden state vector ht is computed using equation  .  figure  e is used to visualize the qrnn layer. the number  of operations and parameters required for a qrnn is shown  in table    where k is the size of the convolution filter.  the sru has two gates and a memory state as well.  the heavy computational blocks  three blocks  are matrix        to vector multiplications  not convolutions. the two gates   forget and update gates  are computed using the equations    the rnn model might have one or more fc layers after  the recurrent layers. non linear functions may be applied  between the fc layers as well. this is called fully connected because each neuron in the input is connected to  each neuron of the output. computationally  this is done by  matrix to vector multiplication using a weight matrix of size  inputsize   outputsize   where inputsize is the size of the  input vector and outputsize is the size of the output vector.  one purpose of the fc layer in rnn models can be to change  the dimension of the hidden state output vector ht to the  dimension of the rnn model output to prepare it for the  output function. in this case  the fc layer might be replaced  by adding a projection layer in the recurrent layer.  volume            rezk et al.  recurrent neural networks  an embedded computing perspective       output function    the output function is the final step in the neural networks inference. it generates the output of the neural network model.  this output can be a prediction  classification  recognition   and so on. for example  in a text prediction problem  the  softmax function is used as an output function. the output  is a vector of probabilities that sum to one. each probability  corresponds to one word. the word with the highest probability becomes the prediction of the neural network     .         d. processing of data in rnn models    there are many ways that processing of data may vary in  rnn models. the first is to vary the way time steps are  treated. this is influenced by the nature of the application   which may have inputs with temporal relations  outputs with  temporal relations  or both. the second form of variation  is related to bidirectional rnns. we discuss below how  a bidirectional rnn can process inputs both forwards and  backwards in time. we also discuss what is meant by a deep  rnn model.         analysis      are two examples. in activity recognition   the model takes a sequence of images as input and  determines the activity taking place in the images. in  sentiment analysis  the model takes a sequence of words   sentence  as input and generates a single emotion at the  end. in this case  the temporal sequence sequence is only  in the input.  many to many a many to many model has a sequence  in the input and a sequence in the output  as shown in  figure  c. language translation     and video description     are two examples. in language translation  the  model has a sequence of words  sentence  as an input  and a sequence of words  sentence  as an output. in  video description applications  the model has a sequence  of image frames as input and a sequence of words   sentence  as output.  one to one there is no rnn model with one to one  unrolling. one to one simply means that there is no  temporal relation contained in the inputs or the outputs   a feedforward neural network .       rnn unfolding variations through time steps       bidirectional rnn    rnn unfolding unrolling is performed to reveal the repetition in the recurrent layer and to show the number of  time steps required to complete a task. unfolding the rnn  illustrates the different types of rnn models one can meet.    one to many a one to many model generates a sequence of outputs for a single input  as shown in figure  a. image captioning is one example    . the model  takes one image as input and generates a sentence as an  output. the words of the sentence compose a sequence  of temporally related data. in this case  the temporal  sequence is only in the output.    in bidirectional rnn  input can be fed into the recurrent  layer from two directions  past to future and future to past.  that requires a duplication of the recurrent layer  so that two  recurrent layers work simultaneously  each processing input  in a different temporal direction. this can help the network to  better understand context by obtaining data from the past and  the future at the same time. this concept can be applied to  different variations of recurrent layers such as bilstm       and bigru     .     a  one to many rnn.     b  many to one rnn.     c  many to many rnn.    figure    unfolding rnn models through multiple time  steps.       many to one a many to one model combines a sequence of inputs to generate a single output  as shown  in figure  b. activity recognition     and sentiment    volume           e. deep recurrent neural networks  drnn     making a neural network a deep neural network is achieved  by adding non linear layers between the input layer and  the output layer     . this is straightforward in feedforward  nns. however  in rnns  there are different approaches that  can be adopted. similarly to feedforward nns  there can be  a stack of recurrent layers  stacked rnn       as shown in  figure    where we have a stack of two recurrent layers.  the output of the first layer is considered as the input for  the second layer. alternately  the extra non linear layers  can be within the recurrent layer computations     . extra  non linear layers can be embedded within the hidden layer  vector ht calculation  where the xt and ht   vectors used to  calculate ht   pass through additional non linear layers. this  model is called the deep transition rnn model. the extra  non linear layers can also be added in computing the output  from the hidden state vector  this model is called the deep  output rnn model. it is possible to have an rnn model that  is both a deep transition and a deep output rnn model     .  one other way to have extra non linear functions within the  recurrent layer is to have them within the gate calculations    a method called h lstm  hidden lstm .           rezk et al.  recurrent neural networks  an embedded computing perspective    the optimization method as  a             figure    stacked rnn. the first layer output is h t and the  second layer output is h t .    iv. optimizations for rnns    as with all neural network applications  rnn applications  are based upon intensive operations performed on high precision values. they therefore require high computation power   large memory bandwidth  and high energy consumption.  because of the resource constraints of embedded platforms   there is a need to decrease the computation and memory  requirements of rnn applications. in this section  we present  optimizations that have been applied to rnns to realize  them on embedded systems. in section v which follows   we discuss hardware implementations of rnns on embedded  platforms and how they relate to the optimizations presented  here. researchers have been working on two types of optimizations. the first type is related to the rnn algorithms  themselves  where rnn algorithms are modified to decrease  computation and memory requirements. the modification  should have no effect or only a limited effect on accuracy.  the second type of optimization is related to the embedded  platform  where hardware improvements are applied to increase the parallelization of the application and decrease the  overhead of memory accesses. figure   illustrates these two  types of optimizations.  a. algorithmic optimizations    in this section  we discuss the different algorithmic optimizations that may be performed on the recurrent layer of an  rnn application to decrease its computation and memory  requirements. we discuss how these optimizations are carried  out  and how they affect accuracy. applying optimizations directly to inference can have unacceptable effects on accuracy.  thus  training the network would be required to enhance  the accuracy. optimizations may be applied during the model  main training or after the model is trained and then the model  is retrained for some epochs  training cycles .  different datasets measure accuracy using different units.  for some units higher values are better  while for others  lower values are better. to provide a unified measure of the  change in accuracy  we calculate the percentage change in  accuracy from the original value to the value after applying        va   vb          vb            where a  is the effect of the optimization method on accuracy as a percentage of the original accuracy value  vb is  the value of accuracy before optimization  va is the value of  accuracy after optimization  and   is an indicator that has a  value of   if higher accuracy values are better and   if lower  accuracy values are better. thus  if the baseline accuracy  achieved by the original model without optimizations is      and the accuracy after optimization is      the effect of  optimization on accuracy is   .  . if the accuracy after  optimization is      the effect of optimization on accuracy  is   .  . if the optimization has no effect on accuracy  then  the effect on accuracy is   .  as shown in figure    the algorithmic optimizations are  quantization  compression  deltarnn  and nonlinear. the  first three optimizations are applied to the matrix to vector  multiplications operations and the last is applied to computation of non linear functions. the table in figure   compares quantization  compression  and deltarnn with their  effect on memory requirements  number of memory accesses   number of computations  and mac operation cost. mac  operation cost can be decreased by decreasing the precision  of operands.     quantization    quantization is a reduction in the precision of the operands.  quantization can be applied to the network parameters only   or to the activations and inputs as well. while discussing  quantization  there are three important factors to consider.  first  the number of bits used for weights  biases  activations  and inputs. second  the quantization method. the  quantization method defines how to store the full precision  values in a lower number of bits. third  discussing whether  quantization was applied with training from the outset or the  model was re trained after applying quantization. these three  factors all affect accuracy. however  they are not the only  factors affecting accuracy  which may also be affected by  model architecture  dataset  and other factors. yet  these three  factors have more relevance when applying quantization to  the rnn model.  in discussing quantization methods  we cover fixed point  quantization  multiple binary codes quantizations  and exponential quantization. we also study whether the selection of the quantized value is deterministic or stochastic.  in deterministic methods  the selection is based on static  thresholds. in contrast  selection in stochastic methods relies  on probabilities and random numbers. relying on random  numbers is more difficult for hardware.  a  quantized values representation    there are different methods for representing quantized values. in the following  we explain three commonly used  methods.  volume            rezk et al.  recurrent neural networks  an embedded computing perspective    figure    optimizations applied to rnn applications with section numbers indicated  comparing the effect of different  algorithmic optimizations on memory and computation requirements.       fixed point quantization in this quantization method   the    bit floating point values are quantized into a  fixed point representation notated as qm f   where m  is the number of integer bits  and f is the number  of fractional bits. the total number of bits required  is k. the sign bit may be included in the number of  integer bits      or added as an extra bit added to  m and f     . for example  in the first case        q .  is used to represent   bits fixed point that has  three values    .     .  . this quantization method is  also called pow  ternarization     . usually  fixed point  quantization is deterministic  in that for each floatingpoint value  there is one quantized fixed point value  defined by an equation  i.e. it is rule based . fixed point  quantization is performed by clipping the floating point  value between minimum and the maximum boundaries   and then rounding it.     exponential quantization exponential quantization  quantizes a value into an integer power of two. exponential quantization is very beneficial for the hardware  as multiplying with exponentially quantized value is  equivalent to shift operations if the second operand is a  fixed point value  and addition to exponent if the second  operand is a floating point value           . exponential  quantization can be both deterministic and stochastic.     binary and multi bit codes quantization the lowest  precision in rnns is binary precision     . each full  precision value is quantized into one of two values.  the most common two values are           but it  can also be             .          .     .    or any  combination of two values     . binarization can be  deterministic or stochastic. for deterministic binarization  a sign function can be used for binarization. for  stochastic binarization  selection thresholds depend on    volume           probabilities to compute the quantized value        with probability p    h  x    xb       with probability     p             where  h is the  hard sigmoid  function defined as  x    x     h  x    clip             max    min       .              binarization has great value for hardware computation  as it turns multiplication into addition and subtraction.  the greatest value comes with full binarization  where  both the weights and the activations have binary precision. in this case  it is possible to concatenate weights  and activations into    bit operands and do multiple  mac operations using xnor and bit count operations.  full binarization can reduce memory requirements by  a factor of    and decrease computation time considerably     .  adding one more value to binary precision is called  ternarization. weights in ternarized nn are restricted  to three values. these three values can be                 . power two ternarization is discussed above as  a form of fixed point quantization  and is an example  of ternarization with three different values    .        .  . both deterministic and stochastic ternarization  have been applied to rnns     .  having four possible quantization values is called quaternarization. in quaternarization  the possible values  can be        .     .           . in order to benefit  from the high computational benefit of having binary  weights and activations while using a higher number  of bits  multiple binary codes         has been used  for quantization     . for example  two bit quantization  has four possible values                                  .  the most common method for deterministic quantization is uniform quantization. uniform quantization may         rezk et al.  recurrent neural networks  an embedded computing perspective    not be the best quantization method as it can change  the distribution of the original data  especially for nonuniform data  which can affect accuracy. one solution  is balanced quantization     . in balanced quantization   data is divided into groups of the same amount of data  before quantization to ensure a balanced distribution of  data following quantization. other suggested solutions  treat quantization as an optimization problem  and include greedy quantization  refined greedy quantization   and alternating multi bit quantization           .  b  training retraining    as mentioned earlier  there are three options to minimize  accuracy loss due to quantization. the first is to apply  quantization with training       where quantized weights are  used during the forward and backward propagation only. full  precision weights are used for the parameters update step  in the  stochastic gradient descent  sgd. copies for both  quantized and full precision weights are kept to decide at  inference time which one to use     . in the second approach   quantization is applied to pretrained parameters and the rnn  model is retrained to decrease the accuracy loss. also  binarization of lstm gate outputs during training have been applied by using the gumbelsoftmax estimator     . authors in  one rnn implementation      adopted a mix of training and  retraining approaches  where only the activations were not  quantized from the beginning. activations were quantized  after training and then the model was retrained for    epochs.  the third approach is to use quantized parameters without  training retraining. this is very commonly used with    bit  fixed point quantization. usually  training happens at training  servers and quantization is applied at the inference platform  without having the opportunity to retrain the model. it is  very common as well to use    bit fixed point quantization  with other optimization techniques such as circulant matrices  compression       pruning       and deltarnn  discussed  later in section iv a       .  c  effect on accuracy    in table    we list studies that included experiments on the  quantization of rnn models. not all of the studies have  a hardware implementation  as the purpose is to show that  quantization can be performed while keeping accuracy high.  in the table  we put the three factors affecting the accuracy  discussed earlier  number of bits  quantization method  and  training  with an addition of the type of recurrent layer   lstm  gru...  and the dataset. then  we show the effect  of quantization on accuracy computed with respect to the  accuracy achieved by full precision parameters and activation using eq.   . for the number of bits  we use w a  where w is the number of bits used for weights and a  is the number of bits used for activations. for the rnn  type  we put the recurrent layers used in the experiments.  all recurrent layers are explained in section iii. we use  x y z  where x is the number of layers  y is the type of the  layers  and z is the number of hidden cells in each layer.        for training  if quantization was applied with training from  the beginning  we write  with training . if quantization was  applied after training and the model was later retrained  we  write  retraining . positive values for accuracy means that  quantization enhanced the accuracy and negative values for  accuracy means that quantization caused the model to be less  accurate.  each experiment in table   is applied to a different model   different dataset  and may also have used different training  methods. thus  conclusions about accuracy from table    cannot be generalized. still  we can make some observations             fixed point quantization  exponential quantization and  mixed quantization have no negative effect on accuracy.  accuracy increased after applying these quantization  methods. quantized models can surpass baseline models in accuracy as weight quantization has a regularization effect that overcomes over fitting     .  regarding binary quantization  the negative effect on  accuracy varied within small ranges in some experiments           . experiments showed that using more  bits for activations may enhance the accuracy     .  using binary weights with convlstm is not solely responsible for the poor accuracy obtained  as ternary and  quaternary quantization resulted in poor accuracy with  convlstm as well     . however  these quantization  methods were successful when applied on lstm and  gru in the same work     .       compression    compression decreases the model size by decreasing the  number of parameters or connections. as the number of  parameters is reduced  memory requirements and the number of computations decrease. table   compares different  compression methods. the compression ratio shows the ratio  between the number of parameters of models before and  after applying compression methods. accuracy degradation  is computed using eq.   .   i  pruning pruning is the process of eliminating redundancy. computations in rnns are mainly dense matrix  operations. to improve computation time  dense matrices are transformed into sparse matrices  which affects  accuracy. however  careful choice of the method used to  transform a dense matrix to a sparse matrix may result  in only a limited impact on accuracy while providing  significant gains in computation time. reduction in  memory footprint along with computation optimization  is essential for making rnns viable. however  pruning  results in two undesirable effects. the first is a loss in the  regularity of memory organization due to sparsification  of the dense matrix  and the second is a loss of accuracy  on account of the removal of weights and nodes from the  model under consideration. the transformation from a  regular matrix computation to an irregular application  often results in the use of additional hardware and  computation time to manage data. to compensate for the  volume            rezk et al.  recurrent neural networks  an embedded computing perspective    table    effect of quantization methods on accuracy.  method  fixed point  exponential  mixed  binary    ternary  quaternary  multi binary          w a       p t real  eq real  eq  fixed     b real  b real  b    b    b real  t real  t real  t real  q real  q real                   rnn type    bilstm        bilstm        gru        bilstm        gru      convlstm    bilstm        bilstm        gru            gru      convlstm    gru        gru      convlstm    lstm        lstm        lstm        training  with training  with training  with training  retraining  with training  with training  with training  with training  with training  with training  with training  with training  with training  with training  retraining  retraining  with training    accuracy    .                 .       .              .            .                  .      .             .           .      paper                                                                                                          accuracy is also affected by the compression scheme and nonlinear functions approximation used in this work.  we calculate the error at the tenth frame  third predicted frame .  in the table we have used the symbols  w a for number of bits for weights number of bits for activations  p t for power  two ternarization  eq for exponential quantization  b for binary quantization  t for ternary quantization  and q for quaternary  quantization.    loss of accuracy caused by pruning  various methods   including retraining  have been applied. the following  sections describe methods of pruning and compensation  techniques found in the literature. table   summarizes  the methods of pruning and its impact on sparsity and  accuracy. sparsity in this context refers to the number  of empty entries in the matrices. in table    sparsity  indicates the impact on the number of entries eliminated because of the method of pruning used. within  rnns  pruning can be classified as either magnitude  pruning for weight matrix sparsification  or structurebased pruning.  magnitude pruning magnitude pruning relies on eliminating all weight values below a certain threshold.  in this method  the choice of threshold is crucial to  minimize the negative impact on accuracy. magnitude  pruning is primarily based on identifying the correct  threshold for pruning weights.    weight sub groups for weight matrix sparsification   the rnn model is trained to eliminate redundant  weights and only retain weights that are necessary.  there are three categories to create weight subgroups  to select the pruning threshold     . these three  categories are class blind  class uniform  and classdistribution. in class blind  x  of weights with the  lowest magnitude are pruned  regardless  blind  of  the class. in class uniform  lower pruning x  of  weights is uniformly performed in all classes. in  class distribution  weights within the standard deviation of that class are pruned.    hard thresholding            identifies the correct  threshold value that preserves accuracy. ese       uses hard thresholding during training to learn which  volume           dataset  ocr dataset  wsj  tidigits  an   imdb  moving mnist  ocr dataset  ocr dataset  tdigits  imdb  moving mnist  tdigits  imdb  moving mnist  wikitext   wikitext   ptb    weights contribute to prediction accuracy.  gradual thresholding this method      uses a set of  weight masks and a monotonically increasing threshold. each weight is multiplied with its corresponding  mask. this process is iterative  and the masks are  updated by setting all parameters that are lower than  the threshold to zero. as a result  this technique gradually prunes weights introduced within the training  process  in contrast to hard thresholding.    block pruning in block pruning       magnitude  thresholding is applied to blocks of a matrix instead of  individual weights during training. the weight with  the maximum magnitude is used as a representative  for the entire block. if the representative weight is  below the current threshold  all the elements in the  blocks are set to zero. as a result  block sparsification  mitigates the indexing overhead  irregular memory  accesses  and incompatibility with array data paths  that characterises unstructured random pruning.    grow and prune grow and prune      combines  gradient based growth      and magnitude based  pruning      of connections. the training starts with  a randomly initialized seed architecture. next  in the  growth phase  new connections  neurons  and feature  maps are added based on the average gradient over  the entire training set. once the required accuracy has  been reached  redundant connections and neurons are  eliminated based on magnitude pruning.  structure pruning modifying the structure of the network by eliminating nodes or connections is termed  structure pruning. connections that may be important are learned in the training phase or pruned using  probability based techniques.              rezk et al.  recurrent neural networks  an embedded computing perspective    network sparsification pruning through network  sparsification      introduces sparsity for the connections at every neuron output  such that each output  has the same number of inputs. furthermore  an optimization strategy is formulated that replaces nonzero elements in each row with the highest absolute  value. this step avoids any retraining  which may  be compute intensive and difficult in privacy critical  applications. however  the impact of this method of  pruning on accuracy has not been directly measured.  design space exploration over different levels of  sparsity measures the quality of output and gives an  indication of the relationship between the level of  approximation and the application level accuracy.    drop out deepiot      compresses neural network  structures into smaller dense matrices by finding the  minimum number of non redundant hidden elements  without affecting the performance of the network. for  lstm networks  bernoulli random probabilities are  used for dropping out hidden dimensions used within  the lstm blocks.  retaining accuracy levels pruning alongside training  and retraining has been employed to retain the accuracy  levels of the pruned models. retraining works on the  pruned weights and or pruned model until convergence  to a specified level of accuracy is achieved. pruning  has shown a regularization effect on the retraining  phase     . the regularization effect might be the reason  for outperforming baseline model accuracy. another  benefit for pruning which might be the reason for outperforming the baseline accuracy is that pruning allows  the finding of a better local minimum. pruning increases  the loss function immediately  which results in further  gradient descent.  handling irregularity in pruned matrices pruning to  maximize sparsity results in a loss in regularity  or structure  of memory organization due to sparsification of the  original dense matrix. pruning techniques that are architecture agnostic  mainly result in unstructured irregular sparse matrices. methods such as load balancingaware pruning      and block pruning  explained earlier within magnitude pruning       have been applied  to minimize these effects. load balancing aware pruning      works towards ensuring the same sparsity ratio  among all the pruned sub matrices  thereby achieving an  even distribution of non zero weights. these techniques  introduce regularity in the sparse matrix to improve  performance and avoid index tracking.   ii  structured matrices  circulant matrices a circulant matrix is a matrix in  which each column  row  is a cyclic shift of the preceeding column  row      . it is considered as a special  case of toeplitz like matrices. the weight matrices are  reorganized into circular matrices. the redundancy of  values in the matrices reduces the space complexity             of the weights matrices. for large matrices  circulant  matrices can use nearly    less memory space. the  back propagation algorithm is modified to allow training of the weights in the form of circulant matrices.  block circulant matrices instead of transforming the  weight matrix into a circulant matrix  it is transformed  into a set of circulant sub matrices           . figure    shows a weight matrix that has    parameters. the  block size of the circular sub matrices is  . the weight  matrix has transformed into two circulant sub matrices  with   parameters    parameters each . the compression  ratio is     where   is the block size. thus  having  larger block sizes will result in a higher reduction in  model size. however  a high compression ratio may  degrade the prediction accuracy. in addition  the fast  fourier transform  fft  algorithm can be used to speed  up the computations. consequently  the computational  complexity decreases by a factor of o  logk k  .    figure    regular weight matrix transformed into blockcirculant sub matrices of block size       .   iii  tensor decomposition tensors are multidimensional  arrays. a vector is a tensor of rank one  and a   d  matrix is a tensor of rank two and so on. tensors can  be decomposed into lower ranks tensors  and tensor  operations can be approximated using these decompositions in order to decrease the number of parameters in  the nn model. canonical polyadic  cp  decomposition   tucker decomposition  and tensor train decomposition  are some of the techniques used to apply tensor decomposition     . tensor decomposition techniques can be  applied to the fc layers       convolution layers        and recurrent layers     . in table    we show an example of applying tensor decomposition on a gru layer  using the cp technique. in another example  adam s  algorithm has been used as an optimizer for the training process     . tensor decomposition techniques can  achieve a high compression ratio compared to other  compression methods.    volume            method    magnitude  pruning    structured  pruning  structured matrices  tensor  decomp.  knowledge  distillation             technique    rnn type    dataset    compression ratio   sparsity for pruning                         training    accuracy    paper    weight subgroups    wmt       hard thresholding  gradual pruning  block pruning  grow prune  network sparsification  drop out      lstm           lstm         lstm        lstm         bilstm         h lstm          lstm        bilstm        retraining      .     .               .           .                       .                .                                       none  with training  with training  with training  none  none          .               .                                                      bilstm        timit  ptb  speech data   coco  coco  librispeech  asr corpus  an     circulant    nearly       with training       .               block circulant  cp      lstm         gru        timit  nottingham      .                   with training  with training      .                               plain   pruning      lstm         lstm         wmt     wmt                  with training  with training    retraining           .              h lstm is hidden lstm. non linear layers are added in gate computations  explained in section iii .  dataset name is not mentioned in the paper.  accuracy is also affected by quantization  table    and nonlinear functions approximation used in this work.    table    effect of deltarnn method on accuracy  rnn model    gru      cnn    gru        dataset  tidigits  open driving    training  with training  with training    accuracy    .          speedup   .            paper                rezk et al.  recurrent neural networks  an embedded computing perspective    volume           table    effect of compression methods on accuracy.           rezk et al.  recurrent neural networks  an embedded computing perspective     iv  knowledge distillation knowledge distillation is a  method that replaces a large model with a smaller model  that should behave like a large model. starting from  a large model  teacher  with trained parameters and a  dataset  the small model  student  is trained to behave  like the large model     . in addition to knowledge  distillation  pruning can be applied to the resulted model  to increase the compression ratio  as shown in table  .     deltarnn    delta recurrent neural networks  deltarnn       makes  use of the temporal relation between input sequences. for  two consecutive input vectors xt and xt     the difference  between corresponding values in the two vectors may be zero  or close to zero. the same holds for the hidden state output  vector. the idea is to skip computations for input hidden state  values that when compared to input hidden state values of the  previous time step  have a difference that is less than a predefined threshold called delta    . improvement comes from  decreasing the number of computations and the number of  memory accesses required by the recurrent unit. however   memory requirements will not decrease because we still  need to store all the weights as we cannot predict which  computations will be skipped.  the value of delta threshold affects both accuracy and  speedup. in table    we summarize the effect of deltarnn  on accuracy for two different datasets. in some occasions  it  was required to train the rnn using a delta algorithm before  inference to obtain better accuracy at inference time. furthermore  the speedup gained by the delta algorithm at one  delta value is not static. it depends on the relation between  the input sequences. the highest speedup could be reached  using video frames  open driving dataset  as input data  as  seen in table  . however  the time consuming cnn before  the recurrent layer negated the speedup gained by deltarnn.  thus  the    x speedup in gru execution dropped down to  a non significant speedup for the model as a whole. on the  other hand  cnn delta      applied a similar delta algorithm  on cnns. applying delta algorithms to both recurrent layers  and cnn layers might prove beneficial.     non linear function approximation    non linear functions are the second most used operations in  the rnn after matrix to vector multiplications  as may be  seen in table  . the non linear functions used in the recurrent  layers are tanh and sigmoid  respectively. both functions  require floating point division and exponential operations   which are expensive in terms of hardware resources. in  order to have an efficient implementation for an rnn  nonlinear function approximations are implemented in hardware.  this approximation should satisfy a balance between high  accuracy and low hardware cost. in what follows  we present  the approximations used in the implementations under study.  look up tables  luts   replacement of non linear  function computation with look up tables is the fastest  method     . the input range is divided into segments with        constant output values. however  to achieve high accuracy   large luts are required and that consumes a large area of  silicon  which is not practical. several methods have been  proposed to decrease the luts size while preserving high  accuracy.  piecewise linear approximation  this approximation  method is done by dividing the non linear function curve  into a number of line segments. any line segment can be  represented by only two values  the slope and the bias. thus   for each segment  only two values are stored in the luts.  the choice of the number of segments affects both accuracy  and the size of luts. thus  the choice of the number of  segments must be made wisely to keep the accuracy high  while keeping the luts as small as possible. the computational complexity of the non linear function changes to  be a single comparison  multiplication and addition  which  may be implemented using shifts and additions. compared to  using look up tables  piecewise linear approximation requires  fewer luts and more computations.  hard tanh   hard sigmoid  hard tanh and hard sigmoid  are two examples of piecewise linear approximation with  three segments. the first segment is saturation to zero or      zero in case of sigmoid and    in case of tanh   the last  segment is saturation to one  and the middle segment is a line  segment that joins the two horizontal lines.  there is a variant of piecewise linear approximation called  piecewise non linear approximation. the line segments are  replaced by non linear segments and the use of multipliers  cannot be avoided as they can in the linear version. this made  the linear approximation preferable in hardware design.  ralut one other method to reduce the size of the luts  is to use ralut  range addressable look up tables      .  in raluts  each group of inputs is mapped into a single  output.  b. platform specific optimizations    in this section  we discuss the optimizations performed on  the hardware level to run an rnn model efficiently. these  optimizations may be related to computation or memory. for  computation related optimizations  techniques are applied  to speedup the computations and obtain higher throughput.  for memory related optimizations  techniques are applied to  carry out memory usage and accesses with reduced memory  overhead.     compute specific    the bottleneck in rnn computations is the matrix to vector  multiplications. it is difficult to fully parallelize matrix to  vector multiplications over time steps as the rnn model  includes a feedback part. each time step computation waits  for the preceding time step computations to complete so it  can use the hidden state output as an input for the new time  step computation.    loop unrolling loop unrolling is a parallelization technique that creates multiple instances of the looped operations to gain speedup at the expense of resources.  volume            rezk et al.  recurrent neural networks  an embedded computing perspective                   there are two kinds of loop unrolling used in rnn implementations. the first is inner loop unrolling  where  the inner loop of the matrix to vector multiplication  is unrolled           . the second kind is unrolling  over time steps. rnn needs to run for multiple timesteps for each task to be completed. the computation of  the recurrent unit can be unrolled over time steps     .  however  this cannot be fully parallelized  as discussed  earlier. only computations that rely on inputs can be  parallelized  while computations relying on hidden state  outputs are performed in sequence. one solution can be  to use qrnn or sru  as discussed in section iii b. in  qrnn and sru  the matrix to vector multiplications  do not operate on the hidden state output and thus can  be fully parallelized over unrolled time steps     .  systolic arrays  d systolic arrays are a good candidate  for matrix to vector multiplication            and convolution units     . systolic arrays are efficient as multiplications operands move locally between neighbor  pes  processing elements      . thus  systolic arrays  require less area  less energy  and less control logic. well  designed systolic arrays can guarantee that pes remain  busy to maximize throughput.  pipelining pipelining is an implementation technique  that can increase throughput. pipelining has been used in  rnn implementations in various ways. coarse grained  pipelining  cgpipe  is used to tailor the lstm variants  data dependency           . lstm computation is performed in three stages  with double buffers in between.  the first stage is for weight matrices multiplications  with inputs and hidden cells vectors  the second stage  is for non matrix to vector operations  and the third  stage is for projection layer computations. fine grained  pipelining  fgpipe  can be used to schedule the operations within the cgpipe stages. the design of the  pipelining scheduler is a critical task due to the data dependency in lstm variants     . some operations need  to be performed sequentially  while some operations can  be done concurrently. having sparse weight matrices   due to applying pruning  increases the complexity of  the scheduler design.  tiling tiling consists of dividing one matrix to vector  multiplication into multiple matrix to vector multiplications. usually  tiling is used when a hardware solution  has built in support for matrix to vector multiplication of  a specific size in one clock cycle. when the input vector  or the weight matrix size is larger than the size of the  vector or the matrix supported by the hardware  tiling  is used to divide the matrix to vector multiplication to  be performed on the hardware in multiple cycles            . thus  tiling can be combined with inner loop  unrolling or systolic arrays. figure   shows a vector  that is broken into three vectors and a matrix that is  broken into nine matrices. thus  one matrix to vector  multiplication is broken into nine matrix to vector multiplications. each vector is multiplied with the matrices    volume           having a similar color. the output vector is built from  three vectors  where each of the three output vectors  are accumulated together to form one vector in the  output. this computation requires nine cycles to be  completed  assuming that new weights can be loaded  into the hardware multiplication unit within the cycle  time.    figure    tiling  converting one matrix to vector multiplication into nine matrix to vector multiplications.                 hardware sharing in the gru recurrent layer  the  execution of rt and e  ht has to be in sequence as e  ht  computation depends on rt as shown in eq.   . thus   the computation of rt and e  ht is the critical path in the  gru computation. while zt can be computed in parallel  as it is independent of e  ht and rt . the same hardware  can be shared for computing rt and zt to save hardware  resources     .  load balancing in the case of sparse weight matrices  resulting from pruning   load balancing techniques  might be needed during parallelization of the matrix  to vector multiplication over processing elements            .  analog computing analog computing is a good candidate for neural network accelerators     . analog  neural networks      and analog cnns       have been  studied recently. interestingly  rnn implementations  using analog computing have started to attract attention  from researchers            . analog computing brings  significant benefits  especially for the critical matrixvector computation  by making it both faster and more  energy efficient. this is true for the non linear functions  that normally are calculated between the nn layers as  well. analog computing also allows for more efficient  communication as a wire can represent many values  instead of only a binary value. the performance of an  analog computer will however  critically depend on the  digital to analog and analog to digital converters  for  both speed and energy consumption.       memory considerations    for the processing of an rnn algorithm  memory is needed  to store weight matrices  biases  inputs  and activations   where the weight matrices have the highest memory requirement. the first decision related to memory is the location of  weights storage. if all the weights are stored in the off chip  memory  accessing the weights comprises the largest cost  with respect to both latency and energy            .         rezk et al.  recurrent neural networks  an embedded computing perspective    on chip memory after applying the algorithmic optimizations introduced in section iv a  the memory requirements of the rnn layer are reduced  which increases the  possibility of storing the weights in the on chip memory.  however  this results in a restriction on the largest model size  that can run on the embedded platform. on chip memory has  been used for storing the weights by many implementations                               .  hybrid memory storing all the weights in the on chip  memory restricts the size of the model executed on the  embedded solution. storing some of the weights in on chip  memory with the remainder in off chip memory might provide a solution     .  in addition to maximizing the use of on chip memory   some researchers use techniques to reduce the number and  the cost of memory accesses.                      multi time step parallelization  the fact that qrnn and sru remove the hidden state  output from the matrix to vector multiplications can be  leveraged to allow multi time step parallelization     .  multi time step parallelization is performed by converting multiple matrix to vector multiplication into a fewer  matrix to matrix multiplications. this method decreases  the number of memory accesses by reusing the weights  for computations involving multiple time steps.  reordering weights reordering weights so they occupy memory in the same order as computation helps  decrease the memory access time     . reordering the  parameters in memory is carried out in a way that  ensures memory accesses will be sequential.  compute load overlap in order to compute matrix to  vector multiplications  weights need to be accessed and  loaded from memory and then used for computations.  the total time is the sum of the access time and computation time. to decrease this time  memory access  and computations can be overlapped. this overlap can  be achieved by fetching the weights for the next timestep while performing the computation of the current  time step. the overlap would require the existence of  extra buffers for storing the weights of the next time step  while using the weights of the current time step     .  doubling memory fetching in this method  twice  the number of required weights for computation are  fetched      . half of the weights are consumed in  the current time step t computations and the rest are  buffered for the following time step t    . doubling  memory fetching can reduce memory bandwidth by  half.    domain wall memory  dwm  dwm is a new technology for non volatile memories proposed by parkin et al.  from ibm in           . dwm technology is based on a  magnetic spin            . information is stored by setting  the spin orientation of magnetic domains in a nanoscopic  permalloy wire. multiple magnetic domains can occupy one  wire which is called race tracking. race tracking allows the        representation of up to    bits. dwm density is hoped to  improve sram by   x and dram by   x      . using  dwm in rnn accelerator can achieve better performance  and lower energy consumption      .  processing in memory  pim  pim gets rid of the data  fetching problem by allowing computation to take place  in memory  eliminating memory access overhead. in such  an architecture  a memory bank is divided into three subarray segments  memory sub arrays  buffer sub arrays  and  processing sub arrays  which are used as conventional memory  data buffer  and processing sub arrays respectively.  reram based pim arrays is one approach used to accelerate cnns             and rnns      . reram that  supports xnor and bit counting operations will only be  sufficient for rnn implementation if binary or multi bit  code  section iv a   quantization has been applied     .  memristors crossbar arrays have successfully been used as  an analog dot product engine to accelerate both cnns        and rnns      .  v. rnn implementations on hardware    in the previous section  we discussed the optimizations applied to decrease the computation and memory requirements  of rnn models. in this section  we study recent implementations of rnn applications on embedded platforms. the  implementations are divided into fpga  asic  and other implementations. we analyze these implementations and study  the effects of the applied optimizations. however  the effect  of each optimization is not shown separately. instead  the  outcomes of applying the mix of optimizations are discussed  with respect to the objectives presented in section ii. first   with regard to efficiency  the implementations are compared  in terms of throughput  energy consumption  and meeting  real time requirements. then  for flexibility  we discuss implementations that support variations in the models  online  training  or different application domains.  table   shows the details of the implementations studied  here. authors names are shown  along with the name of  the architecture  if named  the affiliation  and the year of  publication. table   and table   present the implementations  under study. table   shows implementations performed on  fpgas  while table   shows implementations performed  on other platforms. each implementation has an index. the  index starts with  f  for fpga implementations   a  for  asic implementations  and  c  for other implementations.  for each implementation  the tables show the platform  the  rnn model  the applied optimizations  and the runtime  performance.  in most cases  only the recurrent layers of the rnn model  are shown  as most of the papers provided the implementation  for these layers only. the recurrent layers are written in the  format x y z  where x is the number of recurrent layers  y  is the type of recurrent layers  e.g. lstm  gru  ..   and z  is the number of hidden cells in each layer. if the model has  different modules  e.g. two different lstm models or lstm    cnn   we give the number of executed time steps of the  volume            rezk et al.  recurrent neural networks  an embedded computing perspective    table    detailed information about papers under study  index  f          authors  li et al.    name  e rnn    year               affiliation  syracuse university  northeastern university   florida international university   mellon university   carnegie university of southern california   suny university  peking university  syracuse university   city university of new york  university of kaiserslautern   xilinix research lab  stanford university  deephi tech   tsinghua university  nvidia  university of zurich   eth zurich  university of kaiserslautern   german research center for artificial intelligence  university of illinois  inspirit iot inc   tsinghua university  beihang university  seoul national university  shanghai jiao tong university   chinese academy of sciences   university of cambridge  imperial college  peking university  university of california  pku ucla joint research institute in science and engineering  imperial college london  purdue university  purdue university  hewlett packard enterprise   university of illinois at urbana champaign  nanjing university  louisiana state university  georgia institute of technology  atlanta  fudan university  zhejiang university   university of washington  pohang university of science and technology  ku leuven  arizona state university  goergia institute of technology  goergia institute of technology  arm inc.  university of california  san diego   seoul national university    f          wang et al.    c lstm    f          rybalkin et al.    finn l    f          han et al.    ese    f        f          gao et al.  rybalkin et al.    deltarnn       f          zhang et al.    f         f          lee et al.  sun et al.         f           guan et al.         f         f         a           rizakis et al.  chang et al.  ankit et al.    deeprnn  puma    a        a        a         a          wang et al.  zhao et al.  long et al.  chen et al.    ocean    a        a         a        a        a           park et al.  giraldo et al.  yin et al.  kwon et al.  sharma et al.    laika  maeri  bit fusion    c        c        c           sung et al.  cao et al.    mobirnn    stony brook university            rnn model. both algorithmic and platform optimizations  are shown in the tables. all the optimizations found in the  tables are explained above in section iv using the same  keywords as in the tables. for quantized models   quantization x  is written in the optimizations column  where x is  the number of bits used to store the weights. the effective  throughput and the energy efficiency given in the tables are  discussed in detail in the sub section below.  a. implementation efficiency    to study the efficiency of the implementations understudy   we focus on three aspects  throughput  energy consumption   and meeting real time requirements.     effective throughput    to compare the throughput of different implementations   we use the number of operations per second  op s  as a  measure. some of the papers surveyed did not directly state  the throughput. for these papers  we have tried to deduce the  throughput from other information given. one other aspect to  volume                                                                                                                                                     consider is that compression optimization results in decreasing the number of operations in the model before running it.  consequently  the number of operations per second is not a  fair indicator of the implementation efficiency. in this case   the throughput is calculated using the number of operations  in the dense rnn model  not the compressed model. we  call this an effective throughput. below  we list the methods  used to deduce the throughput values for the different papers.    case q   effective throughput is given in the paper.    case q   number of operations in the dense model and  computation time are given. by dividing number of  operations nop by time  we get the effective throughput  qef f as shown in eq.   . in some papers  the number  of operations and the computation time timecomp were  given for multiple time steps  multiple inputs   which  would require running the lstm nsteps times.  qef f         nop   nsteps  tcomp            case q   the implemented rnn model information is         rezk et al.  recurrent neural networks  an embedded computing perspective    provided in the paper. thus  we calculate the number of  operations from the model information and then divide  it by computation time to get the throughput as in case  q . to compute the number of operations  the number  of operations in the matrix to vector multiplications is  counted as they have the dominant effect on the performance. if the paper does not give enough information to  calculate the number of operations  the number of operations can be approximately calculated by multiplying  the number of parameters by two.    case q   the energy efficiency is given in terms of  op s watt and the power consumption is given in watt.  by multiplying the two values  throughput is calculated.    case q   effective throughput could not be computed.  for a fair comparison between the asic implementations   we have applied scaling to    nm technology at  .  v using  the general scaling equations in rabaey       and scaling  estimate equations for    nm and smaller technologies      .  if the voltage value is not mentioned in the paper  we assume  the standard voltage for the implementation technology. for  example  since a  was implemented on    nm  we assume  the voltage value to be  .  v.  to analyze table   and table   and understand the effect  of different optimizations on throughput  the tables entries  are ordered in descending order  starting with the highest  throughput implementation. there exist two optimization  groups that appear more frequently among the high throughput implementations. the first group is related to decreasing  memory access time. memory access time is decreased either  by using on chip memory for all weights or overlapping the  computation time and the weights loading time.  the second group is related to algorithmic optimizations.  algorithmic optimizations present in all high throughput  implementations are compression  pruning  block circulant  matrices  etc.   deltarnn  and low precision quantization.  non linear function approximations and    bit quantization  are not within the groups of high effect optimizations. quantization with    bit is present in many implementations that  do not aim for lower precision  and it does not have a great  effect on computation cost. thus  it is not a differentiating  factor. non linear function approximations do not contribute  to the most used operations  matrix to vector multiplications .  finally  the throughput values are plotted against the implementations in figure  . the scaled effective throughput  values for the asic implementations are used. implementations that have memory access optimizations and or algorithmic optimizations are highlighted by putting them inside  a square and or circle. it can be observed from figure    that all of the implementations with high throughput have  some algorithmic optimization and applied memory access  optimization. for example  f       applied low precision  quantization and placed all the weights on the on chip memory. f        f        f        and a        all applied both  on chip memory optimization and algorithmic optimizations.  in f        the architecture had a scheduler that overlapped  computation with memory accesses. all the weights required        for computation are fetched before the computation starts.  thus  they managed to eliminate the off chip memory access  overhead by having an efficient compute load overlap.  both f  and f  applied block circulant matrices optimization. in addition  a  applied circulant matrices optimization.  this indicates that restructuring weight matrices into circulant matrices and sub matrices is one of the most fruitful  optimizations. the reason could be that circulant matrices  optimization does not cause the irregularity of weight matrices seen in pruning     . additionally  circulant blockcirculant matrices can be accompanied by low precision  quantization without a harsh effect on accuracy as in a    bit  and f      bit . it is observed in table   that f  and  f  optimizations are almost identical  but their performance  is different. f  and f  have differences in the hardware  architecture and f  applied lower precision than f   but the  most important reason is that f  used a better approach in  training the compressed rnn model. f  was able to reach  the same accuracy level reached by f  with block size    while using block size   . thus  the rnn model size in f  is  approximately  x less than f .  nevertheless  it is noticed that the only computeoptimization in f  and f  is pipelining. in these two implementations  pipelining served in two roles. the first is coarsegrained pipelining between lstm stages  and the second   fine grained pipelining within each stage. it worth knowing  that f  is based on the same architecture as f . f  achieved  higher throughput than f  by applying higher frequency and  using lower precision. assuming linear frequency scaling   the ratio between the two implementations  throughput is  close to the ratio between the precisions used for storing the  weights by the two implementations.  the lack of algorithmic optimizations in a        was  compensated by the use of analog crossbar based matrix to  vector multiplication units. analog crossbar units allowed  low latency matrix to vector multiplications. implementations that used analog computing are marked with an  a   sign in figures   and   . comparing a  to a   both were  using analog crossbars. a  surpassed a  by applying pim   processing in memory   which removes memory access  overhead. therefore  in figures   and     we consider pim  as a memory access optimization.  one implementation that stands out is a         which  has a very low throughput for an asic implementation while  applying on chip and algorithmic optimizations. this particular implementation was meant to meet a latency deadline of    ms while consuming low power   at the micro watt level.  thus  high throughput was not the objective from the beginning. implementations that defined real time requirements  are marked by an  rt  sign in figures   and   . another  implementation that rewards close inspection is f . despite  applying the two mentioned optimizations  it could not reach  as high performance as expected. the conclusion here is that  applying memory access optimization and algorithmic optimization is necessary but not sufficient for high performance.  volume            platform  optimizations  on chip pipelining  inner loop unrolling  unrolling timesteps  tiling  on chip  pipelining    qef f    gop s    q            eef f    gop s watt    e           q              e      .     on chip  pipelining  on chip  pipelining      q        .       e            q        .       e            q             e            q      .         e     .       q      .         e           q             e     .       q      .       e           q     .       e         pruning    on chip  pipelining  inner loop unrolling  compute load overlap  pipelining  load balancing  inner loop unrolling  inner loop unrolling  reordering weights  on chip  inner loop unrolling  tiling inner loop unrolling  pipelining  tiling  inner loop unrolling  pipelining  reordering weights  compute load overlap  tiling  inner loop unrolling      q     .        e         quantization     piecewise approx.    hybrid memory  compute load overlap      q     .       e     .      index    platform    model    f          zync xczu ev      mhz      bilstm        f          alpha data adm  v       mhz      lstm         f          zync           mhz      gru        f          alpha data adm  v       mhz      lstm         f            bilstm        f          zynq       xc z          mhz  xcku         mhz    block circulant     piecewise approx.  quantization     deltarnn  ralut  quantization     block circulant    piecewise approx.  quantization     quantization        lstm         pruning  quantization       f          virtex   vc         mhz    quantization       f          f          xc z          mhz  vc         mhz    alexnet      steps   lstm         steps   lstm          steps   lstm        lstm      fc    f            vc         mhz      lstm        piecewise approx.    f          f            zynq zc         mhz        lstm        zynq      xc z         mhz      lstm                         algorithmic  optimizations  quantization      quantization    hard sigmoid    the cases q  q  are explained in section v a .  the cases e  e  are explained in section v a .  the effective throughput in the paper was computed on a bit basis. for a fair comparison  we recalculated the number of operations on an operand basis.  the throughput is for running cnn and lstm combined together.  the number of time steps the model should run per second to reach real time behavior is given. we computed the number of operations in the model and multiplied by the number  of time steps in one second  then multiplied by the speedup gained over the real time threshold to get the implementation throughput.    rezk et al.  recurrent neural networks  an embedded computing perspective    volume           table    comparison of rnns implementations on fpgas.                 table    comparison of rnns implementations on asic and other platforms.  category      e                 q       .       e                   q             e       .       q     .           e       .           q           e              q      .       e         unrolling timesteps      q     .       e         renderscript       q     .          e         a           cmos   nm   ghz    lstm      quantization       a          tsmc   nm      mhz   v      lstm        a          cmos    nm      lstm       quantization    circulant matrices  piecewise approx.  quantization      a          cmos   nm       mhz  cmos   nm      mhz  cmos   nm       khz  cmos   nm    gru       lstm        quantization    piecewise approx.  quantization       armv      ghz  intel core i      . ghz  adreno     gpu        mhz      sru         sru    memristor pim  analog computing  pipelining  on chip  tiling  inner loop unrolling  analog computing  on chip  on chip  hardware sharing  pipelining  load balancing  inner loop unrolling  on chip  doubling memory fetching  reram pim  analog computing  unrolling timesteps      sru         sru       c        c        c              lstm        lstm         lstm       quantization     piecewise approx.  pruning    the cases q  q  are explained in section v a .  the cases e  e  are explained in section v a .  scaled to    nm at  .  volt using general scaling       and scaling estimates for    nm and smaller technologies      .  the throughput is not high as the purpose was to reach very low power consumption while performing inference within   ms.  renderscript is a mobile specific parallelization framework      .  quantization used   bit for weights and   bits for activations.  a  proposed a gru core without providing specific model details.  a  did not specify which model achieved the provided throughput and energy efficiency.    volume           rezk et al.  recurrent neural networks  an embedded computing perspective      q      .          platform  optimizations    others              e                 algorithmic  optimizations    a          a                       q                 model    a                  eef f   gop s watt   original scaled      e              platform    asic         qef f  gop s   original scaled      q                  index     rezk et al.  recurrent neural networks  an embedded computing perspective    in addition  figure   shows that most of the asic implementations were not exceeding fpga implementations in  terms of throughput. we think the reason is that the asic  implementations under study did not use the latest asic  technologies  as shown in table  .  for the low throughput implementations  figure   shows  that some implementations did not apply either of the two  optimizations  memory access and algorithmic   such as  f       that had a strict accuracy constraint bounding the use  of algorithmic optimizations and c       . in addition  some  implementations applied only one of the two optimizations   including f        and f       .     energy efficiency    to compare the implementations from the energy consumption perspective  we use the number of operations per second  per watt as a measure. the last columns in table   and  table   show the energy efficiency. energy efficiency is  calculated based on the dense model  not the sparse model   as for effective throughput. however  it was not possible to  obtain values for energy efficiency for all implementations.  in some cases  the power consumption was not mentioned  in the paper  while in others  the consumed power was not  provided in a precise manner. for instance  the power of the  whole fpga board may be provided  which does not indicate  how much power is used by the implementation with respect  to the peripherals           .  here  we list the cases used for computing the energy  efficiency in table   and table  . the case number appears  in triangular brackets      before the numeric value          case e   the eef f energy efficiency is given in the  paper.  case e   the power consumption is given in the paper.  to compute the energy efficiency eef f   the effective  throughput qef f  op s  is divided by the power p   watt  as  eef f                qef f  .  p    case e   energy and computation time are provided.  first  we divide energy by time to get power. then  we  divide effective throughput qef f by the power to get  energy efficiency  as in case e .  case e   energy efficiency could not be computed.    figure    is a plot of the energy efficiency found or  deduced for the implementations under study against the  implementation index. implementations are sorted in the plot  according to energy efficiency and the scaled values for the  asic implementations are used. again  to show the effect of  optimizations  we chose the two most effective optimizations  from table   and table   to include in the figure. they are  the same as in figure    memory access optimization and algorithmic optimizations. comparing the effective throughput  and energy efficiency of fpga and asic implementations   it is observed that fpga and asic have close values for  volume           effective throughput while asic implementations are more  energy efficient. the credit should go to asic technology.  it can be observed that the highest energy efficiency was  achieved by a        and a      . both implementations  used analog crossbar based matrix to vector multiplications.  a  managed to save memory access time by computing in  memory. the quantization method used was multi bit code  quantization    bit for weights and   bit for activations .  multi bit code quantization enables replacing the mac operation with xnor and bit counting operations  as discussed  in section iv a . it was sufficient to use an xnor rram  based architecture to implement the rnn.  both a   applying pim and analog computing  and a    applying memory and algorithmic optimizations  were less  energy efficient than expected. they were both less energy  efficient than a   applying only memory optimization . a   had a quite high clock frequency of   ghz. this high frequency helped the implementation to achieve high throughput. however  we suspect that this high frequency is the main  reason for the energy efficiency degradation compared to the  other implementations. a  had the least power consumption  among all implementations       w  . the low throughput  of a  affected the overall energy efficiency value.     meeting real time requirements    in some of the implementations  real time requirements for  throughput and power have been determined. for example   in f         the speech recognition system had two rnn  models. one model for acoustic modeling and the other for  character level language modeling. the real time requirement was to run the first model     times per second and the  second model       times per second. while in a          an lstm accelerator for an always on keyword spotting  system  kws   the real time response demanded that a new  input vector should be consumed every    ms and the power  consumption should not exceed     w .  b. flexibility    flexibility  as defined in section ii is the ability of the  solution to support different models and configurations. the  flexibility of the solution can be met by supporting variations  in the model. models can vary in the number of layers  the  number of hidden units per layer  optimizations applied on  the model  and more. flexibility can be met by supporting  online training or meeting different application domain requirements.  flexibility is not quantitative  like throughput. thus  we  use a subjective measure for flexibility to reach a flexibility  score for each implementation. table    shows the flexibility  aspects supported by each implementation  as discussed in  the papers and the flexibility score for each implementation.  papers that do not discuss any flexibility aspects are omitted  from table   . in a        the architecture should be able to  support various models  but the number of cells and layers  the architecture can support are not mentioned in the paper.  hence  we cannot deduce how the implementation could         rezk et al.  recurrent neural networks  an embedded computing perspective    figure    effective throughput of different implementations and the key optimizations affecting them.    figure     energy efficiency of different implementations and the key optimizations used.    support variations in the rnn model. also  the variations  should be supported on the hardware platform and not only  by the method before fabrication. in a        the design  method can support two different rnn layers. however  the  fabricated chip supports only one of them. thus  we do not  consider a       to meet the flexibility objective.  to understand how far flexibility is met by the implementations  figure    shows the percentage of implementations  supporting each flexibility aspect. flexibility is visualized  as levels. level   is used to indicate no flexibility. level    requires the implementation to support only one recurrent  layer configuration. all papers meet level   requirement but  thereafter they vary in meeting other flexibility aspects. the  flexibility aspects and how they can be met are discussed  below.  supporting variations in rnn layers  level    recurrent  layers can vary in the type of layers  the number of cells  in each layer  and the number of layers  the depth of the  rnn model . one optimization that might have a side effect  on the flexibility of the solution is the choice of using onchip or off chip memory to store the weights. being able to        store all the weights in on chip memory is very beneficial.  it leads to better performance and less energy consumption  by decreasing the cost of memory accesses. however  this  solution may be unfeasible for larger problems. for example   in f         the number of weights in the model and their  precision are restricted by the on chip memory size. it is not  possible to run a model with an increased number of hidden  cells or increased precision. a possible solution is to use  an adaptable approach  where the location chosen to store  the weights is dependent on the model size and thus can a  wide range of models can be supported. another solution was  adopted in f         where some of the weights are stored in  internal memory  and the rest are stored in off chip memory   hybrid memory .  supporting other nn layers  level    supporting other  nn layers allows the solution to run a broader range of nn  applications. also  other nn layers may exist in the rnn  model  such as convolutions used as a feature extractor. supporting such a convolution in the implementation increases  the flexibility of the solution  as it can run rnn models with  visual inputs and run cnn independent applications.  volume            rezk et al.  recurrent neural networks  an embedded computing perspective    figure     percentage of implementations meeting flexibility aspects for different flexibility levels and the definition of  flexibility levels.    table     flexibility score of implementations under study.  index  f          f          f          f        f        f           f          a        a        a           a           a          a           a           c        c            volume           flexibility aspects in papers  varying layer  lstm gru   varying number of cells  varying block size  block circulant matrices   varying layer  lstm bilstm   varying number of layers  varying number of cells  varying layer  lstm bilstm   varying precision  fc supported  varying layer  lstm bilstm   fc supported  convolution supported  fc supported  varying number of layers  varying number of cells  input layer  varying number of layers  varying number of cells  online training  varying number of cells  fc supported  varying number of layers  varying number of cells  linear nonlinear quantization  fc supported  varying type of layer lstm gru   convolution supported  fc supported  varying number of cells  varying number of layers  dense sparse  convolution supported  varying number of cells  varying number of layers  convolution supported  varying precision  varying number of cells  varying number of layers  varying type of layers  convolution supported  fc supported  varying layer  lstm sru qrnn   varying number of cells  varying number of layers  varying number of cells    score  xxx  xxx  xxx  xx  xx  xxx  xx  x  xx  xxxx    xxx  xxxx    xxxx    xxxxx    xx  xx    supporting algorithmic optimization variations  level     variations in the optimizations applied are also considered  as variations in the model. for example  variation due to  applying or not applying pruning is related to the presence  of sparse or dense matrices in the matrix to vector multiplications computations. the design in a       employed  a configurable interconnection network topology to increase  the flexibility of the accelerator. the accelerator in a        supported both lstm and cnn layers. the accelerators supported both sparse and dense matrices. one other variation  in the precision of the weights and activations. the design  in a        supported varying precision models by allowing  dynamic precision per layer for both cnn and rnn models.  similarly  the microsoft npu brainwave architecture        supported varying precision using a narrow precision block  floating point format      . to maximize the benefit of  varying precision  f       applied a parameterizable parallelization scheme. when lower precision is required  lstm  units are duplicated to exploit the unused resources to gain  speedup. and  when higher precision is used simd folding  is applied to save resources for the needed high precision.  online training  level    incremented online training  was included in a       to support retraining pre trained  networks to enhance accuracy. changes in hardware design  were applied to support both training and inference without  affecting the quality of inference. for example  three modes  of data transfer were applied. the first to load new weights   the second to load input sequences  and the third to update  certain weights. extra precision was only used for training.  meeting different applications domains constraints   level    none of the implementations target variations  in the application domain constraints. netadapt is a good  example of an implementation that can adapt to different  metric budgets      . however  it only targets cnns.           rezk et al.  recurrent neural networks  an embedded computing perspective    vi. discussions and opportunities    in the previous section  we studied the implementations of  rnn on embedded platforms. in section ii  we defined  objectives for realizing rnn models on embedded platforms.  in this section  we investigate how these objectives are being  met by the implementations.  throughput it is clear that throughput was the main objective for most of the implementations. as seen in figure     high throughput was achieved by many of them. algorithmic  and memory optimizations are present in most of these high  throughput implementations. the algorithmic optimizations  applied were effective because they decrease both the computation and the memory requirements of the rnn models.  for example  if   bit precision is used instead of    bit  for weights storage  the memory requirement is decreased  to    . multiple   bit weights can be concatenated during  weights fetching. thus  the number of memory accesses can  decrease as well. furthermore  the hardware required for  bit operations is simpler than the hardware required for    bit  floating point operations.  memory specific optimizations are effective because they  decrease or hide the overhead of accessing the large number  of weights used in rnn computations. memory access time  can be decreased by storing all weights in on chip memory.  however  this can bound the validity of the solution for  larger models as on chip memory may not be sufficient to  store the weights. overlapping the memory access time with  computation and computation in memory are also considered  to be memory optimizations.  energy efficiency applying algorithmic and memory access optimizations has a positive effect on energy efficiency  as well. algorithmic optimizations lead to a decrease in the  number of computations  the complexity of computations   and the number of memory accesses  and so decrease the  energy consumed by the implementation. also  minimizing  off chip memory use by storing weights on on chip memory  is an effective way to enhance energy efficiency. analog computing and processing in memory implementations showed  superior energy efficiency in asic implementations.  meeting real time requirements was not an objective for  many of the implementations. in a few of them  real time  deadlines were mentioned and followed in the design of the  solution.  flexibilty in section ii a  flexibility is defined as a secondary objective. thus  we do not expect flexibility to be  fully met by the implementations. variations in the rnn  model was partially fulfilled by many implementations. however  the number of variations covered by each implementation is quite low. few implementations included other nn  layers and variations in algorithmic optimizations. onlinetraining was targeted by only one implementation. embedded implementations do not usually support online training.  however  on the algorithmic side  researchers are carrying  out interesting work based on online or continuous training           . none of the rnn implementations support  different applications  but this has been done by the cnn        solution in      . following a similar method in rnns  and  in addition  also supporting model variations  could lead to  interesting solutions.  opportunities for future research  based on the survey reported on in this article  we list some  opportunities for future research.  qrnn and sru  qrnn and sru  section iii b   are  two alternatives to lstm where the matrix to vector computations for the current time step are independent of previous  time step computations. thus  using them in rnn models  can make the parallelization more efficient and consequently  lead to better performance.  deltarnn      and deltacnn       we believe that  applying the delta algorithm to both recurrent and convolution layers is a logical step because of the temporal relation  between the input sequences. adding a delta step to other  algorithmic optimizations such as pruning and quantization  would decrease memory access and computation requirements.  block circulant matrices using block circulant matrices  as an algorithmic optimization decreases the rnn size while  avoiding irregularity of computation as introduced by pruning     . applying circulant matrices can be accompanied by  low precision parameters and activations  with a small effect  on accuracy     . with the addition of the delta algorithm  as  mentioned earlier  rnn inference can achieve a promising  throughput and energy efficiency.  hybrid optimizations  it has been shown that a mix of  algorithmic optimizations can be applied to an rnn model  with a loss in accuracy that is acceptable     . applying a  mix of optimizations would enable the implementations to  benefit from each optimization. for an rnn implementation   three classes of optimizations can be mixed with tuning.  the first optimization is the delta algorithm  and the corresponding parameter is delta. the second is quantization  and the corresponding parameters are the number of bits and  the quantization method. the third optimization is compression. if the applied compression technique is block circulant  matrices  the parameter is the block size. tuning the three  parameters delta  number of bits  quantization method  and  block size  the designer can achieve the highest performance  while keeping the accuracy within an acceptable range  the  range is dependent on the application .  analog computing and processing in memory  analog  computing      and processing in memory             have  shown promising performance  especially in energy efficiency. analog crossbar based matrix to vector multiplication  units can provide low latency and computing in memory  overcomes the memory access problems.  flexible neural networks and domain specific architectures domain specific architectures  dsas  have been  highlighted as a future opportunity in the field of computer  architecture      . dsas  also called accelerators or custom  hardware  for neural networks applications can reach high  performance and energy efficiency. designing an architecture for neural networks applications as a specific domain  volume            rezk et al.  recurrent neural networks  an embedded computing perspective    with known memory access patterns enhances parallelism  and the use of the memory hierarchy. it is possible to use  lower precision and benefit from domain specific languages   dsls . google edge tpu is an example of a dsa for neural  networks inference using   bit precision      . based on the  study in this article  we add that the neural networks dsa  needs to support flexibility. for the flexibility aspects defined  earlier in section ii to be fulfilled  there are some features  need to be supported in the underlying hardware.    variable bit width operations as in a        to support  different quantization schemes.    some optimizations require pre post processing on input vectors and weights. support for weights reordering   delta vectors computation  retaining circulant matrices  from equivalent vectors  and other operations required  by miscellaneous optimizations would be useful.    support for training that would imply the support of  back propagation and the allowance of weights modification.  vii. summary    today we see a trend towards more intelligent mobile devices  that are processing applications with streamed data in the  form of text  voice  and video. to process these applications   rnns are important because of their efficiency in processing  sequential data. in this article  we have studied the state ofthe art in rnn implementations from the embedded systems  perspective. the article includes all the aspects required for  the efficient implementation of an rnn model on embedded platforms. we study the different components of rnn  models  with an emphasis on implementation rather than on  algorithms. also  we define the objectives that are required  to be met by the hardware solutions for rnn applications   and the challenges that make them difficult to implement. for  an rnn model to run efficiently on an embedded platform   some optimizations need to be applied. thus  we study both  algorithmic and platform specific optimizations. then  we  analyze existing implementations of rnn models on embedded systems. finally  we discuss how the objectives defined  earlier in the article have been met and highlight possible  directions for research in this field in the future.  we conclude from the analysis of the implementations  that there are two optimizations that are used for most of  the efficient implementations. the first is algorithmic optimizations. the second is to decrease the memory access time  for weights retrieval  which can be implemented by relying  on on chip memory for storing the weights  applying an  efficient overlap between weights loading and computations   or by computing in memory. however  using analog crossbar based multipliers can achieve high performance without  relying too much on algorithmic optimizations. a study of  the implementations in the literature shows performance high  enough for many streaming applications while also showing  a lack of flexibility. finally  we deduce some opportunities  for research to fill the gap between the defined objectives and  the research work under study. we highlight some hardware  volume           efficient recurrent layers and algorithmic optimizations that  can enhance implementations  efficiency. additionally  we  describe how custom embedded hardware implementations  can support flexible rnns solutions.  