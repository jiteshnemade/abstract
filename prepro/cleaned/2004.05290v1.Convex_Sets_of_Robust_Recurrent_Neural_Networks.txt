introduction  neural networks are a class of universal function approximator frequently used in machine learning. while there  have been some great successes in solving complex tasks   their lack of robustness often prevents their application   particularly in a safety critical context. this realization has  motivated a large amount of research into the area of  adversarial defenses               . this area seeks guarantees  of robustness against adversarially chosen inputs.  recurrent neural networks are state space models  parametrized by neural networks and are frequently used  to model sequences to sequence mappings. many of the  concerns for ensuring robustness of learned models relate  directly to traditional concerns in robust control  e.g. stability  and continuity of the input output mapping    .  there are many definitions of stability for nonlinear systems  e.g.  rnns . approaches that study the stability of  certain equilibria are difficult to apply when synthesizing  new models as the locations of equilibria are unknown for  unknown inputs. two alternatives that do not depend on prior  knowledge of inputs and trajectories are incremental stability  and contraction analysis    . these approaches define stability in terms of the distance between all trajectories  regardless  of the input.  beyond stability  a robust model must also not be critically  sensitive to small changes in the input. the most commonly  used method for analyzing the input output behavior of a  system is the    gain bound. as noted in      however  a finite     gain bound only guarantees only boundedness  whereas a  this work was supported by the australian research council.  the authors are with the australian centre for field robotics   the university of sydney  sydney  nsw       australia  e mail   ian.manchester sydney.edu.au .    finite incremental    gain bound guarantees continuity. further insight can be gained by noting that the incremental     bound is equivalent to the lipschitz constant of the sequenceto sequence mapping. the lipschitz constant of a mapping  is a quantity that commonly appears in for instance proofs  of generalization bounds      analysis of expressiveness      and guarantees of robustness to adversarial attacks          .  there have been a number of approaches to analyzing  and training stable recurrent neural networks. many of the  approaches to analysis such as                               ensure stability through lmi conditions. the lmis  however   are not jointly convex in both model parameters and the  stability certificate complicating their use during training.  the approaches in      and      both try to learn lyapunov  functions and enforce the lyapunov inequality via projection  and penalization. these approaches  however  construct a  lyapunov function about a particular equilibrium and as  such cannot be easily extended to the deal with unknown   exogenous inputs. on the other hand      and      provide  conditions for contraction  easily dealing with inputs. however  these models suffer from reduced model expressiveness  due to the conservatism of the stability criteria. this conservatism is caused by two issues. firstly  the contraction  metrics are limited to a diagonal structure. secondly  they do  not account for the interactions between the linear component  and the nonlinearity and instead require both the system and  nonlinearity to be contracting in a common metric.  one approach to reducing the conservatism of stability  criteria is provided by the integral quadratic constraint  iqc   framework. this approach deals with the case of when a  known  linear system is in feedback with an uncertain  nonlinear or otherwise troublesome component. by abstracting  away the troublesome component in favour of quadratic  descriptions of the signals it can produce  criteria such as  stability     gain bounds and passivity can be accurately  verified. in the case where there are multiple  troublesome   components that share a common description  very accurate  results can be obtained using the approach presented in       and     . more recently  extensions of the iqc framework  were proposed in      lifting the iqc approach to the tangent  bundle and allowing for the nominal system to be nonlinear.  in this case  the stability criteria take the form of point wise   convex lmis. in the context of neural network analysis   the iqc approach has been recently applied to the  nonrecurrent  neural networks to develop the tightest bounds on  the lipschitz constant known to date      and robustness  guarantees for sets of input perturbations     .  in this work  we develop a convex set of robust rnns.  by treating the rnn as a linear system in feedback with     its activation functions  we can apply methods from robust  control to develop contraction conditions that are less conservative than prior methods. the proposed model set is the  most expressive set of contracting rnns so far and contains  all previously proposed sets. the use of an implicit model  structure leads to conditions that are jointly convex in both  the model parameters and certificate of stability  building  on the approach for polynomial models in                        . this greatly simplifies the problem of training new  models subject to stability constraints as they can be treated  using penalty  barrier or projected gradient methods. we  provide a discussion of future extensions that may allow for  improved expressibility and non constant contraction metrics  and discuss a number of applications of such a model set.  finally  we illustrate the efficacy of the proposed model set  on a system identification task.  notation. we use n  r to denote the set of natural and  real numbers  respectively. the set of all one side sequences  x   n   rn is denoted by  n e . superscript n is omitted when  it is clear from the context. we use xt to represent the value  of the sequence x at time t   n. the notation         rn   r  denotes the standard   norm. the subset        e consists  of all square summable  i.e.  x      if and only if  ppsequences        is finite. given a sequence   x     the    norm kxk     t  t    x     e   the    norm ofqits truncation over     t   with t   n  pt     is written as kxkt     t    xt   . given a piecewise difn  ferentiable vector function f   r   rm   we use d  f  x  v   to denote the one side directional derivative of f     at x in  the direction v  i.e. d  f  x  v     lim   f  x sv  f  x   s.  s      if f is differentiable  then d  f  x  v     f   x v. for matrices  a  we use a     and a     to mean a is positive definite  or positive semi definite respectively and a   b and a      to mean a   b     and a   b     respectively. the set of  diagonal  positive definite matrices is denoted d  .    same input sequence u  the corresponding output trajectories  y a and y b satisfy y a   y b    p  .  this definition implies that initial conditions are forgotten   however  the outputs can still be sensitive to small perturbations in the input. in such cases  it is natural to measure  system robustness in terms of the incremental     gain.  definition  . the system          is said to have an incremental     gain bound of   if for all pairs of solutions with  initial conditions a  b   rn and input sequence ua   ub    m   e    the output sequences y a   y b    p e satisfies  ya   yb       t          ua   ub       t      d a  b      note that the above definition implies the incrementally     stability since ky a   y b k t   d a  b  for all t   n when  ua   ub . it also shows that all operators defined by     and      are lipschitz continuous with lipschitz constant    i.e.  for any a   rn and all t   n  ksa  u    sa  v kt    ku   vkt       u  v    m   e .    problem  . construct a convex rnn model set    such  that for any          the system          has a finite the  incremental     gain.  problem  . construct a convex rnn model set    such  that for any          the system          has an incremental      gain bound of  .  b. contraction analysis  contraction analysis     studies the incremental stability  properties of system          based on its associated differential dynamics  a.k.a. variational  linearized  prolonged     xt     d  f   xt   ut    xt    ut     yt   d g   xt   ut    xt    ut      a. problem setup  consider an input output sequence mapping  operator   p  sa    m   e      e whose model can be described by a  dynamical system with finite dimensional state xt   rn and  initial state x    a  driven by a known input ut   rm and  producing an output yt   rp . we are interested in learning  rnn models parametrized by         rn    xt     f   xk   uk             yk   g   xk   uk             where functions f    g  will be defined later.  it is often desired to learn rnns which have predictable  responses to a wide variety of inputs. one direct approach  is to search for rnns from a model set with stability and  robustness guarantees. first  we introduce some stability  definitions.  definition  . the system          is termed incrementally     stable if for any two initial conditions a and b  given the           in this work we will address the following problems.         ii. p roblem f ormulation and p reliminaries     t   n.                    where the sequence   x    u    y   can be interpreted as the  infinitesimal displacements between two neighboring trajectories of the original system         .  a system          is said to have differential     gain bound  of   if for all t   n  k y k t       k u k t   b x     x              where b x   x       with b x        . for smooth systems  the differential     gain bound is equivalent to the incremental      gain bound     .  a sufficient  and in some cases necessary  condition for      is the existence of a storage function v  x   x       with  v  x         such that the following dissipation inequality  holds for all t   n     v  xt      xt       v  xt    xt        ut        yt    .          in general there are many forms of differential storage functions. one common choice is the quadratic form v   x       x  m  x where m    .          v    w    g    y    u    fig.    feedback interconnection for rnns.    c. bounds and identities for quadratic matrix functions  throughout the paper we will frequently use the following simple property. the matrix functions g e  p       e   p    e with e   rn n   p     obey the upper bound         e p               e  p  e  e           where the right hand side is convex in e  p . inequality      follows directly from the expansion  p   e     e   e   p    e    e   p    p     e   p      .  from this expansion it is also clear that the bound     is tight  if e   p .  iii. robust rnn s  a. model set  our work can be applied to the models with multi layer  network. to streamline the presentation  we will focus on  the case with one layer network.  as shown in fig.    we treat the rnn as a feedback  interconnection of a linear system g and a static  memoryless  nonlinear operator   of the form  w     v .            where   x       v            vq     with vi as the ith  component of the vector v. we assume that the slope of    is restricted to the interval                  y      x         y x     x  y   r  x    y.            in the neural network literature  such functions are referred  to as  activation functions   and common choices such as   e.g. tanh  relu  sigmoid  are slope restricted     .  we will parameterize the linear system g using the  following implicit  redundant parametrization          ext     f xt   b  wt   b  ut        yt   c  xt   d   wt   d   ut        vt   c  xt   b   d   ut  where      e  f  b    b    b  c    d     d     is the model  parameter with e invertible. the choice of c  and d   is  discussed later in remark  .  note that by setting f      e    b    a  e    b    b   c       d     c  d     d  c    i and d        the above  implicit rnn            is reduced to the conventional rnn       of the form   zt       azt   but   b             yt   czt   dut            where zt     xt   b .  the implicit model            is called a robust rnn  if its differential     gain is bounded by some constant  .  to characterize the set of robust rnns  we first derive the  associated differential dynamics  which can also be expressed  as a feedback interconnection of a linear system  g and a  differential operator             e xt     f  xt   b   wt   b   ut   g    yt   c   xt   d    wt   d    ut               vt   c   xt   d    ut           wt   d    vt    vt  .            analysis of the above system is primarily complicated by  the presence of the nonlinear activation function in  . we  will simplify the analysis by replacing    with differential  integral quadratic constraints    iqcs .  b. description of   by   iqcs  a   iqc for the operator   can be viewed as an iqc         for the differential operator        .  definition  . the operator   satisfies the differential integral  quadratic constraint defined by a multiplier m   m      r q  q if for any v   v    w   rq satisfying       the  following inequality holds for all t   n       t    x   v  t    t       wt       m         vt     .   wt            for the nonlinear operator      where each component is  slope restricted to the same interval         we can obtain a  simpler quadratic constraint                 vt      t            t      vt             w t   w t         t        t       z        m        for all t   n  where t       diag      . . .     q     d  . note  that the above   iqc is a conic combination of quadratic  constraints for each activation function    i.e.                  q  x   vt i              vt i   i       wt i            wt i  i      where vt i   wt i are the ith elements of vt and wt   respectively.  c. convex parametrization of robust rnns  to construct the set of stable models with finite differential      gain  we use the following constraint      e   e    p                       f        f     p     b    b                 b   b   c  c          b   m     b        d    d               b      c      and d  b        i . we define the set  where c            of robust rnns with finite differential     gain as follows              p          d  s.t. e   e             .  to obtain a robust rnn with differential     gain bound  of    we propose to use the following constraint                      f  f  e   e    p                     b     p     b            i  b    b                               b   b   c  c  c   c              b       b             d       d       d       m      d                   b   b   d    d    d  d          b   d                    d      to learn models for a wide class of systems  it is of course  beneficial to have as expressive a model set as possible. the  main result regarding expressivity is that the robust rnn  set    contains all ci rnns and stable lti models.  to explain this result  we first introduce the concept of  input output equivalence  i.e.  two rnns with parameter     and    is said to be input output equivalent  denoted            if they admits the same input output trajectory set. a model  set    is said to be a subset of another model set     denoted            if for any           there exists         such that          . and         means    is a strict subset of    .  the set of all stable lti systems will be denoted  lti  and can be described by the state space model   xt     axt   but      yt   cxt   dut            where p     and       . note that if the lmi  condition      is satisfied  there exists a sufficiently large    such that      holds for any choice of b    c    d     d    and d   . we define the set of robust rnns with differential      gain bound of   as follows    with a necessary and sufficient condition for stability given  by the lmi   p   a  pa                           p          d  s.t. e   e             .    theorem  . the robust rnn set    contains all stable lti  models  i.e.  lti      .    remark  . note that the lmi      is not jointly convex in  the weights c    d   and the   iqc multipliers  . we will see  that fixing c  and d   and optimizing over the remaining  parameters still leads to highly expressive models. fixing  c    i and d        the model set still contains the set of  ci rnns  see theorem   below. alternatively  expressibility  can be improved at the cost of computational complexity by  fixing c  and d   as random wide layers  i.e. with large  q . this is similar to the approach taken in the echo state  network     .    proof. for any stable lti system       lti   it is easy to  verify that        where   represents an implicit model with  e    f   a  b       e    b    b  c   c and d   d. we  will show that        . let p   e   p    e we have    theorem  . suppose that          then the robust rnn             has a differential     gain bound of  .  proof. to establish the differential     gain  we  first    bound               left and right multiply      by the vectors  x t    w  ut and  t               xt    wt    ut . then  applying the bound     and taking  summation over     t   gives           t    x      vt     m     vt  vt   v     k u k t   k y k t     w t   wt     t      where vt    xt e   p    e xt . from      the differential    gain condition     follows with b x     x       v  .  theorem  . suppose that          then the robust rnn             has a finite differential     gain.  proof. since      implies      for some sufficiently large     from theorem   the robust rnn            has a finite  differential     gain bound of  .  d. expressivity of the model set  some recent works show that the model sets with additional stability constraints can improve generalizability and  trainability  e.g. contracting implicit rnns  ci rnns        and stable linear time invariant  lti  models     . to be able    for some p    .          e   p    e   f   e    e   p    ee    f       e   e     p   f   p    f            e   e     p   f   p    f  t                  t   t  where t   d  satisfies t      e  e    p  f   p    f .  a ci rnn      is an implicit model of the form   ezt       fzt   but   b      yt   czt   dut    such that the following contraction condition holds        e   et   p ft      f  p                    where p   d  . we define a convex set of ci rnns as   ci          p   d     s.t. e   e             .            note that  ci does not contain all stable lti systems. for  example  the system xt      . xt   ut   yt   xt cannot be  converted into the form      via coordinate transformation.  theorem  . the robust rnn set    contains all ci rnns   i.e.  ci      .  proof. for any ci rnn       ci   we first show that         where   represents the implicit model with f      e    p      b    p    fe      b    eb  c       d     ce       d     d  c    i and d      . by substituting   into       and       the implicit model can be rewritten as  xt     fe      xt   b    but    yt   ce      xt   b    dut .             note that the above system is equivalent to     via the  coordinate transformation zt   e      xt   b  since  ezt       xt     b      fe      xt   b    but   b       fzt   but   b .  second  we will prove that        . let p   t   p     we have        e   e     p   f   p    f       e   p    e   f   p    f       p      e    f   p    pp    fe           p      e    f   p    pp    fe      p    pp           t   b   p    b    t  e   e     p     t            e   e    p   t               .   t   t   b   p    b   and  ci is a strict subset of    as  ci does not contain all  stable lti systems.  we also note that  unlike  ci   the proposed model set does  not require a diagonal metric. this may further reduce the  conservatism of    compared to  ci .  iv. d iscussion  we have focused on the case where g is a linear system  and   is described by simple sector bound quadratic constraints  however both of these assumptions can be relaxed  with some additional technical considerations.  all of our results can be extended to the case where the  nominal system g is a smooth nonlinear state space model.  when g is no longer a linear system however  the stability  conditions take the form of pointwise lmis. this allows  us to deal with state dependent contraction metrics as e  now becomes a state dependent function and the contraction  metric takes the form e x   p    e x      . for the case  where g is a polynomial state space model  sum of squares  programming can be used to enforce the lmis.  the conservatism of the iqc framework is largely associated with the accuracy of the iqc descriptors used  for the activation function. for the non incremental case   extensive libraries of iqc descriptors have been developed   e.g. the zames falb  popov and rl rc multipliers. in the  incremental case was shown that these descriptors cannot  be easily used as they do not preserve incremental positivity      . to the authors  knowledge  it remains an open problem  to to find more expressive iqcs for activation functions that  are valid for incremental and differential analysis.    fig.    nonlinear spring profile.    in order to generate data  we will use a simulation of a  series of four coupled mass spring dampers. the goal is to  identify a mapping from the force on the initial mass to the  position of the final mass. nonlinearity is introduced through  the springs  piecewise linear force profile described by   fspring i  d    ki   d             where   d  is the piecewise linear function depicted in  figure   and ki is the spring constant for that spring.  we excite the system using a pseudo random binary sequence that changes values after a an interval distributed uniformly in         and takes values that are normally distributed  with stand deviation  u . to generate a training batch we  simulate the system for     seconds and sample the system  at  hz to generate      datapoints with an input signal  characterized by       s and  u    n . the training data  consists of     training batches. we also generate validation         s and  u    n   and a test set with       s and   u    n by simulating the system for      seconds and  sampling at   hz.  a. training procedure  fitting robust rnns requires a constrained optimization  problem to be solved subject to a number of lmi constraints.  i.e. we are interested in solving the following optimization  problem   min   y    s u                     v. n umerical e xample    where     is defined in     . in this work  we enforce the  constraint        using an interior point method where we  minimize a series of objective functions of the following  form   x  x  j   mse y   s u        log det mi       log  j .         we will compare the proposed robust rnn with the  commonly used  elman  rnn      and lstm       along  with two stable model sets  the stable rnn  srnn       and  contracting implicit rnn  ci rnn      . all models have a  state dimension of    and all models except for the lstm  use a relu activation function. the lstm uses a tanh  activation.    where mi are the lmis to be satisfied and  j are the  iqc multipliers. as        this approaches the solution  of the problem     . we minimize      using the adam  optimizer      for stochastic gradient descent with an initial  learning rate of  e    . a backtracking line search is used  to ensure strict feasibility though out the optimization. when    i    j     we observe more than    epochs without an improvement in  performance on a validation  we decrease the learning rate  by a factor of  .   and decrease the barrier parameter   by  a factor of    until a final learning rate of  e   is achieved.  we initialize all robust rnns by first solving a linear  subspace identification problem using n sid to find matrices  a b c and d and using the resulting matrices to initialize  e    f   a  e    b    b  c    c and d     d.  b. model evaluation  we will compare models on a number of different metrics  measuring nominal quality of fit and robustness. quality of  fit will be compared in terms of the normalized simulation  error     y    y          nse      y    where y   y      are the simulated and true system outputs  respectively. in order to study the the robustness of the  systems  we will study the the lipschitz constant estimated  via the following        max  u v      s u    s v          u   v      u    v.            while solving      exactly is complicated by non convexity   an approximate solution can be found using gradient ascent.  the values of    mentioned in this work are thus a lower  bound on the true lipschitz constant of the model  which  can be interpreted as a measure of worst case sensitivity to  input perturbations.  c. results  the performance of a number of the models on the various  test sets and estimates of the lipschitz constants are shown  in table i. there are a number of apparent benefits from  the proposed model set. firstly  if we compare the nominal  performance of the stable models  we can see that the robust  rnn       outperforms all other stable models. this is due to  the reduced conservatism of the proposed stability constraint.  we have also plotted the performance on the second test  set versus the observed lipschitz constant in figure  . from  this  we can see the robust rnns have the best trade off  between nominal performance and robustness signified by  the fact that they lie further in the lower left corner. for  instance if we compare the lstm with the robust rnn         we observe similar nominal performance  however the  robust rnn has a much smaller lipschitz constant.  another interesting observation is that all models with  stability constraints have significantly reduced lipschitz constant when compared to those without. this provides additional support for the observation that stability constraints  can improve robustness and generalizability of models            .  in figure   we present boxplots showing the performance  of each of the models for a number of realizations of the  input signal with varying  u . note that we only present the  results for a subset of the models due to space restrictions.  we can see that for all plots  there is a trough corresponding    fig.    test performance on simulated mas spring damper  system versus observed worst case sensitivity.  to where the training data was drawn with  u    . note  however  that for the lstm and the rnn  the performance  of the models quickly degrades as the amplitude of the  input data distribution increases. for the remaining models   however  we can see that this loss in performance is much  more gradual. we interpret this as an improvement in model  generalizability  supporting the assertion that the lipschitz  constant is a fundamental quantity affecting a models ability  to generalize. examples of the outputs and errors for these  models are shown in figure   for  u     and for  u     .  figure   shows the medians of the same dataset for the  models with and without stability constraints. comparing the  stable models  we can see that the nominal nse is improves  as the stability condition used becomes less conservative.  additionally  comparing the unstable models with the stable  models  we can see that the stable models have better  generalization  signified by the reduced slope after  u    .  