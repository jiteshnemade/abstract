introduction      interpretable models by design  in this family of models  the idea is to construct deep learning architectures with the concern of interpretability  raised early on the design phase. this change of  the architecture may take the form of adding special components to traditional models whose role is  to leverage the interpretability issue           using  interpretation friendly activation functions  modifying the loss function by injecting a term that favours  a resulting interpretable model      or using a variant of the back propagation algorithm for training   which enforces that the resulting model is readily  interpretable     . nevertheless  enforcing the constraint of interpretability by design leads inevitably  to a loss of flexibility of the constructed models  and  hence a drop of their expressive power.          dl model and the objective is then to design algorithmic and visualization tools          that attempt  to answer questions related to the interpretability of  the original model such as    a  semantics of hidden units of the network model   or  alternatively which role is played by each hidden unit in the network with respect to the learning  task  e.g. does a neuron serve as a counter in rnns  trained to learn languages recognized by counter  machines      a neuron that stores the state of an  rnn trained to accept a regular language etc.     b  tracing the causal relationship between the predicted output with respect to the input  also called  instance level interpretability                   methods form this class raise the problem of local interpretability and  roughly speaking  aims at designing  algorithmic answers to the following question  what  is the influence degree of each input factor that explains the obtained output   another important sub category of post hoc techniques concerns methods that attempt to extract  interpretable rule based machines  e.g. decision  trees  automata. . .   from dl models         . unlike instance level interpretability techniques  these  methods are global and the challenge is how to convert the continuous representation of information as  encoded in rnns into a discrete  symbolic representation  while maintaining a good quality of prediction of these last structures.     the intersection between a dfa and the cut language  of a weighted rnn lm is undecidable    the equivalence of a pdfa pfa wfa and weighted rnn lm  in a finite support is exp hard   b  for consistent  first order rnn lms with any computable activation  function    the tcheybetchev distance approximation  is decidable    the tcheybetchev distance approximation in a finite support is np hard.  the rest of this article is organized as follows. section   gives a concise literature overview of issues related to our problematic. section   presents our results for the case of general first order rnn language  models rnn lm  with relu activation function. section   is dedicated to the case of consistent rnn lms.         related works    the problem of symbolic knowledge extraction from  connectionist models is not a new issue  and one  can trace back works interested in this problem  since the development of the first neural architectures         . however  with the development of novel  spatio temporal connectionist models in the nineties   the most important of which is ellman rnns       and their great empirical success on inferring language  models with limited amount of data and with performance results that often outscore rule based algorithms  traditionally used in the grammatical inference field       research interests in this issue has regained more  attention. in fact  these works were mostly driven by  a legitimate motivation  if an rnn like structure is  trained to recognize a language belonging to a given  class of languages c  and this latter can be recognized  by a class of computing devices m  then there must be  a close connection between the representation of the  target language as encoded in the rnn like structure  on one hand  and that of the corresponding computing  device in m that is capable of recognizing it on the  other.  this aforementioned motivation raises two fundamental questions  at least from a theoretical viewpoint     in this work  we are interested in this last family of  interpretability methods. more precisely  we address   from a computational viewpoint  the issue of extracting fsm based machines from general rnn language  models.  but this problem would benefit from understanding  better how well a finite state model can approximate  an rnn. which in turn requires solving essential  computational problems  can we compute distances  between these language models   can we decide  equivalence  these questions have received answers  for pfa          . we aim in this work to extend these  results by including rnn language models into the  picture.     . what is the expressive power of different classes of   rnn machines   as compared to classical symbolic  our main results are summarized as follows   a   machines  e.g. deterministic non deterministic fifor general weighted first order rnn lms with relu  nite state automata  deterministic non determinstic  activation function   . the equivalence problem of a  pushdown automata  turing machines etc.    pdfa pfa wfa and a weighted first order rnn lm  is undecidable     as a corollary  any distance metric  between languages generated by pdfa pfa wfa  . how can we design algorithms that extract symand that of a weighted rnn lm is also undecidable   bolic machines from rnn models  what are the        functions were turing complete. later on  kilian  et el. generalized this result to sigmoidal activation  functions     .  the turing completeness of some classes of rnns has  many consequences with respect to the computational  class to which belong many problems related to them.  in      the authors proved that the problem of deciding  whether a rnn language model  rnn lm  with  relu activation function is consistent  encodes a valid  probability distribution  or not is an undecidable  problem. moreover  the consensus string problem  and finding a minimal rnn lm equivalent to a given  rnn lm or testing the equivalence between two  rnn lms are also undecidable.    theoretical guarantees of such methods  what is  the computational complexity of such problems   we should note that these two questions are  in some  sense  interrelated. if a class of rnns is very powerful  say turing equivalent  computational problems  related to the extraction of finite state machines are  more likely to be undecidable. in fact  as a corollary of rice s theorem    the equivalence between a  turing machine and any non trivial class of computing devices is necessarily undecidable  which means in  practice that no algorithm can exist that can answer  the question of equivalence between symbolic machines  and  turing  equivalent  rnn ones. in other words   from the perspective of the theory of computation  the  trade off between expressiveness and interpretability   in connectionist models is unavoidable. as a consequence of the above discussion  we argue that analyzing a class of rnns as a computational model can give  many insights with regard to its interpretability.  guided by questions raised above  we divide the rest  of this section into two parts  in the first part  we examine works present in the literature that focused on  the computational power of recurrent neural networks  and its consequences on some computational problems  concerning rnns. in the second part  we give a brief  overview of existing methods in the literature aimed  at extracting finite state machines from trained rnn  ones.     .     given these pessimistic results about computability  of several important problems related to rnns  a new  line of research suggests to analyze the practical capabilities computational power of neural nets instead  of the classical  unrealistic  theoretical model  by constraining the amount of memory resources of the rnn  hidden units to be finite         . under this constraint   korskky et al.      proved that rnns with one hidden  layer and relu activation  and grus are expressively  equivalent to deterministic finite automata. in        weiss et al. showed that the class of finite precision  lstms were able to simulate counter machines  while  the simple class of elman rnns and grus can t.     .     computational power of rnns    the question of the computational capabilities of  different classes of rnn has been addressed since the  early development of neural systems. to the best of  the author s knowledge  early works that addressed  this dates back to the middle of the previous century  by mcculloch et al.      and kleene      where it was  proven that networks with binary threshold activation  functions are capable of implementing finite state  automata. in       pollack designed a turing complete  class of high order recurrent neural networks with two  types of activation function  linear and heaviside .  this result was later extended in       where authors  relaxed the high order requirement  and showed that  first order rnns with saturated linear activation    extraction of automata based machines from trained rnns    early works investigating the problem of extracting  automata based machines from trained rnns coincide  with the emergence of novel rnn architectures           in early nineteens that have shown promising results  for the task of inferring language models from limited  data. these early works have mainly focused on the extraction of deterministic finite automata  dfas  from  rnns trained to recognize regular languages  and most  of which were based on the assumption that a welltrained rnn to recognize a regular language tend to  group hidden states of the rnn into clusters that maps  directly to states of the minimal dfa recognizing the  target regular language. based on this assumption  the  problem of dfa extraction from rnns boils down to  a clustering quantization problem of the rnn s hidden state space  and many clustering techniques were  proposed for this task  quantization by equipartition            hierarchical clustering      k means            fuzzy clustering     etc.  during the last few years  as rnn based architec       the rice theorem states that any class of non trivial languages recognized by a turing machine is not recursive    in our context  we quantify the interpretability of a model  as a measure of the computational difficulty by which one can  extract a finite state machine. a more rigorous formal definition of what is an intepretable model is still an arguably open  question.          tures became more sophisticated and thus harder to  be a subject of interpretative analysis  the issue has  gained an increasing interest among researchers  and  new methods were proposed in the literature to extract automata based machines from different classes  of rnns. in       weiss et al. proposed an adaptation  of the l  algorithm     to extract deterministic finite  automata  dfa  from rnn acceptors  where an rnn  acceptor model serves as a black box oracle for approximate equivalence and membership queries  hinting that the exact equivalence query is  likely to be  intractable . same authors extended their work in       to extract probabilistic deterministic finite automata  from rnn lms. in order to answer the equivalence  query  authors used a sampling strategy of both models  and gave theoretical guarantees of its convergence  in probability under a relaxed notion of equivalence.  ayache et al.     employed the spectral learning framework     to extract weighted finite automata wfa   from a rnn language model. in       okudono et al.  raised the problem of answering the equivalence query  between a rnn language model and a wfa proposing an empirical regression based technique to perform  this task. however  no theoretical guarantees were provided to back their method.         in section    a   sat formula will be denoted by the  symbol  . a formula is comprised of n boolean variables denoted x    ..xn   and k clauses c    ..ck . for each  clause  we ll use notation li    li    li  to refer to its three  composing literals. for a given string w         n  the  number of clauses satisfied by w will be denoted by nw .  for the rest of this section  we shall first provide  a formal definition of the class of first order weighted  rnn lms that we ll study in this work. also  we ll give  a brief recall of basic definitions of different automatabased machines that we ll encounter throughout the  rest of this article.  definition  . .     a first order weighted rnn language model is a weighted language f        r and is  defined by the tuple      n  h         w   w         e  e      such that                      is the input alphabet   n the number of hidden neurons       q   q is a computable activation function   w   qn  n is the state transition matrix    w           where each w     qn is the embedding  vector of the symbol             o   q    n is the output matrix     o    q   the output bias vector.    definitions and notations    the computation of the weight of a given string w   where   is the end marker  by r is given as follows.   a  recurrence equations     let   be a finite alphabet. the set of all finite strings  is denoted by    . the set of all strings whose size is  equal  resp. greater than or equal  to n is denoted by   n  resp.   n  . for any string w        the size of w is  denoted by  w   and its n th symbol by wn . the prefix  of length n for any string w     n will be referred to  as w n . the symbol   denotes a s  special marker. the  symbol    will refer to the set      .    h t        w.h t    ww  t    et     oh t      o      et      sof tmax   et         b  the resulting weight      w     y  weighted languages  a weighted language f  ei   r w      over   is a mapping that assigns to each word  i    w      a weight f  w    r. a wl f is called  consistent  if it encodes a valid probability dis  where w    w   w         tribution  i.e.  satisfies  the  following  properties   p  remark that  in order to avoid technical  f  w     . two wls f    f    w        f  w        w     issues   we used softmax base   defined as    xi  are said to be equivalent if   w        f   w    f   w . sof tmax  x     for any x   rd instead  n     i  p   xj  the tcheybetchev distance metric between two  j    wls is denoted d   f    f     and defined as  of the standard softmax in the previous definition.  max   f   w    f   w  .  finally  we define  for a in the following  hidden units of the network will be  w      given scalar c      the cut point language of f with designated by lowercase letters n    n    ..  and their  respect to c and denoted lf c   as the set of finite words activations at time t by htn . also  we denote by r  the  whose values are greater or equal to c.  class of rnn lms when   is the activation function.        for example  an important class of rnn lms that  will be used extensively in the rest of the article is  rrelu .    tional viewpoint. weiss et al.      proved both theoretically and empirically that first order rnns with  relu .  can simulate counter machines. korsky et  al.     proved that finite precision first order rnns  weighted finite automata  wfa . wfas with relu are computationally equivalent to deterrepresent weighted versions of nondeterministic finite ministic finite automata. moreover  when allowed arbiautomata  where transitions between states  denoted trary precision  they can simulate pushdown automata.    q     q     where q  q     q represents states of the wfa analyzing rnns with other widely used activation  are labeled with a rational weight t  q     q      and each functions  such as the sigmoid and the hyperbolic tanof its nodes q   q is labeled by a pair of rational gent  are left for future research.  numbers  i q   p  q   that represents respectively the  initial state and final state weight of q. wfas model  .  turing completeness of general  weighted languages where the weight of a string w is  weighted rnns  siegelmann s conequal to the sum of the weights of all paths whose  struction  transitions encode the string w. the weight of a path  p is calculated as the product of the weight labels the basic building block for proving computational reof all its transitions  multiplied by the initial state sults presented in this part of the article is the work  weight of its staring node and the final state weight of done by siegelmann and al. in      to prove the turits ending node.  ing completeness of a certain class of first order rnns.  hence  we propose  in this section  to provide a global  probabilistic finite automata  pfa . a pfa scope of this construction  followed by an equivalent  is a wfa with two additional constraints  first  the reformulation of their main theorem that will be relesum of initial state weights of all states is a valid vant for our work.  probability distribution over the state space. second  the main intuition of siegelmann et al. s work is that   for each state  the sum of weights of its outcoming with an appropriate encoding of binary strings  a firstedges added to its finite state weight is equal to  . this order rnn with a saturated linear function can readadditional constraint restricts the power of pfas to ily simulate a stack datastructure by making use of  encode stochastic languages       which makes it useful a single hidden unit. for this  they used   base enfor representing language models. interestingly  pfas coding scheme that represents a binary string w as a   w   are proven to be equivalent to hidden markov models  p wi  rational  number   enc w       i . backed by this   hmms   and the construction of equivalent hmms  i    from pfas and vice versa can be done in polynomial result  they proved than any two stack machine can  time    . the deterministic version of pfas  a.k.a be simulated by a first order rnn with linear satudeterministic probabilistic finite automata  dpfa   rated function  where the configuration of a running  enforces the additional constraint that for any state q  two stack machine  i.e. the content of the stacks and  and for any symbol   there is at most one outgoing the state of the control unit  is stored in the hidden  transition labeled by   from q.  units of the constructed rnn. finally  given that any  turing machine can be converted into an equivalent  two stack machine  the set of two stack machines is  turing complete        they concluded their result.    computational results for in the context of our work  two additional remarks  to be noted about siegelmann s construction  general rnn lms with relu need  first  although the class of first order rnns examined in their work uses the saturated linear function  activation functions  as an activation function  their result is generalizable  the choice of relu in this part of the article is not to the relu activation function  or  more generally   arbitrary. in fact  due to its nice piecewise linear prop  any computable function that is linear in the support  erty and its wide use in practice  the relu .  function           second  although not mentioned in their work   is first choice to analyze theoretical properties of rnn the construction of the rnn from a turing machine is  architectures. recently  chen et al.     provided an polynomial in time. in fact  on one hand  the numextensive study of first order rnn language models ber of hidden units of the constructed rnn is linear in  with relu as an activation function from a computa  the size of the turing machine  and the construction        of transition matrices of the network is also linear in r   rm w  relu   there exists t   n  such that  t   t     t    time. on the other hand  notice that the   base en  h t         and hnhalt    .  nhalt  coding map enc .  is also computable in linear time.  in light of these remarks  we are now ready to present   .  the equivalence problem between  the following theorem     fsas and general rnns    theorem  . .  theorem          let                        be any computable function  and m be a  turing machine that implements it.  we have   for any binary string w  there exists n     o poly  m      h       enc w   ..     qn   w    qn  n   such that for any finite alphabet               w     qn   o   q      n   o    q        r        n  relu  w  w     o  o     rrelu verifies     the equivalence problem between a dpfa and a general rnn lms is formulated as follows   problem. equivalence problem between a dpfa and  a general rnn  given a general weighted rnn lm r   rrelu and a  dpfa a. are they equivalent   theorem  . . the equivalence problem between a  dpfa and a general rnn is undecidable      if   w  is defined  then there exists t   n such that  the first element of the hidden vector ht is equal to  enc   w    and the second element is equal to       proof. we ll reduce the halting turing machine problem to the equivalence problem. let      a . we first  define the trivial dpfa a with one single state q    and  t   q   a q      p  q           i q       . this dpfa imple   .  ments the weighted language f  an      n    let m be a turing machine and w      . let  r   rm w  relu such that o nhalt   a         everywhere  and o  is equal to zero everywhere. we construct a  rnn r  from r by adding one neuron in the hidden        t     layer  denoted n  such that  hn        t       hn       t      relu hn     o n         .  notice that  by corollary  .   the tm m never halts   t     t    on w if and only if  t    hnhalt   hn              i.e.     n  r a       n   . that is  the tm m doesn t halt on w  if and only if the dpfa a is equivalent to r    which  completes the proof.      if   w  is undefined  i.e. m never halts on w   then  for all t   n  the second element of the hidden vector  ht is always equal to zero.  moreover  the construction of h  and w is polynomial  in  m   and  w .  in the following  we ll denote by rm w  relu the set of  rnns in rrelu that simulate the tm m on w. it  is important to note that the construction of a rnn  that simulates a tm on a given string in the previous  theorem is both input and output independent. the  only constraints that are enforced by the construction  are placed on the transition dynamics of the network  and the initial state. in fact  the input string is placed  in the first stack of the two stack machine before running the computation  i.e. in the initial state h     .  under this construction  the first stack of the machine  is encoded in the first hidden unit of the network. afterwards  the rnn machine runs on this input  and  halts if it ever halts  when the halting state of the  machine is reached. in theorem  .   the halting state  of the machine is represented by the second neuron of  the network. in the rest of the article  we ll refer to  the neuron associated to the halting state by the name  halting neuron  denoted nhalt .  we present the following corollary that gives a characterization of the halting machine problem  that relates  it to the class rrelu      a direct consequence of the above theorem is that  the equivalence problem between pfas wfas and  general rnn lms in rrelu is also undecidable  since  the dpfa problem case is immediately reduced to the  general case of pfas  or wfas . another important  consequence is that no distance metric can be computed between dpfa pfa wfa and rrelu    corollary  . . let      a . for any distance metric  d of      the total function that takes as input a description of a pdfa a and a general rnn lm rrelu  and outputs d a  r  is not recursive.  this fact is also true for pfas and wfas.    corollary  . . let m be any turing machine  and w  be a binary string  m halts on w if and only if for any    proof. let d be any distance metric on    . by definition of a distance  we ll have d a  r      if and only if  a and r are equivalent. since the equivalence problem  is undecidable  d .  can t be computed.      the halting machine problem is defined as follows  given  a tm m and a string w  does m halt on w  this problem is  undecidable.           .     intersection of the cut language of the proof is similar to the used for theorem  . . we  a general weighted rnn lm with are given a tm m   a string w and m   n. let      a .  we construct a general weighted rnn lm r  by auga dfa  m w       menting r   rrelu with a neuron n as in theorem   . . by theorem  .   this reduction runs in polynomial  time. on the other hand  let a be the trivial pdfa  with one single state q    and t   q   a q      p  q                 i q       . note that r doesn t halt in m steps   t    if and only if  t   m    nhalt   n  t               i.e.     r   an       n   for the first m running steps on r    in  which case the language modeled by r  is equal to f in    m . hence  a is equivalent to r in   m if and only  if m doesn t halt on the string w in less or equal than  m steps.    in this subsection  we are interested in the following  problem   problem. intersection of a dfa and the cut point  language of a weighted rnn lm  given a general weighted  s rnn lm r   rrelu   c   q   and a dfa a  is lr c la      before proving that  this problem is undecidable  we shall recall first a result  proved in       theorem  . .  theorem         define the highestweighted string problem as follows  given a weighted  rnn lm r   rrelu   and c           does there exist  a string w such that r w    c   the highest weighted string problem is undecidable.  this problem is also known as the consensus problem       and it is known to be np hard even for pfa.         computational results for consistent rnn lms with general activation functions    corollary  . . the intersection problem is undecidable.    in the previous section  we have seen that many interesting questions related to measuring the similarity  proof. we shall reduce the highest weighted string between weighted languages represented by different  problem from the intersection problem. let r   classes of weighted automata and first order rnn lms  rrelu a general weighted rnn lm  and c         . with relu activation function turned out to be either  construct thetautomaton a that recognizes    . we undecidable  or intractable when restricted to finite  have that la lr   lr     if and only if there exist support. in this section  we examine the case where  no string w such that r w    c  which completes the trained rnn lms are guaranteed to be consistent   proof.  and we raise the question of approximate equivalence  between pfas and first order consistent rnn lms   .  the equivalence problem in finite with general computable activation functions. for  any computable activation function    we formalsupport  ize this question in the following two decision problems   given that the equivalence problem between a  weighted rnn lm and different classes of finite state problem. approximating the tchebychev distance  automata is undecidable  a less ambitious goal is to between rnn lm and pfa  decide whether a rnn lm agrees with a finite state instance  a consistent rnn lm r   r    a consisautomaton over a finite support. we formalize this tent pfa a  c      problem as follows   question  does there exist  w       such that  problem. the eq finite problem between pdfa and  r w    a w     c  weighted rnn lms  given a general weighted rnn lm r   rrelu   m   n problem. approximating the tchebychev distance beand a pdfa a. is r equivalent to a over   m    tween consistent rnn lm and pfa over finite support  instance  a consistent rnn r   r    a consistent  pfa a  c     and n   n     question  does there exist  w    n such that   r w    a w     c  note that there is no constraint on the activation function used for consistent rnn lms in these defined  problems  provided it is computable. the first fact  is easy to prove     theorem  . . the eq finite problem is exp hard.  proof. we reduce the bounded halting problem  the eq finite problem.         to      the    bounded halting problem is defined as follows  given  a tm m  a string x and an integer m  encoded in binary form.  decide if m halts on x in at most n steps  this problem is  exp complete.          t  t    if x     li    li    li     then  qi        qi        a   and  t  f   qi       qi       a    t  t    if x      li    li    li     then  qi        qi        a   and  t  f   qi       qi       a  t  f    otherwise   a        qi     a  qi        a    theorem  . . approximating the tcheybechev distance between rnn lm and pfa is decidable.    proof. let r be a consistent rnn lm and a be a consistent pfa. an algorithm that can decide this problem  runs as follows  enumerate all strings w    .. in    until  c  c  we reach a string that satisfies this property in which    otherwise  a      c    t  f      qi      a  qi         a  case the algorithm returns yes. if there is no such  string  by definition of consistency  there will be a finite   for each clause i and every boolean variable xj  where j       n        t  t  p  p  time t such that  r wt         c   a wt         c  n  s  t    t      if xj    li    li    li     then  qi j       qi j         a  in which case  we have   t   t   r wt     c and  n  t    if x j    li    li    li     then  qi j       qi j         a  a wt     c which implies  t   t    r wt   a wt      c.  n  n  when t is reached  the algorithm returns no.    otherwise   a        qi j   a  qi j        a     .       transition probabilities     approximating the tcheybetchev  distance over a finite support       i       k   a      c    s  n             k   k    c  ta  q    a  qi         proving the np hardness of the tcheybetchev  t  f     i       k   a       ta  qin    a  qin       distance approximation in finite support is more    all the other transitions belonging to  a has a  complicated  and we ll give below the construction of  weight         a pfa and a rnn from a given   sat formula which  will help us prove the result. let              whose    final state probabilities   value will be specified later.  n    for each clause i  pa  qin                construction of a pfa a  the construction    all the other states in a has a final state probaof our pfa is inspired from the work done in      and  bility equal to     illustrated in figure  . intuitively  each clause i in   is  represented by two paths in the pfa  one that encodes    construction of a rnn  the rnn r we ll cona satisfiable assignment of the variables for this clause  struct is simple  and it generates the quantitative lanand the other not. more formally  the pfa a is defined guage r w               w   . more formally  our rnn is  as   defined as   c    qa    q       qi j    i       k   j       n   c    t  f      n        hidden neurons    is the set of states               hn              initial probabilities  ia  q           otherwise           hn     for every i       k   c    t  f   and a             c        q    a  qi         a      transition matrices  win      w    w               t  t     i       k   j       n       a        qi j   a  qi j        a     w          for each clause i                    log        t  t        if x     li    li    li     then  a        qi     a  qi             output matrices  o          o     log             a . and           f  t    if x     li    li    li     then  qi        qi        a   and  where log   .  is the logarithm to the base    f  f   qi       qi       a        w   f  t    if x      li    li    li     then  qi        qi        a   and what s left is to show that r w              defines  n  n  a consistent language model        qi        a   qi   f  f    otherwise a        qi     a  qi        a  proposition  . . for any         the weighted lan     guage model defined as f  w               w    is consistent.    f  t    if x      li    li    li     then  a        qi     a  qi        a   and           t  q      t  q                                  q           f  q                                                                                                                          f  q                    t  q                                                              f  q                                                                                          f       q            t  q                     t  q                                                                                            f  q                                        f  q                        t  q                                                                        f  q                    t  q                                                                                                                    t  q                                    f       q                figure    a graphical representation of the pfa constructed from      x    x    x       x     x    x     on the other hand  for  w    n  we have     proof. we have   x  x  f  w        w       x             n       nw             r w    a w              w      k           n n w  w  n            x            n    note that we have for any             n n    by applying the equality     p    n n    xn           x     w     n    r w    a w      r w n     a w n       for any  x         on the sum present in the right hand term of the  equation above  we obtain the result.    this means that  under this construction  the maximum is reached necessarily by a string whose length is  exactly equal to n. thus  we obtain     proposition  . . let   be an arbitrary   sat formula with n variables and k clauses. let a be the pfa               max nw  d   r  a            n  constructed from   by the procedure detailed above  the  k       w  n  probabilistic language generated by a is given as      note that   is satisfiable if and only if maxn nw   k.      w      if  w    n  w                  k nw  as  a  result   pick  any  s      k     k    and  define  cepsilon    a w               w     nkw       if  w    n  s          k          the  formula  is  satisfiable  if  and only    k         n            k nw n   w  nw n           else               k         k  if d   r  a    c.  proposition  . . for any rational number           there exists a rational number c  such that   is satis  theorem  . . the tchebychev distance approximation problem between consistent rnn lms and pfas  fiable if and only if d   r  a    c  in finite support is np hard.  proof. for any w such that  w    n   r w  a w        .  proof. we reduce the   sat satisfiability problem to  for  w    n  we have   our problem. let   be an arbitrary   sat formula.  construct a pfa a and a rnn r as specified previnw                   r w    a w             n  ously. choose a rational number        . let c      be     k            any rational number as specified in the proof of proposition  .   and n   n    . by proposition  .     is satisfiable if and only if d   r  a    c  which completes  the proof  remarks     np hardness for lstms grus  although our  main focus in this article was on first order weighted  rnns with one hidden layer  it is worth noting the  the np hardness reduction technique from   sat  problem we employed can easily be generalized to the  case of lstms     and grus  two widely used rnn  architectures in practice. indeed  our reduction relies  on the construction of a memoryless first order rnn  which makes abstraction of the state of the hidden  units of the network  and exploits only the output  bias vector o  . hence  provided we have first order  output function for a lstm  or gru  architecture   the np hardness result demonstrated above is easy  to extend our proof to these architectures.    finite precision rnns  as said earlier in section ii  a new line of work considered the analysis of the computational power of rnns with  bounded resources  which is a realistic condition in  practice        . broadly speaking  a finite precision  rnn is one whose weights and values of its hidden  units are stored using a finite number of bits  see       for further details . under our construction  the  same remark raised above about lstms grus can  be applied to rnns with finite precision. in fact  it s  easy to notice that  with a judicious choice of    say               in which case o       o           the toy memoryless rnn we constructed in the proof requires only    bits to encode a hidden unit and a weight value of  the network. this shows that even approximating  the tcheybetchev distance in finite support between  a language represented by pfa and that of a finiteprecision first order rnn with any computable activation function is also np hard.         conclusion and perspectives    in this article  we investigated some computational  problems related to the issue of approximating trained  rnn language models by different classes of finite state  automata. we proved that the equivalence problem of  pdfas pfas wfas and general weighted first order  rnn lm with relu activation function with a single  hidden layer is generally undecidable  and  as a result   trying to calculate any distance between them can t be        computed. when restricting rnn lms to be consistent  we proved that approximating the tcheybetchev  distance between consistent rnn lms with general  computable activation functions and pfas is decidable   and that the same problem when restricted to a finite  support is at least np hard. moreover  we gave arguments that the reduction strategy from   sat problem  we employed to prove this latter result makes this result generalizable to the class of lstms grus and  finite precision rnns.  this work provides first theoretical results of examining equivalence and the quality of approximation problems between automata based models and rnns from  a computational viewpoint. yet  there are still many  interesting problems on the issue that could motivate  future research  such as  is the equivalence problem  between general rnn lms and different classes of finite state machines still undecidable when other highly  non linear activation functions  e.g. sigmoid  hyperbolic tangent ..  are used instead of relus  is the  equivalence problem between the cut point language  of an rnn lm and a dfa decidable  if an rnn lm  is trained to recognize a language generated by a regular grammar  can we decide if its cut point language  is indeed regular  etc.    