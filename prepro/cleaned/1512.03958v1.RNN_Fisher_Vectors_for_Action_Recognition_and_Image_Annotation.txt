introduction  fisher vectors have been shown to provide a significant  performance gain on many different applications in the domain of computer vision                . in the domain  of video action recognition  fisher vectors and stacked  fisher vectors      have recently outperformed state of theart methods on multiple datasets         . fisher vectors   fv  have also recently been applied to word embedding   e.g. word vec       and have been shown to provide state  of the art results on a variety of nlp tasks       as well as  on image annotation and image search tasks     .  in all of these contributions  the fv of a set of local  descriptors is obtained as a sum of gradients of the loglikelihood of the descriptors in the set with respect to the  parameters of a probabilistic mixture model that was fitted  on a training set in an unsupervised manner. in spite of being richer than the mean vector pooling method  fisher vectors based on a probabilistic mixture model are invariant to  order. this makes them less appealing for annotating  for  example  video  in which the sequence of events determines  much of the meaning.  this work presents a novel approach for fv representation of sequences using a recurrent neural network        the proposed rnn fv method achieves state of theart results in action recognition on the hmdb        and  ucf         datasets. in image annotation and image  search tasks  the rnn fv method is used for the representation of sentences and achieves state of the art results  on the flickr k dataset     and competitive results on other  benchmarks.     . previous work  action recognition as in other object recognition problems  the standard pipeline in action recognition is comprised of three main steps  feature extraction  pooling and  classification. many works              have focused on the  first step of extracting local descriptors. laptev et al.       extend the notion of spatial interest points into the spatiotemporal domain and show how the resulting features can  be used for a compact representation of video data. wang  et al.          used low level hand crafted features such as  histogram of oriented gradients  hog   histogram of optical flow  hof  and motion boundary histogram  mbh .  recent works have attempted to replace these handcrafted features by deep learned features for video action  recognition due to its wide success in the image domain.  early attempts              achieved lower results in comparison to hand crafted features  proving that it is challenging to apply deep learning techniques on videos due to the  relatively small number of available datasets and complex  motion patterns. more recent attempts managed to overcome these challenges and achieve state of the art results  with deep learned features. simonyan et al.      designed  two stream convnets for learning both the appearance of  the video frame and the motion as reflected by the estimated optical flow. du tran et al.      designed an effective approach for spatiotemporal feature learning using    dimensional convnets.  in the second step of the pipeline  the pooling  wang et  al.      compared different pooling techniques for the application of action recognition and showed empirically that  the fisher vector encoding has the best performance. recently  more complex pooling methods were demonstrated  by peng et al.      who proposed stacked fisher vectors   sfv   a multi layer nested fisher vector encoding and  wang et al.      who proposed a trajectory pooled deepconvolutional descriptor  tdd . tdd uses both a motion  cnn  trained on ucf     and an appearance cnn  originally trained on imagenet      and fine tuned on ucf   .  image annotation and image search in the past few  years  the state of the art results in image annotation and  image search have been provided by deep learning approaches                                        . a typical  system is composed of three important components   i  image representation   ii  sentence representation  and  iii     matching images and sentences. the image is usually represented by applying a pre trained cnn on the image and  taking the activations from the last hidden layer.  there are several different approaches for the sentence  representation  socher et al.      used a dependency tree  recursive neural network. yan et al.      used a tfidf histogram over the vocabulary. klein et al.      used  word vec      as the word embedding and then applied  fisher vector based on a hybrid gaussian laplacian mixture model  hglmm  in order to pool the word vec embeddings of the words in a given sentence into a single representation. ma et al.      proposed a matching cnn  mcnn  that composes words to different semantic fragments  and learns the inter modal relations between image and the  composed fragments at different levels.  since a sentence can be seen as a sequence of words   many works have used a recurrent neural network  rnn   in order to represent sentences                     . to address the need for capturing long term semantics in the sentence  these works mainly use long short term memory   lstm      or gated recurrent unit  gru      cells. generally  the rnn treats a sentence as an ordered sequence of  words  and incrementally encodes a semantic vector of the  sentence  word by word. at each time step  a new word  is encoded into the semantic vector  until the end of the  sentence is reached. all of the words and their dependencies will then have been embedded into the semantic vector  which can be used as a feature vector representation of  the entire sentence. our work also uses an rnn in order  to represent sentences but takes the derived gradient from  the rnn as features  instead of using a hidden or an output  layer of the rnn.  a number of techniques have been proposed for the task  of matching images and sentences. klein et al.      used  cca      and yan et al.      introduced a deep cca in  order to project the images and sentences into a common  space and then performed a nearest neighbor search between the images and the sentences in the common space.  kiros et al.       karpathy et al.      socher et al.      and  ma et al.      used a contrastive loss function trained on  matching and unmatching pairs of  image sentence  in order to learn a score function for a given pair. mao et al.       and vinyals et al.      learned a probabilistic model for inferring a sentence given an image and  therefore  are able to  compute the probability that a given sentence will be created by a given image and used it as the score.     . baseline pooling methods  in this section we describe two baseline pooling methods  that can represent a multiset of vectors as a single vector.  the notation of a multiset is used to clarify that the order of  the words in a sentence does not affect the representation   and that a vector can appear more than once. both methods     can be applied to sequences  however  the resulting representation will be insensitive to ordering. to address this   we propose in sec.   a novel pooling method  rnn fv.     . . mean vector  this pooling technique takes a multiset of vectors   x    x    x    . . .   xn     rd   and computes its mean   pn  v   n   i   xi . clearly  the vector v that results from the  pooling is in rd .  the disadvantage of this method is the blurring of the  multiset s content. consider  for example  the text encoding  task  where each word is represented by its word vec embedding. by adding multiple vectors together  the location  obtained   in the semantic embedding space   is somewhere  in the convex hull of the words that belong to the multiset.     . . fisher vector of a gmm  given a multiset of vectors  x    x    x    . . .   xn      rd   the standard fv      is defined as the gradient of the  log likelihood of x with respect to the parameters of a  pre trained diagonal covariance gaussian mixture model   gmm . it is a common practice to limit the fv representation to the partial derivatives with respect to the means      and the standard deviations     and ignore the partial derivatives with respect to the mixture weights.  it is worth noting the linear structure of the gmm fv  pooling. since the likelihood of the multiset is the multiplication of the likelihoods of the individual elements  the  log likelihood is additive. this convenient property would  not be preserved in the rnn model  where the probability  of an element in the sequence depends on all the previous  elements.  to all types of fv  we apply the two improvements that  were introduced by perronnin et al.     . the first improvement is to apply an element wise power normalization function  f  z    sign z  z   where           is a parameter  of the normalization. the second improvement is to apply  an l  normalization on the fv after applying the power  normalization function.     . rnn based fisher vector  the pooling methods described above share a common  disadvantage  insensitivity to the order of the elements in  the sequence. a way to tackle this  while keeping the power  of gradient based representation  would be to replace the  gaussian model by a generative sequence model that takes  into account the order of elements in the sequence. a desirable property of the sequence model would be the ability to  calculate the gradient  with respect to the model s parameters  of the likelihood estimate by this model to an input  sequence.  in this section  we show that such a model can be obtained by training an rnn to predict the next element in a    sequence  given the previous elements. having this  we propose  for the first time  the rnn fv  a fisher vector that is  based on such an rnn sequence model.  we propose two types of rnn fvs. one type is based  on training a regression problem  and the other on training  a classification problem. in practice  only the first type is  directly useful for video analysis. for image annotation   the first type outperforms the second.  given a sequence of vectors s with n vector elements x    ...  xn   we convert it to the input sequence x     x    x    ...  xn       where x    xstart . this special element is used to denote the beginning of the input sequence   and we use xstart     throughout this paper. the rnn  is trained to predict  at each time step i  the next element  xi   of the sequence  given the previous elements x    ...  xi .  therefore  given the input sequence  the target sequence  would be  y    x    x    ...xn  .  the training data and the training process are application  dependent  as is described in sec.   for action recognition  and in sec.   for image annotation.     . . rnn trained for regression  given a sequence of input vectors x  the regression  rnn is trained to predict the next vector in the sequence  s  i.e.  the sequence y . the output layer of the network is  a fully connected layer  the size of which would be d  i.e.   the dimension of the input vector space.  there are several regression loss functions that can be  used. here  we consider the following loss function      ky   vk           where y is the target vector and v is the predicted vector.  after the rnn training is done  and given a new sequence s  the derived sequence x is fed to the rnn. denote the output of the rnn at time step i  i      ...  n       by rn n  x    ...  xi     vi   rd . the target at time step i  is xi    the next element in the sequence   and the loss is   loss y  v          kxi     vi k      the rnn can be seen as a generative model  and  likelihood of any vector x being the next element of  sequence  given x    ...  xi   can be defined as            p  x x    ...  xi          d   exp   kx   vi k      loss xi     vi             the  the           we are generally interested in the likelihood of the correct prediction  i.e.  in the likelihood of the vector xi    given x    ...  xi   p  xi    x    ...  xi  .  the rnn based likelihood of the entire sequence x is   p x       n      y  i      p  xi    x    ...  xi              the negative log likelihood of x is   l x      log  p x          n      x    the rnn can be seen as a generative model which gives  likelihood to the sequence u      log  p  xi    x    ...  xi     pr u        i           nd     log                n      x    n      y                 log piwi             i      i      kxi     vi k     n      y    piwi      pr  wi    w    ...  wi        the negative log likelihood of u is     i           in order to represent x using the fisher vector scheme   we have to compute the gradient of l x  with respect to  our model s parameters. with rnn being our model  the  parameters are the weights w of the network. by     and       we get that l x  equals the loss that would be obtained when x is fed as input to the rnn  up to an additive  constant. therefore  the desired gradient can be computed  by backpropagation  we feed x to the network and perform forward and backward passes. the obtained gradient   w l x  would be the  unnormalized  rnn fv representation of x. notice that this gradient is not used to update  the network s weights as done in training   here we perform  backpropagation at inference time.  other loss functions may be used instead of the one presented in this analysis. given a sequence  the gradient of the  rnn loss may serve as the sequence representation  even if  the loss is not interpretable as a likelihood.     . . rnn trained for classification  the classification application is applicable for predicting a sequence of symbols w   w   ... wn that have matching vector representations r w      x    r w       x    ...  r wn     xn . the rnn predicts the sequence u    w    w    . . .   wn   from the sequence x     x    x    . . .   xn     .  denote by m the size of our symbol alphabet  i.e.  the  number of unique symbols in the input sequences. the output layer of the network is a softmax layer with m units   where the j th element in the output is the probability of the  j th symbol to be the next output element. the loss function  for the training of the rnn is the cross entropy loss.  after the rnn is trained  it is ready to be used as a  feature vector extractor for new sequences. denote the  new sequence by u and its vector representation by x as  above. consider feeding the sequence x to the rnn. at  time step i  i      ...  n       the output of the rnn  pm  is rn n  x    ...  xi      pi    ...  pim    where j   pij    .  here  pij is the probability which the rnn gives to the j th  symbol at time step i.  the cross entropy loss at time step i is derived from the  probability given to the correct next symbol         lossi     log piwi       log  pr  wi    w    ...  wi            l u       log  pr u           n      x  i      by     and      we get that l u   equals the loss that  would be obtained when x is fed as input  and u as output  to the rnn. therefore  the desired gradient can be computed by backpropagation  i.e. feeding x to the network  and performing forward and backward passes. the obtained  gradient  w l u   would be the  unnormalized  rnn fv  representation of u .     . . normalization of the rnn fv  it was suggested by      that normalizing the fvs by the  fisher information matrix is beneficial. we approximated  the diagonal of the fisher information matrix  fim   which  is usually used for fv normalization. note  however  that  we did not observe any empirical improvement due to this  normalization  and our experiments are reported without it.  let     w be a single weight of the rnn. the term in     the diagonal of the fim which corresponds to  l x w  is       i  h     r   l x w    dx.  f    x p  x w        since the probabilistic model which determines  p  x w   is the rnn  it is impossible to derive a closedform expression for this term. therefore  we approximated  it directly from the gradients of the training sequences   h  i      by computing the mean of  l x w  for each     w .      the normalized partial derivatives of the fv are then         l x w    fw  .         . action recognition pipeline  the action recognition pipeline contains the underlying  appearance features used to encode the video  the sequence  encoding using the rnn fv  and an svm classifier on top.     . . visual features  the rnn fv is capable of encoding the sequence properties  and as underlying features  we rely on video encodings that are based on single frames or on fixed length  blocks of frames.  vgg using the pre trained vgg convolutional network       we extract a      dimensional representation of  each video frame. the vgg pipeline is used  namely  the  original image is cropped in ten different ways into     by         pixel images  the four corners  the center  and their xaxis mirror image. the mean intensity is then subtracted in  each color channel and the resulting images are encoded by  the network. the average of the    feature vectors obtained  is then used as the single image representation. in order to  speed up the method  the input video was sub sampled  and  one in every    frames was encoded. empirically  we noticed that recognition performance was comparable to that  of using all video frames. to further reduce run time  the  data dimensionality was reduced via pca to    d. in addition  l  normalization was applied to each vector. all  pcas in this work were trained for each dataset and each  training test split separately  using only the training data.  cca using the same vgg representation of video frames  as mentioned above and the code of         we represented  each frame by a vector as follows  we considered the common image sentence vector space obtained by the cca algorithm  using the best model  gmm hglmm  of       trained on the coco dataset     . we mapped each frame  to that vector space  getting a      dimensional image representation. as the final frame representation  we used the  first  i.e. the principal      dimensions out of the     . for  our application  the projected vgg representations were l   normalized. the cca was trained for an unrelated task of  image to sentence matching  and its success  therefore  suggests a new application of transfer learning  from image  annotation to action recognition.  c d while the representations above encode single frames   the c d method      splits the video into sub volumes that  are encoded one by one. following the recommended settings  we applied the du tran et al. pre trained  d convolutional neural network in order to extract     d representation to    frame blocks. the blocks are sampled with an    frame stride. following feature extraction  pca dimensionality reduction     d  and l  normalization were applied.     . . network structure  our rnn model consists of three layers  a    d fullyconnected layer units with leaky relu activation       .     a     units long short term memory  lstm      layer   and a    d linear fully connected layer. our network is  trained for regression with the mean square error  mse   loss function. weight decay and dropouts were also applied.  an improvement in recognition performance was noticed  when the dropout rate was enlarged  up to a rate of  .     due to its ability to ensure the discriminative characteristics  of each weight and hence also of each gradient.     . . training and classification  we train the rnn to predict the next element in our  video representation sequence  given the previous elements   as described in sec.  . . in our experiments  we use only    available    at www.cs.tau.ac.il  wolf code hglmm    the part of gradient corresponding to the weights of the last  fully connected layer. empirically  we saw no improvement  when using the partial derivatives with respect to weights of  other layers. in order to obtain a fixed size representation   we average the gradients over all time steps. the gradient representation dimension is    x            which is  the number of weights in the last fully connected layer. we  then apply pca to reduce the representation size to     d   followed by power and l  normalization.  video classification is performed using a linear svm  with a parameter c    . empirically  we noticed that the  the best recognition performance is obtained very quickly  and hence early stopping is necessary. in order to choose  an early stopping point we use a validation set. some of  the videos in the dataset are actually segments of the same  original video  and are included in the dataset as different  samples. care was taken to ensure that no such similar  videos are both in the training and validation sets  in order  to guarantee that high validation accuracy will ensure good  generalization and not merely over fitting.  after each rnn epoch  we extract the rnn fv representation as described above  train a linear svm classifier  on the training set and evaluate the performance on the validation set. the early stopping point is chosen at the epoch  with highest recognition accuracy on the validation set. after choosing our model this way  we train an svm classifier  on all training samples  training   validation samples  and  report our performance on the test set.     . image sentence retrieval  in the image sentence retrieval tasks  vector representations are extracted separately for the sentences and the images. these representations are then mapped into a common  vector space  where the two are being matched.      have  presented a similar pipeline for gmm fv. we replace this  representation with rnn fv.  a sentence  being an ordered sequence of words  can  be represented as a vector using the rnn fv scheme.  given a sentence with n words w    ...  wn    where wn  is considered to be the period  namely a wend special token   we treat the sentence as an ordered sequence s     w    w    ...  wn       where w    wstart . an rnn is  trained to predict  at each time step i  the next word wi    of the sentence  given the previous words w    ...  wi . therefore  given the input sequence s  the target sequence would  be   w    w    ...wn  .  the training data may be any large set of sentences.  these sentences may be extracted from the dataset of a specific benchmark  or  in order to obtain a generic representation  any external corpus  e.g.  wikipedia  may be used.  the two network alternatives are explored  classification  and regression. as observed in the action recognition case   we did not benefit from extracting partial derivatives with     respect to the weights of the hidden layers  and hence we  only use those of the output layer as our representation.  when the rnn is trained for classification  each word in  the dictionary is considered as a class. the input to the network is the word s embedding  a    d vector in our case.  the hidden layer is lstm with     units  which is followed  by a softmax output layer. this design creates two challenges. the first is dimensionality  the size of the softmax  layer is the size of the dictionary  m   which is typically  large. as a result   w l x  has a high dimensionality.  the second issue is with generalization capability  since the  softmax layer is fixed  a network cannot handle a sentence  containing a word that does not appear in its training data.  when training the rnn for regression  the same    d  input is used  followed by an lstm layer of size    . the  output layer  in this case  is fully connected  where the       dimensional  word embedding of next word is predicted.  we use no activation function at the output layer. notice  that the two issues pointed out regarding the classification  rnn are not present in the regression case. first  the size  d of the output layer depends only on the dimension of the  word embedding. second  the network can naturally handle  unseen words  since it predicts vectors in the word vector  space rather than an index of a specific word.  for matching images and text  each image is represented  as a      dimensional vector extracted using the    layer  vgg  as described in sec.  . . the regularized cca algorithm       where the regularization parameter is selected  based on the validation set  is used to match the the vgg  representation with the sentence rnn fv representation.  in the shared cca space  the cosine similarity is used.  we explored several configurations for training the rnn.  rnn training data we employed either the training data of  each split in the respective benchmark  or the      englishwikipedia  m dataset made available by the leipzig corpora collection     . this dataset contains   million sentences randomly sampled from english wikipedia. word  embedding a word was represented either by word vec  or  by the gmm hglmm representation of       projected to  a    d sentence to vgg encoded image cca space. we  made sure to match the training split according to the benchmark tested. sentence sequence direction we explored  both the conventional left to right sequence of words and  the reverse direction.     . experiments  we evaluated the effectiveness of the various pooling  methods on two important yet distinct application domains   action recognition and image textual annotation and search.  as mentioned  applying the fim normalization   sec.  .   did not seem to improve results. another form  of normalization we have tried  is to normalize each  dimension of the gradient by subtracting its mean and    dividing by its standard deviation. this also did not lead  to an improved performance. two normalizations that  were found to be useful are the power normalization and  the l  normalization  which were introduced in       see  section   . both are employed  using a constant        .     . . action recognition  our experiments were conducted on two large action  recognition benchmarks. the ucf         dataset consists  of        realistic action videos  collected from youtube   and divided into     action categories. we use the three  splits provided with this dataset in order to evaluate our results and report the mean average accuracy over these splits.  the hmdb   dataset      consists of      action  videos  collected from various sources  and divided into     action categories. three splits are provided as an official  benchmark and are used here. the mean average accuracy  over these splits is reported.  table   compares our rnn fv pooling method to mean  and gmm fv pooling. three sets of features  as described  in sec.  .  are used  vgg coupled with pca  vgg projected by the image to sentence matching cca  and c d.  the parameters were set on the validation split that we  created for the provided training set. for gmm fv  the  only parameter is k  which is the number of components  in the mixture. the validated values of k were in the set                      . the parameter for rnn fv was the  stopping point of the rnn training  as described in sec.  . .  classification is conducted in all experiments using a multiclass  one vs all  linear svm with c  .  as can be seen in table    the rnn fv pooling outperformed the other pooling methods by a sizable margin. another interesting observation is that with vgg frame representation  cca outperformed pca consistently in all pooling methods. not shown is the performance obtained when  using the activations of the rnn as a feature vector. these  results are considerably worse than all pooling methods.  notice that the representation dimension of mean pooling  is      like the features we used   the gmm fv dimension  is     k        where k is the number of clusters and the  rnn fv dimension is     .  table   compares our proposed rnn fv method  combining multiple features together  with recently published  methods on both datasets. the combinations were performed using early fusion  i.e  we concatenated the normalized low dimensional gradients of the models and train  multi class linear svm on the combined representation. we  also tested the combination of our two best models with idt       and got state of the art results on both benchmarks.  interestingly  when training the rnns on ucf    and applying to encode hmdb   videos  a comparable results of    .      .   without idt  is obtained  which is also above  current state of the art.     dataset  method  vgg pca  vgg cca  c d    mp    .      .      .     hmdb    gmm fv rnn fv    .     .      .      .      .      .      method  idt       idt   high d encodings       two stream cnn    nets        multi skip feature stacking       c d    net        c d    nets        c d    nets    idt       tdd    nets        tdd    nets    idt       stacked fv       stacked fv   idt       rnn fv c d   vgg cca   rnn fv c d   vgg cca    idt    hmdb      .     .     .     .              .     .     .      .      .      .      mp    .      .      .      ucf     gmmfv rnn fv    .      .      .      .      .      .      ucf       .     .         .     .     .     .     .     .           .      .      table  . comparison to the state of the art on ucf    and  hmdb  . in order to obtain the best performance  we combine  similar to all other contributions  multiple features. we also  present a result where idt      is combined  similar to all other top  results  multi skip extends idt . this adds motion based information to our method.     . . image sentence retrieval  the effectiveness of rnn fv as sentence representation is evaluated on the bidirectional image and sentence  retrieval task. we perform our experiments on three benchmarks  flickr k      flickr  k      and coco     . the  datasets contain                  and          images respectively. each image is accompanied with   sentences  describing the image content  collected via crowdsourcing.  the flickr k dataset is provided with training  validation  and test splits. for flickr  k and coco  no training  splits are given  and we use the same splits used by     .  there are three tasks in this benchmark  image annotation  in which the goal is to retrieve  given a query image   the five ground truth sentences  image search  in which   given a query sentence  the goal is to retrieve the ground  truth image  and sentence similarity  in which the goal is   given a sentence  to retrieve the other four sentences describing the same image. evaluation is performed using  recall k  namely the fraction of times the correct result  was ranked within the top k items. the median and mean  rank of the first ground truth result are also reported. for  the sentence similarity task  only mean rank is reported.  as mentioned in sec.    we explored rnn fv based    table  . comparing pooling techniques   mean pooling  gmm fv and rnn fv   on hmdb   and ucf   . three types  of features are used  vgg pca  vggcca  and c d. the table reports recognition average accuracy  higher is better .    on several rnns. the first rnn is a generic one  it was  trained with the wikipedia sentences as training data and  word vec as word embedding. in addition  for each of the  three datasets  we trained three rnns with the dataset s  training sentences as training data  one with word vec as  word embedding  one with the  cca word embedding  derived from the semantic vector space of       as explained  in sec.    and one with the cca word embedding  and with  feeding the sentences in reverse order. these rnns were  all trained for regression. for flickr k  we also trained an  rnn for classification  with flickr k training sentences   and word vec embedding . in this network  the softmax  layer was of size        corresponding to the number of  unique words in the flickr k dataset. since the resulting  number of weights of the output layer is around   million   we reduced the dimension of the gradient feature vector by  random sampling of        coordinates. training a classification model on the larger datasets is virtually impractical   since the number of unique words in these datasets is much  higher  resulting in a very large softmax layer and a huge  number of weights.  in the regression rnns  we used an lstm layer of size     . we did not observe a benefit in using more lstm  units. we used the part of the gradient corresponding to  all        weights of the output layer  including one bias  per word vec dimension . in the case of the larger coco  dataset  due to the computational burden of the cca calculation  we used pca to reduce the gradient dimension from         to       . pca was calculated on a random subset  of         sentences  around      of the training set. we  also tried pca dimension reduction to a lower dimension of         for all three datasets. we observed no change in performance  flickr k  or slightly worse results  flickr  k  and coco .  the number of rnn training epochs was                and     for the flickr k  flickr  k  coco and wikipedia  datasets respectively.  tables      and   show the results of the different rnnfv variants compared to the current state of the art methods.  we also report results of combinations of models. combining was done by averaging the image sentence  or sentencesentence  cosine similarities obtained by each model.  first  we see that regression based rnn fv should be  preferred over the classification based one. in addition to  its lower dimension and natural handling of unseen words   the results obtained by regression rnn fv are better. sec      mean  rank  na  na  na  na  na  na  na  na  na    .     .     .     r      r       .     .     .     .     .     .     .     .     .     .     .     .     image annotation  r   r    median  rank    .    .     .     .    .     .     .    .     .     .    .    .     .    .    .     .    .    .   na    .    .     .    .     .     .    .    .     .    .    .     .    .    .     .    .    .      .    .     .     .     .     .     .     .     .     .     .     .       .     .     .     .     .     .   na    .     .     .     .     .       .     .     .     .     .     .     .     .     .     .       .     .     .     .     .     .     .     .     .     .       .     .     .     .     .     .     .     .     .     .       .     .     .     .     .     .     .     .     .     .       .     .     .     .     .     .     .     .     .     .     r    sdt rnn       dfe       rvp      dvsa       sc nlm       dcca       nic       m rnn       m cnn       meanvector       gmm fv       mm ens       our rnn fv   wiki w v  w v  w v clsf  cca  cca rvrs  cca   rvrs  cca         cca   rvrs         all rnn fv models  all rnn fv models             .     .     .     .     .     .     .     .     .     .      .    .    .    .    .    .    .    .    .    .     image search  r    median  rank    .     .     .     .     .     .     .     .     .     .     .     .     .    .     .     .     .    .     .    .     .    .     .    .     .     .     .     .     .     .     .     .     .     .      .    .    .    .    .    .    .    .    .    .     mean  rank  na  na  na  na  na  na  na  na  na    .     .     .     sentence  mean  rank  na  na  na  na  na  na  na  na  na    .     .     .       .     .     .     .     .     .     .     .     .     .       .     .     .     .     .     .    .    .    .    .     table  . image annotation  image search and sentence similarity results on the flickr k dataset. shown are the recall rates at       and     retrieval results  higher is better . also shown are the median and mean rank of the first ground truth  lower is better . we compare the  results of previous work to variants of our rnn fv. the  wiki  notation indicates that the rnn was trained on wikipedia and not on the  sentences of the specific dataset.  clsf  uses classification rnn  while the other models were trained for regression. models notated by   w v  employ word vec  while the other models   cca   use the cca embedding of     .  rvrs  models were trained on reversed sentences.  we also report results of combinations   cca  and  reverse  models   cca  and the best model  gmm hglmm  of        mm ens      cca    reverse  and       all rnn fv models  all rnn fv models and     .    ond  we notice the competitive performance of the model  trained on wikipedia sentences  which demonstrates the  generalization power of the rnn fv  being able to perform well on data different than the one which the rnn  was trained on. training using the dataset s sentences only  slightly improves result  and not always. improved results  are obtained when using the cca word embedding instead  of word vec. it is interesting to see the result of the  reverse  model  which is on a par with the other models. it  is somewhat complementary to the  left to right  model  as  the combination of the two yields somewhat improved results. finally  the combination of rnn fv with the best  model  gmm hglmm  of      outperforms the current  state of the art on flickr k  and is competitive on the other  datasets.     . conclusions  this paper introduces a novel fv representation for sequences that is derived from rnns. the proposed representation is sensitive to the element ordering in the sequence  and provides a richer model than the additive  bag  model  typically used for conventional fvs.  the rnn fv representation surpasses the state of theart results for video action recognition on two challenging  datasets. when used for representing sentences  the rnnfv representation achieves state of the art or competitive  results on image annotation and image search tasks. since  the length of the sentences in these tasks is usually short  and  therefore  the ordering is less crucial  we believe that  using the rnn fv representation for tasks that use longer  text will provide an even larger gap between the conventional fv and the rnn fv.  a transfer learning result from the image annotation task  to the video action recognition task was shown. the con      mean  rank  na  na  na  na  na  na  na  na  na  na  na    .     .     .     r      r       .     .     .     .     .     .     .   na    .     .     .     .     .     .     image annotation  r   r    median  rank    .    .     .     .    .     .     .    .     .     .    .    .     .    .    .     .    .    .   na    .    .   na  na  na    .    .   na    .    .    .     .    .    .     .    .    .     .    .    .     .    .    .      .     .     .     .     .     .     .     .     .     .     .     .     .     .       .     .     .     .     .     .   na    .     .     .     .     .     .     .       .     .     .     .     .     .     .     .     .       .     .     .     .     .     .     .     .     .       .     .     .     .     .     .     .     .     .       .     .     .     .     .     .     .     .     .       .     .     .     .     .     .     .     .     .     r    sdt rnn       dfe       rvp      dvsa       sc nlm       dcca       nic       lrcn      rtp      manual annotations   m rnn       m cnn       meanvector       gmm fv       mm ens       our rnn fv   wiki w v  w v  cca  cca rvrs  cca   rvrs  cca         cca   rvrs         all rnn fv models  all rnn fv models             .     .     .     .     .     .     .     .     .      .    .    .    .    .    .    .    .    .     image search  r    median  rank    .     .     .     .     .     .     .    .     .    .     .     .     .    .     .    .     .   na    .    .     .    .     .    .     .    .     .    .     .     .     .     .     .     .     .     .     .      .    .    .    .    .    .    .    .    .     mean  rank  na  na  na  na  na  na  na  na  na  na  na    .     .     .     sentence  mean  rank  na  na  na  na  na  na  na  na  na  na  na    .     .     .       .     .     .     .     .     .     .     .     .       .     .     .     .     .     .     .     .     .     table  . image annotation  image search and sentence similarity results on the flickr  k dataset. for details  see table  . the rtp  method      enjoys additional information that is not accessible to the other methods  manual annotations of bounding boxes in the  images  which were collected via crowdsourcing.    ceptual distance between these two tasks makes this result both interesting and surprising. it supports a human  development like way of training  in which visual labeling  is learned through natural language  as opposed to  e.g.  associating bounding boxes with nouns. while such training  was used in computer vision to learn related image to text  tasks  and while recently zero shot action recognition was  shown           nlp to video action recognition transfer  was never shown to be as general as presented here.    acknowledgments                                this research is supported by the intel collaborative research institute for computational intelligence  icri ci .    