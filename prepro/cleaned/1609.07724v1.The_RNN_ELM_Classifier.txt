introduction    deep learning  using convolutional neural networks with multiple layers of hidden units has in recent  years achieved human competitive or even better than human performance in image classification  tasks          at the expense of long training times and specialised hardware    . in this paper  we combine the random neural network  rnn               and the extreme learning machine   elm      in shallow and deep classifiers and compare their performance.  the rnn  the rnn is a stochastic integer state  integrate and fire system       initially developed  to model biological neurons     and extended to model soma to soma interactions     . it consists of  m interconnected neurons  each of which can receive positive  excitatory  or negative  inhibitory   signals from external sources such as sensory sources or other cells.  the rnn can be described by equations that are possible to be solved analytically. it provides  useful mathematical properties and algorithmic efficiency as seen in           the state of each neuron i is represented at a given time t by a integer ki     which can describe  the neuron s level of excitation.    each neuron i receives excitatory and inhibitory spikes in the form of independent poisson pro   cesses of rate     i and  i . a neuron when excited  i.e. ki      can fire after a delay characterised  by exponential distribution whose average value      depends on the specific neuron.  i    a neuron j which fires  sends an excitatory or inhibitory spike to a neuron i with probability                 p   ji   pji . we write wji   rj pji and wji   rj pji    the state of the system is the joint probability distribution p k  t    p rob k   t   ...kn  t      k    ..  kn    and it satisfies a coupled system of chapman kolmogorov equations    the rnn has a  product form  solution      meaning that in steady state  the joint probability  distribution of network state is equal to the product of marginal probabilities  where the marginal    lim p r ki  t    ki     qiki      qi      t              and  qi          i    pn       j   qj wij  p  n     ri       i  j   qj wji           the rnn was initially developed to model biological neurons     and has been used for landmine  detection          video and image processing              combinatorial optimisation       network  routing    and emergency management citeemergency.  the elm  the extreme learning machine      is a single layer feedforward network  slfn   with one layer of hidden neurons. input weights w  to the hidden neurons are assigned randomly  in the range       and never changed while the output weights w  are estimated in one step by  observing that its output is calculated as in eq.   where   is the hidden neuron activation function.  then   w      w  x   y    y   w    w  x               where a least squares fit to y is calculated and     is the moore penrose pseudo inverse.  elms have been shown to achieve very good classification results and with their one step weight  estimation procedure  achieve very fast learning times. however  elms tend to produce good results  when very large numbers of hidden neurons are used thus reducing their computational complexity  advantage since the computation time is dominated by the calculation of the pseudo inverse of a  very large matrix.         the rnn elm and the pca rnn elm    rnn elm  inspired by the fact that in mammalian brains  among other communication mechanisms  cells exhibit a quasi simultaneous firing pattern through soma to soma interactions      in       an extension of the rnn was presented. a special network was considered that contained n  identical connected neurons  each having a firing rate r and external excitatory and inhibitory spikes  are denoted by    and    . the state of each neuron was denoted by q and each neuron receives  an inhibitory input from some external neuron u which is not part of the cluster  thus any cell i     inside the cluster has an inhibitory weight wu    wu i      from the external neuron u to i. also the        internal spiking rate weights were set to zero wi j   wi j     . whenever one of the neurons fires it  triggers the firing of the others with p i  j    np and q i  j       p   n . in this way instead of exiting  or inhibiting other neurons in the cluster through spikes  the packed neurons excite each other and  provoke firing through soma to soma interactions. the result that      has reached is          q     rq n      p   n qp n       r        qu wu       rqp n       n   qp n                which is a second degree polynomial in q that can be solved for its positive root which is the only  of interest since q is a probability.  q   p n           qu wu      q n      r     p       p    n      r        qu wu                  from the standard method of solving quadratic equation we can define the activation function of  the cth cluster as    bc   c  x         pc  n          c   x     q  bc      pc  n           x n  c     rc    c     x    pc  n       c     x            where   x     u  x       wu c  qu           u      the rnn elm therefore is defined as an elm using equations     and     as the activation function  of the hidden neurons.  an update rule for elm output  in      and      to achieve better accuracies in classification tasks  an update rule was introduced. instead of updating the elm output weights based on the desired  output  the desired output itself was updated and the weights were updated via the moore penrose  pseudo inverse.  t  denoting the labels of the dataset as l    l  l  ...ld   and the desired output as a d   k matrix    y    yd k   where the ld th element in  yd   ...y d k  is initially   while the rest of the are set to  . then  the hidden layer output is then h     xw       where w     denotes the randomly generated input  weights while let w     which is determined by  w       h   y           then the output of the elm is o   hw     .  the rule dictates an iterative approach to adjust y based on the output o using the negative  log likelihood function at the cost function   eo d ld       fd    ln  pk  o d k   k   e            then taking the partial derivative           pm o d       k   o k d    fd  e  k            o d  k    pk o d   k   o k d   e  k       if k   ld  if k    ld            then   y  i     d  k    o i   d  k    s     fd   o i   d  k             where o i  denotes the output after the i th iteration based on y  i  and s     is the step size chosen  by the user.     the pca rnn elm algorithm  the pca algorithm is using singular value decomposition  svd    in that we decompose the input x and its covariance matrix as    x   u vt    c           xxt   u   ut  n  n                where u is an n   n matrix    is a n   m matrix and v is a m   m matrix. comparing  the factorisation of x with that of c we conclude that the right vectors u are equivalent to the  eigenvectors of xxt . so the transformed data are denoted as y and after selecting only the the  eigenvectors corresponding to the m largest eigenvalues the data are denoted as ym . both can be  expressed as  y   xv   u     ym   um  m   xvm                based on the above the complete pca elm algorithm with nh hidden neurons  vm the matrix  of the first m principal components and training set x is   x m   xvm    w    rand nh   m                 h     w  x m      w    h  t                where t is the matrix of target labels. finally repeat for i iterations starting with o   hw   di       fd   o i   d  k     y  i   d  k    o i     d  k    sdi  w i   h  yi                      on the testing set z the algorithm executed is   z m   zvm    y   w    w  z m                  when using the iterative output adaptation method  one must be careful to avoid model overfitting. the training accuracy rapidly converges to      but the training accuracy starts decreasing.  in our simulations we limit the training accuracy to   .   for mnist and     for norb dataset.         simulation results    before we present our simulation results we must note that most algorithms achieving better performance are using image pre processing  e.g.       or require much larger computational resources   e.g.               .  rnn elm vs. elm  the first experiment run was to compare the accuracy of the elm with  various activation functions with the rnn elm using the activation function of eq. . in      an  elm structure              was used to achieve     testing accuracy with a sigmoid activation  function. in contrast  the rnn elm can achieve the same level of performance with      output  neurons. table   provides a more detailed comparison using      hidden neurons using various  activation functions and the activation function of eq.   rnn . these results clearly demonstrate  that the use of the rnn activation function leads to much better accuracies with a small increase  in computational time.     activation function training time testing time training accuracy testing accuracy  sigmoid                 .       .     sine                 .       .     hard limit                 .      .     triangular basis                .      .    radial basis                 .       .     rnn                 .       .       table    activation function comparison without pca with      hidden neurons    fig.    pca rnn performance for variable pcs and hidden neurons    pca rnn elm  the next experiment run was to test the performance of the pca elm varying  the number of principal components and the number of hidden neurons in the elm. in our testing  we used this process with    iterations and a step size of s     getting equivalent results but using  less neurons as seen in figure fig.  .. also its important to note that the time needed to achieve     iterations of the simulation seems to be constant and independent of the number of neurons  used  in contrast to the method used in      where we observe an increase in the time needed as  the number of neurons increases  as seen in table  .  pca rnn elm vs. autoencoder elm  we compared pca rnn elm with autoencoder elm      using the same number of pcs and autoencoder neurons while varying the elm size. essentially  this compares the performance of the elm given the dimensionality reduction obtained by the two  methods. figure   and table   show that the performance of the two methods is roughly equivalent  for any elm size. the pca elm enjoys a slight advantage in testing time while the accuracy is  essentially the same. it must be noted that due to the randomness of the elm weights  results vary  from run to run and differences beyond the second decimal point should be ignored.  norb with pca rnn elm  in testing the pca elm with the norb dataset the input data were  presented to the algorithms by concatenating binocular images of the same object. therefore  each     x training accuracy testing accuracy total time         .        .       .             .       .       .             .       .       .             .        .       .              .        .       .        table    mnist simulation results  classifier of      with         x structure    pair of images had      features  pixel values   similarly to     . we ran two sets of experiments      setting elm hidden neurons to     and varying the number of pcs used  table    and    keeping  the number of pcs fixed and varying the number of elm hidden neurons . finally we ran the deep  rnn elm network of      and obtained a training time of   .  s  training accuracy   .     testing  time  .   and testing accuracy   .   . all results are averages of    trials to minimise the effect of  the randomised initialisation of the input weights in the elm. we observed also that the training  times are faster than the deep autoencoder rnn elm while producing better testing accuracy  by    at best. the pca rnn elm always enjoys an advantage in testing time. given that the  pca rnn elm training and testing times are quite fast  it is possible to run the algorithm on the  same data multiple times and select the parameters  random input weights  that produce the best  results.  principal components training accuracy testing accuracy                                                                                                                                                         table    norb simulation results  pca rnn elm with output adaptation         conclusion    in this paper we compared the rnn elm to the elm with various activation functions and observed that the rnn elm achieves far superior results with far fewer hidden neurons. we also  demonstrated that the rnn elm network with pca preprocessing is a viable alternative to other  image classification algorithms by comparing three versions of the rnn elm network and a deep  rnn architecture on the standard mnist and norb datasets. the results were obtained without  any prior feature extraction or image processing apart from the pca algorithm in order to concentrate on the raw performance of the algorithms tested. we observed that the relatively simple  pca rnn elm can provide high accuracy and very fast training and testing times while the deep  autoencoder elm algorithm can achieve similar results on the mnist dataset.     