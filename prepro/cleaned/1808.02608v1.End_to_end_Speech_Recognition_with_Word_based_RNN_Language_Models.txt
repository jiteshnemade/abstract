introduction  automatic speech recognition  asr  is currently a mature set of  widely deployed technologies that enable successful user interface  applications such as voice search    . however  current systems  lean heavily on the scaffolding of complicated legacy architectures that grew up around traditional techniques  including hidden  markov models  hmms   gaussian mixture models  gmms   hybrid hmm deep neural network  dnn  systems  and sequence  discriminative training methods    . these systems also require  hand made pronunciation dictionaries based on linguistic assumptions  extra training steps to derive context dependent phonetic  models  and text preprocessing such as tokenization for languages  without explicit word boundaries. consequently  it is quite difficult for non experts to develop asr systems for new applications   especially for new languages.  end to end asr has the goal of simplifying the above modulebased architecture into a single network architecture within a deep  learning framework  in order to address these issues. end to end  asr methods typically rely only on paired acoustic and language  data without linguistic knowledge  and train the model with a single  algorithm. therefore  the approach makes it feasible to build asr  systems without expert knowledge. there are several types of endto end architecture for asr such as connectionist temporal classi     fication  ctc       recurrent neural network  rnn  transducer       attention based encoder decoder      and their hybrid models       .  recently  the use of external language models has shown significant improvement of accuracy in neural machine translation      and end to end asr        . this approach is called shallow fusion  where the decoder network is combined with an external language model in log probability domain for decoding. in our previous  work      we have shown the impact of recurrent neural network language models  rnn lms  in japanese and mandarin chinese tasks   reaching a comparable or higher accuracy to those of state of the art  dnn hmm systems. since the japanese and chinese systems were  designed to output character sequences  the rnn lm was also designed as a character based lm  and effectively combined with the  decoder network to jointly predict the next character.  a character based architecture achieves high accuracy asr for  languages with a large set of characters such as japanese and chinese. it also enables open vocabulary asr  in contrast to word based  architectures  which suffer from the out of vocabulary  oov  problem. however  the character based lms generally under perform  relative to word lms for languages with a phonogram alphabet  using fewer distinct characters  such as english  because of the  difficulty of modeling linguistic constraints across long sequences  of characters. actually  english sentences are much longer than  japanese and chinese sentences in the length of character sequence.  to overcome this problem  we have further extended end to end  asr decoding with lms at both the character and word levels      . during the beam search decoding  hypotheses are first scored  with the character based lm until a word boundary is encountered.  known words are then re scored using the word based lm  while  the character based lm provides for out of vocabulary scores. this  approach exploits the benefits of both character and word level architectures  and enables high accuracy open vocabulary end to end  asr.  more specifically  the character based lm yields the following  benefits in the decoding process with the word based lm    . character based lm can help correct hypotheses survive until  they are rescored at word boundaries during the beam search.  before the hypothesis reaches the boundary  the identity of  the last word is unknown and its word probability cannot be  applied. hence  good character level prediction is important  to avoid pruning errors for hypotheses within a word.   . character based lm can predict character sequences even for  oov words not included in the vocabulary of the word based  lm. since the word based lm basically cannot predict unseen character sequences  good character level prediction is  important for open vocabulary asr.  however  the multi level lm approach has a problem that it requires two different rnn lms. to build the two lms  we need to           joint  decoder    ctc    cl      cl          attention decoder    rnn lm    blstm    shared  encoder    deep cnn  vgg net     x           xt          xt    fig.  . hybrid attention ctc network with lm extension  the  shared encoder contains a vgg net followed by blstm layers and  trained by both ctc and attention model objectives simultaneously.  the joint decoder predicts an output label sequence by the ctc  attention decoder and rnn lm.    take additional time and effort  almost twice of them  for training  the models. moreover  the two lms also increase the computational  cost and memory usage for decoding. inherently  rnn lms need  a lot of computation for training and decoding compared with conventional n  gram lms. in addition  text corpora for training lms  are usually much larger than paired acoustic and text data for training end to end asr models. considering this situation  solving the  above problem is crucial for better end to end asr.  in this paper  we propose a novel strategy for language modeling  and decoding in end to end asr to solve the problem. the proposed  method allows us to decode with only a word based rnn lm in  addition to the encoder decoder  leading a competitive accuracy and  less computation in the decoding process compared to the multi level  lm approach. this method employs look ahead word probabilities  to predict next characters instead of the character based lm. although our approach is similar to old fashioned lexical tree search algorithms including language model look ahead           it provides  an efficient way of dynamically computing the look ahead probabilities for end to end asr with a word based rnn lm  which does  not exist in the prior work. we demonstrate the efficacy of the proposed lms on standard wall street journal  wsj  and librispeech  tasks.   . end to end asr architecture  this section explains the hybrid ctc attention network        we  used for evaluating the proposed language modeling and decoding  approach. but the proposed lms can also be applied to standard  attention based encoder decoders for asr.   . . network architecture  figure   shows the latest architecture of the ctc attention network     . the encoder has deep convolutional neural network  cnn   layers with the vgg net architecture       which are followed by  stacked bidirectional long short term memory  blstm  layers. the  decoder network has a ctc network  an attention decoder network   and an rnn lm  which jointly predict the next label. given input    sequence x   x    . . .   xt   the encoder network accepts x and outputs hidden vector sequence h   h    . . .   ht     where t     t    by  using two max pooling steps in the deep cnn. the decoder network  iteratively predicts a single label cl based on the hidden vectors h  and the label context c    . . .   cl     and generates l length label sequence c    cl   u l              l   where u is a set of labels. in  this work  we assume u is a set of distinct characters or alphabet of  the target language.  the hybrid attention ctc network utilizes both benefits of ctc  and attention during training and decoding by sharing the same  cnn  blstm encoder with ctc and attention decoder networks  and training them jointly. unlike the solitary attention model  the  forward backward algorithm of ctc can enforce monotonic alignment between speech and label sequences during training. that  is  rather than solely depending on the data driven attention mechanism to estimate the desired alignments in long sequences  the  forward backward algorithm in ctc helps to speed up the process  of estimating the desired alignment. the objective to be maximized  is a logarithmic linear combination of the ctc and attention based  posterior probabilities pctc  c x  and patt  c x    lmtl     log pctc  c x            log patt  c x             with a tunable parameter              .   . . decoding with external language models  the inference step of ctc attention based speech recognition is performed by output label synchronous decoding with a beam search.  although the decoding algorithm is basically the same as the method  for standard attention based encoder decoders  it also considers the  ctc and lm probabilities to find a better hypothesis. the decoder  finds the most probable character sequence c  given speech input x   according to  c    arg max     log pctc  c x            log patt  c x   c u       log plm  c               where lm probability plm  c  is added with scaling factor   to the  ctc attention probability in the log probability domain.  in the beam search process  the decoder computes a score of  each partial hypothesis  which is defined as the log probability of  the hypothesized character sequence. the joint score   g  of each  partial hypothesis h is computed by    h      ctc  h            att  h      lm  h             where  ctc  h    att  h   and  lm  h  are ctc  attention  and lm  scores  respectively.  with the attention model  score  att  h  can be obtained recursively as   att  h     att  g    log patt  c g  x             where g is an existing partial hypothesis  and c is a character label  appended to g to generate h  i.e.  h   g   c. the score for h is obtained as the addition of the original score  att  g  and the conditional  log probability given by the attention decoder. lm score  lm  h  is  also obtained similarly to the attention model as   lm  h     lm  g    log plm  c g .           on the other hand  ctc score  ctc  h  is obtained differently  from the other scores  where we compute the ctc prefix probability          defined as the cumulative probability of all label sequences that  have h as their prefix   x    p h  . . .  x       p  h     x                u    eos        and use it as the ctc score    ctc  h    log p h  . . .  x             where   represents all possible label sequences except the empty  string  and  eos  indicates the end of sentence.  during the beam search  the number of partial hypotheses for  each length is limited to a predefined number  called a beam width   to exclude hypotheses with relatively low scores  which dramatically  improves the search efficiency.     . . multi level rnn lm  the multi level rnn lm contains character level and word level  rnn lms  but it can be implemented as a function that performs  character level prediction. let v be the vocabulary of the word level  rnn lm and be including an abstract symbol of oov word such as   unk . we compute the conditional character probabilities with     p   wg   g       if c   s  wg   v    pwlm  clm  wg   g    plm  c g          p    unk            if  c   s  wg    v  wlm  g       p  c g   otherwise  clm  where s denotes a set of labels that indicate the end of word  i.e.   s     space    eos    wg is the last word of the character sequence g  and  g is the word level history  which is the word sequence corresponding to g excluding wg . for the above example  g   wg   and  g are set as  g   a   space   c  a  t   space   e  a  t  s  wg   eats   g   a  cat.     . incorporating word based rnn lms  in this section  we explain the basic approach to incorporate wordbased lms into a character based end to end asr  and present two  word based rnn lms  one is a multi level lm we have already  proposed and the other is a look ahead word lm we propose in this  paper.   . . basic approach  in most end to end asr systems  a finite lexicon and an n  gram  language model are compiled into a weighted finite state transducer  wfst   and used for decoding         . the wfst framework efficiently handles frame synchronous or label synchronous  decoding with the optimized search network and reduces the word  error rate         . however  this approach is not suitable for rnnlms because an rnn lm cannot be represented as a static state  network.  in this paper  we extend the character based decoding to enable  open vocabulary end to end asr with a word based rnn lm. we  consider that the character based systems can predict space characters between words as well as letters within the word. note that the  space character has an actual character code  which is different from  the ctc s blank symbol. with the space characters  it is possible  to deterministically map any character sequence to a word sequence   e.g.  character sequence  a  space  c a t  space  e a t s       is a scaling factor used to adjust the probabilities for oov words.  the first condition on the right hand side of eq.     is applied  when the g has reached the end of a word. in this case  the word level  probability pwlm  wg   g   is computed using the word level rnnlm. the denominator pclm  wg   g   is the probability of wg obtained  by the character level rnn lm and used to cancel the characterlevel lm probabilities accumulated for wg . the probability can be  computed as   wg      pclm  wg   g        y    pclm  wg i   g wg         wg i                i      where  wg   is the length of word wg in characters and wg i indicates  the i th character of wg . the second term  pwlm   unk   g   acts as a  weight on the character level lm and ensures that the combined language model is normalized over character sequences both at word  boundaries and in between. if wg is an oov word as in the second condition  we assume that a word level probability for the oov  word can be computed with the word and character level rnn lms  as  poov  wg   g     pwlm   unk   g  pclm  wg   unk    g  .            since the character level probability satisfies    is mapped to a unique word sequence    pclm  wg   unk    g     pclm  wg   g              pclm  wg   unk    g        g   pclm  wg   g               and  a cat eats  where  space  formally represents the space character. accordingly  only when the decoder hypothesizes a space character  it computes the probability of the last word using the word level rnn lm  and simply accumulates it to the hypothesis score. no special treatment is necessary for different types of homonyms  words with the  same spelling but different pronunciation are handled in a contextdependent way by the word language model  whereas words with the  same pronunciation but different spellings are automatically handled  as different word hypotheses in the beam search. similarly  ambiguous word segmentations are automatically handled as different decoding hypotheses.    we approximate    g        and use    as a tunable parameter. in the  second condition of eq.      character based probability pclm  wg   g    is eliminated since it is already accumulated for the hypothesis.  the third case gives the character level lm probabilities to the  hypotheses within a word. although the character level lm probabilities are canceled at the end of every known word hypothesis and  so are only used to score oov words  they serve another important role in keeping the correct word hypotheses active in the beam  search until the end of the word where the word level lm probability  is applied.      unk   a  an  and  anna  at  bit  by                              d  n  t    a    i    b    root  node    n    a    t    y               z    zip       zoo          a  vocabulary with word ids    i    p    o    o     b  prefix tree    fig.  . prefix tree representation of a vocabulary.  a  vocabulary  including word strings and id numbers.  b  prefix tree of the vocabulary  where the shaded circle indicates the root node  each white  circle is a node representing a character and each path from the  root node represents a character sequence of each word  where each  double circle node corresponds to a word end.    finally  the log probability of sentence end label  eos  is  added to the log probability of each complete hypothesis g   as    g         g      log pwlm   eos   g wg      set of succeeding nodes from ng   and   is a scaling factor for oov  word probabilities  which is a tunable parameter.  the first case of eq.      gives the word probability at a wordend node  where pwlm  wg   g   needs to be normalized by pla  ng   g    to cancel the already accumulated look ahead probabilities. the second case computes the look ahead probability when making a transition from node ng to ng c . the third case gives the oov word  probability  where character c is not accepted  which means the hypothesis is going to an oov word. the last one handles the case  that ng is null  which means that the hypothesis is already out of the  tree  and it returns   since the oov probability was already applied  in the third case. in the above procedure  we assume that whenever  the hypothesis is extended by  space  label  the new hypothesis  points the root node of the tree.  although this approach is similar to conventional asr systems  based on a prefix tree search including a lm look ahead mechanism   the look ahead probabilities needs to be computed on the fly using  the word based rnn lm unlike conventional approaches with unigram or bigram lm or weight pushing over a static wfst.  to compute the sum of probabilities in eq.       we assume that  the id numbers are assigned in alphabetical order in the vocabulary.  in this case  the id numbers should be consecutive in each set as a  property of prefix tree. accordingly  we can compute the sum using  the cumulative sums over the word probability distribution by  pla  n      s   max id n     s   min id n                         where s      denotes an array of the cumulative sums given context     which is obtained as    in the beam search process.   . . look ahead word based rnn lm  the look ahead word based rnn lm enables us to decode with  only a word based rnn lm in addition to the encoder decoder. this  model predicts next characters using a look ahead mechanism over  the word probabilities given by the word based lm  while the multilevel lm uses a character level lm until the identity of the word is  determined.  to compute look ahead probabilities efficiently  we use a prefix  tree representation as shown in fig.  . this example shows a vocabulary and its prefix tree representation. during decoding  each  hypothesis holds a link to a node  which indicates where the hypothesis is arriving in the tree. suppose a set of anticipated words at each  node has already been obtained in advance. a look ahead probability at node n can be computed as the sum of the word probabilities  of all the anticipated words as  x  pla  n       pwlm  w            w wset n     where wset n  denotes the set of anticipated words at node n  and  pwlm  w    is the original word probability given by the underlying  word based rnn lm for word level context  .  the character based lm probability with the look ahead mechanism is computed as     p  w      pla  ng   g   if ng   f  c   s       wlm g g  pla  ng c   g   pla  ng   g   if ng    null  c     ng    plm  c g     if ng    null  c      ng         pwlm   unk   g        otherwise        where f denotes a set of word end nodes  ng is the node that g has  arrived  ng c is a succeeding node of ng determined by c    ng   is a    s   i       i  x    pwlm  w k      for i      . . .    v              k      and max id n  and min id n  are the maximum and minimum id  numbers in the set of anticipated words at node n. w k  denotes the  k th word in the vocabulary. once the cumulative sums are computed right after the softmax operation  we can quickly compute the  look ahead probabilities.   . related work  there are some prior work  that incorporates word units into endto end asr. one major approach is acoustic to word ctc                where the input acoustic feature sequence is directly mapped to  the word sequence using ctc. however  this approach essentially  requires a large amount of paired acoustic and text data to learn  acoustic mapping to a large number of words. for example        used         hours of transcribed audio data to train the word ctc  model.  our approach  in contrast  is specially designed for end to end  asr using a character or subword based encoder decoder and an  external rnn language model trained with a large text corpus. thus   this architecture is more suitable for low resource languages  where  the amount of parallel data is limited but large text data are available.  subword units          are also available as an intermediate representation of character and word  where the unit set is automatically  obtained by byte pair encoding      or some chunking techniques.  however  this approach needs to select an appropriate number of  subword units using training data. increasing the number of units  will lead more acoustically expressive units but make it more difficult to train the encoder decoder using a limited amount of data. in  addition  it assumes to use the subword units for both the encoder      . . evaluation with wsj  we used the si    data set for training  the dev   data set for validation  and the eval   data set for evaluation. the data sets are  summarized in table  .  table  . wsj data sets used for evaluation    utterances length  h   training  wsj  si                  validation  dev           .   evaluation  eval           .     as input features  we used    mel scale filterbank coefficients  with pitch features and their delta and delta delta features for the  cnn blstm encoder     . our encoder network is boosted by  using deep cnn  which is discussed in section  . . we used a  layer cnn architecture based on the initial layers of the vgg like  network      followed by eight blstm layers in the encoder network. in the cnn architecture  the initial three input channels are  composed of the spectral features  delta  and delta delta features. input speech feature images are downsampled to             images  along with the time frequency axes through the two max pooling  layers. the blstm layers had     cells in each layer and direction   and the linear projection layer with     units is followed by each  blstm layer.  we used the location based attention mechanism      where the     centered convolution filters of width     were used to extract the  convolutional features. the decoder was a one layer unidirectional  lstm with     cells. we used only    distinct labels     english  letters  apostrophe  period  dash  space  noise  and sos eos tokens.  the adadelta algorithm      with gradient clipping      was  used for the optimization. we also applied a unigram label smoothing technique      to avoid over confidence predictions. in the hybrid attention ctc architecture  we used the      .  for training  and the      .  for decoding. the beam width was set to    in  decoding under all conditions. the joint ctc attention asr was  implemented by using the chainer deep learning toolkit     .  character and word based rnn lms were trained with the wsj  text corpus  which consisted of   m words from  . m sentences.  the character based lm had a single lstm layer with     cells  and a    dimensional softmax layer while the word based lm had  a single lstm layer with      cells. we trained word based rnnlms for   k    k and   k vocabularies  where the softmax layer  had   k    k or   k dimensional output in each lm. we used the  stochastic gradient descent  sgd  to optimize the rnn lms.     .        .        .        .        .        .        .        .        .        .        .         word    lm      chararacter    lm      encoder    decoder        no      lm  ar        ac  te  m  r    l  ul  m              le  ve  lo  l         ok        a  k      he  ad  m            ul        k      le  ve  lo  l       ok   k     a        he  ad  m         ul   k              le  ve  lo  l         ok        a  k      he  ad          k          we evaluate the proposed language models with the wall street journal  wsj  and librispeech corpora. wsj is a well known english  clean speech database          including approximately    hours  data while librispeech is a larger data set of read speech from audiobooks  which contains      hours of audios and transcriptions      .    ch     . experiments    table  . word error rate  wer  with different language models on  wsj.  language models  vocab. size   dev   eval    no lm    .     .   chararacter lm    .    .   word lm    k     .     .   multi level lm    k    .    .   look ahead lm    k    .    .   multi level lm    k    .    .   look ahead lm    k    .    .   multi level lm    k    .    .   look ahead lm    k    .    .     decoding     me    ra o        decoder and the language model  but the appropriate units can be  different for the encoder decoder and the language model. our approach basically employs characters and words  but it is also possible  to combine a character based encoder decoder with a subword based  lm or a subword based encoder decoder with a word based lm.    fig.  . decoding time ratio in the beam search using different language models. every elapsed time was divided by that of the no lm  case.  the first experiment evaluates the contributions of language  models. table   shows word error rate  wer  with different language models. the wers for no language model  no lm   character lm  word lm    k  and multi level lm    k  were already  reported in       where the multi level lm    k  performed the  best in our prior work. when we simply applied the word based  lm without any character level lms or look ahead mechanism  the  wer reduction was very small due to the pruning errors discussed  in introduction.  after that  we conducted experiments with the look ahead lm     k   where the wer for eval   test set increased from  .    to   .   . we analyzed the recognition results  and found that the increased errors mainly came from oov words. this could be because the look ahead lm did not use a strong character based lm  for predicting oov words. to mitigate this problem  we increased  the vocabulary size to   k and   k. then  we obtained a large improvement reaching  .    wer for the look ahead lm. the reason why the   k look ahead lm achieved lower wers than those  of multi level lms is probably that the look ahead mechanism provided better character level lm scores consistent with the word lm  probabilities  which were helpful in the beam search process.  next  we investigated the decoding time when using different  language models. figure   shows the decoding time ratio  where  each decoding time was normalized by the pure end to end decoding time without language models  i.e.  the case of no lm  where  we only used a single cpu for each decoding process   . the character lm and   k word multi level lm increased the decoding time    since the beam search based decoding was implemented in python  the  decoding speed has not been optimized sufficiently.     table  . comparison with other end to end asr systems reported  on wsj.  end to end asr systems  dev   eval    seq seq        .   ctc       .   ctc        .   seq seq        .    .   multi level lm    k         .    .   look ahead lm    k   this work    .    .   table  . librispeech data sets used for evaluation    utterances length  h   train set                dev clean          .   dev other          .   test clean          .   test other          .   by     and      respectively  while the   k word look ahead lm  increased it by    . even when we used the   k word lms  the  decoding time for the look ahead lm was still      which was less  than that of the   k multi level lm. thus  the proposed look ahead  lm has a higher accuracy to multi level lms with less decoding  time.  finally  we compare our result with other end to end systems  reported on the wsj task. table   summarizes the wer numbers  obtained from other articles and this work. since the systems in  the table have different network architectures from each other  it is  difficult to compare these numbers directly. however  we confirmed  that our system has achieved the best wer in the state of the art  systems on the wsj benchmark.   . . evaluation with librispeech  we conducted additional experiments using librispeech to examine  the performance of rnn lms including character lm  multi level  lm and look ahead word lm for a larger corpus. the data sets are  summarized in table  . all the experiments for librispeech were  performed using espnet  the end to end speech processing toolkit        and the recipe for a baseline librispeech setup with pytorch  backend     . according to the baseline recipe  we trained an   layer  blstm encoder including     cells in each layer and direction  and  the linear projection layer with     units followed by each blstm  layer. the second and third bottom lstm layers of the encoder  read every second state feature in the network below  reducing the  utterance length by a factor of four  i.e.  t   . we also used locationbased attention with a similar setting to the wsj model. the decoder  was a one layer unidirectional lstm with     cells. we also trained  different language models as prepared for wsj task  where we used  only transcription of audio data including  . m words. the both  character and word rnn lms had   lstm layers and     cells per  layer. the beam width was set to    for decoding.  table   shows word error rate  wer  with different language  models. we obtained consistent error reduction with wsj s results  in table    where the both multi level and look ahead lms provided  significant error reduction when the vocabulary size was increased  to   k. in this case  the look ahead lm had competitive wers to  the multi level lm. however  the look ahead lm still has the speed  benefit similar to the results in fig.   and the other benefit that we  can completely exclude the training process of the character lm.    table  . word error rate  wer  with different language models on  librispeech  language models  vocab. size   dev  dev  test  test  clean other clean other  no lm   .     .    .     .   character lm   .     .    .     .   multi level lm    k    .     .    .     .   look ahead lm    k    .     .    .     .   multi level lm    k    .     .    .     .   look ahead lm    k    .     .    .     .   multi level lm    k    .     .    .     .   look ahead lm    k    .     .    .     .    . conclusion  in this paper  we proposed a word based rnn language model   rnn lm  including a look ahead mechanism for end to end automatic speech recognition  asr . in our prior work  we combined character based and word based language models in hybrid  ctc attention based encoder decoder architecture. although the  lm with both the character and word levels achieves significant  error reduction  two different lms need to be trained and used  for decoding  which increase the computational cost and memory usage. the proposed method allows us to decode with only a  word based rnn lm  which leads competitive accuracy and less  computation in the beam search process compared to the multi level  lm approach. furthermore  it can completely exclude the training  process for the character level lm. we have shown the efficacy  of the proposed method on standard wall street journal  wsj  and  librispeech tasks in terms of computational cost and recognition accuracy. finally  we demonstrated that the proposed method achieved   .   wer for wsj eval    test set when the vocabulary size was  increased  which is the best wer reported for end to end asr  systems on this benchmark.   . 