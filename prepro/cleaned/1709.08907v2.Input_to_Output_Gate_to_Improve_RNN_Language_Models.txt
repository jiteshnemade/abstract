introduction    a neural language model is a central technology of recently developed neural architectures in  the natural language processing  nlp  field. for  example  neural encoder decoder models  which  were successfully applied to various natural language generation tasks including machine translation  sutskever et al.         summarization  rush  et al.         and dialogue  wen et al.         can  be interpreted as conditional neural language models. moreover  word embedding methods  such as  skip gram  mikolov et al.        and vlbl  mnih  and kavukcuoglu         are also originated from  neural language models that aim to handle much  larger vocabulary and data sizes. thus  language  modeling is a good benchmark task for investigating the general frameworks of neural methods in  the nlp field.  in this paper  we address improving the performance on the language modeling task. in particular   we focus on boosting the quality of existing recurrent neural network  rnn  language models. we  propose the input to output gate  iog  method      our implementation is publicly  https   github.com nttcslab nlp iog.    available    this section briefly overviews the rnn language  models. hereafter  we denote a word sequence  with length t   namely  w    ...  wt as w  t for short.  formally  a typical rnn language model computes  the joint probability of word sequence w  t by the  product of the conditional probabilities of each  timestep t     p w  t     p w       ty        p wt    w  t  .           t      p w    is generally assumed to be   in this literature  that is  p w         and thus  we can ignore  the calculation of this term  see the implementation of zaremba et al.           for example . to  estimate the conditional probability p wt    w  t     we apply rnns. let v be the vocabulary size  and  let pt   rv be the probability distribution of the  vocabulary at timestep t. moreover  let dh and de  respectively be the dimensions of the hidden state  and embedding vectors. then  the rnn language    at       https   github.com wojzaremba lstm     hyper parameter  embedding dimension dg  dropout rate  optimization method  initial learning rate  learning rate decay  max epoch    pt    proposed method   input to output gate  iog     softmax    eq.      gt  computing  gate  eq.       st   eq.       selected value            adam   .           epoch       table    hyper parameters in training iog.    ht  e t    rnn  eq.       ht      formally  let xt be a one hot vector representing  wt   iog calculates the gate gt by the following  equations     et  mapping word  to embedding  eq.       mapping word  to embedding  eq.       xt  wt    figure    overview of computing probability distribution.  models predict pt   by the following equation   pt     softmax st              st   w ht   b            ht   f  et   ht                et   ext           rv    where w    is a matrix  b    is a bias  term  e   rde  v is a word embedding matrix   xt         v is a one hot vector representing the  word at timestep t  and ht   is the hidden state at  previous timestep t    . ht at timestep t     is  defined as a zero vector  that is  h     . let f      represent an abstract function of an rnn  which  might be the elman network  elman         the  long short term memory  lstm   hochreiter  and schmidhuber         the recurrent highway  network  rhn   zilly et al.         or any other  rnn variants.                e t             eg xt .    here  wg   rv  dg is a matrix  bg   rv is a  bias term  and eg   rdg  v is a word embedding  matrix  . then  we compute the probability distribution of the rnn language model by applying the  above gate to the equation     as follows   pt     softmax gt    st              where represents the element wise multiplication  of two vectors.       rv  dh    gt     wg e t   bg        .     experiments  dataset    we conducted word level prediction experiments  on the penn treebank  ptb   marcus et al.         and wikitext    merity et al.      b  datasets.  the ptb dataset consists of    k training words     k validation words  and   k test words. the  wikitext   dataset consists of      k training  words     k validation words  and    k test words.  mikolov et al.        and merity et al.      b   respectively published pre processed ptb  and  wikitext    datasets. we used these pre processed  datasets for fair comparisons with previous studies.    input to output gate   .     in this section  we describe our proposed method   input to output gate  iog . as illustrated in figure    iog adjusts the output of an rnn language  model by the gate mechanism before computing  the probability of the next word. we expect that  iog will boost the probability of the word that may  occur. for example  a word followed by a preposition such as  of  is probably a noun. therefore   if the word at timestep t is a preposition  iog refines the output of a language model to raise the  probabilities of nouns.    training procedure    for the ptb dataset  we prepared a total of   rnn  language models as our baseline models. first  we  replicated lstm with dropout and lstm with  variational inference based dropout  which we refer to as  lstm  and  variational lstm   respectively. following zaremba et al.        and gal     we prepared different embeddings from those used in an  rnn language model.     http   www.fit.vutbr.cz  imikolov rnnlm      https   einstein.ai research the wikitext long termdependency language modeling dataset     model  lstm  medium   zaremba et al.           lstm  medium  replication of zaremba et al.            iog  proposed   lstm  large   zaremba et al.           lstm  large  replication of zaremba et al.            iog  proposed   variational lstm  medium   gal and ghahramani           variational lstm  medium  replication of gal and ghahramani            iog  proposed   variational lstm  large   gal and ghahramani           variational lstm  large  replication of gal and ghahramani            iog  proposed   variational rhn  depth     zilly et al.           variational rhn  depth    replication of zilly et al.            iog  proposed   variational rhn  depth    replication of zilly et al.           wt    iog  proposed   ensemble of   variational rhns    iog  proposed   ensemble of    variational rhns    iog  proposed   neural cache model  grave et al.           pointer sentinel lstm  medium   merity et al.      b     variational lstm  large    wt   al  inan et al.           variational rhn  depth       wt  press and wolf           neural architecture search with base    zoph and le           neural architecture search with base     wt zoph and le           neural architecture search with base     wt  zoph and le           awd lstm   wt  merity et al.      a     awd lstm   wt  result by code of merity et al.      a        iog  proposed   awd lstm   wt   cache  size          merity et al.      a     awd lstm   wt   cache  size           iog  proposed     parameters    m    m    m    m    m    m    m    m    m    m    m    m    m    m    m    m    m     m     m     m     m    m    m    m    m    m    m    m    m    m    m    m    m    m    validation    .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     test    .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     table    comparison between baseline models and the proposed method  represented as    iog   on  the penn treebank  ptb  dataset.   denotes results published in previous studies. the method with wt  shared word embeddings  e in the equation      with the weight matrix of the final layer  w in the  equation     . al denotes that the method used a previously proposed augmented loss function  inan  et al.       .  model  lstm  medium  replication of zaremba et al.            iog  proposed   variational lstm  medium  replication of gal and ghahramani            iog  proposed   variational lstm  medium    cache  size            iog  proposed   pointer sentinel lstm  merity et al.      b     neural cache model  size         grave et al.           neural cache model  size          grave et al.           awd lstm   wt  merity et al.      a     awd lstm   wt  result by code of merity et al.      a      iog  proposed   awd lstm   wt   cache  size          merity et al.      a     awd lstm   wt   cache  size            iog  proposed     parameters    m    m    m    m    m    m    m      m    m    m    m    m    m    m    m    validation     .     .     .     .     .     .     .     .     .     .     .     .     .     test    .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     table    comparison between baseline models and the proposed method  represented as    iog   on the  wikitext   dataset.   denotes results published in previous studies.     and ghahramani         we prepared the medium  setting    layer lstm with     dimensions for  each layer   and the large setting    layer lstm  with      dimensions for each layer  for each  lstm. we also replicated  variational rhn  with  a depth of   described in zilly et al.       . for  the wikitext   dataset  we prepared the medium  setting standard and variational lstms as our baselines  which are identical as those used in merity  et al.      b .  after reproducing the baselines  we incorporated  iog with those models. table   summarizes the  hyper parameters used for training the iog. during  training iog  we fixed the parameters of the rnn  language models to avoid over fitting.   .     results    we show the perplexities of the baselines and those  combined with iog for the ptb in table    and  for the wikitext   in table  . these tables  which  contain both the scores reported in the previous  studies and those obtained by our reproduced models  indicate that iog reduced the perplexity. in  other words  iog boosted the performance of the  baseline models. we emphasize that iog is not  restricted to a neural architecture of a language  model because it improved the rhn and lstm  performances.  in addition to the comparison with the baselines  table   and table   contain the scores published in previous studies. merity et al.      b   and grave et al.        proposed similar methods.  their methods  which are called  cache mechanism   or  pointer    keep multiple hidden states  at past timesteps to select words from previous sequences. inan et al.        and press and wolf         introduced a technique that shares word embeddings with the weight matrix of the final layer   represented as  wt  in table   . inan et al.         also proposed using word embeddings to augment  loss function  represented as  al  in table   . zoph  and le        adopted rnns and reinforcement  learning to automatically construct a novel rnn  architecture. we expect that iog will improve  these models since it can be combined with any  rnn language models. in fact  table   and table       in contrast to other comparisons  we used  the following implementation by the authors   https   github.com salesforce awd lstm lm     the number of parameters is different from the one described in merity et al.      b . we guess that they do not  consider the increase of the vocabulary size.    demonstrate that iog enhanced the performance  even when the rnn language model was combined  with  wt  or the cache mechanism.  table   also shows the scores in the ensemble settings. model ensemble techniques are widely used  for further improving the performance of neural  networks. in this experiment  we employed a simple ensemble technique  using the average of the  output probability distributions from each model as  output. we computed the probability distribution  pt   on the ensemble of the m models as follows   pt      m    x     m pt      m           m      where m pt   represents the probability distribution predicted by the m th model. in the ensemble  setting  we applied only one iog to the multiple  models. in other words  we used the same iog  for computing the probability distributions of each  language model  namely  computing the equation     . table   describes that   and    model ensemble of variational rhns outperformed the single  model by more than   in perplexity. table   shows  that iog reduced the perplexity of the ensemble  models. remarkably  even though the    variational rhn ensemble achieved the state of the art  performance on the ptb dataset  iog improved  the performance by about   in perplexity  .  in addition  as additional experiments  we incorporated iog with the latest method  which was  proposed after the submission deadline of ijcnlp      . merity et al.      a  introduced various  regularization and optimization techniques such  as dropconnect  wan et al.        and averaged  stochastic gradient descent  polyak and juditsky         to the lstm language model. they called  their approach awd lstm  which is an abbreviation of averaged stochastic gradient descent weightdropped lstm. table   and table   indicate the  results on the ptb and the wikitext   respectively.  these tables show that iog was not effective to  awd lstm. perhaps  the reason is that the perplexity of awd lstm is close to the best performance of the simple lstm architecture. we  also note that iog did not have any harmful effect  on the language models because it maintained the  performances of awd lstm with  wt  and the     this result was the state of the art score at the submission  deadline of ijcnlp       i.e.  july          but merity et al.       a  surpassed it on aug        . we mention the effect  of iog on their method in the following paragraph.     model  variational rhn  replicate   variational rhn   iog  proposed   variational rhn   iog with hidden  variational rhn   lstm gate    diff    . m    . m    test    .     .     .     .     table    comparison among architectures for computing the output gate on the ptb dataset. the  column  diff  shows increase of parameters from  iog  proposed .  cache mechanism. moreover  incorporating iog is  much easier than exploring the best regularization  and optimization methods for each rnn language  model. therefore  to improve the performance  we  recommend combining iog before searching for  the best practice.   .     discussion    although iog consists only of word embeddings  and one weight matrix  the experimental results  were surprisingly good. one might think that more  sophisticated architectures can provide further improvements. to investigate this question  we examined two additional architectures to compute the  output gate gt in the equation    .  the first one substituted the calculation of the  gate function gt by the following gt     gt      wg   ht   e t     bg               where wg    rv   dh  dg     and  ht   e t   represents  the concatenation of the hidden state ht of rhn  and embeddings e t used in iog. we refer to this  architecture as    iog with hidden .  the second one similarly substituted gt by the  following gt      gt       wg h t   bg               h t   f    e t   h t                 where f       is the   layer lstm in our experiments.  we set the dimension of the lstm hidden state  to      that is  dg        and the other hyperparameters remained as described in section  . .  we refer to the second one as    lstm gate .  table   shows the results of the above two architectures on the ptb dataset. iog clearly outperformed the other more sophisticated architectures.  this fact suggests that     incorporating additional  architectures does not always improve the performance  and     not always become better even if it  is a sophisticated architecture. we need to carefully    input word  of  in  go  attention  whether    top   weighted words  security  columbia  steel  irs  thrift  columbia  ford  order  labor  east  after  through  back  on  ahead  was  than  heosi  from  to  to  she  estimates  i  ual    table    top   weighted words for each input word  on the ptb experiment.  design an architecture that can provide complementary  or orthogonal  information to the baseline  rnns.  in addition  to investigate the mechanism of iog   we selected particular words  and listed the top    weighted words given each selected word as input in table    . iog gave high weights to nouns  when the input word was a preposition   of  and   in . moreover  iog encouraged outputting phrasal  verbs such as  go after . these observations generally match human intuition.         conclusion    we proposed input to output gate  iog   which  refines the output of an rnn language model by the  gate mechanism. iog can be incorporated in any  rnn language models due to its simple structure.  in fact  our experimental results demonstrated that  iog improved the performance of several different  settings of rnn language models. furthermore   the experimental results indicate that iog can be  used with other techniques such as ensemble.    