introduction    a recurrent neural network  rnn  is a specific type of neural network which is able to process input  and output sequences of variable length and therefore well suitable for sequence modeling. various  rnn architectures have been proposed in recent years based on different forms of non linearity   such as the gated recurrent unit  gru      and long short term memory  lstm     . they have  enabled new levels of performance in many tasks including speech recognition        and machine  translation           .  compared to standard feed forward networks  rnns often take longer to train and are more demanding in memory and computational power. thus  it is of great importance to accelerate computation  and reduce training time of such networks.  previous work showed the successful application of stochastic rounding strategies on feed forward  networks  including binarization     and ternarization      of weights of vanilla deep neural networks  dnns  and convolutional neural networks  cnns        and in     even the quantization of  their activations during training and run time. quantization of rnn weights has so far only been  used with pretrained models     .  in this paper  a condensed and updated version of       we use different methods to reduce the  numerical precision of weights in rnns and test their performance on different benchmark datasets.  we use two popular rnn models  vanilla rnns  and grus. section   covers the three ways of  obtaining low precision weights for the rnn models in this work  and section   elaborates on the  test results of the low precision rnn models on different datasets. in      we additionally provide     results and a possible explanation why binarization of rnns does not work. we make the code for  the rounding methods available.           rounding network weights    this work evaluates the use of   different rounding methods on the weights of two types of rnns.  these methods include the stochastic and deterministic ternarization method  ternaryconnect         the pow  ternarization method       and exponential quantization     . for all   methods  we keep a  full precision copy of the weights and biases during training to accumulate the small updates  while   during test time  we can either use the learned full precision weights or use their deterministic lowprecision version. as experimental results in section   show  the network with learned full precision  weights usually yields better results than a baseline network with full precision during training  due  to the extra regularization effect brought by the rounding method. the deterministic low precision  version could still yield comparable performance while drastically reducing computation and required  memory storage at test time. we will briefly describe the former   low precision methods  and  elaborate on a additional method called exponential quantization we introduced in     .   .     ternarization and pow  ternarization    ternaryconnect was first introduced in     . by limiting the weights to only   possible values  i.e.         or     this method does not require the use of multiplications. in the stochastic version of the  method  the low precision weights are obtained by stochastic sampling  while in the deterministic  version  the weights are obtained by thresholding.  ternaryconnect allows weights to be set to zero. formally  the stochastic form can be expressed as  wtern   sign w   binom p   clip abs  w                 where is an element wise multiplication  binom p  a function that draws samples from a binomial  distribution  and clip x  a  b  clips values outside a given interval to the interval edges . in the  deterministic form  the weights are quantized depending on   thresholds          if  w    .       if  w     .   wtern           otherwise  pow  ternarization is another fixed point oriented rounding method introduced in     . the precision  of fixed point numbers is described by the qm.f notation  where m denotes the number of integer bits  including the sign bit  and f the number of fractional bits. for example  q .  allows    .       .   as  values. the rounding procedure works as follows  we first round the values to be in range allowed by  the number of integer bits        m where  w     m  w  where   m   w    m  wclip          m where  w    m  we subsequently round the fractional part of the values   wp t   round  f   wclip       f   .            exponential quantization    quantizing the weight values to an integer power of   is also a way of storing weights in low precision  and eliminating multiplications. since quantization does not require a hard clipping of weight values   it scales well with weight values.  similar to the methods introduced above  we also have a deterministic and stochastic way of quantization. for the stochastic quantization  we sample the logarithm of weight value to be its nearest    integers  with the probability of getting one of them being proportional to the distance of the weight  value from that integer. for weights with negative weight values we take the logarithm of its absolute  vale  but add their sign back after quantization. i.e.       w    dlog   w  e with probability p    blog         w  c  log   wb           blog   w  c with probability    p       https   github.com ottj quantizedrnn           a      b     figure    training and test set performance of vanilla rnns learning on ptb  a  and text   b   datasets.  for the deterministic version  we set log  wb   dlog  w e if the p in eq.   is larger than  . .  note that we just need to store the logarithm of quantized weight values. the actual instruction  needed for multiplying a quantized number differs according to the numerical format. for fixed point  representation  multiplying by a quantized value is equivalent to binary shifts  while for floating  point representation it is equivalent to adding the quantized number s exponent to the exponent of the  floating point number. in either case  no complex operation like multiplication is needed.         experimental results and discussion    in the following experiments  we test the effectiveness of the different rounding methods on two  different types of applications  character level language modeling and speech recognition.   .     vanilla rnn    we validate the low precision vanilla rnn on   datasets  text  and penn treebank corpus  ptb .  the text  dataset contains the first    m characters from wikipedia  excluding all punctuations. it  does not discriminate between cases  so its alphabet has only    different characters  the    english  characters and space. we take the first   m characters as training set and split them equally into  sequences with    character length each. the last   m characters are split equally to form validation  and test sets.  the penn treebank corpus       ptc  contains    different characters  including english characters  numbers  and punctuations. we follow the settings in      to split our dataset  i.e.      k  characters for training set  as well as    k and    k characters for validation and test set  respectively.  model and training the models are built to predict the next character given the previous ones   and performances are evaluated with the bits per character  bpc  metric  which is log  of the  perplexity  or the per character log likelihood  base   . we use a rnn with relu activation and       hidden units. we initialize hidden to hidden weights as identity matrices  while input to hidden  and hidden to output matrices are initialized with uniform noise.  we can see the regularization effect of stochastic quantization from the results of the two datasets. in  the ptb dataset  where the model size slightly overfits the dataset  the low precision model trained  with stochastic quantization yields a test set performance of  .    bpc that surpasses its full precision  baseline   .    bpc  by around  .    bpc  fig.    left . from the figure   we can see that stochastic  quantization does not significantly hurt training speed and manages to get better generalization when  the baseline model begins to overfit. on the other hand  we can also see  from the results on the  text  dataset where the same sized model now underfits that the low precision model performs worse    .    bpc  than its baseline   .    bpc .  fig.  b and table   .  table    results from vanilla rnns  vrnns  with exponential quantization.  dataset rnn type baseline  exponential quantization  text   vrnn   .    bpc  .    bpc  ptc  vrnn   .    bpc  .    bpc          test accuracy           baseline  full precision   pow  quantization q .   stochastic ternarization t  .           deterministic ternarization t  .   stochastic exponential quantization      .        .        .                   test accuracy          .      .                                                                                    training epoch       a      b     figure    gru models trained on the tidigits speech recognition task.  a  effect of rounding  methods applied on gru weights and biases during training compared against baseline  blue . thick  curves show the mean of    runs  half transparent areas around a curve show the variance.  b  mean  final test accuracy of    runs per method. exponential quantization applied on gru weights and  biases leads to higher than baseline final accuracy.   .     gru rnns    this section presents results from the various methods to limit numerical precision in the weights and  biases of gru rnns which are then tested on the tidigits dataset.  dataset tidigits     is a speech dataset consisting of clean speech of spoken numbers from      speakers. we only use single digit samples  zero to nine  in our experiments  giving us      training  samples and      validation samples. the labels for the spoken  zero  and  o  are combined into one  label  hence we have    possible labels. we create mfccs from the raw waveform and do leading  zero padding to get samples of matrix size   x   . the mfcc data is further whitened before use.  model and training the model has a     unit gru layer followed by a     unit fully connected  relu layer. the output is a    unit softmax layer. weights are initialized using the glorot   bengio  method    . the network is trained using adam       and batchnorm      is not used. we train our  model for up to     epochs with a patience setting of      no early stopping before epoch     . gru  results in figure   are from    experiments  with each experiment starting with a different random  seed. this is done because previous experiments have shown that different random seeds can lead up  to a few percent difference in final accuracy. we show average and maximum achieved performance  on the validation set.  effect of rounding methods to assess the impact of weight quantization  we trained our model  and applied the rounding methods to all gru weights and biases during training. figure    a  shows  how rounding applied to gru weights and biases has an effect on convergence compared to the  full precision baseline. figure    b  shows the mean final accuracy compared to baseline. these  results show that limiting the numerical precision of gru weights and biases using exponential  quantization is beneficial for learning in this setup  the mean final accuracy is higher than baseline   while the convergence is as fast as baseline.         conclusion and outlook    this paper shows how low precision quantization of weights can be performed effectively for rnns.  we presented   existing methods of limiting the numerical precision  and used them on two major  rnn types and determined how the limited numerical precision affects network performance across    datasets. in the language modeling task  the low precision model surpasses its full precision baseline  by a large gap   .    bpc  on the ptb dataset. we also show that the model will work better if  put in a slightly overfitting setting  so that the regularization effect of stochastic quantization will        begin to function. in the speech recognition task  we show that with stochastic ternarization we can  achieve baseline results  and with exponential quantization even surpass the full precision baseline.  the successful outcome of these experiments means that lower resource requirements are needed for  custom implementations of rnn models.         acknowledgments    we are grateful to ini members danny neil  stefan braun  and enea ceolini  and mila members  philemon brakel  mohammad pezeshki  and matthieu courbariaux  for useful discussions and help  with data preparation. we thank the developers of theano      lasagne  keras  blocks      and  kaldi    .  the authors acknowledge partial funding from the samsung advanced institute of technology   university of zurich  nserc  cifar and canada research chairs.    