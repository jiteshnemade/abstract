introduction    music composition is considered creative  intuitive and therefore inherently human. nevertheless  it has a long history of mathematical approaches since hiller  and isaacson proposed to use markov chains for automatic composition    . the  field of automatic composition includes a wide range of tasks such as the composition of melody  chord  rhythm       and even lyrics      i.e. every typical  components of music  and has been subject to numerous research studies. there  are many applications for automatic composition too  automatic background  music generation  ai assisted composition systems and improviser software  for  example.  music can be represented as a sequence of events and thus it can be modelled  as conditional probabilities between musical events. for example  in harmonic  tracks  some chords are more likely to occur than others given the previous  chords  while the whole chord progressions often depend on the global key of the  music. in many automatic composition systems  these relationships are simplified  by assuming that the probability of the current state p n  only depends on the  probabilities of the states in the past p n   k ...p n     . a sequence of musical  events   notes  chords  rhythm patterns   is generated by predicting the following  event given a seed sequence.            this paper has been supported by epsrc grant ep l          fusing audio and  semantic technologies for intelligent music production and consumption.  http   jukedeck.com  http   arpegemusic.com  band in a box  pg music inc.          keunwoo choi  george fazekas  and mark sandler    hidden markov models  hmms  are one of the most popular methods to  model and predict sequences. hmms are based on the assumption of k       markov assumption  given the sequence of the hidden states which determine  the visible states. choral harmonisation is generated after learning chorales by  bach using a hmm in      where     and     chorales are used for training and  testing  respectively. in       chord progressions are generated to accompany a  melody to help non musicians to create music using a hmm. the training set of  the hmm consists of     lead sheets including pop  rock  r b  jazz  and country music. in the prediction  the system generates chords using a       chord  transition probability matrix. in practice  hmms had been the most suitable for  time series modelling given the data  computing power  and feasible optimisation strategies. one of the drawbacks of hmms  however  is the inefficiency of    of k scheme of its hidden states. the memory of hmm is limited to log   n    bits when there is n hidden states  which requires to learn n   parameters for  the transition matrix.  recurrent neural networks  rnns  allow for incorporating long term dependency in the model. jordan net      a simple version of rnns  is used in      to  generate chord sequences. in       melodies were generated by a system named  concert  which is trained on sets of    bach pieces to generate melodies by  note wise prediction. one ability concert lacks is to learn the global structure  this may be due to the difficulty of training an rnns. theoretically  it  can remember infinitely long sequences  although in practice it is limited by the  vanishing gradient problem    . during the training of back propagation through  time  the gradient is extremely diminished by multiplications of sigmoid operations.  lstm  long short term memory  units solved this vanishing gradient problem    . lstm allows the gradient to be flowed by a separate path with not  multiplication but addition operations. lstm is adopted in     to learn    bar  blues chords progressions and melodies.      focuses on the generation of percussive tracks using lstm network. the network in      directly analyses audio  content of drum tracks and learns features using lstm.  in this paper  we introduce applications of character  and word based rnns  with lstm units for the automatic generation of jazz chord progressions and  rock music drum tracks. our work is differentiated from previous works by two  aspects. first  the lstm networks we use are designed to learn from text data  rather than representations of musical symbols or numeric values. directly using  text data minimises the overall design procedures for the encoding decoding  scheme and the network. second  compared to the previous research                the lstm networks is trained using a large dataset  which enables itself to learn  more complex relationship between the chords in a large set.  in the section    we introduce character based rnns and the proposed architecture. in sections   and    two case studies on the applications of rnns  to automatic composition are explained   for jazz chord progressions and rock  music drum tracks. we conclude the work in section  .     text based lstm networks for automatic music composition        .          the architecture  character based rnns    char rnns are rnns with character based learning       which is different from  the conventional approach of word based learning. when applied to the texts of  chords  a char rnn predict a vector that corresponds to a character  e.g. predict  a based on c m  and predict j based on c ma   while a word rnn predicts a  vector  which corresponds to a unique chord  e.g. c maj based on g maj  . using  char rnns in this work has two merits.  first  it is based on the minimal assumption   there is no constraint on the  form of the text representation of music. it is worth inspecting if rnns can learn  musical information with such a weak assumption.  second  fewer number of characters means fewer number of states  which  results in reducing the computational cost. from a linguistics point of view  sequence learning methods such as hmms and rnns used to model each word e.g.  chord  as a state as it is natural to find the relationships between words. one  drawback of word based learning is the large number of states  or the size of  vocabulary   in natural language processing tasks  the vocabulary size easily exceeds few thousands to even few millions. in the proposed method the size of  the chord vocabulary is      . with character based prediction  this decreases  to   .  the price of small vocabulary size is a longer sequence  as we need to learn  character by character  the model should remember a longer sequence of states.  as mentioned above  the lstm unit helps the rnns to learn this long term  dependency better. this trade off does not necessarily benefit as in section  .   .     the proposed architecture    we use two lstm layers  each of which consists of     hidden units. dropout  of  .  is added after every lstm layers     .  we use the keras deep learning framework    . during the optimisation   categorical cross entropy is used as a loss function and optimisation is performed  by adam    . this optimiser shows an equivalent final performance to stochastic  gradient descent with nestrov momentum with faster convergence.  the prediction is stochastic. in each prediction for time index n  the network  outputs the probabilities of every states. to make the system tunable  we employ  a diversity parameter   in the prediction stage  see eqn.      which suppresses          or encourages         the diversity of prediction by re weighting the  probabilities. in detail  the probabilities of i th state  pi   are re weighted as  p i   exp  log pi     . then  one of the states is selected by sampling a state  according to the re weighted probabilities.  as stated in section    we perform experiments with char  and words rnns.  we keep the same size and number of layers for both networks  although they  result in different effective lengths  for example  manifold states are needed to          keunwoo choi  george fazekas  and mark sandler  f    d min   c maj  c maj       g      f            f   f   f   f   d min   d min  g   g   c maj c maj  f   f   c maj c maj c maj  c maj    table    an example of the text representations of chord progressions in score   left  and the training data  right . a   bar chord progression is generally written  in the form on the left  where the positions of the chords loosely indicate the  chord change timings. on the right  the text show how the score on the left is  represented in the training data. here  the chords for every quarter notes are  explicitly written and bar indicators are removed.    be predicted to complete a chord in char rnns while each state correspond to  a chord in word rnns.  the dataset  code and audio files are released on web.          case study    chord progressions     .     representation    the goal of this experiment is to generate chord progressions by training an  lstm network on jazz chord progressions. here  we do not use any musical interpretation of the chords such as binary vectors to represent pitch and chords   as in      but completely rely on their text representations. table   shows an  example of a chord progression and the corresponding texts. the left is an example of a chord notation in the realbook score  where the positions of chords are  loosely related to the timings of chord changes. the score on the left is converted  into the text on the right  which specifies every chord for each quarter note.  we used        scores from the realbooks and the fakebooks as training  data. every score file was parsed from band in a box format to .xlab format.  then they were transposed to the key of c while every blank quarter note was  filled with its preceding chord as in the table  . finally  we put  start  and   end  flags  any distinctive words can be used as flags  at the beginning and  the end of each score.  although the key was transposed to c  only      out of        scores end  with c maj        followed by     g              c maj           f maj        and       scores end with the others       chords      . this is because the  the realbook chord progressions usually end with chords for a turn around to  make the progressions natural to repeat the score.       https   github.com keunwoochoi lstm real book  https   github.com keunwoochoi lstmetallica  https   soundcloud.com kchoi research sets lstm realbook      https   soundcloud.com kchoi research sets lstmetallica drums     text based lstm networks for automatic music composition         there were        unique chords in the training dataset. in other words  the  vocabulary size of word rnn was       . however there were only    characters in total  which significantly reduced the computation of char rnn. the  total numbers of chords  words  and characters were          and               respectively.   .     results    we set the system to output a chord progression for every diversity parameter    after every iteration. in this paper  we present four results from each networks   char rnns and word rnns   part of which are reported in the table  . for  simplicity  we added bar symbols   and removed repeating chords in the same  bar  e.g.   c   c   c   c     reduced to   c     and   c   c   e min e min    reduced to   c   e min  .  first  both char rnn and word rnn showed well structured results. they  learned the local structures of chords and bars after sufficient number of iterations. in the result  the majority of chords continued for multiples of four   implying a single chord for within a bar. they also learned the local relationships  between flags and chord. after one iteration  the flags are not placed properly  as in the table    a   where  end  is not followed by  start  but repeats itself.    i                                     chord progressions   .  c maj   g     ...   g     c maj  a  maj   a     a   d min  d min  d min    d hdim c hdim  c hdim     .   c hdim g   g   d min    d min  d  dim   .  c   f maj f min c maj...c maj g    c maj  c     c     f maj  f  dim   c        c        c      c        .  c      c maj   e   b     a min      a  min        a  min        ... d min   g   c maj   ... g    end   start  c maj   a   c maj     g       ...     g       c maj    .   ...c maj  end  ...  start   start  c  maj a  min a sus      .  c maj     f min  a min  d min  d min ...  start    .     c maj a min   d min  g   b     c maj   a min    d     d     d      d min    g     c maj   c     f maj   f min   c maj    c maj   g min    f maj   d min  d min  d min    g sus  b       .  g min  g min  g min  f       b  b          c      g sus  b      ... ... c min  end   start  c maj   b     table    chord progressions generated by char rnn  a  and word rnn  b .  bar symbols     are inserted for readability and repeated chords in each bar are  omitted.          keunwoo choi  george fazekas  and mark sandler    as training continues  the flags start to appear in a sequence of  end   start   c maj as in the training texts. the last chords of the score  i.e.  the chord before   end  are not always same as the first chord  c   which is also natural as they  vary in the training file.  second  after sufficient training  both results showed chord progressions that  lie in jazz grammar. examples are ii v i progressions  d min   g     c maj    passing chords  a dim   ab dim   g min    modal interchange chords  c min    db maj   and substitutions  b   as a tritone subdominant of f    in char rnn   modal interchanges  g min    circle of fifths  eb sus   gb maj    b maj    and  descending bass  c maj      b dim   a min    ab    in word rnn. the authors  noticed a subtle difference between the results from the two approaches. the  results from word rnn are more conventional progressions than those of charrnn. however  it cannot be the fundamental difference of the two approaches.  instead  it may be caused by the difference of effective lengths between charand word rnns layers   they have the same length of state sequences  but it  results in a longer chord sequence in the word rnn as mentioned in section  . .  in other words  the short memory of char rnn may result in predictions that  seem to be less constrained and stereotyped.         case  . drum tracks     .     representation    there are issues when applying lstm networks to drum tracks including finding a way to create and effective text representation. both chord progressions  and drum tracks are sequences of simultaneous events  pitches and drum components . however  drum tracks do not have a meaningful and compressive representation such as chord and it necessitate an encoding strategy of the track  into text. we also need a finer time resolution as generally there are more than  four events in a bar.  to encode simultaneous events in a track into texts  we used a binary representation of pitches  i.e.  components of drums   kick  snare  hi hats  cymbals   and tom toms. for example            and           represent kick and snare   respectively  and a simultaneous playing of kick and snare can be represented by           .  for efficient representation and learning  only nine components were allowed   kick  snare  open hi hats  closed hi hats  three tom toms  crash cymbal  and ride  cymbal.  we limited the number of events in a bar to    by quantising the drum  track by   th note.  in the experiment  we first loaded    midi files of drum tracks of metallica  and quantised them. then they were encoded into the above described binary       some of the components in the texts also represent other similar components  e.g. a  closed hi hats in the texts can mean either closed hi hats or pedalled hi hats in the  original midi file.     text based lstm networks for automatic music composition         fig.    a score of a generated drum track.    representation. we also added a flag  bar  as an annotation of the bar segments  in order to check if the networks learns the local structure.  there can be theoretically          words  but there are supposedly much  fewer words because the combinations of drum components that are played simultaneously are limited. the size of the word vocabulary in the training file is      and the file consists of           words in total.   .     results    char rnns turned out to fail to learn the drum tracks and output arbitrary    s and   s without any structures  the results have no spaces or  bar  flags .  the length of network may be too short to learn the long term relationship between characters. in char rnns  representing a single bar requires    events     characters     time steps. encoding music sequences with only two characters      and     space to for segmentation    is an extreme approach for char rnns.  in this paper  we therefore only report the result of word rnns.  figure   shows one example of our results   a part of the generated track with       .  after    iterations.  it consists of reasonable rock drum patterns    beat hi hats  combinations of kick and snare  and occasional crash cymbals and  tom toms. although there are occasional kick snare tom toms notes on back  beats  of sixteen notes   hi hats remain consistent  playing on   beat and   beat  pattern  which is very common for instance in drum tracks of metallica.   controlling   provides a way to tune the technical virtuosity of the track.  since large   increases the probabilities of occasional events  large      .    results in tracks with many fill ins with tom toms and a crash cymbal. on the  other hands  when        the track almost never contains anything but kick   snare  and hi hats. as a result  it is possible to use a combination of small and  large   in a drum track generator that is guided by user  who specifies where to  add fill ins.            the score uses the percussion clef where   refers to hi hats  notes on middle and  bottom lines refers to snare and kick  respectively.  https   soundcloud.com kchoi research           bonus for score   the score in the figure starts from    second.               keunwoo choi  george fazekas  and mark sandler    conclusion    we introduced an algorithm of text based lstm networks for automatic composition and reported results for generating chord progressions and rock drum  tracks. word rnns showed good results in both cases while char rnns only  successfully learned chord progressions. the experiments show lstm provides  a way to learn the sequence of musical events even when the data is given as  text. with the diversity parameter  the proposed algorithm can be used as a  tool that helps human composers. in the future  a more complex network with  the capability of learning interactions within music  instruments  melody lyrics   will be examined for a more complete automatic composition algorithm.    