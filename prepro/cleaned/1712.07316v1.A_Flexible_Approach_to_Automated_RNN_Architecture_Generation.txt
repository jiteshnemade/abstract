introduction of non standard rnn  components such as trigonometric curves and layer normalization. using two  different candidate generation techniques  random search with a ranking function  and reinforcement learning  we explore the novel architectures produced by the  rnn dsl for language modeling and machine translation domains. the resulting  architectures do not follow human intuition yet perform well on their targeted tasks   suggesting the space of usable rnn architectures is far larger than previously  assumed.         introduction    developing novel neural network architectures is at the core of many recent ai advances  szegedy  et al.        he et al.        zilly et al.       . the process of architecture search and engineering  is slow  costly  and laborious. human experts  guided by intuition  explore an extensive space of  potential architectures where even minor modifications can produce unexpected results. ideally  an  automated architecture search algorithm would find the optimal model architecture for a given task.  many explorations into the automation of machine learning have been made  including the optimization of hyperparameters  bergstra et al.        snoek et al.        and various methods of  producing novel model architectures  stanley et al.        baker et al.        zoph and le       . for  architecture search  ensuring these automated methods are able to produce results similar to humans  usually requires traversing an impractically large search space  assuming high quality architectures  exist in the search space at all. the choice of underlying operators composing an architecture is  further typically constrained to a standard set across architectures even though recent work has found  promising results in the use of non standard operators  vaswani et al.       .  we propose a meta learning strategy for flexible automated architecture search of recurrent neural  networks  rnns  which explicitly includes novel operators in the search. it consists of three stages   outlined in figure    for which we instantiate two versions.   . a candidate architecture generation function produces potential rnn architectures using a highly  flexible dsl. the dsl enforces no constraints on the size or complexity of the generated tree and  can be incrementally constructed using either a random policy or with an rl agent.   . a ranking function processes each candidate architecture   s dsl via a recursive neural network   predicting the architecture   s performance. by unrolling the rnn representation  the ranking  function can also model the interactions of a candidate architecture   s hidden state over time.         equal contribution. this work was completed while the first author was interning at salesforce research.          candidate architecture generation    ranking  function    evaluator    random architecture generator    full training    ...    evaluation       .     incremental rl generator       .        .     architecture results    figure    a generator produces candidate architectures by iteratively sampling the next node  either  randomly or using an rl agent trained with reinforce . full architectures are processed by a  ranking function and the most promising candidates are evaluated. the results from running the  model against a baseline experiment are then used to improve the generator and the ranking function.     . an evaluator  which takes the most promising candidate architectures  compiles their dsls to  executable code and trains each model on a specified task. the results of these evaluations form  architecture performance pairs that are then used to train the ranking function and rl generator.         a d omain s pecific l anguage for d efining rnn s    in this section  we describe a domain specific language  dsl  used to define recurrent neural network  architectures. this dsl sets out the search space that our candidate generator can traverse during  architecture search. in comparison to zoph and le         which only produced a binary tree with  matrix multiplications at the leaves  our dsl allows a broader modeling search space to be explored.  when defining the search space  we want to allow for standard rnn architectures such as the gated  recurrent unit  gru   cho et al.        or long short term memory  lstm   hochreiter and  schmidhuber        to be defined in both a human and machine readable manner.  the core operators for the dsl are   unary operators    binary operators  and a single ternary operator    mm   sigmoid   tanh  relu       add   mult      gate   .    mm represents a single linear layer with bias  i.e. mm  x     w x   b. similarly  we define   sigmoid  x       x . the operator mult represents element wise multiplication  mult x  y     x     y. the gate  operator performs a weighted summation between two inputs  defined by  gate   x  y  f        f       x             f        y. these operators are applied to source nodes from the set   xt   xt       ht       ct        where xt and xt     are the input vectors for the current and previous timestep   ht     is the output of the rnn for the previous timestep  and ct     is optional long term memory.  the gate  operator is required as some architectures  such as the gru  re use the output of a single  sigmoid for the purposes of gating. while allowing all possible node re use is out of scope for this  dsl  the gate  ternary operator allows for this frequent use case.  using this dsl  standard rnn cell architectures such as the tanh rnn can be defined   tanh add  mm  xt    mm  ht        . to illustrate a more complex example that includes gate     the gru is defined in full in appendix a.   .     a dding support for architectures with long term memory    with the operators defined above it is not possible to refer to and re use an arbitrary node. the best  performing rnn architectures however generally use not only a hidden state ht but also an additional        hidden state ct for long term memory. the value of ct is extracted from an internal node computed  while producing ht .  the dsl above can be extended to support the use of ct by numbering the nodes and then specifying  which node to extract ct from  i.e. ct   node    . we append the node number to the end of the dsl  definition after a delimiter.  as an example  the nodes in bold are used to produce ct   with the number appended at the end  indicating the node   s number. nodes are numbered top to bottom  ht will be largest   left to right.  mult sigmoid  mm  xt     tanh add  mm  ht        mult mm  ct        mm  xt          mult sigmoid  mm  xt     tanh add mm  ht        mult mm  ct        mm  xt          mult sigmoid  mm  xt     tanh add  mm  ht        mult mm  ct        mm  xt             .     e xpressibility of the domain specific language    while the domain specific language is not entirely generic  it is flexible enough to capture most  standard rnn architectures. this includes but is not limited to the gru  lstm  minimal gate  unit  mgu   zhou et al.         quasi recurrent neural network  qrnn   bradbury et al.          neural architecture search cell  nascell   zoph and le         and simple rnns.   .     e xtending the d omain s pecific l anguage    while many standard and non standard rnn architectures can be defined using the core dsl   the promise of automated architecture search is in designing radically novel architectures. such  architectures should be formed not just by removing human bias from the search process but by  including operators that have not been sufficiently explored. for our expanded dsl  we include    sub  div       sin  cos      posenc      layernorm  selu  .    these extensions add inverses of currently used operators  sub a  b    a     b instead of addition   div  a  b    ab instead of multiplication   trigonometric curves  sin and cos are sine and cosine  activations respectively  posenc introduces a variable that is the result of applying positional  encoding  vaswani et al.        according to the current timestep   and optimizations  layernorm  applies layer normalization  ba et al.        to the input while selu is the activation function  defined in klambauer et al.        .   .     c ompiling a dsl definition to executable code    for a given architecture definition  we can compile the dsl to code by traversing the tree from the  source nodes towards the final node ht . we produce two sets of source code     one for initialization  required by a node  such as defining a set of weights for matrix multiplication  and one for the forward  call during runtime. for details regarding speed optimizations  refer to appendix a .         c andidate a rchitecture generation    the candidate architecture generator is responsible for producing candidate architectures that are  then later filtered and evaluated. architectures are grown beginning at the output ht and ordered to  prevent multiple representations for equivalent architectures   growing architectures from ht up beginning from the output node ht   operators are selected to  be added to the computation graph  depicted in figure  . whenever an operator has one or more  children to be filled  the children are filled in order from left to right. if we wish to place a limit on  the height  distance from ht   of the tree  we can force the next child to be one of the source nodes  when it would otherwise exceed the maximum height.  preventing duplicates through canonical architecture ordering due to the flexibility allowed  by the dsl  there exist many dsl specifications that result in the same rnn cell. to solve the        xt                 mm         xt    mm              xt    ht         mm mm    mm mm         add    add    add    add    add         tanh    tanh    tanh    tanh    tanh    tanh    ht    ht    ht    ht    ht    ht    ht    figure    an example of generating an architecture from ht up. nodes which have an empty child        are filled left to right. a source node such as xt can be selected at any time if max depth is exceeded.  issue of commutative operators  i.e. add  a  b    add  b  a    we define a canonical ordering of an  architecture by sorting the arguments of any commutative nodes. special consideration is required  for non commutative operators such as gate    sub  or div. for full details  refer to appendix a .   .     i ncremental a rchitecture c onstruction using r einforcement l earning    architectures in the dsl are constructed incrementally a node at a time starting from the output ht .  the simplest agent is a random one which selects the next node from the set of operators without  internalizing any knowledge about the architecture or optima in the search space. allowing an  intelligent agent to construct architectures would be preferable as the agent can learn to focus on  promising directions in the space of possible architectures.  for an agent to make intelligent decisions regarding which node to select next  it must have a  representation of the current state of the architecture and a working memory to direct its actions. we  propose achieving this with two components    . a tree encoder that represents the current state of the  partial  architecture.   . an rnn which is fed the current tree state and samples the next node.  the tree encoder is an lstm applied recursively to a node token and all its children with weights  shared  but the state reset between nodes. the rnn is applied on top of the encoded partial  architecture and predicts action scores for each operation. we sample with a multinomial and  encourage exploration with an epsilon greedy strategy. both components of the model are trained  jointly using the reinforce algorithm  williams       .  as a partial architecture may contain two or more empty nodes  such as ht   gate                       we introduce a target token  t   which indicates which node is to next be selected. thus  in ht    gate t                 the tree encoder understands that the first argument is the slot to be filled.   .     f iltering c andidate a rchitectures using a r anking f unction    even with an intelligent generator  understanding the likely performance of an architecture is difficult   especially the interaction of hidden states such as ht     and ct     between timesteps. we propose  to approximate the full training of a candidate architecture by training a ranking network through  regression on architecture performance pairs. this ranking function can be specifically constructed  to allow a richer representation of the transitions between ct     and ct .  as the ranking function uses architecture performance samples as training data  human experts  can also inject previous best known architectures into the training dataset. this is not possible for  on policy reinforcement learning and when done using off policy reinforcement learning additional  care and complexity are required for it to be effective  harutyunyan et al.        munos et al.       .  given an architecture performance pair  the ranking function constructs a recursive neural network  that reflects the nodes in a candidate rnn architecture one to one. sources nodes are represented by  a learned vector and operators are represented by a learned function. the final vector output then  passes through a linear activation and attempts to minimize the difference between the predicted  and real performance. the source nodes  xt   xt       ht       and ct       are represented by learned  vector representations. for the operators in the tree  we use treelstm nodes  tai et al.       . all        log perplexity on wikitext       .    .    .    .    .    .    .                                      architecture number  oldest   left  newest   right             figure    visualization of the language modeling architecture search over time. lower log perplexity   y axis  is better.    operators other than  gate    sub  div   are commutative and hence can be represented by child sum  treelstm nodes. the  gate    sub  div   operators are represented using an n  ary treelstm  which allows for ordered children.  unrolling the graph for accurately representing ht     and ct       a strong assumption made  above is that the vector representation of the source nodes can accurately represent the contents of the  source nodes across a variety of architectures. this may hold true for xt and xt     but is not true for  ht     or ct     . the value of ht and ct are defined by the operations within the given architecture itself.  to remedy this assumption  we can unroll the architecture for a single timestep  replacing ht      and ct     with their relevant graph and subgraph. this would allow the representation of ht     to  understand which source nodes it had access to and which operations were applied to produce ht     .  while unrolling is useful for improving the representation of ht       it is essential for allowing  an accurate representation of ct     . this is as many small variations of ct     are possible     such  as selecting a subgraph before or after an activation     that may result in substantially different  architecture performance.         e xperiments    we evaluated our architecture generation on two experiments  language modeling  lm  and machine  translation  mt . due to the computational requirements of the experiments  we limited each  experiment to one combination of generator components. for language modeling  we explore the  core dsl using randomly constructed architectures  random search  directed by a learned ranking  function. for machine translation  we use the extended dsl and construct candidate architectures  incrementally using the rl generator without a ranking function.   .     l anguage m odeling using r andom s earch with a r anking f unction    for evaluating architectures found during architecture search  we use the wikitext   dataset  merity  et al.      b . when evaluating a proposed novel rnn cell c  we construct a two layer c rnn with  a     unit hidden size. aggressive gradient clipping is performed to ensure that architectures such  as the relu rnn would be able to train without exploding gradients. the weights of the ranking  network were trained by regression on architecture perplexity pairs using the adam optimizer and  mean squared error  mse . further hyperparameters and training details are listed in appendix b .  explicit restrictions on generated architectures during the candidate generation phase  we filter  the generated architectures based upon specific restrictions. these include structural restrictions and  restrictions aimed at effectively reducing the search space by removing likely invalid architectures.  for gate  operations  we force the input to the forget gate to be the result of a sigmoid activation.  we also require the cell to use the current timestep xt and the previous timestep   s output ht     to  satisfy the requirements of an rnn. candidate architectures were limited to    nodes  the same  number of nodes as used in a gru  and the maximum allowed distance  height  from ht was   steps.        we also prevent the stacking of two identical operations. while this may be an aggressive filter it  successfully removes many problematic architectures. these problematic architectures include when  two sigmoid activations  two relu activations  or two matrix multiplications are used in succession      the first of which is unlikely to be useful  the second of which is a null operator on the second  activation  and the third of which can be mathematically rewritten as a single matrix multiplication.  if a given candidate architecture definition contained ct       the architecture was queried for valid  subgraphs from which ct could be generated. the subgraphs must contain ct     such that ct is  recurrent and must contain three or more nodes to prevent trivial recurrent connections. a new  candidate architecture is then generated for each valid ct subgraph.  random architecture search directed by a learned ranking function up to        candidate  architecture dsl definitions are produced by a random architecture generator at the beginning of  each search step. this full set of candidate architectures are then simulated by the ranking network  and an estimated perplexity assigned to each. given the relative simplicity and small training dataset   the ranking function was retrained on the previous full training results before being used to estimate  the next batch of candidate architectures. up to    architectures were then selected for full training.     of these were selected from the candidate architectures with the best perplexity while the last    were selected via weighted sampling without replacement  prioritizing architectures with better  estimated perplexities.  ct architectures were introduced part way through the architecture search after     valid ht architectures had been evaluated with ht architectures being used to bootstrap the ct architecture vector  representations. figure   provides a visualization of the architecture search over time  showing valid  ht and ct architectures.  analyzing the bc  cell after evaluating the top    cells using a larger model on wikitext     the top performing cell bc   named after the identifying hash  bc dc a. . .  was an unexpected  layering of two gate  operators   f      w f xt   u f ht        z   v z  x y ct         u z xt       w z xt  ct   tanh f     w g xt          f       z                      o      w o xt   u o ht        ht   o     ct          o      ht                    where     is an element wise multiplication and all weight matrices w  u  v  x     rh  h .    equations   to   produce the first gate  while equations   and   produce the second gate  . the  output of the first gate  becomes the value for ct after passing through a tanh activation.    while only the core dsl was used  bc  still breaks with many human intuitions regarding rnn  architectures. while the formulation of the gates f and o are standard in many rnn architectures  the  rest of the architecture is less intuitive. the gate  that produces ct  equation    is mixing between a  matrix multiplication of the current input xt and a complex interaction between ct     and xt  equation    . in bc   ct     passes through multiple matrix multiplications  a gate  and a tanh activation before  becoming ct . this is non conventional as most rnn architectures allow ct     to become ct directly   usually through a gating operation. the architecture also does not feature a masking output gate like  the lstm  with outputs more similar to that of the gru that does poorly on language modeling.  that this architecture would be able to learn without severe instability or succumbing to exploding  gradients is not intuitively obvious.   . .     e valuating the bc  cell    for the final results on bc   we use the experimental setup from merity et al.      a  and report  results for the penn treebank  table    and wikitext    table    datasets. to show that not any  standard rnn can achieve similar perplexity on the given setup  we also implemented and tuned a  gru based model which we found to strongly underperform compared to the lstm  bc   nascell   or recurrent highway network  rhn . full hyperparameters for the gru and bc  are in appendix        model    parameters    validation    test    inan et al.          variational lstm  tied    augmented loss  inan et al.          variational lstm  tied    augmented loss  zilly et al.          variational rhn  tied   zoph and le          variational nas cell  tied   zoph and le          variational nas cell  tied   melis et al.            layer skip connection lstm  tied   merity et al.      a      layer weight drop lstm   nt asgd  tied       m    m    m    m    m    m    m      .     .     .               .     .       .     .     .     .     .     .     .       layer weight drop gru   nt asgd  tied     layer weight drop bc    nt asgd  tied       m    m      .     .       .     .     table    model perplexity on validation test sets for the penn treebank language modeling task.  model    parameters    validation    test    inan et al.          variational lstm  tied   inan et al.          variational lstm  tied    augmented loss  melis et al.            layer lstm  tied   melis et al.            layer skip connection lstm  tied   merity et al.      a      layer weight drop lstm   nt asgd  tied       m    m    m    m    m      .     .     .     .     .       .     .     .     .     .       layer weight drop gru   nt asgd  tied     layer weight drop bc    nt asgd  tied       m    m      .     .       .     .     table    model perplexity on validation test sets for the wikitext   language modeling task.    b . our model uses equal or fewer parameters compared to the models it is compared against. while  bc  did not outperform the highly tuned awd lstm  merity et al.      a  or skip connection  lstm  melis et al.         it did outperform the recurrent highway network  zilly et al.         and nascell  zoph and le        on the penn treebank  where nascell is an rnn found using  reinforcement learning architecture search specifically optimized over the penn treebank.   .     i ncremental a rchitecture c onstruction using rl for m achine t ranslation    for our experiments involving the extended dsl and our rl based generator  we use machine  translation as our domain. the candidate architectures produced by the rl agent were directly used  without the assistance of a ranking function. this leads to a different kind of generator  whereas  the ranking function learns global knowledge about the whole architecture  the rl agent is trimmed  towards local knowledge about which operator is ideal to be next.  training details before evaluating the constructed architectures  we pre train our generator to  internalize intuitive priors. these priors include enforcing well formed rnns  i.e. ensuring xt   ht        and one or more matrix multiplications and activations are used  and moderate depth restrictions   between   and    nodes deep . the full list of priors and model details are in appendix c .  for the model evaluation  we ran up to    architectures in parallel  optimizing one batch after  receiving results from at least four architectures. as failing architectures  such as those with exploding  gradients  return early  we needed to ensure the batch contained a mix of both positive and negative  results. to ensure the generator yielded mostly functioning architectures whilst understanding the  negative impact of invalid architectures  we chose to require at least three good architectures with a  maximum of one failing architecture per batch.  for candidate architectures with multiple placement options for the memory gate ct   we evaluated  all possible locations and waited until we had received the results for all variations. the best ct  architecture result was then used as the reward for the architecture.  baseline machine translation experiment details to ensure our baseline experiment was fast  enough to evaluate many candidate architectures  we used the multi  k english to german  elliott  et al.        machine translation dataset. the training set consists of        sentence pairs that briefly        figure    distribution of operators over time. initially the generator primarily uses the core dsl   faded colors  but begins using the extended dsl as the architecture representation stabilizes.    describe flickr captions. our experiments are based on opennmt codebase with an attentional  unidirectional encoder decoder lstm architecture  where we specifically replace the lstm encoder  with architectures designed using the extend dsl.  for the hyper parameters in our baseline experiment  we use a hidden and word encoding size of         layers for the encoder and decoder rnns  batch size of     back propagation through time of     timesteps  dropout of  .   input feeding  and stochastic gradient descent. the learning rate starts at    and decays by     when validation perplexity fails to improve. training stops when the learning  rate drops below  .  .  analysis of the machine translation architecture search figure   shows the relative frequency  of each operator in the architectures that were used to optimize the generator each batch. for all the  architectures in a batch  we sum up the absolute number that each operator occurs and divide by the  total number of operators in all architectures of the batch. by doing this for all batches  x axis   we  can see which operators the generator prefers over time.  intriguingly  the generator seems to rely almost exclusively on the core dsl   mm   gate    tanh  sigmoid   xt   ht       when generating early batches. the low usage of  the extended dsl operators may also be due to these operators frequently resulting in unstable  architectures  thus being ignored in early batches. part way through training however the generator  begins successfully using a wide variety of the extended dsl  sub  div   sin  cos  . . . . we  hypothesize that the generator first learns to build robust architectures and is only then capable of  inserting more varied operators without compromising the rnn   s overall stability. since the reward  function it is fitting is complex and unknown to the generator  it requires substantial training time  before the generator can understand how robust architectures are structured. however  the generator  seems to view the extended dsl as beneficial given it continues using these operators.  overall  the generator found     architectures that out performed the lstm based on raw  test bleu score  out of a total of      evaluated architectures      . the best architecture  determined by the validation bleu score  achieved a test bleu score of   .   respectively  compared to the standard lstm   s   .  . multiple cells also rediscovered a variant  of residual networks  add  transformation xt    xt     he et al.        or highway networks   gate   transformation xt    xt   sigmoid  . . .     srivastava et al.       . every operation in the  core and extended dsl made their way into an architecture that outperformed the lstm and many  of the architectures found by the generator would likely not be considered valid by the standards of  current architectures. these results suggest that the space of successful rnn architectures might  hold many unexplored combinations with human bias possibly preventing their discovery.  in table   we take the top five architectures found during automated architecture search on the  multi  k dataset and test them over the iwslt       english to german  dataset  cettolo et al.        . the training set consists of         sentence pairs from transcribed ted presentations  that cover a wide variety of topics with more conversational language than in the multi  k dataset.        dsl architecture description   encoder used in attentional encoder decoder lstm     multi  k  val loss test bleu    iwslt       test bleu    lstm  bc      .     .        .      .        .      .      gate   add  mm  xt    xt    sigmoid  mm  ht          gate   mm  xt    tanh xt    sigmoid  mm  ht            deep nested gate  with layernorm  see appendix c    residual xt with positional encoding  see appendix c    gate   mm  xt    xt   sigmoid  mm  selu  ht              .     .     .     .     .        .      .      .      .      .        .      .      .      .      .      table    model loss and bleu on the multi  k and iwslt      mt datasets. all architectures were  generated on the multi  k dataset other than the lstm and bc  from the lm architecture search.  we did not perform any hyperparameter optimizations on either dataset to avoid unfair comparisons   though the initial opennmt hyperparameters likely favored the baseline lstm model.  this dataset is larger  both in number of sentences and vocabulary  and was not seen during the  architecture search. while all six architectures achieved higher validation and test bleu on multi  k  than the lstm baseline  it appears the architectures did not transfer cleanly to the larger iwslt  dataset. this suggests that architecture search should be either run on larger datasets to begin with  a  computationally expensive proposition  or evaluated over multiple datasets if the aim is to produce  general architectures. we also found that the correlation between loss and bleu is far from optimal   architectures performing exceptionally well on the loss sometimes scored poorly on bleu. it is also  unclear how these metrics generalize to perceived human quality of the model  tan et al.        and  thus using a qualitatively and quantitatively more accurate metric is likely to benefit the generator.  for hyper parameters of the iwslt model  refer to appendix c .         r elated w ork    architecture engineering has a long history  with many traditional explorations involving a large  amount of computing resources and an extensive exploration of hyperparamters  jozefowicz et al.         greff et al.        britz et al.       . the approach most similar to our work is zoph and  le        which introduces a policy gradient approach to search for convolutional and recurrent  neural architectures. their approach to generating recurrent neural networks was slot filling  where  element wise operations were selected for the nodes of a binary tree of specific size. the node to  produce ct was selected once all slots had been filled. this slot filling approach is not highly flexible  in regards to the architectures it allows. as opposed to our dsl  it is not possible to have matrix  multiplications on internal nodes  inputs can only be used at the bottom of the tree  and there is no  complex representation of the hidden states ht     or ct     as our unrolling ranking function provides.  many other similar techniques utilizing reinforcement learning approaches have emerged such as  designing cnn architectures with q learning  baker et al.       .  neuroevolution techniques such as neuroevolution of augmenting topologies  neat   stanley  and miikkulainen        and hyperneat  stanley et al.        evolve the weight parameters and  structures of neural networks. these techniques have been extended to producing the non shared  weights for an lstm from a small neural network  ha et al.        and evolving the structure of a  network  fernando et al.        bayer et al.       .         c onclusion    we introduced a flexible domain specific language for defining recurrent neural network architectures  that can represent most human designed architectures. it is this flexibility that allowed our generators  to come up with novel combinations in two tasks. these architectures used both core operators that  are already used in current architectures as well as operators that are largely unstudied such as division  or sine curves. the resulting architectures do not follow human intuition yet perform well on their  targeted tasks  suggesting the space of usable rnn architectures is far larger than previously assumed.  we also introduce a component based concept for architecture search from which we instantiated  two approaches  a ranking function driven search which allows for richer representations of complex        rnn architectures that involve long term memory  ct   nodes  and a reinforcement learning agent  that internalizes knowledge about the search space to propose increasingly better architectures. as  computing resources continue to grow  we see automated architecture generation as a promising  avenue for future research.    