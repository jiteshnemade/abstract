introduction    recurrent neural networks  rnns  are widely used for sequence modelling tasks in domains such  as natural language processing  sutskever et al.         speech recognition  amodei et al.          and reinforcement learning  hausknecht and stone       . most rnns  including popular variants  such as long short term memories  lstms   introduced by hochreiter and schmidhuber          and gated recurrent units  grus   introduced by cho et al.         contain a non linear dependency  between sequential inputs. these non linear dependencies create a very flexible class of models but  limit the feasibility of training rnns on long sequences as each sequence element must be processed  sequentially. modelling sequences of thousands to millions of elements is important to domains such  as robotics  remote sensing  control systems  speech recognition  medicine  and finance.  the rnn serial evaluation inefficiency problem is usually mitigated by parallelizing the forward and  backward pass over a minibatch of inputs. without minibatches  rnn evaluation is a sequence of  matrix vector multiplications. minibatches transform rnn computation into a sequence of more  efficient matrix matrix multiplications  but this speed up brings several disadvantages. rnn model  size is often limited by gpu memory size  and running a forward and backward pass on a minibatch  requires memory linear in the minibatch size. grouping data into minibatches increases the latency  of each pass and reduces the rate of optimization steps. finally  training with larger minibatches  damages generalization ability  keskar et al.       . given these effects  it is desirable to obtain  high training throughput with small minibatches. persistent rnns  diamos et al.        use a novel  implementation that can achieve high gpu utilization with very small minibatch sizes when the  recurrent state is larger than     elements  but even persistent rnns become limited by the serial  evaluation inefficiency at smaller hidden sizes.  numerous prior works have shown strong performance from neural sequential models with only  linear dependence on earlier sequence elements. balduzzi and ghifary        investigated rnns with  only elementwise linear recurrence relations ht    t ht           t   xt and developed linear       currently at the future of humanity institute  university of oxford  oxford  uk          published as a conference paper at iclr         variants of lstm and gru that perform similarly to standard non linear rnns on text generation  tasks. bradbury et al.         kalchbrenner et al.         gehring et al.         and van den oord  et al.        have successfully applied networks of convolutions over sequences for tasks such as  machine translation  language modelling  and audio generation. these works have observed up to an  order of magnitude increase in training throughput compared to rnn alternatives. convolutional  sequence models typically rely on either an attention mechanism or a  possibly linear  recurrent layer  to integrate information at scales larger than the filter width. introduction of a recurrent layer prevents  full parallelization over the sequence length while attention mechanisms are expensive to apply on  long sequences in online inference use cases.  a linear recurrence is a specific instance of a general form of computation known as a scan. scans  and reductions are computations involving repeated application of a binary operator   over an array  of data. computing the sum or maximum of an array is an example of a reduction  while a cumulative  sum is a common example of a scan operation. throughout this work  the scan of   with initial value  b is defined as  scan     a    a    ...  an    b      a    b    a    a    b   ...   an   an   ...   a    b  .  the reduction of   over array a and initial value b is denoted reduce    a  b  and is the final  element of scan    a  b . despite their dependent computation graph  algorithms exist to parallelize  scans and reductions when   is associative  ladner and fischer       .  blelloch        shows that first order recurrences of the form ht     t   ht       xt can be  parallelized with the parallel scan algorithm if three conditions are met    .   is associative   a   b    c   a    b   c    .   is semiassociative  there exists a binary associative operator   a b    c   .   distributes over    a    b   c     a   b     a   c     such that a    b   c       considering the familiar operations in linear algebra  we see that the associative operation of vector  addition  x   y   x   y   the semiassociative operation of matrix vector multiplication  a   x   ax   and the associative operation of matrix matrix multiplication  a b   ab  satisfy blelloch s three  conditions  allowing ht    t ht     xt to be evaluated in parallel over time steps t for vectors xt  and square matrices  t .  we investigate this idea further and deliver the following contributions     we classify rnns which satisfy the conditions above  and show that many rnns used  in practice such as the quasi rnns  qrnns  introduced by bradbury et al.        are  contained in this class.    we provide an implementation of the parallel linear recurrence algorithm as a cuda kernel   and show that it speeds up training of qrnn and lei and zhang        s simple recurrent  unit  sru  architectures by factors of up to  x.    we describe how several recent linear rnns can be described as linear surrogates for  non linear architectures. we introduce a linear surrogate for the lstm and show that we  are able to train it with a speedup of     x compared to the cudnn lstm when we use  the parallel linear recurrence algorithm.         parallel linear recurrence    as the method is essential to this work  algorithm   presents the parallel linear recurrence algorithm  for the interested reader.   .     t heoretical performance    the cost of a serial scan over a sequence of length t is csscan   o  c    c   t    compared  to the parallel scan cost cpscan   o   c   c    c    t  p   lg p   on p processors  blelloch        . if ht is a vector of dimension n then c   o n     c    o n     c    o n  giving        published as a conference paper at iclr         algorithm   parallel linear recurrence on p processors     let y          x           x     ...    t   xt        let binary operator   act as     x    h    h   x     let s       si   ei   ei       si     ep     t for i in    p                                                                   parfor i      p     do  pi   reduce     si  ei   i   ri   reduce    ysi  ei       end parfor  let z     p    r      p    r     ...   pp   rp   .  c   scan    z  h       . compute ci   pi ci     ri with c     h     parfor i      p     do  hsi  ei   scan    ysi  ei   ci      end parfor  return h    cpscan   o   n    n    n  t  p   lg p   and csscan   o  n    n t  . the o n    cost of the matrix  multiplication in the parallel algorithm can counter act any parallel speedups for sufficiently large  hidden states and lead to a slower algorithm overall.  to avoid this problem  we will only consider diagonal matrices  t   in which case both matrix matrix  and matrix vector multiplication have cost proportional to n and cpscan   o  n t  p   lg p   and  csscan   o  nt  . this gives a parallel speedup factor of pt    t   lg p . assuming p   t   then  cpscan   csscan when p    .  as we are only considering diagonal matrices  we write the linear recurrence as ht    t  where indicates elementwise multiplication.    ht     xt    limiting  t to be diagonal may seem like a severe constraint but there are several reasons to do  so beyond the favorable parallelization performance. relatively few neural network models use  separate recurrent matrices for each sequence element and using these separate matrices would  require potentially prohibitive n  t memory. applying the same matrix   to each sequence element  is also unappealing considering that a matrix multiplication can be thought of as a rotation and a  scaling. the same rotation at every element seems unlikely to be useful  and the scaling is exactly  what s captured in diagonal vectors  t . recurrent coefficient vectors  t provide enough flexibility to  implement schemes such as exponential moving averages or a gating mechanism.   .     backpropagation   l   ht   ht     ht l     ht     h t l         t       l   ht   l   ht   l     ht   ht   l       ht   ht l   ht    ht l    t   xt l    ht l   h    h  l     h  l       h  l   h      t l       l  the backpropagation equations center around a linear recurrence over  h  in the reverse order of  t  the original sequence. this allows for parallelizing both the forwards and backwards pass of a linear  rnn over the sequence length.          published as a conference paper at iclr          .     i mplementation    gpus commonly used for deep learning in      consist of between     and      parallel processors  known as warps. each warp operates on    single precision floating point numbers in parallel.  this work implemented parallel linear recurrence as a cuda kernel with bindings into the tensorflow   abadi et al.        framework. each warp acts as a processor  which means the algorithmic p is up  to      and the theoretical parallelization speedup factor is up to several hundred. the    lanes of  each warp work on different elements of the recurrence vector in parallel. these implementation  details mean that peak performance is only obtained on sequences of at least several thousand steps  on at least a    element vector.  the parallel linear recurrence cuda kernel and tensorflow bindings are available at https     github.com eamartin parallelizing linear rnns .         m odels    parallel linear recurrence can be used to construct a wide variety of differentiable modules that can  be evaluated in parallel. common applications of linear recurrence include gating schemes and  exponential moving averages. although linear recurrence values can depend only linearly on previous  elements  the stacking of linear recurrent layers separated by non linearities allows for a non linear  dependence on the past. in this sense the non linear depth of a linear recurrent network is the number  of layers and not the sequence length.   .     g ated impulse linear recurrent layer    a gated impulse linear recurrent  gilr  layer transforms its m dimensional inputs xt into a sequence  of n dimensional hidden states ht    gt     u xt   bg    it      v xt   bz    ht   gt ht          gt   it  a gilr layer applies the same non linear transform to each sequence element and then accumulates  the sequence elements with a non linear gating mechanism. gate gt uses the sigmoid activation  function to give values in       for reasonable gating semantics  while impulse it can use any activation  function   . stacking gilr layers allows for rich non linear dependence on previous events while  still taking advantage of fast parallel sequence evaluation.   . .     i mpact on effective   batch size      consider evaluating an rnn with recurrence ht     u ht     v xt   b  from m inputs to n hidden  units on a sequence of length t with minibatch size b using a serial evaluation strategy. at each of t  iterations  the naive approach performs two  b  m     m  n  matrix multiplications. larger matrix  multiplications achieve higher throughput due to less io overhead  so the better approach computes  v xt for all t ahead of time in a single  bt  m     m  n  matrix multiply. the non linear recurrence  forces even the better approach to perform t potentially small  b  m     m  n  matrix multiplications  in serial. this makes serial rnn performance heavily dependent on minibatch size.  now consider the gilr  noting that it has the same two matrix vector multiplications per iteration  as the above rnn. the intermediate variables g and i can be evaluated for all t with a single   bt  m     m  n  matrix multiplication each. given g and i  h can be computed using a parallel  linear recurrence over t vectors each of bn elements. rather than t small operations  the gilr can  be evaluated over all sequence elements with two large matrix multiplications and a parallel linear  recurrence. gilr performance is much less dependent on batch size as the matrix multiplication  kernel sees an  effective batch size  of bt and t is typically large.   .     l inear surrogate rnn s    rnns learn a transition function st   f  st     xt   which combines previous state st   with input  xt to compute current state st . non linear f prevents application of the parallel linear recurrence        published as a conference paper at iclr         algorithm and forces slow serial evaluation. to work around this inefficiency  note that st serves dual  purposes. in st   f  st     xt    st   serves as an input to f summarizing the previous inputs while st  serves as the output of f to be passed to other layers of the network. we can decouple these uses and  introduce independent variables for each purpose  st is passed onto other layers of the network and  we introduce the linear surrogate s t which is passed onto the next state  with st   f  s t     xt  . we  are still able to choose a non linear f   our only limitation being that s t must be linearly computable.  we refer to this class of model as a linear surrogate rnn  ls rnn . qrnns  bradbury et al.         are ls rnns using h t     wk xt k   ...w  xt   and strongly typed rnns  balduzzi and ghifary         are ls rnns with h t   xt   . although not a rule  ls rnns can often be parallelized over  sequence length with either convolution or linear recurrence.  consider an lstm   ft   it   ot  zt  ct  ht        uf i o ht     vf i o xt   bf i o         uz ht     vz xt   bz      ft ct     it zt    ot ct    an lstm has state st    ht   ct  . since ct depends only linearly on ct     no surrogate is needed for  ct . ht has a non linear dependence on ht     so ht needs a linear surrogate. introducing a gilr layer  as the surrogate  we obtain the gilr lstm     gt     vg xt   bg    jt      vj xt   bj    h t   gt    h t          gt      jt    ft   it   ot     uf i o h t     vf i o xt   bf i o    zt      uz h t     vz xt   bz    ct   ft ct     it zt  ht   ot ct    for m inputs and hidden size n  a gilr lstm contains  n n   m  more parameters than the  equivalently sized lstm to handle the mapping from x to h . more generally  a ls rnn contains all  of the same parameters as the underlying rnn as well as some additional parameters to compute the  linear surrogate.         e xperiments    we perform several experiments. first we find that our parallel linear recurrence kernel is able to  achieve up to   x higher throughput than a serial implementation when applied to long sequences.  secondly  we confirm that this kernel speedup translates to up to a  x speedup to ls rnns such as  qrnns.  in order to illustrate that the linearization does not necessarily come at the cost of expressibility  we  show that the gilr lstm architecture computed with the parallel linear recurrence algorithm is  able to train significantly faster than an optimized lstm implementation on a pathological long term  dependency problem from the original lstm paper  hochreiter and schmidhuber       .   .    . .     t hroughput benchmarks  k ernel performance    we first illustrate the throughput advantage of the parallel scan algorithm for evaluating the linear  recurrence. for a minibatch comprised of b sequences of length t   we define the number of events  as bt and the throughput as the number of events processed per second. we implement two cuda        published as a conference paper at iclr         table    parallel kernel speedup on m features  minibatch size       sequence length    m      m         m                                     .     .     .      .      .     .     .      .      .     .     .      .     table    parallel kernel speedup for a variety of ls rnns  implemented as two stacked rnn  layers with     hidden units. we keep the gpu memory usage constant by fixing bt           for  minibatch size b and sequence length t  sequence length    sru    qrnn  filter size       qrnn  filter size        gilr lstm                               .     .     .     .       .     .     .     .       .     .     .     .       .     .     .     .      kernels  one which evaluates the parallel linear recurrence described in algorithm    and one which  evaluates the same linear recurrence on gpu in serial over sequence length and in parallel over  features and minibatch. the performance of each kernel depends on two factors  the sequence length  and the product of number of features and minibatch size. the performance measurements for this  experiment are made directly at the kernel level  avoiding any overhead from tensorflow. we find  that the parallel kernel has a distinct advantage at long sequence lengths with a speedup factor of up  to   x  as shown in table  . the parallel kernel does not perform well at short sequence lengths due  to the overhead of multiple passes over data and communication between processors.   . .     accelerating existing rnn architectures    several recently introduced ls rnns can be accelerated with the parallel linear recurrence algorithm.  we implemented srus  qrnns  with filter width   and      and gilr lstms that can be computed  with either the standard serial linear recurrence algorithm or parallel linear recurrence. both methods  compute an identical recurrence  so switching from a serial to parallel implementation does not cause  any numerical changes and takes only a single line of code changes. notably  both srus and qrnns  claim an order of magnitude speedup compared to cudnn lstm when implemented with serial  linear recurrence. any further speedup from parallel linear recurrence applies on top of the existing  speedup. we timed train throughput  forwards and backwards propagation   but the linear time of  each pass also makes the results applicable to forwards  inference  performance. however  parallel  linear recurrence can only accelerate inference in scenarios where the entire input sequence is known  at the start of the inference phase. we controlled for gpu memory usage within these experiments by  fixing bt           for minibatch size b and sequence length t   and chose a popular architecture  consisting of two stacked rnn layers with     hidden units and an input size of  .  table   shows that the throughput advantage from using parallel linear recurrence compared to serial  linear recurrence reaches up to  x. simpler architectures  for which the linear recurrence is a higher  proportion of the total computational load  are more affected by the switch to the parallel kernel.  this is particularly clear in the case of the qrnn  where including wider convolutional filters results  in more time spent outside of the linear recurrence and therefore reduces the speedup from linear  recurrence parallelization.   .     s ynthetic e xperiment    one of the key strengths of the lstm is that it is capable of dealing with long term dependencies. in  order to demonstrate that the gilr lstm is also able to handle long term dependencies we tackle a  canonical example of inference over many time steps from hochreiter and schmidhuber       . we  show that in fact the gilr lstm is able to outperform the cudnn lstm and extend to sequence        published as a conference paper at iclr         figure    the structure of the synthetic example and the gilr lstm architecture we used to tackle  it. we feed in one hot unit vectors x which are chosen uniformly at random  with replacement . the  class is determined by the very first vector x    which has a fixed direction. the sign of x  determines  the class. in the diagram  each rounded block indicates a cell of the rnn  whilst the square indicates  a linear unit.  table    performance of the gilr lstm compared to the cudnn lstm on problem  b from  hochreiter and schmidhuber       .  sequence length                           iterations      s   wall time  hours     cudnn    gilr    cudnn    gilr    cudnn    gilr     .     .    .      .       .      .     .       .        .      .     .      .       .      .     .      .                    .     .     lengths orders of magnitude longer than dealt with previously. the input consists of sequences of  length n where for n     each element is a randomly chosen one hot vector x in p dimensional  space. the first vector in each sequence  x    is always either        . . .      or         . . .     . the  sequential model must read in an entire sequence and then output the sign of the first sequence  element. this sequence classification problem requires remembering the first element over the  length of the sequence  and early rnns struggled with this for p as small as a few dozen. in the  original formulation of the problem  dealing in the regime with around one hundred timesteps    the dimensionality of the input p is set equal to n. since this would make the size of the input  data grow impractically large as o n    for long sequences  we fix p       as we vary n. we  generated sequences for n equal to               and          . for each of these we compared a  two layer gilr lstm with     hidden units to a two layer lstm with     hidden units  per layer  implemented by cudnn.  we ran all experiments on a nvidia k   gpu  with five runs per configuration allowing us to  find the average and standard deviation of the time and number of iterations to convergence. we  continually generated random sequences to serve as input data. a brief search over learning rate and  batch size was carried out to find the parameters which allow the network to converge most rapidly  for all runs. the criterion for convergence was five consecutive minibatches giving      accuracy.  the learning curves in figure   give support to this being a reasonable convergence criteria. for the  longest sequence length  we did not observe the cudnn lstm converging  even after several days   training.  the results as show in table  .  illustrate that the gilr lstm is able to converge between   and     times faster than the cudnn lstm. this is somewhat surprising given the lstm was specifically  constructed for problems of this sort  and the cudnn lstm implementation is highly optimized  to  the extent that the monolithic interface it exposes is difficult to modify or extend . the gilr lstm  is implemented entirely in standard tensorflow with the exception of using the new linear recurrence  op instead of a tensorflow symbolic loop. convergence of the gilr lstm models leads to the  conclusion that the non linearities present in lstm are not necessary for solving this instance of     for the longest sequence length  the number of hidden units was decreased to    for both architectures so  that the net could fit in memory.          published as a conference paper at iclr         figure    learning curves for gilr lstm and cudnn lstm architectures for various sequence  lengths. each plot shows the moving mean and standard deviation of classification accuracy over five  training runs  with the exception of a single run for cudnn lstm on   million sequence length.    the long term dependency problem. the time to convergence further leads to the conclusion that  inclusion of a non linearity at every step incurs a significant training time slowdown. furthermore   the gilr lstm is able to learn to carry dependencies over a one million element sequence. as far  as we know  this one million step sequence experiment is the longest sequential learning problem to  be handled by neural networks to date.         c onclusion    a significant portion of the success of deep learning can be attributed to access to massive amounts  of computation. most of this computation is accessed through two highly efficient and parallelizable  building blocks  matrix multiplication and convolution. recent research has demonstrated that linear  rnns can achieve similar prediction accuracy to non linear rnns on a wide variety of tasks in a  fraction of the training time. we propose the framework of ls rnns as a way to tame the growing  zoo of sequential neural nets. we identify linear recurrence as another parallelizable building block  for current and future sequential models and we use it to obtain significant speedups on already fast  models. with the power of parallel linear recurrence we are able to solve a sequential dependency  problem multiple orders of magnitude larger than anything done prior. future applications of parallel  linear recurrence within neural nets could include parallel training of memory augmented models  or providing a new sort of image filter on very high resolution images. we hope that parallel linear  recurrence can be to large scale sequence modelling what fast convolution algorithms are to image  recognition.  acknowledgments  we would like to acknowledge kevin bowers  alex meiburg  jd co reyes  carson mcneil  andy  palan  s ren mindermann  and several others for fruitful conversations and guidance.        published as a conference paper at iclr         