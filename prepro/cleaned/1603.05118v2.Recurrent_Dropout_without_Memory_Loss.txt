introduction  recurrent neural networks  lstms in particular   have recently become a popular tool among nlp  researchers for their superior ability to model and  learn from sequential data. these models have  shown state of the art results on various public  benchmarks ranging from sentence classification  wang et al.        irsoy and cardie         liu et al.        and various tagging problems  dyer et al.        to language modelling  kim et al.         zhang et al.          text  generation   zhang and lapata         and  sequence to sequence  prediction  tasks  sutskever et al.       .  having shown excellent ability to capture and  learn complex linguistic phenomena  rnn architectures are prone to overfitting. among  the most widely used techniques to avoid overfitting in neural networks is the dropout regularization  hinton et al.       . since its intro     duction it has become  together with the l   weight decay  the standard method for neural  network regularization. while showing significant improvements when used in feed forward  architectures  e.g.  convolutional neural networks  krizhevsky et al.         the application of  dropout in rnns has been somewhat limited. indeed  so far dropout in rnns has been applied  in the same fashion as in feed forward architectures  it is typically injected in input to hidden  and hidden to output connections  i.e.  along the  input axis  but not between the recurrent connections  time axis . given that rnns are mainly  used to model sequential data with the goal of capturing short  and long term interactions  it seems  natural to also regularize the recurrent weights.  this observation has led us and other researchers   moon et al.        gal        to the idea of applying dropout to the recurrent connections in  rnns.  in this paper we propose a novel recurrent  dropout technique and demonstrate how our  method is superiour to other recurrent dropout  methods recently proposed in  moon et al.         gal       . additionally  we answer the following questions which helps to understand how to  best apply recurrent dropout   i  how to apply  the dropout in recurrent connections of the lstm  architecture in a way that prevents possible corruption of the long term memory   ii  what is  the relationship between our recurrent dropout  and the widely adopted dropout in input to hidden  and hidden to output connections   iii  how the  dropout mask in rnns should be sampled  once  per step or once per sequence. the latter question  of sampling the mask appears to be crucial in some  cases to make the recurrent dropout work and  to  the best of our knowledge  has received very little  attention in the literature. our work is the first one  to provide empirical evaluation of the differences     between these two sampling approaches.  regarding empirical evaluation  we first highlight the problem of information loss in memory  cells of lstms when applying recurrent dropout.  we demonstrate that previous approaches of dropping hidden state vectors cause loss of memory  while our proposed method to use dropout mask in  hidden state update vectors does not suffer from  this problem. we experiment on three widely  adopted nlp tasks  word  and character level  language modeling and named entity recognition. the results demonstrate that our recurrent dropout helps to achieve better regularization and yields improvements across all the tasks   even when combined with the conventional feedforward dropout. furthermore  we compare our  dropout scheme with the recently proposed alternative recurrent dropout methods and show that  our technique is superior in almost all cases.      related work  neural network models often suffer from overfitting  especially when the number of network parameters is large and the amount of training data is  small. this has led to a lot of research directed towards improving their generalization ability. below we primarily discuss some of the methods  aimed at improving regularization of rnns.  pham et al.        and zaremba et al.         have shown that lstms can be effectively regularized by using dropout in forward connections.  while this already allows for effective regularization of recurrent networks  it is intuitive that introducing dropout also in the hidden state may force  it to create more robust representations. indeed   moon et al.        have extended the idea of dropping neurons in forward direction and proposed to  drop cell states as well showing good results on  a speech recognition task. bluche et al.         carry out a study to find where dropout is most  effective  e.g. input to hidden or hidden to output  connections. the authors conclude that it is more  beneficial to use it once in the correct spot  rather  than to put it everywhere. bengio et al.         have proposed an algorithm called scheduled sampling to improve performance of recurrent networks on sequence to sequence labeling tasks. a  disadvantage of this work is that the scheduled  sampling is specifically tailored to this kind of  tasks  what makes it impossible to use in  for example  sequence to label tasks. gal        uses    insights from variational bayesian inference to  propose a variant of lstm with dropout that  achieves consistent improvements over a baseline  architecture without dropout.  the main contribution of this paper is a new  recurrent dropout technique  which is most useful in gated recurrent architectures such as lstms  and grus. we demonstrate that applying dropout  to arbitrary vectors in lstm cells may lead to  loss of memory thus hindering the ability of the  network to encode long term information. in  other words  our technique allows for adding a  strong regularizer on the model weights responsible for learning short and long term dependencies without affecting the ability to capture longterm relationships  which are especially important  to model when dealing with natural language. finally  we compare our method with alternative  recurrent dropout methods recently introduced  in  moon et al.        gal        and demonstrate  that our method allows to achieve better results.      recurrent dropout  in this section we first show how the idea  of feed forward dropout  hinton et al.        can  be applied to recurrent connections in vanilla  rnns. we then introduce our recurrent dropout  method specifically tailored for gated architectures such as lstms and grus. we draw parallels and contrast our approach with alternative  recurrent dropout techniques recently proposed  in  moon et al.        gal        showing that our  method is favorable when considering potential  memory loss issues in long short term architectures.   .  dropout in vanilla rnns  vanilla rnns process the input sequences as follows   ht   f  wh  xt   ht       bh              where xt is the input at time step t  ht and ht    are hidden vectors that encode the current and  previous states of the network  wh is parameter  matrix that models input to hidden and hidden tohidden  recurrent  connections  b is a vector of  bias terms  and f is the activation function.  as rnns model sequential data by a fullyconnected layer  dropout can be applied by simply  dropping the previous hidden state of a network.     specifically  we modify equation   in the following way   ht   f  wh  xt   d ht        bh              where d is the dropout function defined as follows      mask   x  if train phase  d x               p x  otherwise   where p is the dropout rate and mask is a vector   sampled from the bernoulli distribution with success probability     p.   .     dropout in lstm networks    long  short term  memory  networks   hochreiter and schmidhuber         have introduced the concept of gated inputs in  rnns  which effectively allow the network to  preserve its memory over a larger number of  time steps during both forward and backward  passes  thus alleviating the problem of vanishing  gradients  bengio et al.       . formally  it is  expressed with the following equations                   it    wi  xt   ht       bi      ft      wf xt   ht     bf                      ot       wo xt   ht     bo            gt  f  wg xt   ht     bg             ct   ft   ct     it   gt           ht   ot   f  ct              where it   ft   ot are input  output and forget gates at  step t  gt is the vector of cell updates and ct is the  updated cell vector used to update the hidden state  ht     is the sigmoid function and   is the elementwise multiplication.  gal        proposes to drop the previous hidden state when computing values of gates and updates of the current step  where he samples the  dropout mask once for every sequence                   it    wi  xt   d ht        bi      ft      wf xt   d ht       bf                      ot       wo xt   d ht       bo                gt  f  wg xt   d ht       bg      moon et al.        propose to apply dropout directly to the cell values and use per sequence sampling as well     ct   d ft   ct     it   gt             in contrast to dropout techniques proposed by  gal        and moon et al.         we propose to  apply dropout to the cell update vector gt as follows   ct   ft   ct     it   d gt             different from methods of  moon et al.         gal         our approach does not require sampling of the dropout masks once for every training sequence. on the contrary  as we will show in  section    networks trained with a dropout mask  sampled per step achieve results that are at least as  good and often better than per sequence sampling.  figure   shows differences between approaches to  dropout.  the approach of  gal        differs from ours  in the overall strategy   they consider network s  hidden state as input to subnetworks that compute gate values and cell updates and the purpose of dropout is to regularize these subnetworks. our approach considers the architecture  as a whole with the hidden state as its key part  and regularize the whole network. the approach  of  moon et al.        on the other hand is seemingly similar to ours. in section  .  we argue that  our method is a more principled way to drop recurrent connections in gated architectures.  it should be noted that while being different  the  three discussed dropout schemes are not mutually  exclusive. it is in general possible to combine our  approach and the other two. we expect the merge  of our scheme and that of  gal        to hold the  biggest potential. the relations between recurrent  dropout schemes are however out of scope of this  paper and we rather focus on studying the relationships of different dropout approaches with the  conventional forward dropout.  gated recurrent unit  gru  networks are  a recently introduced variant of a recurrent network with hidden state protected by  gates  cho et al.       . different from lstms   gru networks use only two gates rt and zt to  update the cell s hidden state ht                    zt    wz  xt   ht       bz         wr xt   ht     br    rt        gt   f  wg xt   rt   ht     bg                     ht    f  ct           g    o    i         ht    ht    f            ct    ct       a  moon et al.          g    o    i         ht    ht    f                 ct    ct       b  gal          g    o    i         ht         ct             c  ours    figure    illustration of the three types of dropout in recurrent connections of lstm networks. dashed  arrows refer to dropped connections. input connections are omitted for clarity.  ht        zt     ht     zt   gt            similarly to the lstms  we propoose to apply  dropout to the hidden state updates vector gt    ht        zt     ht     zt   d gt              we found that an intuitive idea to drop previous hidden states directly  as proposed in  moon et al.         produces mixed results. we  have observed that it helps the network to generalize better when not coupled with the forward  dropout  but is usually no longer beneficial when  used together with a regular forward dropout.  the problem is caused by the scaling of neuron activations during inference. consider the hidden state update rule in the test phase of an lstm  network. for clarity  we assume every gate to be  equal to     ht    ht     gt  p             where gt are update vectors computed by eq.    and p is the probability to not drop a neuron. as  ht   was  in turn  computed using the same rule   we can rewrite this equation as   ht     ht     gt    p   gt  p            recursively expanding h for every timestep results in the following equation   ht       h    g   p   g   p   ... p   gt  p       pushing p inside parenthesis  eq.    can be written  as   t  x  t    pt i   gi        ht   p h     i      since p is a value between zero and one  sum components that are far away in the past are multiplied by a very low value and are effectively removed from the summation. thus  even though    the network is able to learn long term dependencies  it is not capable of exploiting them during  test phase. note that our assumption of all gates  being equal to   helps the network to preserve hidden state  since in a real network gate values lie  within        interval. in practice trained networks  tend to saturate gate values  karpathy et al.         what makes gates to behave as binary switches.  the fact that moon et al.        have achieved an  improvement can be explained by the experimentation domain. le et al.        have proposed a  simple yet effective way to initialize vanilla rnns  and reported that they have achieved a good result  in the speech recognition domain while having  an effect similar to the one caused by eq.   . one  can reduce the influence of this effect by selecting a low dropout rate. this solution however is  partial  since it only increases the number of steps  required to completely forget past history and does  not remove the problem completely.  one important note is that the dropout function  from eq.   can be implemented as      mask   x p  if train phase        d x     x  otherwise  in this case the above argument holds as well  but  instead of observing exponentially decreasing hidden states during testing  we will observe exponentially increasing values of hidden states during  training.  our approach addresses the problem discussed  previously by dropping the update vectors g.  since we drop only candidates  we do not scale  the hidden state directly. this allows for solving  the scaling issue  as eq.    becomes   ht   ph       t  x  i      p gi   ph    p    t  x    gi            i      moreover  since we only drop differences that are  added to the network s hidden state at each timestep  this dropout scheme allows us to use per step     mask sampling while still being able to learn longterm dependencies. thus  our approach allows to  freely apply dropout in the recurrent connections  of a gated network without hindering its ability to  process long term relationships.  we note that the discussed problem does not  affect vanilla rnns because they overwrite their  hidden state at every timestep. lastly  the approach of gal        is not affected by the issue  as well.      experiments  first  we empirically demonstrate the issues linked  to memory loss when using various dropout  techniques in recurrent nets  see sec.  .  .  for this purpose we experiment with training  lstm networks on one of the synthetic tasks  from  hochreiter and schmidhuber         specifically the temporal order task. we then validate  the effectiveness of our recurrent dropout when  applied to vanilla rnns  lstms and grus on  three diverse public benchmarks  language modelling  named entity recognition  and twitter  sentiment classification.   .     synthetic task    data. in this task the input sequences are generated as follows  all but two elements in a sequence  are drawn randomly from  c  d  and the remaining two symbols from  a  b . symbols from  a   b  can appear at any position in the sequence.  the task is to classify a sequence into one of four  classes   aa  ab  ba  bb   based on the order  of the symbols. we generate data so that every sequence is split into three parts with the same size  and emit one meaningful symbol in first and second parts of a sequence. the prediction is taken  after the full sequence has been processed. we use  two modes in our experiments  short with sequences of length    and medium with sequences  of length   .  setup. we use lstm with one layer that contains     hidden units and recurrent dropout with   .  strength. network is trained by sgd with a  learning rate of  .  for  k epochs. the networks  are trained on     mini batches with    sequences  and tested on   k sequences.  results. table   reports the results on the  temporal order task when recurrent dropout is  applied using our method and methods from   moon et al.        and   gal       .  us     ing dropout from  moon et al.        with persequence sampling  networks are able to discover  the long term dependency  but fail to use it on  the test set due to the scaling issue. interestingly   in medium case results on the test set are worse  than random. networks trained with per step sampling exhibit different behaviour  in short case  they are capable of capturing the temporal dependency and generalizing to the test set  but require       times more iterations to do so. in  medium case these networks do not fit into the  allocated number of iterations. this suggests that  applying dropout to hidden states as suggested in   moon et al.        corrupts memory cells hindering the long term memory capacity of lstms.  in contrast  using our recurrent dropout methods  networks are able to solve the problem in all  cases. we have also ran the same experiments for  longer sequences  but found that the results are  equivalent to the medium case. we also note that  the approach of  gal        does not seem to exhibit the memory loss problem.   .  word level language modeling  data. following mikolov et al.        we use  the penn treebank corpus to train our language  modeling  lm  models. the dataset contains approximately   million words and comes with predefined training  validation and test splits  and a  vocabulary of   k words.  setup. in our lm experiments we use recurrent networks with a single layer with     cells.  network parameters were initialized uniformly in     .     .   . for training  we use plain sgd  with batch size    with the maximum norm gradient clipping  pascanu et al.       . learning rate   clipping threshold and number of backpropagation through time  bptt  steps were set to        and    respectively. for the learning rate decay we use the following strategy  if the validation error does not decrease after each epoch  we  divide the learning rate by  . . the aforementioned choices were largely guided by the work  of mikolov et al.       . to ease reproducibility  of our results on the lm and synthetic tasks  we  have released the source code of our experiments  .  results. table   reports the results for lstm  networks. we also present results when the  dropout is applied directly to hidden states as  in  moon et al.        and results of networks       https   github.com stas semeniuta drop rnn     sampling  per step  per sequence    moon et al.         short sequences medium sequences  train  test train  test    gal         ours  short sequences medium sequences  train  test train  test                                                                                                                 table    accuracies on the temporal order task.  dropout rate    sampling    moon et al.         valid test    gal         valid test    ours  valid test     .    .     .    .     .        per step  per step  per sequence  per sequence       .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .     .      .     .      .    .     .    .     .        per step  per step  per sequence  per sequence      .      .      .      .      .       .      .      .     .      .       .     .     .     .     .       .     .     .     .     .       .     .      .     .      .       .     .     .     .      .     table    perplexity scores of the lstm network on word level language modeling task  lower is better .  upper and lower parts of the table report results without and with forward dropout respectively. networks  with forward dropout use  .  and  .  dropout rates in input and output connections respectively. values  in bold show best results for each of the recurrent dropout schemes with and without forward dropout.  trained with the dropout scheme of  gal       .  we make the following observations   i  our approach shows better results than the alternatives    ii  per step mask sampling is better when dropping hidden state directly   iii  on this task our  method using per step sampling seems to yield results similar to per sequence sampling   iv  in this  case forward dropout yields better results than any  of the three recurrent dropouts  and finally  v  both  our approach and that of  gal        are effective  when combined with the forward dropout  though  ours is more effective.  we make the following observations   i  dropping hidden state updates yields better results than  dropping hidden states   ii  per step mask sampling is better when dropping hidden state directly    iii  contrary to our expectations  when we apply  dropout to hidden state updates per step sampling  seems to yield results similar to per sequence sampling   iv  applying dropout to hidden state updates rather than hidden states in some cases leads  to a perplexity decrease by more than    points   and finally  v  our approach is effective even when    combined with the forward dropout   for lstms  we are able to bring down perplexity on the validation set from     to   . .  to demonstrate the effect of our approach on the  learning process  we also present learning curves  of lstm networks trained with and without recurrent dropout  fig.   . models trained using  our recurrent dropout scheme have slower convergence than models without dropout and usually have larger training error and lower validation  errors. this behaviour is consistent with what is  expected from a regularizer and is similar to the  effect of the feed forward dropout applied to nonrecurrent networks  hinton et al.       .   .  character level language modeling  data. we train our networks on the dataset described in the previous section. it contains approximately   million characters  and a vocabulary of     characters. we use the provided partitions train   validation and test partitions.  setup. we use networks with      units to solve  the character level lm task. the characters are     dropout rate    sampling    moon et al.         valid  test    gal         valid test    ours  valid test     .    .     .    .     .        per step  per step  per sequence  per sequence     .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .    .     .    .     .        per step  per step  per sequence  per sequence     .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .       table    bit per character scores of the lstm network on character level language modelling task   lower is better . upper and lower parts of the table report results without and with forward dropout respectively. networks with forward dropout use  .  and  .  dropout rates in input and output connections  respectively. values in bold show best results for each of the recurrent dropout schemes with and without  forward dropout.            with dropout  without dropout                                                              figure    learning curves of lstm networks  when training without and with  .   per step recurrent dropout. solid and dashed lines show  training and validation errors respectively. best  viewed in color.    embedded into     dimensional space before being processed by the lstm. all parameters of  the networks are initialized uniformly in    .      .   . we train our networks on non overlapping  sequences of     characters. the networks are  trained with the adam     algorithm with initial  learning rate of  .    for    epochs. we decrease  the learning rate by  .   after every epoch starting  from epoch   . to avoid exploding gradints  we  use maxnorm gradient clipping with threshold set  to   .  results. results of our experiments are given  in table  . note that on this task regularizing only the recurrent connections is more beneficial than only the forward ones. in particular  lstm networks trained with our approach  and the approach of  gal        yield a lower  bit per character  bpc  score than those trained  with forward dropout onlywe attribute it to pronounced long term dependencies. in addition   our approach is the only one that improves over  baseline lstm with forward dropout. the overall best result is achieved by a network trained  with our dropout with  .   dropout rate and perstep sampling  closely followed by network with  gal        dropout.   .  named entity recognition  data. to assess our recurrent named entity  recognition  ner  taggers when using recurrent  dropout we use a public benchmark from conll           tjong kim sang and de meulder       .  the dataset contains approximately    k words  split into train  validation and test partitions. each  word is labeled with either a named entity class  it belongs to  such as location or person  or  as being not named. the majority of words are  labeled as not named entities. the vocabulary size  is about   k words.  setup. previous state of the art ner systems  have shown the importance of using word context features around entities. hence  we slightly  modify the architecture of our recurrent networks  to consume the context around the target word  by simply concatenating their embeddings. the  size of the context window is fixed to   words   the word to be labeled  two words before and  two words after . the recurrent layer size is       units. the network inputs include only word  embeddings  initialized with pretrained word vec  embeddings  mikolov et al.        and kept static   and capitalization features. for training we use the  rmsprop algorithm  dauphin et al.        with    fixed at  .  and a learning rate of  .   and multiply the learning rate by  .   after every epoch.  we also combine our recurrent dropout  with persequence mask sampling  with the conventional  forward dropout with the rate  .  in input and  .   in output connections. lastly  we found that using relu x    max x     nonlinearity resulted in  higher performance than tanh x .  to speed up the training we use a length expansion approach described in  ng et al.          where training is performed in two stages   i  we  first sample short   words input sequences with  their contexts and train for    epochs   ii  we  fine tune the network on input    words sequences  for    epochs. we found that further fine tuning  on longer sequences yielded negligible improvements. such strategy allows us to significantly  speed up the training when compared to training  from scratch on full length input sentences. we  use full sentences for testing.  results. f  scores of our taggers are reported in  table   when trained on short   word and longer     word input sequences. we note that the gap  between networks trained with and without our  dropout scheme is larger for networks trained on  shorter sequences. it suggests that dropout in recurrent connections might have an impact on how  well a network generalizes to sequences that are  longer than the ones used during training. the    dropout rate    rnn    lstm    gru      word long sequences   .     .     .     .     .      .     .     .       word long sequences   .     .     .     .     .      .     .     .    table    f  scores  higher is better  on ner task.  gain from using recurrent dropout is larger for  the lstm network. we have experimented with  higher recurrent dropout rates  but found that it  led to excessive regularization.   .  twitter sentiment analysis  data.  we use twitter sentiment corpus from semeval      task     subtask  b   rosenthal et al.       .  it contains   k  labeled tweets split into training and validation  partitions. the total number of words is approximately    k and the vocabulary size is   k.  the task consists of classifying a tweet into three  classes  positive  neutral  and negative.  performance of a classifier is measured by the average of f  scores of positive and negative  classes. we evaluate our models on a number of  datasets that were used for benchmarking during  the last years.  setup. we use recurrent networks in the standard  sequence labeling manner   we input words to a  network one by one and take the label at the last  step. similarly to  severyn and moschitti          we use   million of weakly labeled tweets to pretrain our networks. we use networks composed of      neurons in all cases. our models are trained  with the rmsprop algorithm with a learning rate  of  .   . we use our recurrent dropout regularization with per step mask sampling. all the other  settings are equivalent to the ones used in the ner  task.  results. the results of these experiments are presented in table  . note that in this case our algorithm decreases the performance of the vanilla  rnns while this is not the case for lstm and  gru networks. this is due to the nature of the  problem  differently from lm and ner tasks   a network needs to aggregate information over a  long sequence. vanilla rnns notoriously have  difficulties with this and our dropout scheme im      dropout rate    twitter      livejournal          .        .      .        .      .          .        .      .        .      .          .        .      .        .      .      twitter      rnn    .      .    lstm    .      .    gru    .      .      twitter      sms      sarcasm        .      .        .      .        .      .        .      .        .      .        .      .        .      .        .      .        .      .      table    f  scores  higher is better  on sentiment evaluation task  pairs their ability to remember even further. the  best result over most of the datasets is achieved  by the gru network with recurrent dropout. the  only exception is the twitter     dataset  where  the lstm network shows better results.      conclusions  this paper presents a novel recurrent dropout  method specifically tailored to the gated recurrent  neural networks. our approach is easy to implement and is even more effective when combined with conventional forward dropout. we  have shown that for lstms and grus applying  dropout to arbitrary cell vectors results in suboptimal performance. we discuss in detail the cause of  this effect and propose a simple solution to overcome it. the effectiveness of our approach is verified on three different public nlp benchmarks.  our findings along with our empirical results  allow us to answer the questions posed in section    i  while is straight forward to use dropout  in vanilla rnns due to their strong similarity with  the feed forward architectures  its application to  lstm networks is not so straightforward. we  demonstrate that recurrent dropout is most effective when applied to hidden state update vectors in lstms rather than to hidden states   ii   we observe an improvement in the network s performance when our recurrent dropout is coupled  with the standard forward dropout  though the  extent of this improvement depends on the values of dropout rates   iii  contrary to our expectations  networks trained with per step and persequence mask sampling produce similar results  when using our recurrent dropout method  both  being better than the dropout scheme proposed by  moon et al.       .    while our experimental results show that applying recurrent dropout method leads to significant improvements across various nlp benchmarks  especially when combined with conventional forward dropout   its benefits for other tasks   e.g.  sequence to sequence prediction  or other domains  e.g.  speech recognition  remain unexplored. we leave it as our future work.    acknowledgments  this project has received funding from the european union s framework programme for research and innovation horizon                 under the marie skodowska curie agreement no.       . stanislau semeniuta thanks  the support from pattern recognition company  gmbh. we gratefully acknowledge the support of  nvidia corporation with the donation of the titan x gpu used for this research.    