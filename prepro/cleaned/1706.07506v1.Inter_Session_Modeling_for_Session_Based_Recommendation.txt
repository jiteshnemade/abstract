introduction    in a session based setting  the actions of the user within a session are correlated. this means that a recommender system can observe the user s actions    this work was carried out at the telenor ntnu ai lab  hosted by the department of  computer science  norwegian university of science and technology.          and improve the recommendations as the system learns more about the user s  interest. recently  rnns has been shown to work well in the session based  setting hidasi et al.         twardowski         zhang et al.         liu et al.        . rnns are naturally good at working with sequences of data  because  they have an internal memory storing the past observations  and the ability to  update and discard information in their memory. therefore  a rnn will make  more accurate recommendations as it learns more about a user. this also means  that a simple rnn will struggle to make good recommendations at the start  of a session. the advantage of employing an rnn over many other recommendation prediction models  is that it naturally considers the order of sequences.  many other models use the relaxed assumption that the order does not matter.  solutions that take the sequence into account are possible  but rnns considers  the order of sequences in a very natural way that few other models do.  in many of the services where user interaction is session based  the users  are logged in and their actions can be stored. if this is done  a recommender  system can get access to the user s history  which it can use to improve the  recommendations. a rnn could possibly use information from a users history  to make precise recommendations from the start of a new session  and possibly  improve all recommendations in that session. in the session based scenario  the  user history consists of a ordered sequence of sessions  and the rnn could also  be used to process user history  as we show in our proposed architecture.  session based recommender systems that only consider the current session  face the task of doing recommendations based only on small set of interactions.  collaborative filtering approaches fall short here  and usually content based  filtering is used instead  i.e. recommending similar items. the data in a session  consists of a sequence of user actions. the sequences may vary in length. also   the actions within a session are likely to be dependent. these properties fit well  with rnns  and therefore they can perform well in this setting. intuitively  a  rnn should be able to capture dependencies between items  like content based  filtering  but with its memory capabilities a rnn should also be able to consider  the whole session  which could lead to more accurate predictions. recent papers  have shown promising results in using rnns for session based recommendation  hidasi et al.         tan et al.      a .  when user history is available to the recommender system  collaborative  filtering approaches  such as matrix factorization  can perform well. a rnn can  still perform well  but it will probably struggle at the start of the sessions until it  has learned what the user is interested in. this problem could of course be fixed  if the rnn was able to learn from the user history before starting the session   and thus have a foundation to make recommendations on  right from the start.  this extra information could potentially improve the overall recommendations   and especially the initial recommendations in each session. in a session based  setting where user history is available  the user history consists of past sessions.  so the user history is a sequence of sessions  and each session is a sequence of  events. this brings us back to our motivation for this work. the idea is to use  a rnn to do predictions within a session  and to employ another rnn layer  to contribute in the prediction of the users interests for the next session. let s        consider the example of a user shopping on a e commerce site. one day he  might buy a laptop  some days later he will buy some hiking gear  and some  days after that he buys accessories to the laptop he bought in the first session.  this illustrates that the users interest in a session can be dependent on what  he did in earlier sessions  and that just considering the previous session is not  enough.  in this work we will investigate how a rnn can be used to learn from user  histories  and thereby improve the straightforward use of rnn in session based  recommendations. in particular  the contributions will be the following      we  introduce the concept of inter session learning      we propose a way to learn  the inter session behavior together with the intra session one     we validate  the effectiveness of the model by an extensive set of experiments      we show  the effectiveness of the method in tackling the cold start problem.         related work    in recent year  different deep learning techniques have been successfully employed in the recommendation context. in particular the use of rnn has been  shown to be promising in the area of session based recommendation. in this  section we will present the state of the art in these areas. the idea of using  rnn in a straightforward way for session based recommendation has been first  introduced in hidasi et al.       . further works  extending this initial work  still employing an rnn for the recommendation task  have been presented in  tan et al.      b   hidasi et al.         jing and smola         wu et al.          song et al.         liu et al.         twardowski       . in hidasi et al.         the authors shows that a basic rnn for session based recommendations  can  achieve remarkable results. they also deal with sparsity issues  and introduce a  new ranking loss function for training the network. they experimented with  two different datasets. both datasets contain sequences of user clicks with  timestamps. one dataset has clicks on items from an e commerce site from  the recsys challenge         while the other contains clicks on videos from a  youtube like platform. various modifications of the network were tested. in  tan et al.      b  the authors explored various ways for improving the model  proposed in hidasi et al.         used as baseline. they experiment with techniques that have worked well when neural networks have been applied to other  problems  to see if those techniques can improve performance of a rnn sessionbased recommender as well. they experimented their proposed models on the  same dataset based on four different way of improving the original model. they  improved the original methods by mainly a  applying a data augmentation technique for tackling with the problem of session varying in length  b  accounting  for temporal shifts in the data distribution related to users behavior to tackle  cases in which the products are released in different periods of time. so far   the presented works refer to models that performs predictions solely based on  the items clicked  where the items are only represented by an id often in the    recsys    challange       http       .recsyschallenge.com challenge.html          form of a one hot vector. clearly  additional information  both about the item  and about the sessions  could help improve the predictions. some possible additional information about the item is be the category of the item  an image   and a textual description of the item. additional information about the session  could be timestamps of the clicks  geo location of the user  and weather. in  liu et al.         for example  the authors suggest that modeling the time of  a session and the transition time between events in the session both can give  better performance. assumption about the temporal dimension is also done in  other works. in song et al.         the main assumption of the authors is that  user interests change over time. as an example  in elkahky et al.         it was  shown that users who visited spiegel.de  a popular german news portal  were  likely to be interested in football related news. the reason was that the data  was collected around the time of the football world cup of     . similarly   user interests may change over time  e.g. during summer and christmas. the  authors propose to use a model that combines static and temporal user features.  the static features are learned by using the full training set  while the temporal features are learned by only training on the most recent examples. the  time aspect is also considered in jing and smola        for the task of  just in  time  recommendation  where the objective is to recommend the right items  at the right time. the inter session dependency are here considered to learn  recurrent user activities by a lstm based architecture. the work proposed in  wu et al.        start from the assumption that many of the current state of  the art approaches and methods for recommendation are lacking when it comes  to temporal and causal aspects inherent in the data. in particular they state  that user profiles and movie attributes are generally considered static.they proposed a rnn based model considering these aspects and modelling the user and  movie dynamics. the model is shown to be able to capture temporal patterns  in rating data  outperforming previous works in term of prediction accuracy often  other extra information are available. in twardowski         the authors  propose a model employing the item extra information available for example  in some e commerce site such as the type of action the user performed  i.e.   viewing an item  adding it to the basket  removing it from the basket  or buying it . they propose a rnn based model that makes use of this information  for recommendations. the proposed model sends embedded event information  through a rnn layer  the output is concatenated with an embedded item representation  before being sent through feed forward layers to produce a prediction.  on a dataset with rich search contextual information  the proposed rnn model  performs significantly better than other compared models and baselines. while  on a dataset with less events and data  the rnn based model performed worse  than a matrix factorization model that was also customized to utilize event information. sometimes  in some e commerce site  items are also described and  associated with information such as picture and textual description. in hidasi  et al.         the authors explore the possibility of employing this richer features  representation in a number parallel rnn architectures to model sessions on the  clicks and the rich features  text and images  of the clicked item.               the ii rnn architecture    in the session based setting  the user s actions might depend on all earlier actions  in the session  not just the previous one. how the dependencies between the  actions work  will vary between different domains. for example  on a news site   if a user reads articles about german news and international sports  that user  will probably be interested in reading news articles about german sport  while  for a online grocery shopping site  past actions might indicate that the user will  not be interested in similar items. if the user has added bread and milk to his  basket  he will probably not add anymore bread or milk to that basket. but  if the user has only added milk to the basket  it might be interested in adding  bread as well.     .     main idea    rnns work well in the session based recommendation scenario because it can  process sequences of user actions  and create an internal representation of the  user s interests. also  it does not assume that all actions indicate interest in  something  it can learn to interpret actions as sign of disinterest. as discussed  in the previous section  the rnn model achieves state of the art performance  on session based recommendation problems.  in addition to the short term dependencies between actions within a session   there are usually long term dependencies between actions from different sessions. e.g.  a user that was interested in news articles about golf in his previous  session s   will probably also have that interest in his current session. or a user  that bought a new laptop in a recent previous session  will probably not be interested in buying another one in the current session  but he might be interested  in accessories to the laptop he bought. this means that it should be possible to  improve the recommendations for a session based recommender system  by giving it information about the user s interaction history. furthermore  one of the  reasons that a rnn works well for recommendations within a session  is that  it is able to process the sequence of the session events. similarly  we believe  that the order of the sequence of earlier sessions can be important. en example  could be a person that regularly does his grocery shopping online. if he buys  bread in one session  then he will probably not be interested in buying another  one within the next few sessions. on the other hand  he is probably going to  buy bread soon if he has not done so during the last few sessions.  since rnns work well for recommendations on sequences of events within a  session  and because the sessions themselves form a sequence  we think that a  rnn could work well to process the sequence of sessions as well.  the main idea is to use one rnn to process the events within a session  as  has been done before  and to enhance the recommendations from this by using  a second rnn to process a user s recent sessions and help the first rnn with a  initial prediction about the current session. in other words  a rnn that works  on a inter session level  provides the initial hidden state for a rnn that works on  a intra session level. we will refer to this model as ii rnn  inter intra rnn .         .     problem formulation    in the session based recommendation scenario  there is a system with a set of  items that a user can interact with  note that the term  item  is used in a  broad sense here. we experiment with the proposed models using two different  datasets  where the possible recommendations are sub forums of a discussion  site and artists on a music website  respectively. the datasets are described in  section  .  let n be the set of items in the system  and nv   rd is the embedded representation of item v. each user u has a interaction history s u    stu    stu    . . .    where s u is a session of interaction by user u at time ti . the session history is  ordered temporally by ti . the session length is  su  . each session stui consists  of a collection of events  euti  j   rm  j         ...   sti  u     where euti  j is the representation of event j in the session. while events can be any type of interaction  in general  events in this work will simply be items the user interacts with.  hence  an event will relate directly to an item v. all recommendation models  we experiment with use an item id  iv          ...   n     as input for each item.  however  the rnn models retrieves the corresponding embedded representation  nv for each iv   and feed those into the rnn layer of the model. the common  task for all the recommendation models we experiment with is to predict each  consecutive item in a session stui . that is  for a sub session  euti      euti      . . .   euti  j    of stui   the system is to predict euti  j   . this is repeated for j         . . .  stui      .  a recommendation rj is an ordered list of k recommended items  where we  would want to see the next item  euti  j     as close to the top as possible.     .     model description    ii rnn combines the modeling of the inter session with the intra session behavior of a single architecture. the first the model is similar to the one used in  hidasi et al.         and the model proposed in hidasi et al.        will therefore  serve as a baseline to compare the ii rnn model to.  intra session rnn the intra session rnn produces recommendations by  processing the sequence of items in a session. figure   illustrates the model.  this model is very similar to the one in hidasi et al.        and other papers.  we do not use one hot encodings as input  but use item embeddings directly.  mathematically these two methods are equivalent  but in practice this saves us  the computation required to create the one hot vectors. when the set of items  is huge  creating a mini batch of one hot vectors will require a large amount of  memory  which can be a problem.  the embedded item representation is sent through one or multiple layers of  gru  and dropout is applied to these layers. afterwards a feed forward layer  is used to scale up the vector to r n   . the output vector is then  ov  ov  ...ov n      where ovi is a score for item vi   n . the list of recommendations  rj is then  created by taking items corresponding to the k highest scores  sorted by their  score. training is done with the adam algorithm for stochastic gradient descent          figure    the intra session rnn  kingma and ba         and the loss is calculated with cross entropy. the target  output is a score of   for all items  except for the relevant item which should  get a score of  . this means that we treat the recommendation problem as a  classification problem. that is  given the users recent activity  predict the next  item he will interact with. this works because the model predicts scores for  how likely it believes that each item is the correct class  and these scores then  form a natural way of ranking the recommendations.  ii rnn model although the intra session rnn can achieve a strong performance  it starts out in each session without any knowledge about the user.  it learns about the user s interests throughout the session  but all that information is discarded again at the end of that session. the ii rnn can improve  upon the intra session rnn  because it takes the user s previous sessions into  account  and supplies the intra session part with information at the start of  each new session. figure   illustrates the ii rnn. for each session stui in a  user s interaction history su   let suti be an embedded vector representation of  that session. the input to the inter session rnn layer  the gru layer in figure     is then the sequence  sutz g   sutz g     . . .   sutz    where sutz is the representation  of the most recent session  and g is the number of recent sessions that should  be processed. the initial hidden state  h    of the intra session rnn is then set  to final output of the inter session rnn. in other words  the inter session rnn  produce the initial hidden state of the intra session rnn  based on a series of  vector representations of the most recent sessions for the given user. the output of the inter session rnn is calculated before the intra session rnn starts        producing predictions.  we apply two different methods of producing the session representations suti .  one is the average of the embedded vector representations of the items in the  session  as illustrated in figure  a. the other is to simply use the the last hidden  state of the intra session rnn as the session representation  illustrated in figure   b. even though the final hidden state can contain more useful information  learned by the intra session rnn  it is more a representation of the end of the  session  rather than the whole session. since the hidden state is produced by a  rnn  it will depend on the order of the sequence of items in a session  while  the average of the embeddings is unaffected by the order of the items.        .     experimental setting  datasets    we experimented with two different datasets  the first is a dataset on user  activity on the social news aggregation and discussion website reddit  . this  dataset contains tuples of usernames  a subreddit where the user made a comment to a thread  and a timestamp for the interaction. the second dataset  contains listening habits of users on the music website last.fm bertin mahieux  et al.       . this dataset contains tuples of user  timestamp  artist  and song  listened to.  reddit dataset the reddit dataset contains a log of user interaction on  different subreddits  sub forums   with timestamps. here  an interaction is when  a user adds a comment to a thread. since the dataset itself  does not split the  interactions into sessions  we did this manually when preprocessing the dataset.  to do this we analyzed the dataset and specified a time limit for inactivity.  using the timestamps  we let consecutive actions that happened within the  time limit belong to the same session. that is  for a specified time limit  t    and a list of a user s interactions  at    at    . . .   atn    ordered by their timestamps  ti   two consecutive interactions ati and ati   belong to the same session if and  only if ti     ti    t . we set the time limit to   hour       seconds . note  that users  in addition to commenting on threads  also do browsing and reading.  therefore it makes sense to set a time limit that allows for some time between  the interactions captured in the dataset. also some users are more active than  others  some users are mostly passive consumers who rarely comments. so  it is  impossible to set a time limit that fits all users. however  it is important that  the time limit is large enough that the average session contains a fair amount  of interactions  but small enough so that it is reasonable to assume that the  interactions are dependent on each other.    subreddit  interactions  subreddit interactions    dataset     https   www.kaggle.com colemaclean            a  the ii rnn model with average pooling to create session representations from items.     b  the ii rnn model where the last hidden state of the intra  session  rnn is stored as the session representation.    figure    the proposed ii rnn architectures          number  number  sessions  average  number    of users  of sessions  per user  session length  of items    reddit    last.fm      .      .   .                  .               .                   .       table    statistics for the datasets after preprocessing  last.fm dataset we also had to split each user s history into sessions manually for the last.fm dataset. we used the same approach as for the reddit  dataset  but here we used    minutes       seconds  as the time limit. also  we  faced the problem that the dataset contains an overwhelming amount of songs.  since our recommendation models produce a score for each possible item  the  huge amount of songs caused a memory requirement problem. to solve this  we  simplified the dataset by ignoring the specific song of each user interaction and  only use the artists. this reduce the item set to a manageable size.     .     preprocessing    after the initial manual splitting into sessions  we used the same preprocessing  for all three datasets. in the reddit and last.fm datasets  there were many items  that repeated consecutively. we are not interested in a recommender system  that learns to predict the last seen item  therefore we removed all consecutively  repeating items  and only kept one instance. furthermore  the rnn models  need to have a specified maximum length of the sessions  because they must be  unrolled in order to be trained. to deal with this  we set the maximum length  l   of a session to l     . sessions that had a length l of l   l    l were split into  two sessions. this was done because we did not want to throw away all sessions  that were too long  but splitting very long sessions create many sessions that  should not be separate sessions  since the events in them depend on each other.  however  there were some unreasonable long sessions that probably originate  from bots or some other error source. these were removed with the  l limit for  session lengths. with this scheme  the majority of the sessions from all datasets  were kept. sessions of length l     were removed  and users with less than    sessions were also removed. finally  the datasets were split into a training set  and a test set on a per user basis. for each user      of his sessions were placed  in the training set  and the remaining in the test set. each user s sessions were  sorted by the timestamp of the earliest event in the session  and the test set  contains the most recent sessions of each user. table   shows statistics for the  two datasets after preprocessing  before splitting into training and test sets .            .     baselines    in addition to the following baselines  the intra session rnn itself forms a baseline for the ii rnn.  most popular the most popular baseline is a very simple baseline  but it  can perform decently in some cases. all items are sorted by their number of  occurrences in the training set  and the top k items are recommended at each  time step. although a very basic baseline  it provides a nice sanity check. any  serious model should be able to beat this model.  most recent even though we removed consecutive repetitions of items in all  sessions  there could still be a high repetitiveness of items within sessions  i.e.  some items can occur multiple times in a session . especially in the reddit  and last.fm datasets  where users can tend to interact with some subreddits or  artists multiple times in their sessions. we believe that it is less likely to see  such repetitiveness in the instacart dataset  because users probably only add  each item to their cart once. the most recent baseline behaves as a stack. it  is initially filled with k random items. for each time step  the item interacted  with is added to the top of the stack  and the item at the bottom is pushed  out of it. however  if the new item is already in the stack  it is just moved to  the top. the recommendation at each time step is then the stack of recently  seen items  where the top recommendation is the item just interacted with. our  model should be able to beat this baseline significantly. but the most recent  baseline gives us information about the diversity of items within sessions  item knn item k nearest neighbors  item knn  is a simple  but usually  strong baseline. it is commonly used in practice as a item to item recommender  linden et al.       . different implementations are possible. we implemented it  as follows. for each item in the dataset  we count the number of co occurrences  with the other items in the dataset. a co occurrence is when two items appear  in the same session. when testing  the algorithm recommends the top k items  with highest co occurrences with the last seen item.  bpr mf the bayesian personalized ranking for matrix factorization  bprmf  rendle et al.        is a commonly used matrix factorization method. it  tries to predict personal pairwise rankings of unseen items  i.e. given a user  and two items  bpr  mf tries to predict which of the two items the user would  rate higher . we use an existing implementation    that we tweak slightly to fit  our use case. the original implementation does not recommend already seen  items  but in our case  users often interact with items they have already seen.  bpr mf computes feature vectors for users and items based on the users earlier  interactions  and is then able to make a recommendation based on this. this  means that the recommendations will be the same throughout future sessions     theano bpr     https   github.com bbc theano bpr           embedding size  learning rate  max. recent session representations  mini batch size  number of gru layers  intra session level  number of gru layers  inter session level    reddit    last.fm         .                            .      .                 table    best configurations for the rnn models. we found that the configurations that worked well for the ii rnn  worked well for the standalone intrasession rnn as well. not all configurations are applicable to the standalone  intra session rnn.  unless the model is re trained. in other words  bpr mf cannot be applied  directly to session based recommendations. to make a more fair comparison   we create a new split of the datasets. only the last session of each user is put  in the test set. bpr mf still produce the same recommendations for all time  steps in the test session for a given user.     .     evaluation and hyperparameters tuning    we used recall k and mrr k with k             to evaluate all models. in  addition to the baselines already discussed  we also compared the intra session  rnn to the ii rnn on the two presented datasets. we experimented with minibatch sizes  embedding sizes  learning rate  dropout rate  using multiple gru  layers  and number of session representations to find the best configurations for  each dataset. the best configurations we found are summarized in table  . we  employed two different configurations of the ii rnn  one using average pooling  and the other using last hidden state as session representations for past sessions.  we used the same size for the item embeddings and internal vectors in the gru  layers. we found tanh to work well as activation function in the gru layers   and did not investigate other alternatives.     .     creating mini batches    we want our model to be biased towards recent user trends. this is often  desirable in practice  and we find it reasonable to assume that it applies for  our datasets. furthermore  the way we split our dataset into training  and  test sets reflect this. i.e. the test set contains the most recent samples for  each user. this leaves us with two desirable properties for how the training  samples should be processed. first  more recent samples should be processed  last. second  each mini batch should contain a variety of users. i.e. no user  should be over represented with samples in any mini batch. to achieve these  properties  we constructed the following scheme for creating mini batches. each         training sample  a session  is associated with a user. all sessions belonging to  the same user  are grouped together and sorted oldest to newest.     .     implementation details    the implementation is done in python  . .   with the tensorflow machine learning software library. we run our experiments on three different computers  all  with the ubuntu   .   operating system. all computers have at least    gb of  ram  and a nvidia geforce gtx        gb or better. the code is available  on github here  .         results and discussion    we evaluate the performance of the proposed models by using the standard evaluation metrics presented in the previous section. the comparison is performed  over the baselines from literature over the two datasets  last.fm and reddit.     .     inter session model effectiveness    we found that using multiple gru layers did not improve performance neither  when applied at the inter session level  nor at the intra session level. dropout  was crucial in order to get good results on the last.fm dataset  while on the  reddit dataset the models got better results without dropout. to achieve the  best results  dropout had to be used on all gru layers.  table  a shows an overview of how the models and baselines scored on the  reddit dataset. relative scores are given compared to the standalone intrasession rnn  considered the strongest baseline. we ran the rnn model three  times and the results presented in the table are averages of three runs  even  though the results were usually consistent between runs. similarly  table  b  presents the results for the last.fm dataset. for both datasets  item knn and  rnn were the strongest baselines  but were both clearly outperformed by the  intra session rnn.  when it comes to the two versions of ii rnn  ap using average pooling  and lhs using the last hidden state  cf. figures  a and  b   table  a shows  that using the last hidden state of the intra session rnn as the representation  of a session is slightly better than using average pooling for the reddit dataset   while the results for the last.fm dataset  table  b  are reversed  this time with  ap being better than lhs.  finally  table  a and  b shows how the bpr mf baseline performed on the  hold one out version of the dataset. due to limited space  we only show the  results for the bpr mf. in all cases  the ii rnn significantly outperformed the  standalone intra session rnn and the bpr mf method.    https   github.com olesls master thesis           item knn  most recent  most popular  rnn  ii rnn ap  ii rnn lhs    r      r       r       mrr      mrr       mrr        .       .       .       .       .          .      .          .        .       .       .       .       .          .      .          .        .       .       .       .       .          .      .          .        .       .       .       .       .          .      .          .        .       .       .       .       .          .      .          .        .       .       .       .       .          .      .          .       r      r       r       mrr      mrr       mrr        .       .       .       .       .         .      .         .        .       .       .       .       .          .      .         .        .       .       .       .       .          .      .          .        .       .       .       .       .         .      .         .        .       .       .       .       .         .      .         .        .       .       .       .       .         .      .         .        a  reddit dataset.  item knn  most recent  most popular  rnn  ii rnn ap  ii rnn lhs     b  last.fm dataset.    table    recall and mrr scores for the ii rnn models and the baselines.  relative scores are given compared to the standalone intra session rnn. the  best results per dataset are highlighted. the two ii rnn models differ by how  they feed information to the inter session model  either using average pooling   ii rnn ap  or the last hidden state  ii rnn lhs .    bpr mf  rnn  ii rnn lhs    r      r       r       mrr      mrr       mrr        .       .       .         .       .       .         .       .       .         .       .       .         .       .       .         .       .       .         a  reddit dataset.  bpr mf  rnn  ii rnn ap    r      r       r       mrr      mrr       mrr        .       .       .         .       .       .         .       .       .         .       .       .         .       .       .         .       .       .         b  last.fm dataset.    table    recall and mrr scores for the bpr mf baseline and the rnn models  on the hold one out version of the dataset. only the best performing ii rnn  model is included for each dataset.            .     impact on session cold start problem    the intra session rnn learns about the user as it observes item interactions  throughout a session. it is therefore reasonable to believe that the model s  prediction accuracy increases throughout the session. as discussed  the ii rnn  can improve both the overall recommendations  and especially the first few  recommendations in each session  impacting strongly on the cold start problem  within a session. to evaluate this  we test the rnn models both on the overall  recommendations and on the first n recommendations in a session  for n       . . .      l  where l is the maximum session length. that is  we evaluate the  models on recommendations for the first n time steps  and note that when  n   l we retain the overall score already reported. a comparison between  rnn and ii rnn lhs in terms of recall   are shown in figure  a for the  reddit dataset. notice how the ii rnn already at the first recommendation  of a new session achieves r     .   a substantial     improvement over the  rnn model. while the rnn catches up somewhat as more interactions are  seen in the current session  the ii rnn also improves with more information   and holds a   .   improvement over the rnn at the end of the session. similar  results can be seen in figure  b  where ii rnn ap is compared to the rnn  using the last.fm dataset. again  we see a dramatic improvement early on in a  new session  and even though the rnn catches up some of the ii rnn s   .    lead  the ii rnn remains superior throughout the session.         conclusion    in this paper we have investigated a new rnn architecture for session based  recommendations  termed ii rnn. ii rnn combines modeling of recommendations inside a single session with an inter session rnn that serves as a memory  of user interactions from historical sessions. the two parts are combined into  a single architecture. we have evaluated ii rnn using two publicly available  datasets  and show considerable improvements over strong baselines. furthermore  we found the ii rnn model to be particularly adept to making recommendations early in a user session  thereby helping to alleviate the well known  cold start problem session based recommender systems are confronted with.  we anticipate at least two paths for future research  firstly  while the iirnn model already works well using either of the two methods for creating  session representations  ap and lhs   we will consider other approaches as  well. further improvement can potentially be achieved by for example considering more complex methods for representing each session  or by using other  more advanced attention mechanisms. secondly  we are currently utilizing timeinformation only implicitly through the notion of sessions. we believe that explicitly representing the time difference between sessions will improve the recommendations  and are currently investigating how to efficiently incorporate  this information into the recommendation process.            a  reddit dataset.     b  last.fm dataset.    figure    effect of cold start at the offset of a new session.    