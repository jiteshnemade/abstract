introduction  neural networks have a long history in speech recognition   usually in combination with hidden markov models       .  they have gained attention in recent years with the dramatic  improvements in acoustic modelling yielded by deep feedforward networks       . given that speech is an inherently  dynamic process  it seems natural to consider recurrent neural networks  rnns  as an alternative model. hmm rnn  systems     have also seen a recent revival         but do not  currently perform as well as deep networks.  instead of combining rnns with hmms  it is possible  to train rnns  end to end  for speech recognition           .  this approach exploits the larger state space and richer dynamics of rnns compared to hmms  and avoids the problem of using potentially incorrect alignments as training targets. the combination of long short term memory       an  rnn architecture with an improved memory  with end to end  training has proved especially effective for cursive handwriting recognition         . however it has so far made little  impact on speech recognition.    rnns are inherently deep in time  since their hidden state  is a function of all previous hidden states. the question that  inspired this paper was whether rnns could also benefit from  depth in space  that is from stacking multiple recurrent hidden layers on top of each other  just as feedforward layers are  stacked in conventional deep networks. to answer this question we introduce deep long short term memory rnns and  assess their potential for speech recognition. we also present  an enhancement to a recently introduced end to end learning  method that jointly trains two separate rnns as acoustic and  linguistic models     . sections   and   describe the network  architectures and training methods  section   provides experimental results and concluding remarks are given in section  .   . recurrent neural networks  given an input sequence x    x    . . .   xt    a standard recurrent neural network  rnn  computes the hidden vector sequence h    h    . . .   ht   and output vector sequence y     y    . . .   yt   by iterating the following equations from t      to t    ht   h  wxh xt   whh ht     bh             yt   why ht   by           where the w terms denote weight matrices  e.g. wxh is the  input hidden weight matrix   the b terms denote bias vectors   e.g. bh is hidden bias vector  and h is the hidden layer function.  h is usually an elementwise application of a sigmoid  function. however we have found that the long short term  memory  lstm  architecture       which uses purpose built  memory cells to store information  is better at finding and exploiting long range context. fig.   illustrates a single lstm  memory cell. for the version of lstm used in this paper       h is implemented by the following composite function   it      wxi xt   whi ht     wci ct     bi             ft      wxf xt   whf ht     wcf ct     bf             ct   ft ct     it tanh  wxc xt   whc ht     bc             ot      wxo xt   who ht     wco ct   bo             ht   ot tanh ct             where   is the logistic sigmoid function  and i  f   o and c  are respectively the input gate  forget gate  output gate and     a crucial element of the recent success of hybrid hmmneural network systems is the use of deep architectures  which  are able to build up progressively higher level representations  of acoustic data. deep rnns can be created by stacking multiple rnn hidden layers on top of each other  with the output sequence of one layer forming the input sequence for the  next. assuming the same hidden layer function is used for  all n layers in the stack  the hidden vector sequences hn are  iteratively computed from n     to n and t     to t       hnt   h whn   hn hn      whn hn hnt     bnh        t  where we define h    x. the network outputs yt are  yt   whn y hn  t   by  fig.  . long short term memory cell            deep bidirectional rnns can be implemented by replacing  each hidden sequence hn with the forward and backward se            quences h n and h n   and ensuring that every hidden layer  receives input from both the forward and backward layers at  the level below. if lstm is used for the hidden layers we get  deep bidirectional lstm  the main architecture used in this  paper. as far as we are aware this is the first time deep lstm  has been applied to speech recognition  and we find that it  yields a dramatic improvement over single layer lstm.   . network training    fig.  . bidirectional rnn  cell activation vectors  all of which are the same size as the  hidden vector h. the weight matrices from the cell to gate  vectors  e.g. wsi   are diagonal  so element m in each gate  vector only receives input from element m of the cell vector.  one shortcoming of conventional rnns is that they are  only able to make use of previous context. in speech recognition  where whole utterances are transcribed at once  there  is no reason not to exploit future context as well. bidirectional rnns  brnns       do this by processing the data in  both directions with two separate hidden layers  which are  then fed forwards to the same output layer. as illustrated in        fig.    a brnn computes the forward hidden sequence h          the backward hidden sequence h and the output sequence y  by iterating the backward layer from t   t to    the forward  layer from t     to t and then updating the output layer                       xt   w         h t     b      h t   h wx        h  h h  h                      x   w         h t     b      h t   h wx        h t  h h  h                h t   w     h t   by  yt   w         hy  hy  combing brnns with lstm gives bidirectional lstm        which can access long range context in both input directions.    we focus on end to end training  where rnns learn to map  directly from acoustic to phonetic sequences. one advantage  of this approach is that it removes the need for a predefined   and error prone  alignment to create the training targets. the  first step is to to use the network outputs to parameterise a  differentiable distribution pr y x  over all possible phonetic  output sequences y given an acoustic input sequence x. the  log probability log pr z x  of the target output sequence z  can then be differentiated with respect to the network weights  using backpropagation through time       and the whole system can be optimised with gradient descent. we now describe  two ways to define the output distribution and hence train the  network. we refer throughout to the length of x as t   the  length of z as u   and the number of possible phonemes as k.   . . connectionist temporal classification  the first method  known as connectionist temporal classification  ctc          uses a softmax layer to define a separate output distribution pr k t  at every step t along the input sequence. this distribution covers the k phonemes plus  an extra blank symbol   which represents a non output  the  softmax layer is therefore size k     . intuitively the network decides whether to emit any label  or no label  at every  timestep. taken together these decisions define a distribution over alignments between the input and target sequences.  ctc then uses a forward backward algorithm to sum over all     possible alignments and determine the normalised probability  pr z x  of the target sequence given the input sequence    .  similar procedures have been used elsewhere in speech and  handwriting recognition to integrate out over possible segmentations           however ctc differs in that it ignores  segmentation altogether and sums over single timestep label  decisions instead.  rnns trained with ctc are generally bidirectional  to ensure that every pr k t  depends on the entire input sequence   and not just the inputs up to t. in this work we focus on deep  bidirectional networks  with pr k t  defined as follows                   n h n  hn  yt   w   t   w   t   by  h ny  h y  exp yt  k       pr k t    pk     k     exp yt  k                     where yt  k  is the k th element of the length k     unnormalised output vector yt   and n is the number of bidirectional  levels.   . . rnn transducer  ctc defines a distribution over phoneme sequences that depends only on the acoustic input sequence x. it is therefore  an acoustic only model. a recent augmentation  known as an  rnn transducer      combines a ctc like network with a  separate rnn that predicts each phoneme given the previous  ones  thereby yielding a jointly trained acoustic and language  model. joint lm acoustic training has proved beneficial in  the past for speech recognition         .  whereas ctc determines an output distribution at every  input timestep  an rnn transducer determines a separate distribution pr k t  u  for every combination of input timestep t  and output timestep u. as with ctc  each distribution covers the k phonemes plus  . intuitively the network  decides  what to output depending both on where it is in the  input sequence and the outputs it has already emitted. for a  length u target sequence z  the complete set of t u decisions  jointly determines a distribution over all possible alignments  between x and z  which can then be integrated out with a  forward backward algorithm to determine log pr z x      .  in the original formulation pr k t  u  was defined by taking an  acoustic  distribution pr k t  from the ctc network   a  linguistic  distribution pr k u  from the prediction network  then multiplying the two together and renormalising.  an improvement introduced in this paper is to instead feed  the hidden activations of both networks into a separate feedforward output network  whose outputs are then normalised  with a softmax function to yield pr k t  u . this allows a  richer set of possibilities for combining linguistic and acoustic information  and appears to lead to better generalisation.  in particular we have found that the number of deletion errors  encountered during decoding is reduced.                denote by h n and h n the uppermost forward and  backward hidden sequences of the ctc network  and by p  the hidden sequence of the prediction network. at each t  u              the output network is implemented by feeding h n and h n  to a linear layer to generate the vector lt   then feeding lt and  pu to a tanh hidden layer to yield ht u   and finally feeding  ht u to a size k     softmax layer to determine pr k t  u                    n h n  hn        lt   w   t   bl  t   w   h nl  h l  ht u   tanh  wlh lt u   wpb pu   bh              yt u   why ht u   by            exp yt u  k    pr k t  u    pk        k     exp yt u  k               where yt u  k  is the k th element of the length k     unnormalised output vector. for simplicity we constrained all non            output layers to be the same size    h nt       h nt      pu       lt      ht u     however they could be varied independently.  rnn transducers can be trained from random initial  weights. however they appear to work better when initialised  with the weights of a pretrained ctc network and a pretrained next step prediction network  so that only the output  network starts from random weights . the output layers  and  all associated weights  used by the networks during pretraining are removed during retraining. in this work we pretrain  the prediction network on the phonetic transcriptions of the  audio training data  however for large scale applications it  would make more sense to pretrain on a separate text corpus.   . . decoding  rnn transducers can be decoded with beam search      to  yield an n best list of candidate transcriptions. in the past  ctc networks have been decoded using either a form of bestfirst decoding known as prefix search  or by simply taking the  most active output at every timestep    . in this work however  we exploit the same beam search as the transducer  with the  modification that the output label probabilities pr k t  u  do  not depend on the previous outputs  so pr k t  u    pr k t  .  we find beam search both faster and more effective than prefix search for ctc. note the n best list from the transducer  was originally sorted by the length normalised log probabilty  log pr y   y   in the current work we dispense with the normalisation  which only helps when there are many more deletions than insertions  and sort by pr y .   . . regularisation  regularisation is vital for good performance with rnns  as  their flexibility makes them prone to overfitting. two regularisers were used in this paper  early stopping and weight  noise  the addition of gaussian noise to the network weights  during training      . weight noise was added once per training sequence  rather than at every timestep. weight noise     tends to  simplify  neural networks  in the sense of reducing  the amount of information required to transmit the parameters           which improves generalisation.   . experiments  phoneme recognition experiments were performed on the  timit corpus     . the standard     speaker set with all  sa records removed was used for training  and a separate  development set of    speakers was used for early stopping. results are reported for the    speaker core test set.  the audio data was encoded using a fourier transform based  filter bank with    coefficients  plus energy  distributed on  a mel scale  together with their first and second temporal  derivatives. each input vector was therefore size    . the  data were normalised so that every element of the input vectors had zero mean and unit variance over the training set. all     phoneme labels were used during training and decoding   so k        then mapped to    classes for scoring     .  note that all experiments were run only once  so the variance due to random weight initialisation and weight noise is  unknown.  as shown in table    nine rnns were evaluated  varying along three main dimensions  the training method used   ctc  transducer or pretrained transducer   the number of  hidden levels        and the number of lstm cells in each  hidden layer. bidirectional lstm was used for all networks  except ctc  l    h tanh  which had tanh units instead of  lstm cells  and ctc  l    h uni where the lstm layers  were unidirectional. all networks were trained using stochastic gradient descent  with learning rate        momentum  .   and random initial weights drawn uniformly from    .    .  .  all networks except ctc  l    h tanh and pretrans  l    h  were first trained with no noise and then  starting from the  point of highest log probability on the development set  retrained with gaussian weight noise       .     until the  point of lowest phoneme error rate on the development set.  pretrans  l    h was initialised with the weights of ctc l    h  along with the weights of a phoneme prediction network  which also had a hidden layer of     lstm cells   both  of which were trained without noise  retrained with noise  and  stopped at the point of highest log probability. pretrans  l   h was trained from this point with noise added. ctc  l   h tanh was entirely trained without weight noise because  it failed to learn with noise added. beam search decoding was  used for all networks  with a beam width of    .  the advantage of deep networks is immediately obvious   with the error rate for ctc dropping from   .   to   .    as the number of hidden levels increases from one to five.  the four networks ctc  l    h tanh  ctc  l    h  ctc l    h uni and ctc  l    h all had approximately the same  number of weights  but give radically different results. the  three main conclusions we can draw from this are  a  lstm  works much better than tanh for this task   b  bidirectional    table  . timit phoneme recognition results.  epochs  is  the number of passes through the training set before convergence.  per  is the phoneme error rate on the core test set.  n etwork  ctc   l      h   tanh  ctc   l      h  ctc   l      h  ctc   l      h  ctc   l      h   uni  ctc   l      h  ctc   l      h  t rans    l      h  p re t rans    l      h    w eights   . m   . m   . m   . m   . m   . m   . m   . m   . m    e pochs                                              per    .      .      .      .      .      .      .      .      .      fig.  . input sensitivity of a deep ctc rnn. the heatmap   top  shows the derivatives of the  ah  and  p  outputs printed  in red with respect to the filterbank inputs  bottom . the  timit ground truth segmentation is shown below. note that  the sensitivity extends to surrounding segments  this may be  because ctc  which lacks an explicit language model  attempts to learn linguistic dependencies from the acoustic data.  lstm has a slight advantage over unidirectional lstmand   c  depth is more important than layer size  which supports  previous findings for deep networks     . although the advantage of the transducer is slight when the weights are randomly  initialised  it becomes more substantial when pretraining is  used.   . conclusions and future work  we have shown that the combination of deep  bidirectional  long short term memory rnns with end to end training and  weight noise gives state of the art results in phoneme recognition on the timit database. an obvious next step is to extend the system to large vocabulary speech recognition. another interesting direction would be to combine frequencydomain convolutional neural networks      with deep lstm.      . 