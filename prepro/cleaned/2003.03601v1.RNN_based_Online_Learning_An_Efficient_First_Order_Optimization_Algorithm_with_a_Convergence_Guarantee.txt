introduction  prediction of individual sequences is one of the main  subjects of interest in the contemporary online learning literature    . in this problem  we sequentially receive a data  sequence related to a desired signal to predict the signal s next  value    . this problem is also known as online regression  and extensively studied in the neural network           signal  processing           and machine learning literatures         .  in these studies  nonlinear approaches are generally employed  since linear modeling is inadequate for a wide range of  applications due to constraint of linearity         .  for online regression  there exists a wide range of nonlinear approaches in the fields of machine learning and signal  processing        . however  these approaches usually suffer  from prohibitive computational requirements and may provide  poor performance due to overfitting and stability issues    .  adopting neural networks is another method for online nonlinear regression due to their success in approximating nonlinear  functions         . however  neural network based regression  algorithms are shown to be prone to such issues as overfitting  or demonstrating inadequate performance in certain applications           . to overcome the limitations of the shallow  networks  neural networks composed of multiple layers  i.e.   deep neural networks  dnns   have recently been introduced.  in dnns  each layer performs a feature extraction based on the  previous layers  which enables them to model highly nonlinear  structures           . however  this layered structure poorly  this work is supported in part by tubitak contract no.    e   .  n. m. vural  s. f. yilmaz  f. ilhan and s. s. kozat are with the department of electrical and electronics engineering  bilkent university  ankara         turkey  e mail  vural ee.bilkent.edu.tr  syilmaz ee.bilkent.edu.tr   filhan ee.bilkent.edu.tr  kozat ee.bilkent.edu.tr.    performs in capturing time dependencies of the data due to  its lack of temporal memory. therefore  dnns provide only  limited performance in processing temporal data and modeling  time series     . to remedy this issue  recurrent neural networks  rnns  are used  as these networks have a feed back  connection that enables them to store past information           . through their many variants  recurrent neural networks  have seen remarkable empirical success in a broad range of  sequential learning domains               . in this study  we  are interested in online nonlinear regression with rnn based  networks due to their superior performance in capturing time  dependencies.  for rnns  there exists a wide range of online training methods to learn network parameters               . among them   the first order methods are commonly preferred in practice due  to their computational efficiency                 . however   using gradient based optimization methods for rnn based  online learning is challenging due to the divergence problems  caused by the exploding gradient problem     . in addition to  this  finding an effective learning rate for first order methods  requires time consuming search algorithms       which costs a  significant amount of time and effort in practical applications.  to resolve these problems  several heuristic methods have  been proposed. for the divergence problems  krueger et  al.      penalized the squared distance between the norms of  successive hidden states to keep the rnn model stable during  training. mikolov et al.      and pascanu et al.      showed  that gradient clipping helps to reduce exploding gradients in  practice. to overcome the learning rate related problems  blier  et al.      applied multiple learning rates with adaptive randomization  which is shown to perform close to the stochastic  gradient descent  sgd  with the optimal learning rate. as an  alternative approach  orabona et al.      randomly performed  gradient descent updates with a fixed learning rate  which is  again empirically shown to be successful in neural network  training.  while these heuristics are reasonably successful in practice   they are ad hoc and based on empirical observations  which  may not necessarily be applicable in online settings. there  are also mathematical studies in the literature to provide  theoretical performance guarantees for rnn based learning   hardt et al.      showed that the gradient descent algorithm  learns the globally optimum weights when the learning model  is a single input single output linear dynamical system. oymak  et al.      extended this result to the contractive nonlinear  dynamical systems by assuming that the ground truth hidden     state vectors are observed. additionally  allen zhu et al.       showed the elman network trained with sgd is capable of  minimizing the regression loss with the assumption that the  number of neurons is polynomial in the training data size.  although these studies provide definite solutions for potential problems of the first order methods  the conditions of  their results are restrictive for online settings. therefore  their  results are usually inapplicable to practical online learning  scenarios. in this study  differing from the previous works  we  introduce a first order optimization algorithm that theoretically  guarantees to learn the locally optimum parameters without  any assumption on the input output sequences  except their  boundedness  or the system dynamics of the model. we  emphasize that our convergence guarantee is valid when our  algorithm is used with commonly used rnn models  e.g.   elman networks      and lstms     .  to obtain this result  we model rnn optimization as a  sequential learning problem and assume each time step as  a separate loss function  whose argument is the network  weights  detailed in the following . by using the lipschitz  characteristics of these loss functions  we develop an online  gradient descent algorithm that guarantees to converge to the  weights with zero derivatives. in the simulations  we verify our  theoretical results and show that our algorithm improves the  error performance of the state of the art methods                   on several real life datasets. therefore  in this paper  we  introduce a both practical and theoretically justified algorithm  that can be used safely in rnn based online learning settings.  our contributions can be summarized as follows     to the best of our knowledge  we  as the first time in  the literature  introduce an online first order optimization  algorithm that guarantees to learn the locally optimum  parameters when used with practical rnn models. we  note that previous studies in the literature either make  ad hoc assumptions in their results or give a theoretical  justification for restricted settings that do not sufficiently  describe the practical scenarios.    our algorithm is truly online such that it does not make  any assumption on the desired data sequence to guarantee  convergence. therefore  it can be safely used in any  practical application.    through simulations involving real life datasets  we illustrate significant performance gains achieved by our  algorithm with respect to the state of the art methods                  .  this paper is organized as follows  in section ii  we  formally introduce the online regression problem and describe  our rnn model. in section iii  we develop a first order optimization algorithm with a theoretical convergence guarantee.  in section iv  we verify our results and demonstrate the  performance of our algorithm with numerical simulations. in  section v. we conclude the paper with final remarks.  ii. m odel and p roblem d efinition  all vectors are column vectors and denoted by boldface  lower case letters. matrices are represented by boldface capital  letters. we use k k  respectively k k    to denote the        respectively      vector or matrix norms depending on the  argument. we use bracket notation  n  to denote the set of the  first n positive integers  i.e.   n                n .  we study online regression  where we observe a desired  signal  dt  t   and regression vectors  xt  t   to sequentially  estimate dt with d t . for mathematical convenience in the        proofs  we assume dt      nh   nh    with a user dependent  nh     and xt          nx . however  our derivations hold  for any bounded input output sequence after proper shifting  and scaling. given our estimate d t   which is a function  of          xt     xt   and          dt     dt      we suffer the loss    dt   d t  . our aim is to optimize the network with respect to  the loss function        . in this study  we particularly work with  the squared loss  i.e.    d t   dt      .  dt   d t    .  however   since our work uses a generic approach for the gradientbased non convex optimization  it can be extended for any  continuous cost function. an extension for the cross entropy  loss is given in appendix c.    n    in this paper  we study online regression with continually  running rnns     . for this task  we use the elman network  model  i.e.     ht     tanh wht   uxt    d t   ct ht .                r    nh  nh  here  as the weight matrices  we have w       u   nh  nx and c   nh   with kck    .  moreover   ht          nh is the hidden state vector  xt          nx is the        input vector  d t      nh   nh   is our estimation  and tanh  applies to vectors point wise. we note that we use the elman  network model with tanh and linear activations in         due  to the wide use and simplicity of this model     . however   our derivations can be extended for any differentiable neural  network architecture  given that the lipschitz properties of the  architecture can be explicitly derived. the sketch of such an  extension for lstms is given in appendix d. additionally   although we do not explicitly write the bias terms  they can  be included in         by augmenting the input vectors with a  constant dimension.    r    r    iii. a lgorithm d evelopment    in this section  we develop an online first order algorithm  that guarantees convergence to the locally optimum weights.  we develop our algorithm in three subsections. in the first  subsection  we describe our sequential learning approach and  make definitions for the following analysis. in the second  subsection  we present some auxiliary results that will be  used in the development of the main algorithm. in the last  subsection  we introduce our algorithm and mathematically  prove its convergence guarantee.     output layer     ct  t ht   t    t     dt    initial  h  state      rnn pass  defined in      h   parametrized by   t and  t    input  vectors    x     rnn pass  parametrized by   t and  t    h            ht      rnn pass  parametrized by   t and  t    x     ht   t    t      xt    unfolded version of the rnn model in         over all the time steps up to the current time step t.  note that all forward passes share the same parameters  i.e.   t and  t .    fig.    in this figure  we visually describe ht   t    t    where ht   t    t   is defined as the hidden state vector obtained by running the model in         with   t and  t from the initial time step up to the current time step t. in the figure  the rnn sequence is initialized with a predetermined initial state h    which  is independent of the network weights. then  the same rnn forward pass  given in      is repeatedly applied to the input sequence  xt  t     where all the  iterations are parametrized by  t and  t . the resulting hidden vector after t iterations is defined to be ht   t    t  . here  we note that the dependence of  ht        on t is due to the increased length of the recursion at each time step.    a. sequential learning approach  we investigate the rnn based regression problem problem  in the sequential learning framework. here  we make no  statistical assumptions on the data in order to model chaotic   non stationary or even adversarial environments. hence  we  model our problem as a game between a learner and an  adversary  where the learner is tasked with predicting the  weight matrices from some convex sets in each round t. in  the following  we use the vectorized forms of the weight  matrices  i.e.   t   vec wt   and  t   vec ut    for mathematical convenience. therefore  we construct the game as  follows  at each round t  the learner declares his prediction   t and  t   concurrently  the adversary chooses a target value        dt      nh   nh    an input xt          nx   and a weight  vector ct   kct k      then  the learner observes the loss function       t   t    t       .  dt   ctt ht   t    t             z     d t    and suffers the loss  t   t    t    where ht   t    t   denotes the  hidden state vector obtained by running the model in          with  t and  t from the initial time step up to the current  time step t  for the detailed description of ht   t    t   see fig.    . this procedure of play is repeated across t rounds  where  t is the total number of input instances. here  we note that  we constructed our setting for adversarial ct selections for  mathematical convenience in our proofs. however  since the  selected  t   t    t   is convex with respect to ct   we will use  the online gradient descent algorithm      to learn the optimal  output layer weights  simultaneously with the hidden layer  weights  during the training.  since we assume no statistical assumptions on the input output sequences  we analyze our performance with the  notion of regret. however  we note that the standard regret    we scale the squared loss with  .  for mathematical convenience when  computing the derivatives.    we note that the boundedness of c will be required in our proofs. however   our algorithm will guarantee to keep c bounded with a proper projection  onto a convex set. here  we assume in particular kck     for mathematical  convenience in the proofs.    definition for the convex problems is intractable in the nonconvex settings due to the np hardness of the non convex  global optimization           . therefore  we use the notion  of local regret recently introduced by hazan et al.       which  quantifies the objective of predicting points with a small  gradient on average.  to formulate the local regret for our setting  we first define  the projected partial derivatives of  t        with respect to    and   as follows   h       k       t           t        i                   k                      h   k       t               t        i            k          .                  here   k   denotes the projected partial derivative operator  defined with some convex set k and some learning rate    the  learning rates    and    are used to update  t and  t   the  operators  k     and  k     denote the orthogonal projections  onto k  and k  .  we define the time smoothed loss at time t  parametrized  by some window size w    t    as  lt w              w      x   t i       .  w i             then  we define the local regret as  rw  t         t    x   k      lt w   t    t        t                 k      lt w   t    t                .           our aim is to derive a sublinear upper bound for rw  t    in order to ensure the convergence of our algorithm to the  locally optimum weights. however  before the derivations   we first present some auxiliary results  i.e.  the lipschitz and  smoothness properties of lt w         which will be used in  the convergence proof of our algorithm.  b. lipschitz and smoothness properties  in this section  we derive the lipschitz and smoothness  properties of the time smoothed loss function lt w       . we     note that lt w        is defined as the average of the most  recent w instant loss functions  see       where the loss  function  t        recursively depends on   and   due to  ht         see     and fig.   . we emphasize that since we are  interested in online learning  this recursion can be infinitely  long  which might cause lt w        to have unboundedly large  derivatives. on the other hand  online algorithms naturally  require loss functions with bounded gradients to guarantee  convergence     . therefore  in the following  we first analyze  the  potentially infinite  recursive dependency of lt w         on   and    where we derive sufficient conditions for its  derivatives to be bounded. then  we use the results of this  analysis to find the explicit formulations of the lipschitz and  smoothness constants of lt w        in terms of the model  parameters defining our rnn model in        .  to analyse the effect of recursion  we first write the hidden  state update in     with the vectorized weight matrices as  ht     tanh ht     xt              where ht   i   htt   xt   i   xtt   and   is the kronecker  product.  we  then  provide the following lemma  where we derive  the lipschitz and smoothness properties of the single rnn  iteration defined in    .  lemma  . let w and u the hidden layer weight matrices  in the model          which satisfy kwk      and kuk      for some     . by using the equivalent hidden state update  formula in      the lipschitz and smoothness properties of the  single rnn iteration can be written as     r      tanh ht     xt            ht              tanh ht   xt       nh                   tanh ht   xt       nx                    tanh ht     xt               h t               tanh ht   xt          tanh ht     xt        nh         nh           ht                          tanh h    x      t  t       tanh ht     xt        nx         nx          ht                           tanh ht     xt         nx nh .                proof. see appendix a.  in the following lemma and remark  we derive the lipschitz  properties of ht         and observe the effect of infinitely long  recursion on the derivatives of lt w       .  lemma  . let w  w   u  u  satisfy kwk  kw  k      and  kuk  ku  k    . by using the notation in      let ht        and  ht             be the state vectors obtained at time t by running  the model in         with the matrices w  u  and w    u   on common input sequence  x    x            xt      respectively.    if h           h               then  kht          ht            k      t  x     i         nh k        k              nx k       k .    i            proof. see appendix a.  remark  . we note that  by       to ensure ht        has a  bounded gradient with respect to   and   in an infinite time  horizon    should be in         i.e.            . in this case  the  right hand side of      becomes bounded  i.e.         nh  nx  k        k    k       k  kht          ht            k                    for any t    t  .  recall that ltw        is dependent on ht        due to      and    . hence  to ensure the derivatives of ltw        stay  bounded  we need to constrain our parameter space as k      vec w    kwk      and k     vec u    kuk      for  some           . we note that since k  and k  are convex  sets for any             our constraint does not violate the  setting described in the previous subsection.  now that we have found            is sufficient for  lt w        to have bounded derivatives in any t    t    in the  following theorem  we provide its lipschitz and smoothness  constants.  theorem  . let     vec w  and     vec u    where w  and u satisfy kwk      and kuk     for some           .  then  lt w        has the following lipschitz and smoothness  properties                          lt w              lt w                  lt w                    lt w                   lt w                  nh  .            nh nx  where r     .           nh nh  where       .                nx nh  where       .                nh nx  where        .                r    where r                 r                                                              proof. see appendix b.  in the following section  we use these properties to derive  an rnn learning algorithm with a convergence guarantee.  c. main algorithm  here  we present our main algorithm  namely the windowed  online gradient descent algorithm  wogd   shown in algorithm  .  in the algorithm  we take the window size w    t   and             as the inputs. we  then  define the parameter spaces  k    k    and kc in line  . here  we define k  and k  as given  in remark   to ensure that the derivatives of the loss functions  are bounded. furthermore  we define kc as kc    c   kck       to satisfy our assumption of kck    .     algorithm   windowed online gradient descent algorithm   wogd      parameters  w    t              .     initialize           c  and h  .     let k     vec w    kwk                               k     vec u    kuk       kc    c   kck       for t     to t do  predict  t    t and ct .  receive xt and generate d t .  observe dt and the cost function  t   t    t  .    updates   h      t        i  ct      kc ct       ct  t   k      lt w   t    t     t      t            k      lt w   t    t     t      t       .                               end for    in the learning part  we first predict the hidden layer weight  matrices in their vectorized forms  i.e.   t and  t   and the  output layer weights  i.e.  ct  see line   . then  we receive the  input vector xt and generate d t by running the model in        .  we next observe ground truth value dt and the loss function   t   t    t   in line  . having observed the label  we update the  weight matrices in line    or in           . here  we update  the output layer weights ct with the projected online gradient  descent algorithm       since  t   t    t   is convex with respect  to ct . we update the hidden weights in           by using  their projected partial derivatives defined with  k         and   k        .  we note that since we constructed our setting for adversarial  ct selections  the update rule for the output layer in      does  not contradict with our analysis. moreover  by using      theorem     we can prove that this update rule converges to the best  possible output layer weights satisfying kck    . therefore  in  the following theorem  we provide the convergence guarantee  of wogd specifically for the hidden layer weights.  theorem  . let  t        and lt w        be the loss and timesmoothed loss functions defined in     and      respectively.  moreover  let    and    be the smoothness parameters defined  in          . then  if wogd is run with the parameters                    and                                       nh  t     min           w            it ensures that  rw  t        where rw  t   is the local regret defined in    . by selecting  t  a window size w such that w    o t    one can bound rw  t    with a sublinear bound  hence  guarantee convergence of the  hidden layer weights to the locally optimum parameters.     we use little o notation  i.e.  g x    o f  x    to describe an upper bound  that cannot be tight  i.e.  limx   g x  f  x     .    proof. see appendix b.  theorem   shows that with appropriate parameter selections  wogd is guaranteed to converge to the locally optimum hidden layer weights without any assumption on the  input output sequences and output layer weights ct . additionally  recall that by      theorem     the output layer weights  also converge to the best weights in hindsight. therefore   we conclude that wogd with the learning rates in       guarantees to learn the locally optimum rnn parameters for  any bounded input output sequences.  now that we have proved the convergence guarantee of  wogd  in the following remark  we investigate the computational requirement of wogd and comment on the window  size selection for the algorithm.  remark  . we note that the most expensive operation of  wogd is the computation of the projected partial derivatives  in            which requires the computation of the partial  derivatives of lt w        with respect to   and    and their  corresponding projections onto k  and k  . to compute the  partial derivatives  we use the truncated backpropagation   through time algorithm       which has o hnh  nh   nx    computational complexity with a truncation length h. since  wogd uses the partial derivatives of the last w losses  we  can approximate these partial derivatives with a single backpropagation by using a truncation length h   w   n for some  n     which results in o wnh  nh   nx   computational  requirement for computing the partial derivatives. in addition  the projection step can be performed by computing the  singular value decomposition  svd  of the weight matrices  w and u  and clipping their singular values with    . here      since performing svd requires o min nh   nx  nh  nh   nx      the computational requirement  of wogd becomes o  w       min nh   nx   nh  nh   nx   per time step.  we highlight that wogd introduces a tradeoff between  the convergence speed and computational complexity. for  example  one can choose a large window size w to ensure  fast convergence  see       by increasing the computational  requirement of wogd  or vice versa. however   as we will  show in the following section  selecting w   d t e usually  provides the best trade off between performance and efficiency.    n    in the next remark  we discuss the effect of choosing higher  learning rates than the theoretically guaranteed ones given in  theorem  .  remark  . we note that wogd is constructed by assuming  the worst case lipschitz constants derived in theorem  . on  the other hand  our experiments suggest that in practice  the  landscape of the objective function is generally nicer what  is predicted by our theoretical development. for example  in  the simulations  it is observed that the smoothness parameters  of the error surface  i.e.     and      is usually     to       times smaller than the values given in          . therefore  it is  practically possible to obtain vanishing gradient with wogd  by using much higher learning rates than theoretically guaranteed learning rates in     . as the regret bound of wogd  is inversely proportional with the learning rates  see        in     the following  we use wogd with the higher learning rates  than suggested in theorem   to obtain faster convergence.  iv. s imulations  in this section  we verify our theoretical analysis and illustrate the performance of our algorithm on different real life  datasets. in particular  we consider the regression performance  for elevators       and pumadyn      datasets. to demonstrate  the performance improvements of our algorithm  we compare  it with the three widely used first order optimization algorithms  adam       rmsprop      and sgd     .  for all the simulations  we randomly draw the initial rnn  weights from a gaussian distribution with zero mean and  standard deviation of  .   and set the initial values of all  internal state variables to  . for a fair comparison  in each  experiment  we choose the hyper parameters such that all the  compared algorithms reach their maximum performance in  that setup. we run each experiment    times and provide the  mean performances.  a. elevators dataset  in the first simulation  we consider the regression performance on the elevators dataset       which includes        input output pairs obtained from a procedure that is related to  controlling an f   aircraft  i.e.  t        . here  our aim is  to predict the scalar output that expresses the actions of the  f   aircraft. for this  we use    dimensional input vectors of  the dataset with an additional bias dimension  i.e.  nx     . to  get small loss values with relatively lower run time  we use   dimensional state vectors in rnns  i.e.  nh     . in wogd   we use       .           .           .    w      . in  adam  rmsprop and sgd  we respectively choose the learning  rates as  .      .    and  .   . we note that we do not use  momentum in sgd.  in fig.  a  we demonstrate the temporal loss of the compared algorithms for the elevators dataset. here  we observe  that despite the differences in the performances at the beginning  rmsprop  adam and sgd converge to the similar loss  values. we also observe that since wogd obtains small loss  values much faster than the other algorithms  it can track the  desired data sequence with lower error values throughout the  simulations.  to observe how window size affects the performance  we  run wogd with six different window sizes  namely w                                 and plot their resulting temporal  performance in fig.  b. here  we see that as consistent with  our regret bound in theorem    the final loss values obtained  by the algorithms are inversely proportional to their window  sizes. moreover  we observe that in the initial part of the  experiment  the algorithms with larger window sizes tend to  suffer larger losses  as averaging gradients in a large window  prevents them to overfit to the small amount of data observed  in the earlier stage. from the figure  we can observe that the  wogd with w       provides comparable performance with  the others at both initial and final parts of the experiments.  as the computational requirement of our algorithm is linear    in w  see remark bla   selecting w       provides a highly  preferable trade off for practical purposes.  in figs.  c and  d  we compare the empirical smoothness  parameters of the smoothed loss functions with the theoretical  upper bounds derived in theorem  . to observe the behaviour  of the empirical smoothness parameters without calculating the  exact hessian matrix  we use the empirical lipschitz constants   i.e.   emp     t    emp     t               lt w   t     t                  lt w   t   t                  k t      t k   lt w   t     t                  lt w   t   t          k t      t k    .            emp  emp  in the figures  we observe that    t  and    t  are practically much lower than the theoretical upper bounds  where  the theoretical values are given in the titles of the plots.  we note that the difference between the theoretical upperbounds and empirical smoothness parameters are expected   since we derived the worst case upper bounds by considering  the saturation region of rnns  which is rarely encountered in  practice due to variations in real world datasets. furthermore   in figs.  b and  c  we see that the learning rates we used  satisfy the requirement of theorem     stated in     .  to verify our theoretical analysis in theorem    we plot  the normalized regret of wogd  i.e.  rw  t  t for t    t     and the regret bound in       scaled with  .    in fig.  d.  here  we see that as consistent with our theoretical derivation   the normalized regret vanishes. moreover  it is bounded by  the regret bound scaled with  .  . we believe that the gap  between the normalized regret and actual regret bound is due  to our adversarial model for the output layer weights  which is  mainly used for mathematical convenience. since we update  the output layer weights simultaneously with the hidden layer  weights  deriving a tighter regret bound requires a cooperative  learning model for neural network optimization  which  to the  best of our knowledge  has not been studied in the literature.  the construction of such model and its analysis is left as an  open problem for the future studies.    b. pumaydn dataset  in our second simulation  we consider the pumaydn dataset        which includes      input output pairs obtained from  the simulation of unimation puma     robotic arm  i.e.  t        . here  our aim is to estimate the angular acceleration  of the arm by using the angular position and angular velocity  of the links. for this simulation  we use   dimensional input  vectors of the dataset with an additional bias dimension  i.e.   nx      and    dimensional state vectors in rnn  i.e.  nh      . in wogd  we use       .          .         .    w     . in adam  rmsprop  and sgd  we choose the learning  rates as  .      .     and  .    respectively. as in the previous  experiment  we do not use momentum in sgd.  in fig.  a  we demonstrate the temporal loss of the compared algorithms for the pumaydn dataset. here  we observe  that while all algorithms provide comparable performances   wogd enjoys a smaller error value at the end of the      a      b      c      d     fig.     a  sequential prediction performances of the algorithms for the elevators dataset  b   c  comparison between the empirical smothness parameteres  of lt w        and their theoretical upper bounds given in      and       d  comparison between the normalized regret bound of wogd  i.e.  rw  t  t for  t    t    and its theoretical upper bound given in     .    simulation. in figs.  b and  c  we compare the empirical  smoothness parameters  as defined in            and their  theoretical upper bound derived in theorem  . as in the  previous experiment  we observe that the empirical smoothemp  emp  ness parameters  namely    t  and    t    are considerably  smaller than the theoretical upper bounds and our learning  rate selection satisfies the requirement in     . in fig.  d   we plot the normalized regret of wogd  i.e.  rw  t  t for  t    t    and the regret bound in      scaled with  .  .  as in the previous experiment  here also  we see that the  normalized regret vanishes and the regret bound scaled with   .   bounds the normalized regret by above  which is parallel  with our derivations in theorem  . since our algorithm enjoys  this guarantee by providing comparable performance with the  state of the art methods  it can be a theoretically grounded  alternative of the widely used heuristics                  in  rnn based online learning settings.  v. c onclusion  we studied online nonlinear regression with continually  running rnns  i.e.  rnn based online learning. for this problem  we introduced a first order gradient based optimization  algorithm that provides convergence guarantee to the locally  optimum parameters. we emphasize that unlike previous theoretical results  which holds in restricted settings             our    algorithm is generic such that it guarantees convergence with  any smooth rnn architecture  e.g.  the elman networks       or lstms       without any assumption on the input output  sequences.  to achieve this result  we model the rnn based online  regression problem as a sequential learning problem  where we  assumed each time step as a separate loss function assigned by  an adversary. we characterize the lipschitz properties of these  loss functions with respect to the network weights and derived  sufficient conditions for our model to have bounded derivatives. then  by using these results  we introduce an online  gradient descent algorithm that is guaranteed to converge to the  locally optimum parameters. in the simulations  we verify our  theoretical analysis. here  we also demonstrate that our algorithm achieves considerable performance improvements with  respect to the state of the art gradient descent methods                  .  a ppendix a  for the following  we denote the derivative of tanh as  tanh    where tanh   x        tanh x   . due to the space  restrictions  we denote the elementary row scaling operation  with   i.e.  x w   diag x w  where x   n   w   n m  and diag x    n n is the elementary scaling matrix whose    r    r    r      a      b      c      d     fig.     a  sequential prediction performances of the algorithms for the pumaydn dataset  b   c  comparison between the empirical smothness parameteres  of lt w        and their theoretical upper bounds given in      and       d  comparison between the normalized regret bound of wogd  i.e.  rw  t  t for  t    t    and its theoretical upper bound given in     .    diagonal elements are the components of x. before the proofs   we give three inequalities that will be used frequently in the  following     r    lemma  . for any x  y   n   w    the following statements hold      kx    rn m  where n  m   n     wk   kxk  kwk               ktanh   x    tanh   y k     kx   yk               ki   xt k   kxk.               proof of      we note that     and     are equivalent. by  using      and tanh   x      on      we write      tanh wht   uxt      ktanh   wht   uxt   wk    ht    ktanh   wht   uxt  k  kwk    .    proof.    since x w   diag x w  we have kx wk    kdiag x kkwk  where we use the cauchy schwarz inequality for bounding. since by definition kdiag x k    kxk    kx wk   kdiag x kkwk   kxk  kwk.     recall that tanh   x        tanh x   . since tanh x              tanh is   lipschitz   smooth. then  by using  kxk    kxk for any x   n   we have ktanh   x     tanh   y k    ktanh   x    tanh   y k. since tanh is  smooth  we have ktanh   x    tanh   y k    kx   yk.     see      theorem   .       proof of       by using      and       we write    proof of lemma  . in the following  we prove each statement  separately        proof of       we note that since tanh is twice differentiable  the order of partial derivatives is not important. then     r      tanh wht   uxt     tanh w  ht   uxt         ht    ht       ktanh  wht   uxt   w   tanh   wh t   uxt                ktanh  wht   uxt     tanh     kwkkht         ht kkwk     wh t    wk      uxt  k  kwk         kht   h t k.        by using                   and kht k           ht           share the same    in the following  between            we abbreviate them as ht     and ht            nh   we write      tanh ht     xt      tanh h t     xt                     ktanh  ht     xt    ht  tanh   h t     xt            ht k                     ktanh  ht     xt      tanh  ht     xt   k  kht k                h t       ktanh    xt   k  kht   h t k           ktanh  wht   uxt     tanh   wh t   uxt  k  kht k   kht   h t k                        kwk nh    kht   ht k       nh    kht   ht k     where we add   tanh   h t     xt    ht inside of the norm  in       and use the triangle inequality for     . here  we  omit    term for mathematical convenience in the following  derivations.     proof of       this can be obtained by repeating the steps  in the proof of      for   and ht .        proof of       by using                   kht k   nh and     kxt k   nx   we write    tanh ht     xt      tanh ht     xt                       ktanh  ht     xt    ht   tanh   ht     xt                 ht k    kht       ht       k                        ktanh wht         uxt     tanh w ht          uxt  k         ktanh wht         uxt     tanh wht            uxt  k   ktanh wht            uxt     tanh w  ht            uxt  k                          kht         ht       k   nh k      k  t         x         i nh k        k        i            here  to obtain       we add   tanh wht          uxt   inside  of the norm in       and use the triangle inequality. then  we  use     and      to get     . until we reach       we repeatedly  apply the same bounding technique to bound the norm of the  differences between the state vectors.  now  we bound the second term in     . since ht            and ht             share the same      in the following  between              we abbreviate them as ht     and ht                  ktanh  ht     xt      tanh  ht     xt    k  kht k          kxt kk       kkht k     nh nx k       k.     proof of       by using       tanh   x       and kht k       nh      tanh ht     xt       ktanh   ht     xt    ht k           ktanh   ht     xt   k  kht k   nh .     proof of       this can be obtained by repeating the steps  in the proof of      for  .        proof of       by using                   and kht k   nh    we write    tanh ht     xt      tanh ht       xt                     ktanh  ht     xt    ht   tanh   ht       xt    ht k    ktanh   ht     xt      tanh   ht       xt   k  kht k     kht kk        kkht k    nh k        k.    kht       ht      k                                    ktanh w ht        uxt   tanh w ht         u xt  k         ktanh w  ht        uxt   tanh w  ht          uxt  k   ktanh w  ht          uxt   tanh w  ht          u  xt  k                    kht         ht        k   nx k       k  t         x         i nx k       k        i            here  for       we add   tanh w  ht           uxt   and use  the triangle inequality. we  then  use     and      to get     .  until we reach       we repeatedly apply the same technique  to bound the norm of the differences between state vectors. in  the end  we use      and      to bound       which yields the  statement in the lemma.       proof of       this can be obtained by repeating the steps  in the proof of      for  .  a ppendix b       proof of lemma  . before the proof  let ht         be the state  vector obtained at time t by running the model in     with the  matrices w    u   input sequence  x    x            xt      and the  initial condition h              h           h             . then   kht          ht            k              kht          ht          k   kht             ht            k        where we add  ht           inside of the norm in       and  use the triangle inequality. we will bound the terms in       separately. we begin with the first term. since ht        and    proof  of theorem  . recall  that  lt w            pw                 .  hence   if  we  can  bound  the  derivative  t i  i    w  of  t        for an arbitrary t    t    the resulting bound  will be valid for lt w       . therefore  in the following  we  analyse the lipschitz properties of  t        for an arbitrary  t    t   and extend the result for lt w       . in addition  in        the following  we note that since dt   d t      nh   nh   and     kct k      the    norm of   dt   d t  c is bounded by   nh       i.e.  k  dt   d t  ck     nh .        we write    then       d t    t             dt   d t            t    d t   x   ht   h         dt   d t      ht        h          dt   d t  c         nh            t  x    ht   h     h              t      ht   h      h              hi     h     hi       i                nh            t  y      hi     hi    i           h                             nh                     ht  h   here  to get      from       we add    h  inside the norm         and use the triangle inequality. to get       we use     and      .  in the following  we will bound the terms in      separately.  we begin with the first term. note that h    tanh wh        ux        and h     tanh w  h        ux      . then     t    where we use     and      to get     . by realizing that       holds for an arbitrary t  the statement in the theorem can be  obtained.     by using similar steps in            we write      x t     h    h      nh                                       nh      t           d t      dt   d t            t    d t   x   ht   h         dt   d t      ht        h          dt   d t  c         t   x               n     t          nh nh                      h       nh k      k              k      k       tanh wh      ux                                   where we add      and use the triangle      inequality       and      for     . we use lemma   for     .  now  we bound the second term in     . to bound the term    h    ht  we first focus on the term inside of the sum  i.e.   h     h t         t  x    ht   h      h                   t        x t        nh nx               t       x t           nh        nh kh       h      k  nh k      k                 nh nx                            t        x t        nh nh         ht   h t         h    h            t  t  x    x t     h    h      ht   h t      nh    nh  .                     h    h                  t   y  t  x         nh            h      h    h                          x    ht      nh    h           t  x    t  x      t          t                       t  t   x   x    h t   h        ht   h       t  t           dt   dt  c    d     d     c  t  t    h         h                        t    x   ht   h     h    h             nh     t      h        h                                      where we use     and      to get     . by realizing that       holds for an arbitrary t  the statement in the theorem can be  obtained.     let us use ht and h t for the state vectors obtained by  running the model in     from the initial step up to current time  step t with the same initial condition  same input layer matrix     common input sequence  x            xt     but different   and        respectively. let us also say  t              .  dt   d  t       where d  t is the prediction of the second model producing h t .      ht   h t       h    h        h t    h t        ht       ht      h t      h t    ht                  h t      h     h            t          kht     h t   k    nh k      k    ht     h t                   h     h          n          ht     h t    h     t           nh k      k              h     h                   n     h     t     t           nh k        k                     ht      h      h     where we add    h  t  h ht      and use the triangle inequality for     t          utilize      and      for       and lemma   for     . we   then  repeat the same manipulations in           to bound the     terms with partial derivatives. then  the second term in       can be bound as      nh    t  x      ht       h                nh    t  x                nh      h t    h               t       t                        n         h            nh k        k                   nh nh               k        k                                    pt    where we use the upper bound of the series        t        t        i.e.               to get     . then  by using       and       we can bound      as    t          t                                        nh nh                   k      k                                   nh nh     k        k.                  by realizing that      holds for an arbitrary t  the statement  in the theorem can be obtained.     this part can be obtained by adapting the steps in the  previous proof for    and use the lipschitz conditions in                   and lemma   accordingly.     we use the same notation in the proof of the  rd statement.  then     t          t                       t  t   x   x    h t   h        ht   h      t  ct     dt   d  t  ct    d     d  t    h         h                  t    x   ht   h     h    h       nh     t      h        h            t      x     ht      nh    h         t      x t        nh             tanh w  h        ux      . then                   t  x     t                h    h                        t       x t              nh        nx kh       h      k   nx nh k      k                        n n           x h  t           nx nh k      k      nh                      nh nx            k      k            t   x      tanh wh      ux                                     and use the triangle  where we add        inequality       and      for     . we use lemma   for     .  now  we bound the second term in          t    x   ht       h      t    nh nx    h     h                   t       n  x          h  t              nh nx   t              nh k        k                       nh nx                  k        k                                 h      ht  where we use      to bound the terms  h     h t . then  by        using      and       we bound      as follows       t          t                                    nh nx                       k      k                                  nh nx     k        k.                  by realizing that      holds for an arbitrary t  the statement  in the theorem can be obtained.      h    h      h                         ht   h t         h    h          t        x       h    h     ht   ht         .     nh nx            h     h                     ht  h   here  to get      from       we add    h  inside the norm         and use the triangle inequality. to get       we use     and      .  we bound the terms in      separately. we begin with the  first term. note that h    tanh wh       ux        and h        proof of theorem  . in the following  we use h    i to denote  the inner product. due to the space constraints  we omit the  arguments in the partial derivative terms  i.e.     t          lt w                   t          lt w                    k      lt w   k      lt w   t    t                    k      lt w   t    t     k      lt w      .            we start our proof by bounding the term lt w   t      t           lt    w   t       t         . then  by using       we write    lt w   t    t     lt w   t      t       lt w   t    t    e    d  l     t w        t      t   k t      t k            d  l  e    t w           t      t    k t      t k              k t      t kk t      t k        e  d  l  e  d  l     l     l  t w  t w  k     t w  k     t w                                                     k      lt w           k      lt w                         k      lt w  k      lt w                            d  l  e  d  l  e  t w  k      lt w  t w  k      lt w                                       p  p   k      lt w   k      lt w               .                                l  k      t w                          k      lt w                                   where we use      lemma  .   for       thepupdate rules in            for       and the fact that             for        see           . moreover  for       we use      lemma  .    and the fact   a    b       a   b   for any a  b   .  for the following  we define lt            for t    .  we continue our proof by bounding lt    w   t       t      as  follows     r    lt    w   t       t                      t  x    lt   w   t      t     lt w   t    t      t    t    x  t    t    x                           t   x   k      lt w      t      t   x   k      lt w      t           t   x   k      lt w                     t                              t      nh .  w    a ppendix c  in this part  we describe how to extend our work for the  cross entropy loss  which is denoted as re      . since the  cross entropy loss is mainly used for classification  we describe our extension by using the following rnn architecture   ht     tanh wht   uxt    d t   f  ct ht    et   re dt   d t  .  here  f is assumed to be the sigmoid or softmax function  depending on the dimension of the desired sequence. as in         ht          nh is the hidden state vector  xt          nx  is the input vector  and dt   d t          is our estimation.  moreover  et denotes the cross entropy loss at time instance  t.  as in the squared loss  the cross entropy is convex with  respect to output layer weights  i.e.  c. therefore  we can use  the projected online gradient descent   as in       to ensure  the convergence of the output layer update. moreover  since  the formula for the derivative of the cross entropy function  with respect to c is the same with that of the squared loss           t  dt    t   dt    i.e.   .  d c     re d     dt   d t  c  the lipschitz   c  properties derived in theorem   applies to the the crossentropy loss as well. since theorem   uses only the lipschitz  properties of the rnn architecture to prove convergence  it  can be extended for the cross entropy with the same updateprojection steps in          . therefore  algorithm   can be  used for the cross entropy case without any change as well.  a ppendix d  in this part  we describe how to extend our work for lstms.  the equations of lstm and the loss function is given as          nh t             w    where we add  lt w   t      t     to      to obtain       and        use      and dt   d t      nh   nh   for any t    t    which     implies lt   w   t      t       lt w   t      t         nh  w   to obtain     .  we note that since the  t   t    t   is defined as the  square loss between the ground truth value and our prediction  it is non negative for all t    t    which implies    t   x   k      lt w      t      by choosing                                      and  dividing both sides of      with min              we can obtain  the statement in the theorem.    t                                            lt w   t      t       lt w   t    t         lt   w   t      t       lt w   t      t                  zt   tanh w  yt     u  xt              it     w  yt     u  xt              ft     w  yt     u  xt              ct   it    zt   ft    ct               ot     w  yt     u  xt               yt   ot tanh ct    d t   ct yt  et    .  dt   d t    .                           r    where  denotes the element wise multiplication  ct   nh  is the state vector  xt          nx is the input vector  and        yt   nh is the output vector  and d t      nh   nh   is our  final estimation. furthermore  the sigmoid function   .  and    r     the hyperbolic tangent function tanh .  applies point wise to  the vector elements. the weight matrices are wi   nh  nh    ui   nh  nx for i                 and c   nh   with kck    .  as in the vanilla rnn case  the boundedness of c can be  guaranteed with a proper projection onto a convex set. note  that although we do not explicitly write the bias terms  they  can be included in            by augmenting the input vector  with a constant dimension.  similar to the vanilla rnn case  the loss function et is  convex with respect to the output weights c. therefore  we can  use the projected gradient descent to ensure the convergence  of the output layer update. for the hidden layer weights  note  that we define the projected gradients as in         and the  regret as as in     to ensure to find a stationary point for the  gradient descent updates. then  by using the same intuition   we can extend the projected gradient definition for lstm as  h    t   i    i   i        k       t   i    i         i    k   i            i        i  h   k       t   i    i      t   i    i   i            i    k   i         i        i    r    r    r    and the regret definition as  rw  t         t x       x   k      lt w   i t    i t      i  t   i                 k      lt w   i t    i t      i                      where  i t and  i t are the vectorized forms of the weight  matrices wi t and ui t at time t  i.e.   i t   vec wi t   and   i t   vec ui t  .  we note that the convergence analysis for our algorithm  requires the lipschitz properties of the architecture of interest.  to this end  we can use the lipschitz properties of lstms  derived in      proposition   . then  we can use these results  in theorem   to upper bound the regret defined in      .  accordingly  algorithm   can be extended by for the lstm  optimization.  