introduction    recurrent neural networks  rnns  seem  unreasonably  effective at modeling patterns in noisy realworld sequences. in particular  they seem effective at recognizing grammatical structure in sequences   as evidenced by their ability to generate structured data  such as source code  c    latex  etc.   with  few syntactic grammatical errors  karpathy et al.       . the ability of rnns to recognize formal  languages   sets of strings that possess rigorously defined grammatical structure   is less well studied.  furthermore  there remains little systematic understanding of how rnns recognize rigorous structure.  we aim to explain this internal algorithm of rnns through comparison to fundamental concepts in  formal languages  namely  finite automata and regular languages.  in this paper  we propose a new way of understanding how trained rnns represent grammatical  structure  by comparing them to finite automata that solve the same language recognition task. we  ask  can the internal knowledge representations of rnns trained to recognize formal languages be  easily mapped to the states of automata theoretic models that are traditionally used to define these  same formal languages  specifically  we investigate this question for the class of regular languages   or formal languages accepted by finite automata  fa .  in our experiments  rnns are trained on a dataset of positive and negative examples of strings  randomly generated from a given formal language. next  we ask if there exists a decoding function   an isomorphism that maps the hidden states of the trained rnn to the states of a canonical fa. since  there exist infinitely many fa that accept the same language  we focus on the minimal deterministic  finite automaton  mdfa    the deterministic finite automaton  dfa  with the smallest possible  number of states   that perfectly recognizes the language.  our experiments  spanning     regular languages  suggest that such a decoding function exists and  can be understood in terms of a notion of abstraction that is fundamental in classical system theory.  an abstraction a of a machine m  either finite state  like an fa  or infinite state  like a rnn  is a  machine obtained by clustering some of the states of m into  superstates . intuitively  an abstraction        published as a conference paper at iclr                                                                                                                     figure    t sne plot  left  of the hidden states of a rnn trained to recognize a regular language  specified by a   state dfa  right . color denotes dfa state. the trained rnn has abstracted dfa  states   green  and   blue   each independently model the pattern         into a single state.  a loses some of the discerning power of the original machine m  and as such recognizes a superset  of the language that m recognizes. we observe that the states of a rnn r  trained to recognize  a regular language l  commonly exibit this abstraction behavior in practice. these states can be  decoded into states of an abstraction a of the mdfa for the language  such that with high probability   a accepts any input string that is accepted by r. figure   shows a t sne embedding  maaten and  hinton        of rnn states trained to perform language recognition on strings from the regex                              . although the mdfa has   states  we observe the rnn  abstracting two states into one. remarkably  a linear decoding function suffices to achieve maximal  decoding accuracy  allowing nonlinearity in the decoder does not lead to significant gain. also  we  find the abstraction has low  coarseness   in the sense that only a few of the mdfa states need be  clustered  and a qualitative analysis reveals that the abstractions often have simple interpretations.         r elated w ork    rnns have long been known to be excellent at recognizing patterns in text  kombrink et al.         karpathy et al.       . extensive work has been done on exploring the expressive power of  rnns. for example  finite rnns have been shown to be capable of simulating a universal turing  machine  neto et al.       . funahashi and nakamura        showed that the hidden state of a  rnn can approximately represent dynamical systems of the same or less dimensional complexity.  in particularly similar work  rabusseau et al.        showed that second order rnns with linear  activation functions are expressively equivalent to weighted finite automata.  recent work has also explored the relationship between rnn internals and dfas through a variety  of methods. although there have been multiple attempts at having rnns learn a dfa structure based  on input languages generated from dfas and push down automata  firoiu et al.        gers and  schmidhuber        giles et al.        miclet and de la higuera        omlin and giles      a    most work has focused on extracting a dfa from the hidden states of a learned rnn. early work in  this field  giles et al.        demonstrated that grammar rules for regular grammars could indeed be  extracted from a learned rnn. other studies  omlin and giles      b  tried to directly extract a dfa  structure from the internal space of the rnn  often by clustering the hidden state activations from  input stimuli  noting the transitions from one state to another given a particular new input stimuli.  clustering was done by a series of methods  such as k nearest neighbor  das and mozer          k means  krakovna and doshi velez         and density based spatial clustering of applications  with noise  dbscan   das and mozer        lawrence et al.       . another extraction effort   ayache et al.        uses spectral algorithm techniques to extract weighted automata from rnns.  most recently  weiss et al.        have achieved state of the art accuracy in dfa extraction by  utilizing the l  query learning algorithm. our work is different from these efforts in that we directly  relate the rnn to a ground truth minimal dfa  rather than extracting a machine from the rnn s  state space.  the closest piece of related work is by tin o et al.       . like our work  this seeks to relate a rnn  state with the state of a dfa. however  the rnn in tin o et al.        exactly mimics the dfa  also   the study is carried out in the context of a few specific regular languages that are recognized by  automata with     states. in contrast  our work does not require exact behavioral correspondence  between rnns and dfas  dfa states are allowed to be abstracted  leading to loss of information.  also  in our approach the mapping from rnn states to fa states can be approximate  and the accuracy  of the mapping is evaluated quantitatively. we show that this allows us to establish connections        published as a conference paper at iclr         figure    an overview of the state comparison experimental setup.    between rnns and dfas in the setting of a broad class of regular languages that often demand  significantly larger automata  with up to    states  than those studied by tin o et al.       .         d efinitions    we start by introducing some definitions and notation. a formal language is a set of strings over a finite  alphabet   of input symbols. a deterministic finite automaton  dfa  is a tuple a    q        q    f    where q is a finite set of states    is a finite alphabet      q       q is a deterministic transition  function  q    q is a starting state and f   q is a set of accepting states. a reads strings over    symbol by symbol  starting from the state q  and making state transitions  defined by    at each step.  it accepts the string if it reaches a final accepting state in f after reading it to the end. the set of  strings accepted by a dfa is a special kind of formal language  known as a regular language. a  regular language l can be accepted by multiple dfas  such a dfa al is minimal if there exists  no other dfa a     al such that a  exactly recognizes l and has fewer states than al . it can be  shown that this minimal dfa  mdfa   which we denote by a l   is unique  hopcroft and ullman        .  abstractions. a nondeterministic finite automaton  nfa  is similar to a dfa  except that the  deterministic transition function   is now a non deterministic transition relation   n f a . this means  that for a state q in the nfa and a      we have that   n f a  q  a  is now a subset of nfa states.  for a given regular language l we denote by anl a nondeterministic finite automaton  nfa  with  n states that recognizes a superset of the language l. an abstraction map is a map     q    q  n    that combines two states in nfa anl   resulting in an nfa an      an    l   that is  al     l . since     every dfa is also an nfa  we can apply   to the mdfa al to obtain a nfa a l . intuitively     creates a new nfa by combining two states of an existing nfa into a new  superstate . an  nfa anl is an abstraction of a l   if anl can be obtained from a l by repeated application of an  abstraction map  . every state of anl can be viewed as a set of the states of the mdfa a l   i.e  q n   qn    q n    qi   i i with qi    q  for all i.  we define the coarseness of an abstraction anl   as the number of applications of   on the mdfa  required to arrive at anl . intuitively  repeated applications of   create nfas that accept supersets of  the language l recognized by the mdfa  and can hence be seen as coarse grained versions of the    q        original mdfa. the coarsest nfa  given by al    is a nfa with only one accepting node and  it accepts all strings on the alphabet  .  given a regular language l  we define rl to be a rnn that is trained to recognize the language  l  with a certain threshold accuracy. each rnn rl will have a corresponding set of hidden states  denoted by h. more details about the rnn are provided in   . . note that a rnn can also be viewed  as a transition system with   tuple r    h       r   h    f r    where h is a set of possible  hidden   states  typically h   rk      r is the transition function of the trained rnn  h  is the initial state of  the rnn  and f r is the set of accepting rnn states. the key distinction between a dfa and a rnn  is that the latter has a continuous state space  endowed with a topology and a metric.  decoding dfa states from rnns. inspired by methods in computational neuroscience  astrand.  et al.         we can define a decoding function or decoder f   h   q  as a function from the hidden  states of a rnn rl to the states of the corresponding  for l  mdfa a l    q               q     f    .  we are interested in finding decoding functions that provide an insight into the internal knowledge  representation of the rnn  which we quantify via the decoding and transitional accuracy metrics  defined below.        published as a conference paper at iclr         decoding abstraction states. let anl be an abstraction of a l   obtained by applying   to a l  repeatedly n times  and let qn be the set of states of anl . we can define an abstraction decoding  function f    h   qn   by f  h        n    f   h   that is the composition of f with   n  . the function    n  is the function obtained by taking n compositions of   with itself. given a dataset of input  strings d        we can define the decoding accuracy of a map f  for an abstraction anl from rnn  rl by    w       x x   f  ht         n   qt        f  rl   anl       d    w   t    w d    where   c  is the boolean indicator function that evaluates to   if condition c is true and to    otherwise  ht       r  ht   at   and qt          qt   at  . note in particular  that for decoding abstraction  states the condition is only checking if the  t      rnn state is mapped to the  t      nfa state  by f   which may be true even if the  t      rnn state is not mapped to the  t      mdfa state  by the decoding function f . therefore a function f  can have a high decoding accuracy even if the  underlying f does not preserve transitions.  decoding abstract state transitions. we now define an accuracy measure that takes into account  how well transitions are preserved by the underlying function f .  intuitively  for a given decoding function f  and nfa anl   we want to check whether the rnn  transition on a is mapped to the abstraction of the mdfa transition on a. we note that in the  definition of the decoding function  we take into account only the states at  t      and not the  underlying transitions in the original mdfa a l   unlike we do here. more precisely  the transitional  accuracy of a map f  for a given rnn and abstraction  with respect to a data set d  is defined as    f  rl   anl         w       x x   f    r  ht   at        n        f  ht    at       d    w   t    w d    our experiments in the next section demonstrate that decoding functions with high decoding and  transitional accuracies exist for abstractions with relatively low coarseness.         e xperimental r esults    our goal is to experimentally test the hypothesis that a high accuracy  low coarseness decoder exists  from rl to a l . we aim to answer   fundamental questions related to the transitional accuracy  of a l and rl       how do we choose an appropriate abstraction decoding function f       what  necessitates the abstraction function        can we verify that a low coarseness   and high accuracy  f  exists  and lastly      how can we better understand rl in the context of   and f     .  e xperimental d esign  to answer the above questions and evaluate our claims with statistical significance  we have designed  a flexible framework that facilitates comparisons between states of a l and rl   as summarized in  figure  .  we first randomly generate a regular expression specifying  language l with mdfa a l . using a l    s  we randomly generate a training dataset d   d  d  of positive  x    l  and negative  x       l   example strings  see appendix for details . we then train rl with d on the language recognition  task  given an input string x        is x   l  thus  we have two language recognition models  corresponding to state transition systems from which state sequences are extracted. given a length t  input string x    x    x    xt   ...  xt     d  let the categorical states generated by a l be denoted by  q    q    q    qt   ...  qt   and continuous states generated by the rl be h    h    h    ht   ...  ht  . the  recorded state trajectories  q and h  for all input strings x   d are used as inputs into our analysis..  for our experiments  we sample a total of       unique l  and thus perform an analysis of        recognizer mdfas and       trained recognizer rnns.   .  l earning an accurate d ecoder  as mentioned in the beginning of     we must first determine what is a reasonable form for the  decoders f and f  to ensure high accuracy on the decoding task. figure  b shows decoding accuracy        published as a conference paper at iclr         figure    a  left   average linear decoding accuracy as a function of m .  b  right   average  decoding accuracy showing no statistically significant difference between linear  green  and nonlinear   blue  decoders for all mdfas tested.  ed   f  r  a l   f   for several different decoding functions f . we test two linear classifiers  multinomial logistic regression and linear support vector machines  svm   and two non linear classifiers   svm with a rbf kernel  multi layer perceptrons with varying layers and hidden unit sizes . in  order to evaluate whether accuracy varies significantly amongst all decoders  we use a statistically  appropriate f test. surprisingly  we find there to be no statistical difference umong our sampled  languages  the nonlinear decoders achieve no greater accuracy than the simpler linear decoders. we  also observe in our experiments that as the size of the mdfa m increases  the decoding accuracy  decreases for all decoders in a similar manner. figure  a shows this relationship for the multinomial  logistic regression classifier.  taken together  these results have several implications. first  we find that a highly expressive nonlinear decoder does not yield any increase in decoding accuracy  even as we scale up in mdfa  complexity. we can conclude from this finding and our extensive hyperparameter search for each  decoder model that the decoder models we chose are expressive enough for the decoding task. second   we find that decoding accuracy for mdfa states is in general not very high. these two observations  suggest linear decoders are sufficient for the decoding task  but also suggests the need for a different  interpretation of the internal representation of the trained rnn.   .     w hy a bstractions are n ecessary    given the information above  how is the hidden state space of the rl organized  one hypothesis that  is consistent with the observations above is that the trained rnn reflects a coarse grained abstraction  of the state space q   figure     rather than the mdfa states themselves.   to test this hypothesis  we propose a simple greedy algorithm to find an abstraction mapping     a   given an nfa anl with n unique states in qn   consider all  n      partitions of qn    i.e. two  nfa states s  s  have merged into a single superstate  s  s       b  select the partition with the highest  decoding accuracy   c  repeat this iterative merging process until only a   partition remains. we note  that this algorithm does not explicitly take into consideration the transitions between states which are  essential to evaluating  f  rl   anl  . instead  the transitions are taken into account implicitly while  learning the decoder f at each iteration of the abstraction algorithm. decreasing the number of states  in a classification trivially increases ed   f  r  anl   f  . we compare to a baseline where the states  abstracted are random to validate our method. we compute the normalized area under the curve   auc  of a decoder accuracy vs coarseness plot. higher normalized auc indicates a more accurate  abstraction process. we argue through figure  a that our method gives a non trivial increase over the  abstraction performance of a random baseline.  the abstraction algorithm is greedy in the sense that we may not find the globally optimal partition  i.e.  with the highest decoding accuracy and lowest coarseness   but an exhaustive search over all partitions  is computationally intractable. the greedy method we have proposed has o m     complexity instead   and in practice gives satisfactory results. despite it being greedy  we note that the resulting sequence  of clusterings are stable with respect to randomly chosen initial conditions and model parameters.  recognizer rnns with a different number of hidden units result in clustering sequences that are  consistent with each other in the critical first few abstractions.     this idea can be motivated by recasting the regular expression for  say  e mails into a hierarchical grammar  with production rules.          published as a conference paper at iclr         figure     a  left  average normalized area under the curve  auc  for all decoding accuracy vs  coarseness plots  similar to figure   .  b  right   average ratio of coarseness that must be created  relative to m in the mdfa to achieve     testing accuracy.    figure     a  left   average linear decoder testing accuracy as a function of coarseness  the number  of times   is applied   sorted by the number of nodes in the mdfa.  b  right   average transitional  accuracy vs. coarseness  sorted by the number of nodes in the mdfa while using a linear decoder.   .  d ecoding a bstractions and t ransitions  once an abstraction   has been found  we can evaluate whether the learned abstraction decoder f  is  of high accuracy  and whether the   found is of low coarseness. results showing the relationship  between high decoding accuracy  f  rl   anl   as a function of coarseness is presented in figure  a  conditioned on the number of nodes in the original mdfa. as stated in   .   as m increases    f  rl   anl   decreases on the mdfa  i.e. n     . we attribute this to two factors      as m  increases  the decoding problem naturally increases in difficulty  and     rl abstracts multiple states  of al into a single state in h as can be seen empirically from figure  . we validate the second factor  by training a overparameterized non linear decoder on the decoding task and find no instances where  the decoder obtains    training error. alongside the decoding accuracy  we also present transitional  accuracy  f  rl   anl   as a function of coarseness figure  b. both of these figures showcase that  for a given dfa  in general we can find a low coarseness nfa that the hidden state space of rl  can be decoded to with high accuracy. figure  b shows the average ratio of abstractions relative to  m needed to decode to     accuracy  indicating low coarseness relative to a random baseline. for  completeness  we also present decoder and transition accuracy for a nonlinear decoder in figures  a  and  b showing similar results as the linear decoder.  our fundamental work shows a large scale analysis of how rnns rl relate to abstracted nfas anl  for hundreds of minimal dfas  most of which are much larger and more complex than dfas typically  used in the literature. by evaluating the transition accuracy between r and anl we empirically validate  our claim. we show that there does exist high accuracy decoders from r to an abstracted nfa anl .   .  i nterpreting the rnn h idden s tate s pace with respect to the m inimal dfa  with an established high accuracy f  with low coarseness   reveals a unique interpretation of h with  respect to a l . using   and f to relate the two  we uncover an interpretation of how r organizes h  with respect to anl   n    m  . we can then determine the appropriate level of abstraction the network  uses to accomplish the logical language recognition task in relation to the underlying mdfa. we  provide two example  real world  dfas to illustrate this interpretation and show several interesting  patterns.        published as a conference paper at iclr         figure     a  left   average nonlinear testing accuracy as a function of coarseness  sorted by the  number of nodes in the mdfa.  b  right   average transitional accuracy vs coarseness  sorted by  the number of nodes in the mdfa while using a non linear decoder.   a d    a d  . v z                                                                                                                                                                                                                                                                            figure     a  top   the mdfa of the s imple e mails language with a dendrogram representing the  the sequence of abstractions created while using a linear decoder. showing the initial abstractions are  those of the same pattern  a d  .  b  bottom  the mdfa of the dates language with a dendrogram  representing the the sequence of abstractions created while using a linear decoder. showing the initial  abstractions are those representing states that represent the same moment in time.    we present in figure   the clustering sequences of two regular expressions that have real world  interpretations  namely the s imple e mails and dates languages that recognize simple emails  and simple dates respectively. to explain  figure  b shows the dates language with its clustering  sequence superimposed on the mdfa in the form of a dendrogram. the dendrogram can be read  in a top down fashion  which displays the membership of the mdfa states and the sequence of  abstractions up to n   m    . a question then arises  how should one pick a correct level of  abstraction n . the answer can be seen in the corresponding accuracies  f  rl   anl   in figure  .  as n increases and the number of total nfa states decreases  the linear decoding  ldc  prediction  task obviously gets easier       accuracy when the number of nfa states q q    is     and hence it  is important to consider how to choose the number of abstractions in the final partition. we typically  set a threshold for  f  rl   anl   and select the minimum n required to achieve the threshold accuracy.        published as a conference paper at iclr         figure     a  left   linear decoder accuracies as a function of coarseness for the s imple e mails  language in figure  a.  b  right   linear decoder accuracies as a function of coarseness for the  dates language corresponding to figure  b.  consider the first two abstractions of the s imple e mails dfa. we notice that both states   and    represent the pattern matching task  a d    because they are agglomerated by the algorithm. once  two abstractions have been made  the decoder accuracy is at a sufficient point  as seen in figure   . this suggests that the collection of hidden states for the two states are not linearly separable.  one possible and very likely reason for this is the network has learned an abstraction of the pattern   a d   and uses the same hidden state space regardless of location in string to recognize this pattern   which has been indicated in past work  karpathy et al.       . this intuitive example demonstrates  the rnn s capability to learn and abstract patterns from the dfa. this makes intuitive sense because  rl does not have any direct access to a l   only to samples generated from a l . the flexibility of  rnns allows such abstractions to be created easily.  the second major pattern that arises can be seen in the dendrogram in the bottom row of figure  .  we notice that  generally  multiple states that represent the same location in the input string get  merged    and      and      and   . the s imple e mails dendrogram shows patterns that are  location independent  while the fixed length pattern in the dates regex shows location dependent  patterns. we also notice that the algorithm tends to agglomerate states that are within close sequential  proximity to each other in the dfa  again indicating location dependent hierarchical priors. overall   our new interpretation of h reveals some new intuitions  empirically backed by our decoding and  transitional accuracy scores  regarding how the rnn rl structures the hidden state space h in the  task of language recognition. we find patterns such as these in almost all of the dfa s tested. we  provide five additional random dfa s in the appendix  figures       to show the wide variability of  the regular expressions we generate evaluate on.         c onclusions    we have studied how rnns trained to recognize regular formal languages represent knowledge in  their hidden state. specifically  we have asked if this internal representation can be decoded into  canonical  minimal dfa that exactly recognizes the language  and can therefore be seen to be the   ground truth . we have shown that a linear function does a remarkably good job at performing such  a decoding. critically  however  this decoder maps states of the rnn not to mdfa states  but to  states of an abstraction obtained by clustering small sets of mdfa states into  abstractions . overall   the results suggest a strong structural relationship between internal representations used by rnns  and finite automata  and explain the well known ability of rnns to recognize formal grammatical  structure.  we see our work as a fundamental step in the larger effort to study how neural networks learn formal  logical concepts. we intend to explore more complex and richer classes of formal languages  such as  context free languages and recursively enumerable languages  and their neural analogs.          published as a conference paper at iclr         