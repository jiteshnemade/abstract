introduction    fully connected neural networks are known to have many redundant weights               which  makes training challenging and inference costly. there is an need for increasingly large networks       but the compute and memory cost increases quadratically with network size if fully connected  which  makes the cost of increasing network size prohibitive.  large rnns are increasingly being replaced by convolutional sequence to sequence networks because  of the rnn s huge number of trainable weights. this causes overfitting and training instability  and  limits the potential of parallellization        . techniques like pruning or factorization have been  proposed to reduce the inference cost of these networks but often increase the training time and  complexity and introduce additional hyperparameters. in addition  achieving real world speedups  remains difficult.  in contrast to fully connected rnns  neurons in the human brain and other real world resourceconstrained systems are not fully connected at all  with the majority of the connections being local             . we review structurally sparse  locally recurrent neural network architectures  and discuss  implications for hardware acceleration.  compared to unstructured sparsity  for example by pruning  structural sparsity also reduces the  number of weights and computations  but it does so in a way much more suitable for parallel  hardware acceleration. the sparse recurrent connections provide the additional advantage of reduced  communication between neurons  enabling more parallelism. in addition  learning about ways to  simplify the network architecture grants insight into the functioning of these networks.  preprint. under review.          related work    it has been shown that a large sparse network performs better than a small dense network             .  han et al.      combined iterative pruning retraining  quantization and encoding to achieve      model compression on alexnet. achieving real world speedups remains difficult  as the technique  results in unstructured sparsity and irregular memory access patterns.  structured pruning modifies the loss function to create more hardware friendly memory access  patterns  resulting in block sparse weight matrices  for example. narang et al.      achieved the  same performance as a full lstm using    times less weights and a similar speedup. an alternative  approach is to factorize the weight matrices         . both techniques are effective for improving  speed on current hardware  but offer little architectural insight into the reasons the resulting sparse  networks are so good. in addition  training is still fairly complex and expensive  which limits the  practical use of large networks.    self connected recurrent neural networks. the sequential modeling capabilities of rnns  stem from the feedback connections. the most direct feedback connections are self connections  of neurons through which neurons can access information about their own past. this idea leads to  the diagonalrnn architecture  where neurons only have self connections  and the recurrent weight  matrix is a diagonal matrix  in fact  simply a vector .  diagonalrnns have been used successfully in systems control since the    s             . there  are no recurrent connections between neurons in the same layer except for self connections. this  reduces the recurrent transformation to an o n  element wise vector vector multiplication instead of  an o n    matrix vector multiplication. the number of recurrent weights is reduced correspondingly.  different variations of this architecture have been studied  varying the location of the feedback  and the type of recurrence transformation  in general this feedback can be considered an iir filter      . alternatively  fir type recurrences are possible  which are closely related to convolutional  sequence to sequence models  van den oord et al.      . tsoi and back      provide an overview.  the diagonalrnn idea was recently rediscovered and applied to lstms and grus by subakan  and smaragdis     . it was also explored by li et al.      for deep vanilla rnn networks with  skip connections between them. both articles show performance similar to fully connected lstms   however  at much lower cost. mikolov et al.      use the diagonalrnn with fixed recurrent weights  close to unity as contextual units in addition to a fully connected vanilla rnn  obtaining performance  close to lstms yet with a much simpler architecture.  one disadvantage of diagonalrnn is that neurons can no longer see the hidden states of other  neurons in the same layer  placing constraints on the model s temporal dynamics. however  even  with this constraint the models perform well on many challenging tasks as shown in our experiments.    locally recurrent neural networks. to combine the advantages of parallelism  structured memory access patterns  linear relation between number of neurons and the computational cost and  memory footprint  as well as the temporal modeling capacity of the fully connected models  locally  connected networks are a promising alternative to self recurrent networks. this approach is inspired  by both biological neural networks and other real world networked systems  where each connection  incurs a cost. such systems exhibit local connectivity and operate extremely well even under strict  connectivity constraints    .  chan and young     propose locally connected rnns  within one layer each neuron is only connected  to itself and a few neighbours instead of connecting to all other neurons or only to itself  see figure  b .  this allows for greater temporal modeling capability  while reducing the size of the hidden weight  matrix from n   n to n   c  with c the number of connections per neuron being much smaller  than n . by varying the number of neighbours a neuron is connected to  a trade off between the  complexity of temporal dynamics and the computational complexity can be achieved.  the limited local connectivity also allows for better control of network stability during training   which is a challenge with fully connected rnns. the constraint imposed by local connectivity can  act as a regularizer  making learning easier and reducing overfitting     .        a network with local recurrent connections only results in a band diagonal recurrent weight matrix  wrec . there exist fast linear algebra routines for computing banded matrix vector products  so this  recurrent transformation is straightforward to accelerate on current hardware.  a variant of the locally connected rnn is the  grouped  rnn              where a single layer  is replaced by several smaller ones in parallel. neurons are partitioned in several groups  with  full connectivity within each group. groups are not connected to each other  but their outputs are  concatenated when passed to the next layer. this can be seen as a generalization of diagonalrnns   where the independent units are now groups of several neurons instead of single neurons. interestingly   this idea is also very similar to the idea of grouped convolutions  which is gaining traction in state ofthe art convolutional neural networks     .  gray et al.      introduced fast gpu kernels for computations with block sparse matrices  and  demonstrated good performance of block sparse and small world rnns on large scale sentiment  analysis and speech recognition tasks.  both tao lei and artzi      and bradbury et al.     propose alternative gated rnns with a very  efficient element wise recurrent transformation. the gates are computed based only on the current  input  so these operations can be easily executed in parallel across time steps. the recurrence is very  fast as compared to full matrix multiplications at every time step. in several tasks this architecture has  been shown to perform well  however the lack of a hidden state may limit the model capacity     .         structurally sparse rnns for hardware acceleration    as described in section    there are a variety of rnn architectures that make use of local  as opposed  to global  connectivity. table   lists an overview of the architectures  together with the complexity of  the recurrent weight matrix. figure   shows the connectivity patterns  while figure   shows a visual  representation of these sparse weight matrices.  in unstructured sparse networks  see figure  b   non zero weights are spread far apart in an irregular  pattern with lots of zeros in between. when loading the weight matrices from memory  these zeros  have to be loaded as well  and cause higher latency as well as high energy consumption     .  the structurally sparse matrices of grouprnn bandrnn diagonalrnn have large advantages with  regards to memory access patterns  as their non zero weights are all located close together in memory.  in addition  there are some straightforward parallellization opportunities. the following paragraphs  provide an overview of the benefits for hardware acceleration of various rnn architectures.  full rnn  in a standard fully connected recurrent neural network all neurons have recurrent connections to all other neurons. the connectivity pattern is visualized in figure  a  and the  recurrent weight matrix in figure  a. there is no opportunity for parallellization across  neurons  and the recurrent transformation requires a full matrix multiplication.  grouprnn  here  neurons are divided in g groups  with full connectivity within each group  but no  connections between groups. the recurrent weight matrix structure is shown in figure  c.  this allows for complex temporal dynamics and specialization across groups  while the  recurrent computation is much reduced due to the lower number of weights. each neuron  group operates independently of the other groups in the same layer  so there is an opportunity  for parallellization as well.  bandrnn  this is a locally connected rnn  where neurons have self connections in addition to  connections to their immediate neighbours. this creates a sparse band diagonal recurrent  weight matrix  which can be exploited for efficient computation. there is a linear relation  between the number of recurrent weights and the number of neurons. the recurrent weight  matrix is shown in figure  d and figure  e for several band widths. the connectivity pattern  is visualized in figure  b.  diagonalrnn  this is an extreme form of bandrnn with zero connections to neighbours  or  grouprnn with g   nh . this means every neuron is completely independent of the other  neurons in the same layer  and can be computed in parallel. there are only nh recurrent  weights  and the recurrent transformation consists of elementwise operations only. the  connectivity pattern is visualized in figure  c.          table    the number of recurrent weights nrec for a layer of nh neurons  depending on the network  connectivity. the number of recurrent weights for diagonalrnn and bandrnn is linear in the  number of neurons instead of quadratic  rnn cell type   recurrent weights  nh   nh  nh   nh  g  nh  nh   c    full rnn  grouprnn  diagonalrnn  bandrnn                                                                                                                                            b  bandrnn  connections   a  full rnn  connections to  to some neighboring neurons  all neurons in the layer  plus self connection          c  diagonalrnn   self connections only    figure    recurrent connectivity for various rnn types                         neuron incoming       neuron incoming    neuron incoming                                      neuron outgoing           a  full rnn                                                  neuron outgoing     d  bandrnn  c                        neuron incoming                  neuron outgoing     c  grouprnn  g           neuron incoming    neuron incoming            neuron outgoing     b  sparse rnn                                                                 neuron outgoing     e  bandrnn  c                           neuron outgoing           f  diagonalrnn  c        figure    recurrent weight matrices for various rnn types. grey indicates a nonzero weight  while  white indicates a zero weight  i.e. there s no connection            .     diagonalrnns  high speed and low memory footprint    table   compares the speed and cost of full rnns with diagonalrnns. diagonalrnns provide large  speedup opportunities through   . vastly reduced computation in the  sequential  recurrent operation    . parallellization across neurons    . much lower memory footprint.    table    speed experiment in pytorch  comparing cudnn rnn  and diagonalrnn with custom  cuda kernel. diagonalrnn does away with almost all of the recurrent weights  and achieves large  speedups of  x at inference compared to full rnn.  model  train time batch infer time batch model size   recurrent  l      nh        weights  cudnn rnn  cuda diagonalrnn       ms     ms     .  ms   .  ms      .  mb    .  mb       .            in diagonalrnns  the recurrent computation is much simpler  as it only requires elementwise  operations instead of full matrix multiplications. this is of great benefit as the recurrent transformation  is the sequential part of the rnn operation  and usually the performance bottleneck. secondly  every  neuron in a layer is completely independent of every other neuron in that layer  so it s possible to  parallelize across neurons. this creates an additional level of parallelism  making them very well  suited for execution on parallel hardware.  due to the much lower cost of the sequential recurrent computation  plus the parallellization across  neurons  diagonalrnns are  x faster than full rnn for inference.  in deep learning hardware accelerators  the cost of loading weights from memory accounts for a  large portion of the total energy cost     . with diagonalrnns  the storage cost for the recurrent  weights is o nh   instead of o nh     i.e. linear instead of quadratic in the number of neurons.  this brings the number of recurrent weights to almost zero compared to fully connected networks   see table  . the much smaller memory footprint allows keeping the model weights fully in registers   as proposed by zhu et al.       which allows for further large increases in efficiency  reduced latency  and higher performance.   .     prernn with bandrnn    rnns are widely applied to capture motion dynamics and to provide temporal contexts in video  understanding. since the processing unit of a video is an image frame or a short video snippet  cnns  pre trained on large scale image or video data sets are usually used as backbone networks  which  rnns are then built upon         .  instead of simply stacking a cnn and a full rnn and training the whole network from scratch   prernn has recently been proposed in      to convert convolutional layers into recurrent layers.  prernn employs one or more pre trained feedforward layers of a cnn as the input to hidden  transformations to the rnn and therefore avoids the default input to hidden transformation  which  uses a matrix multiplication with nh   ninputs weights of each gate. the only new weights  introduced by prernn are then the nh   nh hidden to hidden weights for each gate.      also  propose the  prernn sih  architecture to further simplify gating functions by binding all gates to  the same single input to hidden transformation. this technique allows for large weight savings  faster  training  and in many cases better performance.  prernn and bandrnn are highly complementary as the former significantly simplifies the inputto hidden transformations and the latter largely reduces the hidden to hidden weights. therefore   the combination of prernn and bandrnn leads to a very efficient  high performance video action  recognition model  as demonstrated in section  .        table    ptb language modeling. diagonallstm performs equivalently to full lstm  while  requiring about       less recurrent weights. bandlstm improves on full lstm  and requires      less recurrent weights.  model  test perplexity  rnn weights  l      nh         full lstm  grouplstm  g      diagonallstm  c      bandlstm  c       bandlstm  c        bandlstm  c                lower is better     .      .      .      .      .      .      total    recurrent        .        .         .         .         .         .           .         .         .         .         .         .          experiments    networks are in the experiments are specified by the recurrent cell type  rnn or lstm   the number  of layers lh   and the number of neurons per layer  nh . bandrnn networks are additionally specified  by the number of recurrent weights per neuron  c. for example  bandrnn           c      is an  rnn with   layer of     neurons  with    connections per neuron    self connection    to neighbours  on the left and   to neighbours on the right .   .     language modeling  ptb    ptb is a well known language modeling data set     . we perform word level language modeling  on the ptb data set  with a vocabulary size of        embedding size of     and tied weights for  encoder decoder  using the adam optimizer and a learning rate of  .   . the remaining training  setup is according to merity et al.       using training code at   .  table   shows the perplexity results for different models. diagonallstm achieve test perplexity very  close to the fully connected lstm  and both grouplstm and bandlstm improve the perplexity.  when increasing the band  performance actually decreases  so it appears that limiting the recurrent  connectivity has a regularizing effect  which is especially important in the ptb benchmark.   .     phoneme recognition  timit    timit is a well known speech recognition data set containing frame level phonetic annotations  labeled by experts garofolo et al.    . it contains  .  hours of audio data from     different speakers  of eight american english dialects  with rich and varied phonetic content    .  for the task of phoneme recognition  the goal is to predict the sub word phoneme corresponding to  every frame of audio. this is a more low level task than speech recognition  and considering the small  size of the data set a good task for fast benchmarking of networks. we use    mfcc coefficients  plus energy with first and second derivatives for a total of    input features. labels are the timit  phoneme labels  collapsed to the reduced set of    phonemes proposed by lee and hon     . the  initial learning rate is  .     with a decay factor of  .  every time the accuracy doesn t improve for    subsequent epochs. we use the adam optimizer. networks are trained for    epochs.  table   shows an overview of the experiments. the sparse networks achieve encouraging results   with diagonallstm performing slightly worse than the full lstm  but while requiring      less  recurrent weights. bandlstm performs significantly better than the fully connected network  while  still requiring    less recurrent weights.   .     speech recognition  vctk    vctk is a standard speech recognition data set consisting of    hours of audio from     native  english speakers  reading sentences selected from a newspaper veaux et al.     . it is a large scale   open vocabulary problem and a challenging task for speech recognition systems     .       https   github.com salesforce awd lstm lm          table    phoneme recognition on timit. structurally sparse networks perform close to or better than  the fully connected network  while requiring much less weights.  model  test accuracy  rnn weights  l      nh           higher is better     total    recurrent      .     .     .      .         .         .           .         .         .          full lstm  diagonallstm  bandlstm  c          table    speech recognition on vctk. for the same number of neurons full gru outperforms  diagonalgru  but larger structurally sparse networks perform better with lower cost.  model  test cer  rnn weights   lower is better   full gru          diagonalgru          c      bandgru          c        diagonalgru           c      bandgru           c           .     .     .     .     .      total    recurrent        .        .         .         .         .           .         .         .         .         .          models are trained to perform character level speech recognition. the network architecture and  training setup is very similar to deepspeech   amodei et al.       consisting of   convolutional layers   followed by   recurrent layers  with the ctc loss function. there is no language model used  and  greedy decoding is used. we use a       train test split  selecting randomly. the optimizer is sgd  with a momentum of  .   with an initial learning rate of  .     decreasing by factor  .  every epoch   and we train for    epochs. we use small tempo and gain for data augmentation. training code is  based on   . further details can be found there.  a comparison of the character error rate  cer  during training is shown in figure  . we observe that  the larger sparse networks converge faster and achieve lower errors than the fully connected network.  the results are shown in table  . diagonalgru and bandgru perform worse than the full gru by  a relatively significant margin. however  the number of recurrent weights in diagonalgru is almost     x lower  in addition to the parallellization opportunity  see section  .    so this is still a good  option for resource constrained applications.  with more neurons in the locally connected networks  both bandgru and diagonalgru outperform  full gru  while still having a much more efficient recurrent transformation and    x less recurrent  weights for diagonalgru.   .     video action recognition  ucf       another important category for sequential data analysis is the processing of videos. we use video  action recognition as a representative testbed to evaluate bandrnn and diagonalrnn. our experiment is performed on the public dataset ucf          which contains     action classes and         videos. we follow the two stream method      to use two separate cnns on the spatial  rgb  and  temporal  optical flow  streams. cnns of each stream in our evaluations are combined with rnns.  resnet        pre trained on imagenet is used as the backbone cnn. following prernn        we transform the last conv layer of resnet   into a recurrent layer and then apply bandrnn and  diagonalrnn. we use the standard experimental setting and report results on the first split.  table   shows the video action recognition accuracy and number of recurrent weights of different  networks. diagonalrnn and bandrnn coupled with prernn achieve comparable accuracy as full  prernn and perform better than traditional rnns  while using over   x less recurrent weights  in  particular for the combination of diagonalrnn and prernn sih which requires about    x less  recurrent weights and achieves the same accuracy.       https   github.com seannaren deepspeech.pytorch          cerresults      .     .     .     .     .     .     .     .     .     .     .    .    .    .    .     banddiagonalgru  x     banddiagonalgru  x      fullgru  x     diagonalgru  x     diagonalgru  x                                                            epoch    figure    character error rate  cer  on vctk. diagonal gru and band gru perform worse than  full gru  but allow larger network sizes for the same budget. these larger networks perform better.  table    comparison of video action recognition accuracy     and number of recurrent weights for  different recurrent networks on the ucf    dataset.  model connectivity  full rnn  diagonalrnn  bandrnn  c       bandrnn  c        bandrnn  c        traditional rnn         prelstm    prelstm sih         .   .             .   .              .   .              .   .              .   .                     .   .             .   .              .   .              .   .              .   .              lstm    .   .              pregru    pregru sih         .   .             .   .              .   .              .   .              .   .                .   .              .   .              .   .              .   .              .   .              gru    .   .              discussion    our experiments on the tasks of language modeling  speech and phoneme recognition  and action  recognition as well as work by other authors                 show that structurally sparse networks can  match and improve performance of fully connected networks  while reducing the cost significantly.  for the same number of neurons  sparse rnns  especially diagonalrnns  are much more costeffective than fully connected rnns. alternatively  for the same parameter or compute budget sparse  rnns can be larger  thereby increasing performance.  future work could involve optimizing the elementwise recurrent computations  as well as integrating  diagonalrnn with work by zhu et al.      which stores weights fully on chip to achieve even larger  speedups.         conclusion    we presented a simple recurrent architecture with high degrees of structured sparsity in the recurrent  weight matrix  allowing for large speedups and model size reduction during both training and  inference. the models were evaluated on several challenging sequence modeling tasks  and the  structurally sparse networks were found to perform as well or better than fully connected networks in  several tasks.  diagonalrnns provide straightforward parallellization opportunities  and reduce the cost of the  recurrent transformation by a large factor  removing the sequential bottleneck which limits speed of  fully connected rnns on parallel hardware  and greatly reduce the network s memory footprint.  these results are extremely encouraging for further research and practical adoption of structural  sparsity in real world applications  opening the way both to ever larger neural network sizes and to  adoption of high performance rnns on low power and low cost devices.        