introduction    recurrent neural networks  rnn        are neural models able to take some context  into account in their decision function. for this reason  they are particularly suitable for  several nlp tasks  in particular sequential information prediction      . in rnns  the  contextual information is provided to the model by a loop connection in the network  architecture. this connection allows to use at the current time step one or more pieces  of information predicted at previous time steps. this architecture seems particularly  effective for neural networks since it allows to combine the power of distributional  representations  or embeddings  with the effectiveness of contextual information.  in the literature about rnns for nlp  two main variants have been proposed  also  called  simple  rnns  the elman     and the jordan     rnn models. the difference  between these models lies in the position of the loop connection giving the recurrent  character to the network  in the elman rnn  it is put in the hidden layer whereas in        the jordan rnn it connects the output layer to the hidden layer. in this last case  the  recurrent connection allows to use  at the current time step  the information predicted  at previous time steps. in the last few years  these two types of rnns have been  very successful for language modeling          and for some sequence labeling tasks               .  the intuition at the origin of this article is that embeddings allow a fine and effective  modeling not only of words  but also of labels and their dependencies  which are very  important for sequence labeling. in this paper  we define two new variants of rnn to  achieve this more effective modeling.  in the first variant  the recurrent connection is between the output and the input  layers. in other words  this variant gives labels predicted at previous positions in a sequence as input to the network. such contextual information is added to the usual input  context made of words  and both are used to predict the label at the current position  in the sequence. moreover we modified the hidden layer activity computation with  respect to elman and jordan rnn  so that to take the different information provided  by words and labels into account. from our intuition  thanks to label embeddings and  to features learning abilities of the hidden layer  this variant models in a more effective  way label dependencies. the second variant we propose combines an elman rnn and  our first variant. this variant can thus exploit both contextual information provided by  the previous states of the hidden layer  and the labels predicted at previous positions of  a sequence.  a high level schema of the elman s  jordan s and our first variant of rnn are  shown in figure  . the schema of our second variant can be obtained by adding the recursion of the first one to the elman architecture. in this figure  w is the input word  y is  the predicted label  e  h  o and r are the parameter matrices between each pair of layers  they will be described in details in the next section. before  it is worth discussing  the advantages our variants can bring with respect to traditional rnn architectures   and explaining why they are expected to provide better modeling abilities.     a  elman rnn     b  jordan rnn     c  our variant of  rnn    figure    high level schema of the main rnns studied in this work.  first  since the output at previous positions is given as input to the network at the        current position  the contextual information flows across the whole network  affecting  each layer s input and output  at both forward and backward phases. in contrast  in  elman and jordan rnns  not all layers are affected at both forward and backward  phases.  a second advantage of our variants is given by label embeddings. indeed  the first  layer of our rnns is just a look up table mapping sparse  one hot  representations into  distributional representations.  since in our variants the output of the network at previous steps is given as input at the current step  the mapping from sparse representations  to embeddings involves both words and labels. label embeddings can be pre trained  from data as it is usually done for words. pre trained word embeddings  e.g. with  word vec  have already shown their ability to capture very attractive syntactic and semantic properties        . using label embeddings  the same properties can be learned  also for labels. more importantly  using several predicted labels as embeddings provide  a more effective modeling of label dependencies via the internal state of the network   which is the hidden layer s output.  another advantage coming from the use of label embeddings and different previous  labels as context  is an increased robustness of the model to prediction mistakes. this  effect comes from the syntactic and semantic properties that embeddings can encode      .  all these advantages are also supported by our second variant  which uses both a  label embedding context  like the first variant  and the loop connection at the hidden  layer  like an elman rnn.  all rnns in this article are studied in their forward  backward and bidirectional  versions    . in order to have a fair and straightforward comparison  we give the results  of our new variants of rnns together with those obtained with our implementation  of elman and jordan rnns. these implementations are very close to state of the art   even if we did not implement every optimization feature.  all models are evaluated on four tasks. two are spoken language understanding   slu  tasks       atis      and media       which can be both modeled as sequence  labeling problems. two are pos tagging tasks  one on the french treebank  ftb            and one on the penn treebank  ptb      . the results we obtain on these tasks  with our implementations  despite they are not always better than the state of the art   provide a stable ranking of different rnn architectures  at least one of our variants   most of the time and surprisingly the simpler one  is always better than jordan and  elman rnns.  in the remainder of the paper  we introduce rnns and we describe in more details  the variants proposed in this work  section   . in section    we describe the corpora  used for evaluation and all the results obtained  in comparison with state of the art  models. in section    we draw our conclusions.    the  one hot  representation of an element at position i in a dictionary v is a vector of size  v   where  the i th component has the value   whereas all the others are  .               improving recurrent neural networks    the rnns we consider in this work have the same architecture also used for feedforward neural network language models  nnlm   described in     . in this architecture  we have four layers  input  embedding  hidden and output. words are given as  input to the network as indexes  corresponding to their position in a dictionary v .  the index of a word is used to select its embedding  or distributional representation  in a real valued matrix e   r v  xn    v   being the size of the dictionary and n  the dimensionality of the embeddings  which is a parameter to be chosen . we name  e v wt    the embedding of the word w given as input at the position t of a sequence.  v wt     i is the index of the word wt in the dictionary  and it can be seen alternatively  as a  one hot  vector representation  the vector is zero everywhere except at position  v w   where it is   .  in contrast to nnlm  rnns have one more connection  the recursive connection   between two layers  depending on the type of rnns. as mentioned previously  elman  rnns have a recursion loop in the hidden layer. since this layer encodes the internal representation of the input to the network  the recurrent connection of an elman  network allows to keep  in memory  words used as input at previous positions in the  sequence. jordan rnns have instead a recursion between the output and the hidden  layer. this means that a jordan rnn can take previous predicted labels into account to  predict the label at the current position in a sequence. for every type of rnn  we call  r the matrix of parameters of the recursion connection.  our implementation of jordan and elman rnns is like in the literature.      .       .     rnn learning    learning the described rnns consists in learning the parameters      e  h  o  r   between each pair of layers  see figure     and we omit biases to keep notations lighter.  we use a cross entropy cost function between the expected label ct and the predicted  label yt at the position t in the sequence  plus a l  regularization term                             is an hyper parameter of the model. since yt is a probability distribution over  output labels  we can also view the output of a rnn as the probability of the predicted label yt   p  yt  i    . i is the input given to the network plus the contextual information provided by the recurrent connection. for elman rnn ielman    wt w ...wt ...wt w   ht     that is the word input context and the output of the hidden  layer at the previous position in the sequence. for jordan rnn ijordan   wt w ...wt ...wt w   yt      that is the same word input context as elman rnn  and the label predicted at the previous position. we associate the following decision function to predict the label at  position t in a sequence   c    ct   log yt        l t   argmaxj   ...  l  p  ytj  i        we    also find     quite easy to understand  even for readers not familiar with rnns.                 l t is a particular discrete label.  we use the back propagation algorithm and the stochastic gradient descent with  momentum      for learning the weights  .     .     learning variants    an important choice for learning rnn models concern the back propagation algorithm. indeed  because of the recurrent nature of their architecture  in order to properly learn rnns the back propagation through time algorithm  bptt  should be  used     . the bptt algorithm constists basically in unfolding the recurrent architecture for a choosen number of steps  and then learning the network as a standard  feed forward network. this is supposed to allow rnns to learn arbitrarily long past  context. however       has shown that rnns for language modelling learn best with  just   time steps in the past. this may be due to the fact that  at least in nlp tasks   the past information kept  in memory  by the network via the recurrent architecture   actually fades away after some time steps. moreover  in many nlp tasks  using an  arbitrarily long context on either input or output side  doesn t garantee better performances  as increasing the context size also increases the noise. since bptt is quite  more expensive than the traditional back propagation algorithm      has preferred to  use explicit output context in jordan rnns and to learn the model with the traditional  back propagation algorithm  not surprisingly without loosing performance.  in this work we use the same variant as    . when using an explicit context of  output labels from previous time steps  the hidden layer activity of a jordan rnn is  computed as   ht     it   h    yt c   yt c   ...yt       r            where c is the size of the history of previous labels that we want to explicitly use as  context to predict next label.     indicates the concatenation of vectors.  all the modifications applied to the jordan rnn so far can be applied in a similar  way to the elman rnn.     .     new variants of rnn    as mentioned in the introduction  the new variants of rnn proposed in this work  present two differences with respect to traditional rnns  i  the recurrent connection  is from the output to the input layer  meaning that predicted labels are converted into  embeddings in the same way as words  ii  the hidden layer activities are computed  in a slightly different way. indeed  in elman and jordan rnns the contextual information provided by the recurrent connection is summed to the input information  see  equation   above . in our variants of rnns instead  word and label embeddings are  concatenated and provided to the hidden layer as different inputs.  the most interesting consequence of modifications in our rnn variants  is the fact  that output labels are mapped into distributional representations  as it is usually done for  input items. indeed  the first layer of our network  is just a mapping from sparse  onehot  representations to distributional representations. such mapping results in fine          features and attractive syntactic and semantic properties  as shown by word vec and  similar works     . such representations can be learnt from data the same way as for  words. in the simplest case  this can be done by using sequences of output labels. when  structured information is available  like syntactic parse trees or structured semantic  labels such as named entities or entity relations  more sophosticated embeddings can  be learnt. in this work  we learn label embeddings using sequences of output labels  associated to word sequences in annotated data. it is worth noting that the idea of using  label embeddings has been introduced by      in the context of dependency parsing. in  this paper  we focus on the use of several label embeddings as context  thus encoding  label dependencies  which are very important in sequence labeling tasks.  using the same notation as above  we name ew the embedding matrix for words   and el the embedding matrix for labels. we name  it    ew  v wt w   ...ew  v wt   ...ew  v wt w      the concatenation of the vectors representing the input words when processing the  position t in a sequence  while  lt    el  v yt c     el  v yt c     ...el  v yt        is the concatenation of the vectors representing the output labels predicted at the  previous c steps. the hidden layer activities are computed as   ht      it lt     h     is the sigmoid activation function           means the concatenation of the two  matrices and we omit biases to keep notations lighter. the remainder of the layer  activities  as well as the error computation and back propagation are computed the  same way as in traditional rnns.  note that in this variant of rnn there is no r matrix at the recurrent connection. the recurrent connection here means that the output is given back as input to  the network and it is thus converted explicitely from probability distribution given by  the softmax into a label index  which is used in turn to select a label embedding from  the matrix el . basically the role played from matrix r in elman and jordan rnn  is  played from matrix el in our variant.  another important interest of having the recursion between output and input layers  is robustness. this is a direct consequence of using embeddings for output labels. since  we use several predicted labels as context at each position t  see lt above   at least in  the later stages of learning  when the model is close to the final optimum   it is unlikely  to have several mistakes in the same context. even then  thanks to the properties of  distributed representations       wrong labels have very similar representations to the  correct ones. taking an example cited in       if we use paris instead of rome  it has no  effect for many nlp tasks  as they are both proper nouns for pos tagging  locations for  named entity recognition etc. distributed representations for labels provide the same  robustness on the output side.   jordan rnns cannot provide in general the same robustness. we can interpret  hidden activity computation in a jordan rnn in two ways.  on the one hand  if we interpret a jordan rnn as using sparse label representations  as input to the hidden layer  such representations are either  one hot  representations of    sometimes in pos tagging  models mistake verbs and nouns. they make such errors because some  particular verbs occur in the same context of nouns  e.g.  the sleep is important    and so have similar  representations.          labels or probability distributions given as output by the softmax at the output layer. in  the first case it is clear that a mistake may have more effect than in the second one  as the  only value that is not zero is in a wrong position. but when probability distributions are  used  we have found that most of the probability mass is  picked  on one or few labels   which thus does not provide much more softness than a  one hot  representation. in  this interpretation  sparse labels are an additional input for the hidden layer  the matrix  r on the recurrent connection plays the same role as h does for input words. the two  matrices are then summed to compute the total input of the hidden layer.  on the other hand  we can see the multiplication of sparse representation of labels  in equation   as the selection of an embedding for labels from matrix r.  even in  this interpretation there is a substantial difference between the jordan rnn and our  variant of rnn  i rnn henceforth  i stands for improved . in order to understand in  detail this difference  we focus on the equations for computing the hidden activities.  for jordan rnn we have   ht     it   h    yt c   yt c   ...yt       r   for i rnn we have   ht      it lt     h   where lt is  el  v yt c     el  v yt c     ... el  v yt        that is the concatenation of c previous label embeddings.  in this second interpretation  in jordan rnn labels are not directly concerned in the  computation of the total input for the hidden layer  since they are not directly multiplied  by matrix h. only input context it is multiplied by h. the result of this multiplication  is summed to the result of label embedding selection  performed as y t   i    r  i      . . . c  . finally the hidden non linear function   is applied. mixing input processing  it   h and label processing y t   i    r with a sum  can make sense for the tasks for  which the jordan network was designed      as output units were of the same nature as  input units  speech signal . however we believe that it doesn t express sufficiently well  words and labels interactions in nlp tasks. also  since y t   i    r is an embedding  selection for labels  labels and words are not processed in the same way  it is already  made of word embeddings  which are further transformed by it   h. this further  transformation is not applied to label embeddings.  in i rnn in contrast  sparse labels are first converted into embeddings with el  v yt i       i     . . . c  and their concatenation results in lt . this matrix is further concatenated  to the input context it . the result of the concatenation is multiplied by matrix h to  compute the total input to the hidden layer.  finally the hidden non linear function    is applied. this means that information provided by the input context it is not mixed  with lt with a sum like in jordan rnn. these two data are given to the hidden layer  as separated inputs. more in particular  the concatenation of it and lt is performed  neuron wise  that is each hidden neuron receives as input all context words and all context labels  encoding them as a network internal feature. thus we let the hidden layer  itself to learn labels interactions  and words labels interactions. this is indeed in agreement with the  philosophy  of neural networks  where features designing is turned into  features learning. since words and labels have different nature in sequence labeling    multiplying    we    a  one hot  representation by a matrix is equivalent to selecting one row of the matrix.  remind the reader that we are omitting biases to keep notation lighter          tasks  we believe that modeling interactions in this way is more effective. also  with  the i rnn architecture  words and labels are processed in the same way  as both are  first converted into embeddings  and then  transformed  again via multiplication by h  matrix. in order to make results obtained with different rnns comparable  we used  the same number of hidden neurons for all rnns. in i rnn thus  each hidden neuron  receives as input much more information than jordan and elman hidden neurons.  in order to make the explanation more clear  i rnn architecture is detailed in  figure  . symbols have the same meaning as equations in the paper  the only exception  is that labels are indicated in the figure with upper case l. matrix h is replicated at  the hidden layer computation meaning that all neurons receive the whole  it lt   input   which is made of     w       c d dimensional embeddings   w     word embeddings  and c label embeddings. cat is the concatenation operator. please note that the  concatenations of embeddings are performed at two different steps just for the sake  of clearness and to be coherent with equations in the paper  all concatenations can be  performed in one step.  our second variant of rnn combines the characteristics of an elman rnn and of  our first variant. in this variant  the only difference with the first one is the computation  of the hidden layer activities  where we use the concatenation of the c previous hidden  layer states in addition to the information already used in the first variant   ht      it lt     h    ht c   ht c   ...ht       r      .     forward  backward and bidirectional rnns    all rnns described in this work are studied in their forward  backward and bidirectional versions    . forward rnns work as described so far. backward rnns have  exactly the same architecture  the only difference being that they process sequences in  the reverse order  from the end to the begin. backward rnns can be used to predict  future labels in a sequence.  bidirectional rnns use both past and future information to predict the next label   which is both words and labels in our variants and in jordan rnns  or both words and  hidden layers in elman rnns. when labeling sequences with bidirectional rnns  a  backward network is first used to predict labels backward. the bidirectional rnn then  processes sequences in forward direction using past contextual information as usual   and future contextual information provided by the states and labels predicted by the  backward network. the hidden layer of the bidirectional version of our fist variant of  rnns is thus computed as   ht      it lpt lft     h   p  where lt   lt introduced above  while  lft    el  v yt      . . . el  v yt c     el  v yt c      is the concatenation of the vectors representing the c future labels predicted by the  backward model. it is very similar for our second variant  refer to     for details.     .     recurrent neural network complexity    we provide here an analysis of the complexity in terms of the number of parameters  involved in each model. in jordan rnn we count         figure    detailed architecture of i rnn. symbols have the same meaning as equations in the paper   the only exception is that labels are indicated in the figure with upper case l. please note that matrix h is  replicated at the hidden layer computation meaning that all neurons receive the whole  it lt   input  which  is made of     w       c d dimensional embeddings   w     word embeddings and c label embeddings.  cat is the concatenation operator.     v     d      w     d   c o      h     h     o   where  v   is the size of the input dictionary  d the dimensionality of the embeddings   h  and  o  are the size of the hidden and output layers  respectively  w the size  of the window of words used as context on the input side  and c is the size of the context of labels  which is multiplied by the dimensionality of the output label dictionary   o .  with the same symbols  in an elman rnn and in our first variant we have  respectively    v     d      w     d   c h      h     h     o   and   v     d    o    d      w     d   cd     h     h     o     the word input context is thus made of w words on the left and w on the right of the word at a given  position t  plus the word at t itself  which gives a total of  w     input words          the only difference between the jordan and elman rnns lies in the factors c o   and c h . their difference in complexity depends thus on the size of the output layer   jordan  with respect to the size of the hidden layer  elman . since in sequence labeling  tasks the hidden layer is often bigger  elman rnn is more complex than jordan rnn.  the difference between the jordan rnn and our first variant is in the factors  o    d  and cd. the first is due to the label embeddings    the second is due to the use of such  embeddings as input to the hidden layer. since often d and o have sizes in the same  order of magnitude  and thanks to the use of vectorized operations on matrices  we  didn t found a noticeable difference in terms of training and testing time between the  jordan rnn and our first variant. this simple analysis also shows that our first variant  roughly needs the same number of connections in the hidden layer as a jordan rnn.  our first variant is thus architecturally equivalent to a jordan rnn.  in contrast  for the second variant we have    v     d    o    d      w     d   cd   c h      h     h     o   the additional term c h   is due to the same recurrent connection as in an elman  rnn. using vectorized operations for matrix calculations  we found the second variant  slower for both training and testing time by a factor  .   with respect to the other  rnns. the same complexity analysis holds for backward rnns. but bidirectional  rnns are even more complex. without deriving any new formula  we note that they  are slower with respect to their corresponding forward backward models by a factor of  roughly  . .         evaluation     .     corpora    we used four distinct corpora   the air travel information system  atis  task      has been designed to automatically provide flight information in slu systems. the semantic representation  is frame based and the goal is to find the correct frame and the corresponding semantic slots. for example  for the sentence  i want the flights from boston to philadelphia today   the correct frame is flight and the words boston  philadelphia and  today must be annotated with the concepts departure.city  arrival.city  and departure.date  respectively.  atis is a relatively simple task dating from     . the training set is made of       sentences taken from the  context independent  data in the atis   and atis    corpora. the test set is made of     sentences  taken from the atis   nov   and  dec   datasets. there are no official development data provided with this corpus  we  have thus taken a part of the training data at random to play this role.    the french corpus media      has been created to evaluate slu systems providing tourist information  in particular hotel information in france. it is composed of       dialogues acquired with a wizard of oz protocol where     speakers have applied   hotel reservation scenarios. the dialogues have been annotated following a rich    we       use embeddings of the same size d for words and labels.  please see      for more details on the atis corpus.             sentences    words    vocab.    oov     training          words  concepts                                     development         words  concepts                           .     .      test         words  concepts                              .     .      table    statistics on the corpus media  section  ftb train  ftb dev  ftb test      sentences                        tokens                            unk. tokens                              table    statistics on the ftb corpus used for pos tagging  semantic ontology. semantic components can be combined to create complex semantic  labels.  in addition to this rich annotation  another difficulty lies in the coreferences.  some words cannot be correctly annotated without information about previous dialog  turns. for example in the sentence  yes  the one at less than fifty euros per night    the one refers to an hotel previously introduced in the dialog. statistics on training   development and test data from this corpus are shown in table  .  both atis and media can be modelled as sequence labeling tasks using the bio  chunking notation     . several different works compared on atis          .     is the  only work providing results on media with rnns  it also provides results obtained  with crfs       allowing an interesting comparison.  the french treebank  ftb  corpus is presented in     . the version we use for  pos tagging is exactly the same as in     . but  in contrast to them  who reach the best  result on this task with an external lexicon  we don t use any external resource here    .  statistics on the ftb corpus are shown in table  .  the penn treebank  ftb  corpus is presented in     . in order to have a direct  comparison with previous works                we split the data as they do  sections         are used for training          for validation and         for testing. statistics  on the ptb corpus are shown in table  .     .     rnns parameters settings    in order to compare with some published works on the atis and media tasks  we  use the same dimensionality settings used by          and      that is embeddings have      dimensions  hidden layer has     dimensions. we also use the same context size  for words  that is w      and we use c     as labels context size in our variants and  in jordan rnn. we use the same tokenization  basically consisting of words lowercasing.  in contrast  in our models the sigmoid activation function at the hidden layer and  the l  regularization. while               and      use the rectified linear activation  function and the dropout regularization          .    for example the label localisation can be combined with city  relative distance   general relative place  street etc.          also provides results without using the external lexicon           data set  training  development  test    sections                        sentences                          tokens                               unknown                     table    statistics of the training  development and test data texts of the penn treebank  corpus  for the ftb pos tagging task we have used     dimensional embeddings     dimensional hidden layer  again w     for the context on the input side  and   context  labels on the output side. the bigger hidden layer gave better results during validation  experiments  due to the larger word dictionary in this task with respect to the others   roughly       for the ftb against      for media and      for atis. in contrast  to       which has used several features of words  prefixes  suffixes  capitalisation information etc.   we only performed a simple tokenization for reducing the size of the  input dictionary  all numbers have been mapped to a conventional symbol  num    and nouns not corresponding to proper names and starting with a capital letter have  been converted to lowercase. we preferred this simple tokenisation without using rich  features  because our goal in this work is not obtaining the best results ever  it is to  compare jordan and elman rnns with our variants of rnn and show that our variants  works better for sequence labeling. adding many features and or building sophisticated models would make the message less clear  as results would be probably better  but the improvements could be attributed to rich and sophisticated models  instead of  to the model itself.  for the ptb pos tagging task we use exactly the same settings and pre processing  as for the ftb task  except that we used     hidden neurons. during validation we  found that this works better  again due to the size of the dictionary which is       for  this task  after pre processing .  we trained all rnns with exactly the same protocol  i  we first train neural language models to obtain word and label embeddings. this language model is like the  one in       except it uses both words labels in the past and in the future to predict next  word label. ii  we train all rnns using the same embeddings trained at previous step.  we train the rnn for word embeddings for    epochs  the rnn for label embeddings  for    epochs  and we train the rnns for sequence labeling for    epochs. the number of epochs has been roughly optimized on development data. at the end of training  we keep the model which gave the best tagging accuracy on development data. also  we roughly optimized on development data the learning rate and the parameter   for  regularization  the best values found are  .  and  .     respectively.     .     training and tagging time    since implementations of rnns use in this work are prototypes     it does not make  sense to compare them to state of the art in terms of training and tagging time. however it is worth providing training times at least to have an idea and to have a comparison among different rnns.     our    implementations are basically written in octave https   www.gnu.org software octave            as explained in section  .   our first variant i rnn and the jordan rnn have  the same complexity. also  since the size of the hidden layer is in the same order of  magnitude as the size of the output layer  i.e. the number of labels   also elman rnn  has roughly the same complexity as the jordan rnn. this is reflected in the training  time.  the training time for label embeddings is always relatively short  as the size of the  output layer  that is the number of labels  is always relatively small. this training time  thus can vary from few minutes for atis and media  to less that   hour for the ftb  corpus.  training word embeddings is also very fast on atis and media  taking less than     minutes for atis and roughly    minutes for media. in contrast training word  embeddings on the ftb corpus takes roughly   days  training time goes to roughly    days for the ptb word embeddings.  training the rnn taggers takes roughly the same time as training the word embeddings  as the size of the word dictionary is the dimension that most affects the  computational complexity at softmax used to predict next label.  concerning the second variant of rnn proposed in this work  i e rnn  this is  slower as it is more complex in terms of number of parameters. training i e rnn  on atis and media takes roughly    minutes and   hour  respectively. in contrast  training i e rnn on the ftb and ptb corpora takes roughly   days and   days   respectively.  we didn t keep track of tagging time  however it is always negligible with respect  to training time  and it is always measured in few minutes. all times provided here are  with a  .  ghz cpu  single process.     .     sequence labeling results    the evaluation of all models described here is shown in table       and    in terms of f   measure for atis and media  accuracy on the ftb and ptb. our implementations  of elman and jordan rnn are indicated in tables as e rnn and j rnn. our new  variants are indicated as i rnn and i e rnn.  table   shows results on the atis corpus. in the higher part of the table we show  results obtained using only words as input. in the lower part  results are obtained using  both words and word classes available for this task. such classes concern city names   airports and time expressions. they allow the models to generalize from specific words  triggering concepts.    note that our results on the atis corpus are not always comparable with those  published in the literature because  i  models published in the literature use a rectified  linear activation function at the hidden layer   and the dropout regularization. our  models use the sigmoid activation function and the l  regularization. ii  for experiments on atis we have used roughly     of the training data for development  we thus     for example the cities of boston and philadelphia in the example above are mapped to the class  city name. if a model has never seen boston during the training phase  but has seen at least one city  name  it will possibly annotate boston as a departure city thanks to some discriminative context  such as the  preposition from.     f  x    max    x .           used a smaller training set. iii  the works we compare with do not always give details  on how classes available for the task have been integrated into their models. iv  layer  dimensionality and hyper parameter settings do not always match those of published  works. in fact  to avoid running too much experiments  we have based our settings on  known works  but this doesn t allow a straightforward comparison with other published  works.  despite this  the message we want to claim in this work is still true for two reasons  i  some of the results obtained with elman and jordan rnns are close  or even  better  than state of the art. thus they are not weak baselines. ii  we provide a fair  comparison of our variants with traditional elman and jordan rnns.  the results in the higher part of table   show that the best model on the atis  task  with these settings  is the elman rnn of     . note that it is not clear how the  improvements of      with respect to      in part due to same authors  have been obtained. indeed  in     authors obtain the best result with a jordan rnn  while in      an  elman rnn gets the best performance. during our experiments  using the same experimentation protocol as       we could not reach the same performances. we conclude  that the difference between our results and those in      are due to reasons mentioned  above. beyond this  we note that our elman and jordan rnn implementations are  equivalent to those of    . also  our first variant of rnns  i rnn  obtains the second  best result    .   in bold   which is the best result we could reach. our second variant  is roughly equivalent to a jordan rnn on this task.  the results in the lower part of the table    classes   obtained with both words and  classes as input  are quite better than those obtained with words only. they roughly  follow the same behavior  except that in this case our jordan rnn is slightly better  than our second variant. the first variant i rnn obtains again the best result among  our implementations    .   in bold . in this case also  we attribute the differences with  respect to published results to the different settings mentioned above. for comparison   in this part of the table we show also results obtained using crf.  on the atis task  using either words or both words and classes as inputs  we can  see that results are always quite high. this task is relatively simple. label dependencies  can be easily modeled  as there is basically no segmentation of concepts over different  consecutive words  one concept corresponds to one word . in this settings  the potential  of our new variants of rnns cannot be fully exploited. this limitation is confirmed by  the results obtained by     and      using crfs. indeed  rnns don t take the whole  sequence of labels into account in their decision function. in contrast  crfs use a  global decision function taking all the possible labelings of a given input sequence into  account to predict the best sequence of labels. the fact that crfs are less effective  than rnn on atis is a clear sign that label dependency modeling is relatively simple  in this task.  table   shows the results on the corpus media. as already mentioned  this task is  more difficult because of its richer semantic annotation  and also because of the coreferences and of the segmentation of concepts over several words. this creates relatively  long label dependencies  which cannot be taken into account by simple models. the  difficulty of this task is confirmed by the magnitude of results     f  points lower than  on the atis task .  as can be seen in table    crfs are in general much more effective than jordan         and elman rnns on media. this outcome could be expected as rnns use a local  decision function not able to take long label dependencies into account. we can also  see that our implementations of elman and jordan rnns are comparable  even better  in the case of elman rnn  with state of the art rnns of    .  more importantly  results on the media task shows that in this particular experimental settings where taking label dependencies into account is crucial  our new variants are remarkably more effective than both our implementations and state of the art  implementations     of elman and jordan rnns. this holds for forward  backward  and bidirectional rnns. moreover  the bidirectional version of our variants of rnns  outperforms even crf. we attribute this effectiveness to a better modeling of label  dependencies  due to label embeddings.  table   shows the results obtained on the pos tagging task of the ftb corpus. on  this task  we compare our rnn implementations to the state of the art results obtained  in      with the model m elt f r . we would like to underline that m elt f r   when it does  not use external resources like in the model obtaining the best absolute result in        nevertheless uses several features associated with words that provide an advantage over  features used in our rnns. as can be expected thus  m elt f r outperforms the rnns.  results in table   shows that forward and backward rnns are quite close to each other  on this task  i rnn  providing a little improvement. in contrast  the bidirectional  version of i rnn and i e rnn provide a significant improvement over jordan and  elman rnns.  table   shows the results obtained on the wsj pos tagging task. we also provide  the results of      and      for comparison with the state of the art. this is just for  a matter of comparison  as the results shown in those works were achieved with rich  features and more sophisticated models. instead  we can roughly compare our results  with those of      which were also obtained with neural networks. in particular  we  can compare with the model called nn sll. note that this is a rough comparison  as the model of      though not a rnn  integrates capitalisation features and uses a  convolution and a max over time layer to encode large context information.  results for pos tagging on the ptb corpus roughly confirm the conclusions reached  for ftb results  with the only difference that in this case i e rnn is slightly better  than i rnn. our rnns don t improve the state of the art  but are all more effective  than the model of    . this result is particularly important  as it shows that rnns  even  without using a sophisticated encoding of the context like the model nn sll in       are intrinsecally a better model for sequence labeling. this claim is enforced also by  the fact that nn sll of     implements a global probability computation strategy similar to crf  sll stands for sentence level likelihood   while all rnns presented here  use a local decision function  see equation   . again  the ranking of rnn models on  the ptb pos tagging task is stable  the variants of rnns proposed in this work being  more effective than traditional elman and jordan rnns.  we would like to notify that we have also performed some experiments on atis  and media without pre training label embeddings. the results reached are not substantially different from those obtained with pre training label embeddings. indeed  on  relatively small data  it is not rare to have similar or even better results without pretraining. this is due to the fact that learning effective embeddings requires a relatively  large amount of data.     also shows results obtained with embeddings pre trained with         word vec and without embedding pre training. his conclusions are indeed very similar. more generally  reaching roughly the same results on a difficult task like media  without label embedding pre training is a clear sign that our variants of rnns are superior to traditional rnns because they use a context made of label embeddings. as a  matter of fact  the gain with respect to elman and jordan rnns cannot be attributed in  this case to the use of pre trained embeddings. on relatively larger corpora like ftb  and ptb  label embedding pre training seems to provide a slight improvement.  finally  we have run experiments also modifying jordan and elman rnns so that  to model words and labels interactions more like i rnn does  that is word and label embeddings  or hidden states in elman rnn  are not summed together  instead  they are concatenated. results obtained were not substantially different from those obtained with  traditional  jordan and elman rnn and in any case i rnn was always  performing best  still keeping a large gain over elman and jordan rnn on the media task. the explanation of this outcome is that keeping word and label embeddings  separated  and then multiplying both by matrix h to compute hidden activities  as we  do in i rnn  is more effective than concatenating it   h and y t        r  as we did  for jordan rnn  and analogously with previous hidden layer for elman rnn. this is  not surprising  as i rnn also in this case is applying one transformation more when  multiplying label embeddings by h to compute the total input to the hidden layer.  it is someway surprising that i rnn systematically outperforms i e rnn  the  latter model integrates more information at the hidden layer and thus should be able  to take advantage of both elman and i rnn characteristics. while an analysis to  explain this outcome is not trivial  our interpretation is that using two recursions in  a rnn gives actually redundant information to the model. indeed  the output of the  hidden layer keeps the internal state of the network  which is the internal  distributed   representation of the input n gram of words around position t and the previous c labels.  the recursion at the hidden layer allows to keep this information  in memory  and to  use it at the next step t    . however using the recursion of i rnn the previous c  labels are also given explicitely as input to the hidden layer. this may be redundant   and constrain the model to learn an increased amount of noise. a similar idea of hybrid  rnn model has been tested in      without showing a clear advantage on elman and  jordan rnns.  what can be said in general from the results obtained on all the presented tasks  is  that rnn architectures using label embedding context can model label dependencies in  a more effective way  even when these dependencies are relatively simple  like in atis  and pos tagging tasks . the two variant of rnns proposed in this work  in particular  the i rnn variant  are for this reason more effective than elman and jordan rnns on  sequence labeling tasks.     .     comparison of jordan rnn and i rnn label representations    we compare jordan rnn and i rnn label representations under the interpretation  where jordan rnn hidden activity computation uses sparse labels as an additional input to the hidden layer  as explained in section  . . as explained also in the same  section  under the other interpretation i rnn provides the advantage of performing an           model        e rnn      j rnn       e rnn  e rnn  j rnn  i rnn  i e rnn      e rnn      crf      e rnn       e rnn       crf  e rnn  j rnn  i rnn  i e rnn    f  measure  forward  backward  bidirectional  words    .       .          .       .       .       .          .       .       .       .       .       .       .       .       .       .       .       .       .     classes    .                   .       .             .          .             .       .       .       .       .       .       .       .       .       .       .       .       .       table    results of slu on the atis corpus  model      e rnn      j rnn      crf  e rnn  j rnn  i rnn  i e rnn    forward    .       .          .       .       .       .       f  measure  backward  bidirectional                   .       .       .       .       .       .       .       .       .       table    results of slu on the media corpus  additional transformation on labels  and gives words and labels embeddings as separated inputs to the hidden layer.  under the first interpretation  the advantage of using label embeddings in i rnn  instead of  one hot  or probability distribution representations like in jordan rnns  is  an increased amount of signal flowing across the network. a semantic interpretation  of the interaction of these two representations with the network is not trivial. indeed   in the probability representation output by the softmax in a jordan rnn  the different  dimensions are just probabilities associated to different labels. in contrast  in label  embeddings used in i rnn  the dimensions are different distributional features  related  to how a particular label is used in particular label contexts. a comparison between  these two representation is thus not really meaningful.  instead  we performed a simple analysis of the magnitude of values found in the  probability distribution used as representations in jordan rnns  when using development data of the media corpus. we summarize this analysis as follows    .      out of              of the time  the maximum value is greater than  .    .      out of              of the time  the sum of the   highest probabilities is  greater than  .    . excluding the   highest probabilities  the remaining values in the distribution         model       m elt f r  e rnn  j rnn  i rnn  i e rnn    accuracy  backward  bidirectional       .       .       .       .       .       .       .       .       .       forward       .       .       .       .       table    results of pos tagging on the ftb  model                  nn sll  e rnn  j rnn  i rnn  i e rnn    forward             .       .       .       .       accuracy  backward  bidirectional       .          .          .       .       .       .       .       .       .       .       .       table    results of pos tagging on the ptb  have very small values  less than  .      this simple analysis shows that the probability distributions used for label representations in jordan rnns do not provide much more information to the network than  a  one hot  representation  and not much signal into the network. this problem is  someway similar to the  vanishing gradient  problem       as the network learns  the  probability gets concentrated on few dimensions and all the other values get very small   limiting network learning. this problem is all the more obvious that label dependency  modeling is more important for the task. on an absolute scale however  this is less  serious than the vanishing gradient problem  as jordan rnns still reach competitive  performances.         conclusions    in this paper we have studied different architectures of recurrent neural networks for  sequence labeling tasks. we have proposed two new variants of rnns to better model  label dependencies  and we have compared these variants to the traditional architectures of elman and jordan rnns. we have explained the advantages provided by the  proposed variants with respect to previous rnns. we have evaluated all rnns  either  new or traditional  on   different tasks  two of spoken language understanding and  two of pos tagging. the results show that  even though rnns don t always improve  the state of the art  our new variants of rnns always outperform the traditional elman  and jordan rnns.    