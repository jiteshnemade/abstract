introduction    a framework called reservoir computing  rc  was  proposed as a novel brain inspired information processing       . one of interesting features of rc is that it exploits the inherent transient dynamics of recurrent neural  networks  rnns  as a computational resource in inputdriven rnns. because of this feature rc has been focused as a theoretical framework to explain the mechanism of information processing of the central nervous  system  cns             and rc has been used to emulate  the human activity like motor action       . as illustrated  in fig.   a   the architecture of rc generally consists of  three layers  an input layer  a reservoir layer implemented  with a recurrent neural network  rnn   and an output  layer. one of the significant features of this framework is  the ability to embed the information of the previous input into the transient dynamics of rnns  which enables  real time information processing of input streams. this  feature is often called a short term memory property.  further  nonlinearity is another feature of the framework  that is defined by the ability to emulate nonlinear information processing. additionally  the supervised learning  of the reservoir layer is not required if the dynamics of  reservoir layer is appropriate to required dynamical systems. the connection weights from the rnns to the  output  which are called readout weights  are adjusted to  learn the required dynamical systems. although rc has  a simple learning scheme  it has been observed to exhibit  high performance in many machine learning tasks       .  the information processing capability of an rc system is dependent on the inherent dynamics of the reservoir layer. consequently  many authors have investigated         iwade acs.i.kyoto u.ac.jp    the information processing capabilities of various dynamical systems. one of the most well known networks is a  dynamical system at the edge of chaos  or at the edge  of stability   which is the transition point from ordered  to chaotic dynamics. such systems exhibit superior information processing capability         . some studies  have investigated the information processing capabilities  of physical phenomena. these approaches exploit the  dynamics of physical phenomena as reservoirs  such as  the surface of water       optoelectronic and photonic  systems          ensemble quantum systems       neuromorphic chips       and the mechanical bodies of soft  robots        .  in this study  we exploit the rnns that are optimized by an optimization principle called recurrent infomax  ri       and investigate the information processing capabilities of these networks. ri was proposed as  an extension of feedforward infomax  which was originally proposed by linsker     . ri provides an unsupervised learning scheme to maximize information retention  in an rnn  which is quantified with mutual information.  the study which introduced ri explained that the rnns  optimized by ri exhibit the characteristic dynamics of  neural activities such as cell assembly like and synfire  chain like spontaneous activities as well as critical neuronal avalanches in cns     . an rnn optimized by ri  is expected to exhibit superior information processing capability because the principle of ri is to maximize the  information retention. however  the method by which ri  affects the information processing capability of an rnn  remains unclear. in this study  we investigate to clarify  this method in detail based on the framework of rc. initially  we optimize rnns using ri and analyze the basis  of their information processing capabilities using benchmark tasks.        ii.    model    we consider an input driven network consisting of n  neurons where the state of each neuron xi  t            i       ...  n   is updated synchronously and stochastically at  discrete timesteps. the firing probability of neuron i is  determined by its interaction with another neuron j  j       ...  n   that is connected with the weight wij and the  input u t           that is connected with the weight wiin  as  pmax          p xi  t                  exp  ui  t    ui  t       n  x    wij  xj  t    p j     wiin  u t    p       hi  t      j      updated at the end of each block  which consists of          timesteps. the connection weights at the b    th  block w b      are calculated using the steepest gradient  method as follows   wij  b        wij  b         hi  t        hi  t      xi  t        p i              where   is the learning rate  which remains constant at   .  . the initial conditions of the network  wij and wiin    are sampled from the gaussian distribution with        and        .  .  iii.    recurrent infomax    further  we briefly describe the algorithm of ri     . in  this study  ri is applied to the connection weights within  the rnn and from the input to the rnn. the connection weights are represented by solid lines in fig.   a .  as illustrated in fig.    b   the connection weights are       log  d                   e   . . . e n     ..      c     ... . . .  .    en   . . . en n           i b    log  c     where            e   . . . e n    . .  ..  ..    ..  .       e   . . . e n     d      eb   . . . eb n    .. . .  ..    .  .  .  enb   . . . enb n     t     ebij    t     eibj    t     ebibj    t    eij      e b  . . . e nb  .. . .  ..  .  .  .  en b  . . . en nb  eb b  . . . eb nb  .. . .  ..  .  .  .  enbb  . . . enb nb                                              x     xi  t    p i   xj  t    p j         a     x     xi  t        p i   xj  t    p j         b     x     xi  t    p i   xj  t        p j         c     x     xi  t        p i   xj  t        p j  .      d     finally  the gradient of the mutual information with  respect to the connection weights is approximated by     i b    x           ij   ebibk ebjl   ebil ebj bk     c     ji    d    ji    d    j n i n     wij                p k        p l  ebkl   p k p l      p k       p l     d    lk n      d    k n l  .       typical instances of the firing activity of the network op            where   is the learning rate that remains constant at       .  throughout this study. the input connection  weights wiin and the input u t  are represented by wi   and x   t   respectively. the gradient of the mutual information with respect to wij is formulated using several  approximations     . the mutual information between  two successive states at the bth block is denoted using  gaussian approximation and is represented as follows          where pmax is the maximal firing probability  p   is the  expected input value  p j is the mean firing rate of neuron  j and hi  t  is the bias for neuron i. the firing frequency  of each neuron is indicated by the mean firing rate. for  example  the neuron i is activated once in    timesteps  on an average at p i    . . the maximal firing probability  pmax indicates the reliability of the activation in response  to the input. in this study  the maximal firing probability  and the mean firing rate are fixed at pmax    .  and p i     .   respectively. the input u t  is sampled randomly  from        with the expected value p      . . the bias  hi  t  is used to fix the average firing probability p i and  is updated at each timestep by     i b       wij           timized by ri at the  th and     th block are depicted in          update the    update the    and    and    fig.  .  a  the architecture of a conventional rc. the connections represend by the solid lines and dashed lines are optimized  using ri and the least square method  respectively  for a specific task.  b  the scheme for ri. the connection weights are  updated between two consecutive blocks and each block consists of         timesteps. the first        timesteps are for hi to  converge to a steady state and the last        timesteps are used to calculate mutual information.    figs.   a  and   b . the neurons in the network optimized by ri tend to fire simultaneously. similar dynamics were also exhibited in     . as depicted in fig.   c    the mutual information is increased by ri and it is almost  saturated at the     th block. therefore  we terminate  ri at the     th block for our study.    iv.    information processing  capability    we use three benchmark tasks  memory capacity   bit boolean function  and   bit boolean function tasks to  evaluate the information processing capability of a network optimized using ri. each benchmark task consists  of washout         timesteps   learning       timesteps    and testing       timesteps  phases. the washout phase  is used to eliminate the influence of the initial state of  the network and to converge the bias hi  t  to a steadystate value. further  the learning phase is used to learn  readout weights wout      whereas the testing phase        timesteps  is used to evaluate the performance of each  task accordingly.  the memory capacity task has been extensively used  as a benchmark task in the framework of rc         .  this task evaluates the extent of decoding the information of past inputs from the network state with memoryless readout. the performance of this task represented  by mc is the summation of    delay memory functions   mf          ...       defined by  mc          x    mf  .    the readout weights  wout      are trained in accordance  t     t      x  x   x  y  where y is the vector of  with wout        timestep past inputs and x is the matrix of network  states during the learning phase.  the n bit  n         boolean function task evaluates  whether the network can process the boolean operation  using previous n bit inputs. we have depicted an example in table.  . each rule of the boolean function  tasks requires separate information processing. hence   the execution of each rule may provide us with useful insights about the property of information processing for     rnns optimized by ri. the total number of rules is  n    and all rules except for two  are applied. exceptional  rules are those for which output f  is   or   regardless of  the input. thus  the number of rules  k  for each n bit     boolean function task is  n    .  table i. an example of a rule table for a   bit boolean  function.  u t       u t          f   t                                         the performance of this task is termed boolean capacity  bc   which is the summation of boolean function bfk  at each rule k  k      ...  k  and time delay            ...      and is represented as follows             bc              k       xx  bfk     k                  k      the memory function mf  is the determination coefficient between u t       and z   t    wout    x t  defined by  mf    max  out  w     cov   z   t   u t         .       z   t       u t                   where  bfk    max    w k out    cov   zk  n   f k  n    .       zk  n       f k  n                  a    c    b    d    fig.  .  color online   a  b  the raster plots for the firing activity of the network at the  th and the     th blocks from         to        timesteps. the circular and triangular points indicate the firing of the input and the neurons in rnn.  c  the  result of mutual information as a function of the block. the solid line indicates the mean for the different initial connection  weights  whereas the dotted line indicates the standard deviation for the networks with the different initial conditions. the  number of trials is     which remains constant throughout this study  d  the results of mc and bcs as a function of the  block. the solid line indicates the result of the memory capacity task. the dashed and dotted lines indicate the result of a    bit and   bit boolean function task  respectively. the error bar indicates the standard deviation for networks with different  initial conditions.    the results of the benchmark tasks are depicted in  fig.   d . we suspended the update and performed the  benchmark tasks for the networks while evaluating the  information processing capability of the networks during ri. the process of updating ri was resumed  after  evaluating the performance of benchmark tasks. though  both the memory capacity and boolean capacities are  improved using ri  they exhibit a peak at the     th  block. this is in sharp contrast to the mutual information  which is observed to saturate at about the     th  block.  we visualized the network structure to investigate the  reason that both mc and bcs exhibit a peak. the network structure is depicted in fig.   a f   which represents the connections with the    largest absolute values.  most of the visualized connections are those within the  rnn  and a few are connections from the input to the  rnn. additionally  we calculated the mean of the absolute connection weights  w   within the rnn and those  from the input to the rnn  as illustrated in fig.   g .  the former is calculated using the    largest absolute  values in w while the latter is calculated using win . we  can confirm that the value of w within the rnn become  much higher than that of w from input to the rnn as  the block of ri increases. this result suggests that the  information of the input is not stored in the network  but  the only information existing in the network is preserved  when the network is optimized by ri for long term. consequently  the performances of both mc and bcs become    worse.    v.    a.    introduction of input  multiplicity    recurrent infomax and information processing  capability    in the previous section  we depicted that the information processing capability exhibits an increase at the  beginning of ri but peaked at the     th block because  the connection weights within the rnn became stronger  than the input connection weights. hence  the input information may not be preserved in the network. we tried  to increase the number of input neurons having common  input information to handle this problem. this is equivalent to introducing the weight coefficient  k   which is  entitled input multiplicity  in the updated formula of the  connection weights from the input to the rnn. the updated formula is  wi   b        wi   b     k     i b       wi             where k    k k   z      k      . we optimize the  input driven rnn by ri using the above formula to update the connection weights from the input to rnn and  subsequently executed the memory capacity and boolean  function tasks. the parameter values except for the        weight coefficient k and benchmark tasks of ri are similar to those mentioned in previous section.  a    a    b    c    d    b    c    d    e    f    fig.  .  color online   a  the mutual information as a function of input multiplicity  k. the color indicates the block  number.  b  the performance of the memory capability task.   c  the performance of the   bit boolean function task.  d   the performance of the   bit boolean function task.    g    fig.  .  color online   a  c  e  the heatmap for the connection weights at the  th       th and     th blocks. the  neuron    indicates the input.  b  d  f  the network structure is designed using each left heatmap. the connections  with the    largest absolute values are visualized. the blue  and red nodes represent the neurons in the rnn and the input  respectively.  g  the mean of the absolute strength of  the connection weights from the input to the rnn and those  within the rnn as a function of the block. the mean and  standard deviations of the networks under distinct initial conditions are illustrated  but the error bar is not substantial due  to the small standard deviation.    initially  we provide an overview of the results for the  mutual information and benchmark tasks. as depicted  in fig.   a   a large k value decreases the mutual information at the learning phase  although ri increases  the mutual information for all values of k  a large k de     creases the mutual information at the end of the learning  phase. furthermore  the memory capacity illustrated in  fig.   b  also increases throughout the optimization using ri at all values of k  except for k      is maximized  around k    . the results of the   bit and   bit boolean  function tasks illustrated in fig.   c  exhibit similar tendencies as that of the memory capacity task.  figures.   a d  illustrate typical network structures at  k        . at k      the strong connection weights from  the input to the rnn and those within the rnn coexist.  the postsynaptic input neurons become the presynaptic neurons of other neurons. hence  a delay line structure originates from the input. conversely  the strong  connection weights are mostly observed to be the connections from the input to the rnn at k     . the  mean strength of the connection weights as a function  of the block exhibits opposite tendency to that of connection weights mentioned in the previous section  which  increases more rapidly than those within the rnn  as depicted in fig.   e  f . these network structures suggest  that for a large k value  the input information at the  final timestep may be stored dominantly in the network  and that majority of the previous input may be lost.  we confirm this speculation quantitatively by investigating the mutual information between the input at the  last timestep and each neuron in rnn  i xi  t   u t         i      ...  n   as plotted in fig.   a  using boxplot. this  result indicates that the network optimized by ri at large  values of k contains information about the input given  at the last timestep. the results of the memory    bit        a  k         b  k         a    b    c  k          d  k          c    d    e  k         f  k          fig.  .  a  the boxplot of mutual information between last  input and the rnn. it is estimated by sampling the          timesteps after washout timesteps          timesteps .  b   the performance of the memory capacity task as a function  of time delay for k        and   .  c  the performance of  the   bit boolean function task as a function of time delay for  k        and   .  d  the performance of the   bit boolean  function task as a function of time delay for k        and    . the error bar indicates the standard deviation for the  networks under the different initial conditions.  fig.  .  color online   a  c  the heatmap for the connection  weights for k     and    at the     th block. neuron     indicates the input.  b  d  the network structure is designed  using each left heatmap. the    largest absolute connection  weights are visualized. the blue nodes represent the neurons  in the rnn  whereas the red nodes represent the input.  e  f   the mean of the absolute strength of the connection weights  from the input to the rnn and those within the rnn is a  function of the block number.    boolean  and the   bit boolean functions as a function  of time delay are plotted in fig.   b  c . the performance of the network when k      at       is observed  to be higher than that when k    . as the time delay    becomes longer  the performance of the network when  k      is inferior to the performance when k    . all  theses results are consistent with the speculation that the  network optimized by ri with a large k value can only  store the recent input information. as discussed in the  preceding sections the learning with ri using a small k  value optimizes the network such that the last input information is not stored dominantly  but the information  about its past state is stored.    b. comparison of information processing  capability between linear and nonlinear rules    we classified the rules of the boolean function tasks  into two classes  i.e.  linear and nonlinear rules  to investigate the property of information processing of the  network optimized by ri. the classification criterion is  based on whether the rule is linearly separable or not      . figure   plots the performance of each rule sorted by  score and colored to denote whether it is linear or nonlinear. initially  we discuss the results of the   bit boolean  function task depicted in fig.   a . the performance of  the majority of the rules are improved using ri  and the  networks exhibit high performance in the execution of  linear rules as compared to nonlinear rules. this result  suggests that the network optimized by ri performed the  linear information processing better than nonlinear information processing. the initial two rules were performed  more efficiently than the other linear rules because the  difficulty of executing a rule is dependent on both linear separability as well as short term memory. these  two rules are identical to the memory capacity task  and  which does not require the information of u t         but  it only requires the information of u t      . therefore   these rules are comparatively simple to execute than the  other linear rules.  the results of the   bit boolean function task illus         a    b    fig.  .  color online  the performance of each rule of the  boolean function tasks for k     at the  th block  the points  with light colors  and     th block  the points with deep colors . the blue points indicate the linear rule while the red  indicate the nonlinear rule. the error bar indicates the standard deviation for the different initial networks.  a    bit and   b    bit boolean function tasks.    trated in fig.   b  exhibit the tendency that linear rules  are performed better than nonlinear rules. however   some nonlinear rules are performed better than linear  rules. consequently  the difference between the two kinds  of rules  which is observed in the   bit boolean function task  is unclear  because some rules require previous information about the input u t          while the  performance of each rule may be influenced not only by  nonlinearity but also the short term memory in the   bit  boolean function task.  vi.    discussions    ported to be one of the structure that can achieve optimal  short term memory             . however  our model is  stochastic  and the complete delay line network may not  be suitable for short term memory because the optimal  short term memory of the delay line network is based on  an assumption that the information of presynaptic neurons is conveyed to postsynaptic neurons without any  loss  and the retention of the input information of the  delay line network is fragile to the unreliable firing of  neurons.  the performance for each rule of the   bit boolean  function task clearly demonstrate the information processing property of the network optimized by ri. although the performance for majority of the rules is improved by ri  the degrees of improvement depend majorly on the linearity of the rules. the linear rules are  processed in an efficient manner in contrast to the nonlinear rules which include exclusive or operations  i.e.   u t         u t         . the tradeoff between shortterm memory and nonlinearity can be observed in multiple dynamical systems such as logistic maps  diffusion  equations and echo state network     . though there  is no theoretical proof about this tradeoff  it is possible  that ri may optimize the network under this tradeoff  constraint. thus  the short term memory of the network  that is optimized by ri may be superior to that of nonlinearity.  we clarify the relationships between the network optimized by ri and its information processing properties  under constrained system. the dynamics of the network  to which current ri can be applied is limited to stochastic dynamics under gaussian approximation. therefore   it is interesting to extend ri to other stochastic or deterministic dynamics of the networks and investigate the relationships between ri and information processing properties of rnn.    in this study  we optimized the input driven rnn using ri and evaluated the information processing capability of the network using the memory capacity and the  boolean function tasks. though naive ri did not improve the information processing capability  ri with input multiplicity improves the performances of the memory capacity and boolean function tasks because the connection weights of the input increase preferentially and  the input information is stored in the network. we tackled this problem by introducing input multiplicity. an  appropriate input multiplicity optimizes the network to  store the information about the past input  and the network partially acquires a delay line structure  which is re     this work was supported by mext kakenhi grant  numbers   h      and           jsps kakenhi  grant numbers   k         kt               and    kt      and jst presto grant numbers jpmjpr  e   japan  kakenhi no.   k       no.            and no.         .  