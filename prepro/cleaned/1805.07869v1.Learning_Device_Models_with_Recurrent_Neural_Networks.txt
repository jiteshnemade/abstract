introduction  in this paper we consider whether rnns can learn functionally equivalent models of unknown computer hardware peripherals through input output observation. peripheral devices  attach to a main computer and use both hardware within the  device and driver software running on the main computer to  perform a task  such as printing a page or sending a message.  however  there are instances when hardware is accessible from  the main system but driver software is not  rendering the  peripheral unusable. this situation is prevalent in open source  operating systems where driver software may not be available  from the vendor. without driver software or development  documentation  it is incumbent on the system s owner to write  software to make use of the peripheral. the device itself  is a  black box   with no information directly available to  the developer beyond a set of memory addresses to interact  with the device and the observable output of the hardware  itself. this leads to labor intensive reverse engineering efforts  with varying degrees of success  see e.g.     . ideally  future  adaptable systems should be able to automatically probe   observe  and develop models of unknown devices to either  inform the development process  or write interface software  directly. such solutions would also be useful in the areas of  evolutionary robotics     and hardware validation        .  traditional automated black box learning techniques which  learn exact models  such as l      or ttt      are prohibitively  expensive for this task since computer peripherals have large  input and output spaces  a large number of internal states   and require a complex series of commands to perform a  task. alternatively  recurrent neural networks  rnns  offer  an intriguing solution to peripheral device modelling as they  are able to learn approximate models without the compu     tational overhead required for traditional techniques. rnns  are versatile and powerful constructs that add memory to  traditional feed forward neural networks via backwards  or  loop  connections from output layers to previous layers. rnns  are capable of modeling turing machines    . recent advances  in machine learning hardware and software allow powerful   multi layer rnns to be trained efficiently.  the central scientific contribution of this paper is an empirical study of the effectiveness of using rnns to model  computer peripheral devices. we include a dataset of simple  machines that mimic real device behavior  section iv   and  then compare how well rnns model those machines. our  experiments show  section v  that rnns are capable of  learning functionally equivalent models of simple hardware  devices  and lay the groundwork for future adaptable systems.    ii. p roblem s tatement and a ssumptions  we approach the problem of learning device models as a  black box learning problem. each device d has the ability  to accept a sequence of input commands i  such as writes  to a command register or memory mapped addresses  which  in turn produce a sequence of observable outputs o  such as  data on a wire or lights on the device. our goal is to learn an  approximate functional model m of the device d such that  d   m      such that   is minimized. d   m is expressed  in this work as the observed functional difference  or loss   between d s response to input i and m  s response to the  same input.  we assume no knowledge of the inner workings of the  device being modeled. we assume that the learning algorithm  either has access to a set of observations of the device   or has the ability to generate such a set. we also assume  that the observed output sequence of a device is in some  way influenced by the input sequence  which we believe is  a reasonable assumption for most peripherals. finally  we  assume that the set of observations for the device contains  at least some characteristic traces that exercise a significant  portion of the device s capability. these assumptions will  not hold true for all potential scenarios  and indeed learning  complete models from black box systems is known to be  infeasible in the general case  but we assert they should hold  for a significantly large subset of target peripherals.     iii. r elated w ork  the first area of related work is the area of black box  automata learning techniques. black box automata learning has  two main approaches  active and passive learning. angluin      proposed the l  active learning algorithm which can infer a  mealy machine given the presence of an oracle who knows  the real state machine. variations of this algorithm are in use  today          and are applied to such areas as hardware and  software component testing      formal model verification        hardware reverse engineering      and network protocol inference     . recently  work has begun on learning register  automata  which allows a memory stack within the learned  automata       which may improve performance.  in passive learning  it is infeasible to actively query the  device under test so the learner is limited to the set of  input and output sequences previously gathered from the  device. gold      produced early work in this field  showing  that inferring a minimal moore machine from examples was  np complete     . passive learning algorithms descended  from gold s work include rpni       ostia       and  mooremi     .  automata learning approaches such as those above suffer  from their need to learn complete and accurate  though not  necessarily minimal  representations of the systems under test.  in general  the complexity of active learning grows linearly  with the number of inputs and quadratically with the number  of states     . thus  active learning is only tractable for small  problems  or large problems that can be broken down into  smaller problems ahead of time by an expert with domain  knowledge. passive learning in general is known to be npcomplete. the work in this paper aims to overcome these  limitations on black box algorithms by using rnns to learn  approximate models that are close enough to the real models  to be functionally equivalent.  two related areas of rnn research include using rnns or  rnn like structures to model computing systems  such as the  neural turing machine      and zaremba and sutskever       who train neural networks with memory to perform computation  and automata extraction from trained neural networks.  examples of extraction methods for feed forward networks  include fernn       deepred       and hypinv     .  more recent examples targeting rnns include murdoch and  szlam       and weiss  et al.     .    iv. e xperimental s etup  to test the ability of rnns to learn models from devices  we  created a set of simulated devices that perform actions which  mimic those of real hardware peripherals . the simulated  devices contain interesting state transitions that test how well  an rnn is able to learn complex concepts yet are simple  enough for manual verification of the results. these simulated  test devices are shown in figure  .    inputs    inputs    outputs    outputs    x              x              x              x              latch                   latch              latch              latch              latch              x              latch              x              latch                   latch               a  eightbitmachine  inputs    latch    x          b  singledirectmachine  outputs    inputs         x              x         x              x         x    x              latch                        latch  outputs         x              x         x              x         x              x         x               c  singleinvertmachine         xor    latch     d  simplexormachine  inputs    outputs    outputs    inputs         latch         latch         latch    parity   even odd     parity         command    baud  register         latch         latch         latch         latch         latch           uart    wordlen    data    stopbits     e  paritymachine     f  serialportmachine    fig.  . simulated machines of increasing complexity used for testing.  all inputs are latched  meaning they retain the latest written value for all  subsequent commands.    a. simple machines  we define a set of five simple artificial machines described  below in order of increasing complexity. in each machine the  inputs are latched  meaning that setting an input to a value of    or   can have an effect on future outputs depending on the  internal structure of the machine. in these machines  the input  value is stored in memory  and remains the same for future  commands in the sequence until explicitly changed.            eightbitmachine  a simple mapping of   inputs to    outputs.  singledirectmachine    inputs are ignored and one leads  directly to a single output.      future    work will target real hardware devices.     setting    values    description    word length  baud rate  stop bits  parity  tx data            bits  value between               .     bits  none odd even high low  value          size of the data to send  base wire clock rate  end of frame bits  meaning of parity bits  data to transmit    table i  r elevant rs     settings with descriptions                   singleinvertmachine  same as a above  but output is  inverted.  simplexormachine    inputs are ignored  and the remaining two are xor d together to produce the single  output.  paritymachine  the output is set to   if an odd number  of inputs are set  and   if an even number are set.    an evaluation of the magnitude of the state spaces of these  machines is available in appendix a. these simple models  provide an easily decomposable testing ground to test what  rnns can learn about a machine. success in modeling these  devices will provide confidence that the rnn will be able to  handle more complex systems in the future.  b.       uart  while the simple models detailed above test the basic ability  of rnns to learn discrete concepts used by devices  the        uart model shows a practical application of the rnn  approach to a real world peripheral. the       uart       is the component behind the modern pc serial port and  communicates using the rs     serial communications standard     . the device is controlled via a series of commands  written to     bit registers. these registers control the desired  communications parameters  notably baud rate  word length   parity  and the number of stop bits used when sending or  receiving data. the device then either listens for incoming  data  or the programmer writes data to the transmit register  which is transmitted over the serial bus using the configured  parameters. table i shows the set of possible values for each  output state used in our software model.  the       uart peripheral makes a good  complex use  case for model learning for several reasons                       relevance  the       uart is used in many systems  today because it is simple enough for even the most basic  operating systems to control.  hidden registers  the device has    internal registers  mapped to   register addresses. the learner must discover  how to access all registers before it can change some  states.  output interdependence  certain outputs can only be  observed if other outputs are set to specific values. for  example  a setting of  .  stop bits can only happen if the  word length is  .  diverse output types  some observable outputs can  take on one of a small set of values  but the baud rate  is a single value from a set of         possible values     fig.  . example command sequence with encoding for eightbitmachine. the  input and output for the sequence is encoded as a   dimensional floating point  array.         and the data transmitted over the wire is drawn from     values.  hidden mathematical formula  the baud rate is determined via an inherent mathematical formula of         divided by the concatenated value of two other registers.  the learner will have to discover this hidden constant to  accurately model the device.    many devices have traits similar to the above. our simulated        machine simulates register inputs and their meanings  at the bit level. it simulates data transmission only  read  commands are recognized but return no data  as there is no  simulated peer from which to receive data. while limited   this software model is sufficient to simulate the interesting  complexities of a uart detailed above. if a rnn can model  a       uart accurately then that is a good indication that  it can successfully model more complex devices.  c. generating observations  we generated a uniform random set of input sequences for  each simulated machine under test  ran those through each  machine and recorded the corresponding output set to create  a dataset of observations used to train the rnn. random  sampling of the input space is not ideal  but is sufficient for  this experiment as it represents a worst case scenario versus a  more intelligent sampling scheme.  for the simple machines  the input sequences consisted of  a tuple of a  set  or  clear  command  and the number of the  input to modify. the output is the vector of results from the  machine  with each output bit set to  .  or  .   representing if  the bit was set or cleared. see figure   for a detailed example  of a command sequence and its input and output encoding. for  the       uart  the input sequences are a tuple of command   register  and data  where command is either  read  or  write    register is the offset of the register to act upon  and data is  the   bit value to write  it is ignored for read commands . the  output of the machine is the state of each output specified in  table i  with parity  stop bits  and word length encoded as  one hot entries  baud rate encoded as a scaled floating point  value between  .  and  .   a single output flag that determines  whether data was transmitted at that time step  and if so  the  data transmitted encoded as   binary digits. see figure   for  an example encoding of a sequence.     machine      params    epochs      success    eval loss                                                                                                                   .         .         .         .         .         .          eightbitmachine  singledirectmachine  singleinvertmachine  simplexormachine  paritymachine  serialportmachine    fig.  . example command sequence with encoding for serialportmachine.  note that parity  word length  and stop bits are one hot encoded  data is binary  encoded  baud rate is linearly scaled between   and    and the transmit flag  is true false.    fig.  . neural network structure learning device models. the width of each  hidden layer is chosen to be larger than the maximum size of either the input  or output layer.    using the above encodings  we generated      training  sequences       validation sequences used during training   and a separate set of     sequences used for evaluation after  training is complete. each training  validation  and evaluation  sequence contains      commands.  d. recurrent neural network  our experimental rnns are multi layer neural networks  consisting of gru      cells. early experiments confirmed that  non recurrent networks were unable to learn models of these  systems. both gru and lstm      cells were considered   and while both were able to learn these models  networks  with gru cells trained faster and were more stable. the same  network structure is used for each test case to make sure  that the same method will work regardless of the underlying  device. the basic structure is shown in figure    with four  hidden layers and an activation function f after each gru  cell. each layer is fully connected to its neighbors.  each hidden layer has more cells than the maximum width  of either the input or the output layers. the experiments shown  here use the heuristic max iwidth   owidth       to determine  the number of recurrent cells in each hidden layer. this allows  the network to learn the model without artificial pressure to  compress its internal representation. this heuristic does create  over sized networks when there are large disparities between  the input and output widths which we speculate could lead to  overfitting and memorization  although that was not observed  in these experiments.  the number of hidden layers was determined experimentally. early tests showed that two hidden layers was sufficient  to learn models of all test machines except paritymachine.  expanding the network to four hidden recurrent layers allowed  this basic network structure to learn models for all test  machines considered.  v. r esults  we divide the experimental results into two sets. the first  set is the aggregated results of training a large number of    table ii  r esults of training    networks for each machine    networks for each test machine to verify that learning is taking  place. we compare each network s output to the validation  dataset s output  and verify that the difference between the  two  or validation loss  is both decreasing with training time   and achieves a small value in a reasonable amount of time.  the second set of results pushes even further and tests  whether  with continued training  a rnn can exactly model  the behavior of a real device at every time step. this requires  not only achieving a low validation loss  but also accurate  results when presented with previously unknown inputs.  a. successful learning  we trained    recursive networks with the training and  validation dataset sequences for each machine type using  keras      with the tensorflow      backend  for a total of      different network instances. the full set of parameters  used for training is covered in appendix b.  each network was trained until the validation loss for the  model dropped below  .     .     for more than    consecutive epochs  or a maximum of      epochs  whichever came  first. when training was complete  each network was evaluated  further by computing the difference between predicted and  expected output on the previously unseen evaluation dataset.  the average evaluation loss among all networks of a particular type is shown in table ii. a network is considered   successfully trained  if the loss on the evaluation dataset  is less than     a threshold chosen to show that significant  learning had occurred even if the network failed to achieve  the stopping criteria of  .  . experiments in the next section  will determine if either threshold is sufficient to accurately  mimic the underlying device s output. by this criteria  the  paritymachine type proved hardest to learn  with only     of  the test networks successfully trained  while all other machine  types achieved successful learning in     of instances or  higher.  figure   shows the average validation loss per epoch for  each machine type. this shows that most machine types  on  average  approach our terminal value of  .   validation loss  within the first     epochs  with the notable exception of paritymachine  which often takes hundreds or thousands of epochs  before terminating  if it learns at all. the serialportmachine  type in particular is much more complex than the others yet  still converges quickly  indicating rnns can model a large  subset of devices.  rnns are susceptible to unpredictability caused by random  weight initialization. despite attempts to control all other  variables  we still observe variability in the success rate and     machine   .      singleinvertmachine  singledirectmachine  paritymachine  serialportmachine  simplexormachine  eightbitmachine     .    validation loss     .     .     .      eightbitmachine  singledirectmachine  singleinvertmachine  paritymachine  simplexormachine  serialportmachine      outputs    epochs    epochs     accuracy                                                                                 n a                                                       .             n a    table iii  m aximum evaluation accuracy with output mapping .     .     .     .                       epochs                    fig.  . average validation loss per epoch using only the networks that  successfully trained for each machine type.    validation loss     .     .     .     .     .                                         epoch                            fig.  . validation loss for each of    trained eightbitmachine networks    learning performance of each individual network. for example   figure   shows the complete set of successful validation loss  performances for each of the    trained networks for the  eightbitmachine type. as shown  the majority of networks  started with a loss between     and      and achieved less  than  .   loss by epoch    . there are three outliers  one  which starts off much better and finishes in less than     epochs  one that gets stuck around     error until epoch       and one that gets stuck at     error  and then fluctuates wildly  until it finally converges near epoch     . similar patterns reoccur for each machine type. this indicates it is necessary to  train multiple instances in parallel to guarantee good learning  performance.  b. real world effectiveness  having shown that learning is possible for each machine  type  the next set of results explore how accurately the learned  rnn model can mimic the behavior of the original device.  this is different than calculating the global loss over the  output sequence because the network is returning an encoded  floating point representations of predictions on each output  using the encoding scheme described in sections iv a and  iv b. the predicted output sequences for each network need to  be converted back to their original output values  via rounding     in most cases  to properly mimic the output of the original  machine.  to evaluate model effectiveness  we choose one of the  successful rnns from the previous set of results at random   and continue training it until it is capable of accurately  modelling the evaluation dataset for the underlying machine.  this means that every output  at every time step  must be  identical to that of the original machine. the results of this  experiment are shown in table iii. the number of outputs  for each machine indicates the total number of outputs the  network must correctly predict for     evaluation instances   which is output vector width   sequence length           number of instances      . the number of extra epochs of  training required to hit the highpoint of accuracy is shown  under  epochs    while  epochs  is the total number of  epochs required to achieve the result. training halted when the  network achieved complete accuracy on the evaluation dataset  for    consecutive epochs.  these results clearly show that a low validation loss in  training does not always translate into perfect real world  performance. while it is true that the singledirectmachine  did not require any more training to achieve perfect evaluation accuracy  every other machine required more epochs to  approach that goal. two models were unable to achieve       accuracy. the paritymachine type was able to correctly predict  all but one output correctly with      additional epochs  but  was unable to move beyond that value despite letting the  experiment run for several thousand more epochs. given the  observed variability in learning this particular machine  we  expect a      accurate paritymachine model is possible.  the serialportmachine type was also unable to achieve  perfect mimicry by continuing training on this test  despite  having a global error loss approaching          . this model  is complex  with    values in its output vector representing    different values. we continued to train the serialportmachine  instance for over       extra instances  but failed to achieved  perfect accuracy on the evaluation dataset. table iv shows  the results of a simple  hello world   command sequence at  three different settings for the network at approximately        epochs. we can observe that while the word length  parity   and stop bits settings are correctly predicted  the baud rate  and data outputs are far less accurate.  to determine why  we first examine which outputs are  contributing the most to the error. figure   shows validation  loss heatmaps of each output at different stages of training.  each cell represents the loss of a particular output value  x              n         e         o     baudrate    wordlen    parity    sbits                                    none  even  odd               output  haxxo wofxp   haxxo wofxp   haxxo wofxp     table iv  e xample  h ello w orld    output after         epochs   single  network    output    encoding    output size    epochs    val. loss    parity  word length  stop bits  baud rate  tx  data    one hot  one hot  one hot  float  true false  binary                                                          .           .           .           .           .           .            command in sequence    target          .             .             .             .             .           parity    c. decomposed model  we speculate a further contributing negative factor to       accuracy with the serialportmachine is that the value the  network is optimizing for is the global loss over that instance.  the serialportmachine type has                   outputs  per instance  nearly three times as large as any other tested  machine. with this large output space  even small amounts of  noise or error from each cell mask where the network needs  to minimize the real error. to test this theory  we created a  new model made up of multiple neural networks  one for each  output type  parity  word length  stop bits  baud rate  tx   and data. we call this a decomposed model  as it decomposes  the problem into smaller networks. intuitively  if the network  only has to optimize for one output type it may be able to  learn faster  as it is optimizing over fewer outputs.  we are careful to use the same network structure as when  training with all outputs in one network  and only change    stopbits b r tx    data     .      a     epochs      global loss    command in sequence    table v  c haracteristics of decomposed model networks for uart          .             .             .             .             .           parity    wordlen    stopbits b r tx    data     .      b      epochs   .   global loss    command in sequence    axis  for each command in the      command evaluation sequence  y axis . darker colors mean the value at that output is  closer to the correct value within the sequence. we show three  maps  one towards the beginning of training  and two more as  training continues. we note the evolution of the training over  time  and observe the overall loss approaching  . . but while  the parity  word length  and stop bits outputs significantly  decreased their loss over time  the data transmitted and baud  rate are the most difficult for the network to learn.  we speculate this is for a few reasons. first  the baud rate  can take any one of     possible values  and is encoded as a  single    bit floating point number. thus  the network must  predict the value to within  .         .         to be correct.  this may be just too difficult for the network to do accurately.  future work will use a different encoding for this value.  second  due to randomly generating commands for training  data  the number of commands in a sequence that actually  transmit data is small compared to the number of commands  overall. thus  we speculate that learning the data outputs is  hard due to a low proportion of data transmit commands in  the command sequence.    wordlen          .             .             .             .             .           parity    wordlen    stopbits b r tx    data     .      c       epochs  .    global loss  fig.  . local error sources contributing to global error for an example  evaluation sequence of      commands issued to a successful uart model.  target          n         e         o     baudrate    wordlen    parity    sbits                                    none  even  odd               output  h llo wop d   hello w rld   hello w rld     table vi  e xample  h ello w orld    output  multiple networks    the width of the output layer. the       uart outputs  are not always independent  so the network must model all  possible states even if it is only predicting one. therefore it  is important to keep the same hidden layer widths from the  non decomposed network to avoid state compression pressure  when training the decomposed models.  metadata about these networks is shown in table v  in      cluding how long each took to reach the stopping criteria and  the validation loss at that time. the result of the example   hello world  from before is shown in table vi. while still  not able to achieve      accuracy  the decomposed model is  significantly better at predicting the final data transmitted  and  trains in much less time than training the entire model at once.  vi. d iscussion  these results show that recurrent neural networks can be  trained to learn information about the inner workings of black  box devices. for the simple test machines  the results are  very accurate  and while the uart model was not able to  precisely learn all outputs  it was able to accurately model  several components of the system and make progress towards  the remainder. we are confident that perfect accuracy on all  models is achievable.  it is important to note that the structure of the networks used  for each machine type is identical. with expert knowledge  one  could create network structures tailored for a specific target  machine  but this generic structure shows that modeling can  be successful without specific knowledge of the underlying  machine. thus  this method is applicable to a wide range of  use cases.  these results suggest that minor tweaks can be made to the  methods presented here to achieve even more accurate results  in the future. while true  the experimental results here are  sufficient to support the notion that rnns are useful tools for  modeling systems like computer peripherals.  furthermore  compared to traditional black box methods  like l   rnns are able to model more complex systems.  state of the art black box systems can only accurately model  devices with a few hundred unique internal states. while our  simple test machines have up to    possible internal states  real  devices like the       uart has on the order of     possible  internal states  see appendix a. such systems are simply too  complex to learn with traditional black box algorithms.  the observed difficulty in learning a larger  complex systems like the uart suggests that global output loss may not be  the optimal parameter to optimise for  as the larger the number  of outputs and the longer the sequence  the less impact each  individual output has on that value. our decomposed model is  able to overcome this issue for our test case  delivering better  results within a fraction of the time.  finally  we note that the observed validation loss highlights  the difficulty of knowing when to terminate training. some  networks had long periods of little to no improvement in  validation loss  only to suddenly learn the model hundred or  even thousands of epochs later.  vii. f uture w ork  we plan to expand testing to include peripheral devices  which utilize large memory maps  such as vga text mode   or dma and interrupts  such as a simple network card. the  uart model is being improved to generate and interpret raw  rs     waveforms to infer settings and data  as opposed to the  current software model which simply supplies that information    at each time step. this leads to interesting learning challenges  as the same waveform can map to multiple meanings  introducing ambiguity in the training data. this will also allow the  learned models to interact with physical uart devices.  further research is needed into output encodings and their  impact on learning. the baud rate  encoded as floating point   appeared to be the hardest for the networks to learn  so  different encoding techniques will be explored to quantify  the practical limits on regression accuracy when predicting  floating point values.  future work will also focus on efficient methods to extract  the learned automata from the neural network. this will  allow automated documentation and explanation of unknown  peripherals to the user.  viii. c onclusions  this paper shows empirically that rnns are capable of  modeling even complex real world devices accurately using  single  generic network structure. in addition  we introduced  a sample test machine dataset useful for evaluating other  techniques for modeling peripheral devices. with time  we  hope the technique can be improved and combined with  automata extraction to gain unprecedented insight into the  inner workings of unknown peripherals.  acknowledgements  the author would like to thank dr. tim oates for helping  develop the ideas in this paper  and dr. vincent weaver for  providing computing resources for these experiments.  