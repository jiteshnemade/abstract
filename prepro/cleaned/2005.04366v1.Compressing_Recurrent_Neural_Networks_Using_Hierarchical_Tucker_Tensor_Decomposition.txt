introduction    recurrent neural networks  rnns   especially their advanced variants such as long short term memory  lstm   and gated recurrent unit  gru   have achieved unprecedented success in sequence analysis and processing. thanks  to their powerful capability of capturing and modeling the  temporary dependency and correlation in the sequential data   the state of the art rnns have been widely deployed in many  important artificial intelligence  ai  fields  such as natural  language processing  nlp   sutskever et al.         speech  recognition  mikolov et al.         and computer vision  yu  et al.       .  despite their current prosperity  the efficient deployment of  rnns is still facing several challenges  especially the large    model size problem. due to the widespread existence of  high dimensional input data in many applications  e.g. nlp  and video processing  the input to hidden weight matrices of  rnns are often extremely large. for instance  as pointed  out in  yang et al.         even with small size hidden layer  such as     hidden states  an lstm working on ucf    video recognition dataset  liu et al.        already requires  more than    million parameters. such ultra high model size   consequently  brings a series of deployment challenges for  rnns  including but not limited to high difficulty of training  susceptibility to overfitting  long processing latency and  inefficient energy consumption etc.  prior work on rnn compression. to address rnns   ultra large model size problem  several model compression  approaches  such as pruning and quantization  song et al.          have been proposed and studied in prior work. among  them  the most promising solution is tensor decomposition   a technique that represents a large tensor with the combination of multiple small tensor cores. by its nature  tensor decomposition approach is inherently a powerful tool  for identifying and exploring the higher order data correlation. from the perspective of model compression  such strong  correlation capturing capability makes tensor decomposition  very attractive and well suited for exploiting and reducing the  model redundancy in large scale rnns. recent advances in  model compression research already show that  various tensor  decomposition based compression methods  including tensor  train  tt   yang et al.         tensor ring  tr   pan et al.         and block term  bt   ye et al.         can bring several  orders of magnitude fewer parameters for large size rnns  with still maintaining high classification prediction performance.  limitations of prior work. although the existing tensor  decomposition based rnn compression approaches already  show their promising potentials  these state of the art methods are still facing two inherent limitations     the tensor decomposition approaches used in  yang et al.          ye et  al.        and  pan et al.        have strict constraints on  either the shapes or the combination manners of the component tensor cores  thereby limiting the representation ability  of the corresponding compressed rnn models. for instance   tt decomposition requires the border tensor cores have to  be rank    which directly hinders the representation power  of tt based rnns. more generally  when using tt  tr or       ht based rnns exhibit stronger representation power  than the existing tensor decomposition based models.  more specifically  the hierarchical structure imposed on  the input to hidden layers makes rnns can exploit and  extract the important representation and pattern from  high dimensional data in a much more hierarchical  precise and comprehensive way  thereby significantly improving rnn models  representation capability. this  benefit on representation capability is also verified by  the empirical experiments. our proposed ht lstm  as  the the compressed lstm models using ht decomposition  achieves higher accuracy than vanilla rnn  the  state of the art compressed rnns as well as non rnn  models on various video recognition datasets.    ht based rnn models have much lower storage and  computational costs than the state of the art. compared  with tt  tr and bt decomposition adopted in prior  work  ht decomposition inherently provides higher  complexity reduction on the same size tensor data with  the same selected rank. by leveraging such theoretical advantage  we can compress large size rnn models in the ht format with requiring very few parame     root    node     non leaf    frame   transfer  tensor     leaf    bt  the important hierarchical structure  which is important to  capture many inherent hierarchical patterns or representation  in the data  is missing at the inter tensor core level  thereby  limiting the representation capability of the entire neural network models  and    from the perspective of complexity  analysis  tt  tr and bt are not the best tensor decomposition approaches that provide the most promising space or  computational complexity reduction. for instance  executing bt based rnn models suffers computation overhead due  to the extra flatten and permutation operations. more generally  when we consider to apply tensor decomposition to compress rnn models  in many settings the state of the art tt   tr and bt solutions are inferior to some other types of tensor decomposition option in terms of numbers of parameters  and or operations saving. consequently  the existing tensor  decomposition based compression methods are not the optimal solutions to fully exploit and minimize the rnn model  redundancy.  technical preview   benefits. to overcome these limitations and fully unlock the potentials of tensor decomposition  in model compression  in this work we propose to develop  compact rnn models by using hierarchical tucker  ht  decomposition  hackbusch and ku hn         a little explored  but powerful tool for capturing and modeling the correlation  and structure in the high dimensional data. unlike other popularly used tensor decomposition methods such as tt  tr  and bt  ht decomposition enables the decomposed tensors  exhibit strong hierarchical structure  which is very useful and  important for enhancing the representation capability of rnn  models. meanwhile  comparing to its well explored counterparts  ht decomposition can bring more complexity reduction with the same rank setting  thereby enabling ht based  rnn models have lower storage and computational costs  than the prior work with the same or even higher accuracy.  in overall  the features and benefits of ht based rnn models are summarized as follows     figure    an illustration of ht decomposition with d  . all  the dashed lines and boxes describe a binary tree with root d    t          u  where the dashed boxes represent the nodes. here node  t u is a leaf node  whose father and brother are node t    u and  node t u  respectively. for leaf nodes  they are only associated with  leaf frame  and for non leaf nodes  they are associated with transfer tensors and non leaf frames. here x is decomposed to a set of  orange colored transfer tensors and blue colored leaf frames.    ters. experimental results show  compared with vanilla  lstm  ht lstm achieves very high compression ratio with even higher accuracy. meanwhile  compared  with the state of the art compressed lstm such as ttlstm  tr lstm and bt lstm  ht lstm consistently achieve simultaneous and significant increase in  both compression ratio and test accuracy on different  datasets.         hierarchical tucker based rnn models    in this section  we describe the details of ht based rnn  models. first  we introduce the preliminaries of tensor basics  tensor computation and hierarchical tucker decomposition. then  we reformulate the forward propagation and backward propagation procedure on the original input to hidden  layer to the ht format  and thereby forming a new ht layer   which is the building component of our proposed compact  ht based rnn models. furthermore  in order to evaluate  the efficiency of ht based compression approach  we analyze the computational and storage complexity of ht based  rnn models and make comparison with the other methods.     .     preliminaries    notation. throughout the paper we use boldface calligraphic  script letters  boldface capital letters  and boldface lower case  letters to represent tensors  matrices  and vectors  respectively  e.g. x p rn   n       nd   x p rn   n    and x p rn  .  in addition  x pi        id q p r denotes the entry of tensor x .  similarly  xpi jq represents the entry of matrix x.  tensor contraction. an ht decomposed tensor is essentially the consecutive product of multiple tensor contraction  results  where tensor contraction is executed between two  tensors with at least one matched dimension. for instance   given two tensors a p rn   n   l and b p rl m   m     where the  rd dimension of a matches the  st dimension     contraction    element                        matrix              vector    tensor   a      b     figure     a  graphical representation of element and computation in the tensor diagram. here the each line represents one dimension  and  the variable near the line is the size of that dimension.  b  representation of matrix vector multiplication on a ht decomposed layer using  tensor diagram. here weight matrix is already decomposed to the ht format with a set of   d leaf frames and   d transfer tensor.    of b with length l  the tensor contraction result is a sizen    n    m    m  tensor as  pa     bqpi   i   j   j  q      l       api   i    q bp  j   j  q .           hierarchical tucker decomposition. the hierarchical  tucker decomposition is a special type of tensor decomposition approach with hierarchical levels with respect to  the order of the tensor. as illustrated in figure    an htdecomposed tensor can be recursively decomposed into intermediate components  referred as frames  from top to bottom in a binary tree  where each frame corresponds to a  unique node  and each node is associated with a dimension  set. in general  for a ht decomposed tensor x p rn       nd    we can build a binary tree with a root node associated with  d   t              du and x   u d as the root frame. for each  non leaf frame u s p rrs  n s      n s   where s   d is associated with the node corresponding to u s   s    s    s are associated with the left and right child nodes of the s associated  node  and  s   minpsq   s   maxpsq  u s can be recursively  decomposed to its left and right child frames  u s  and u s     and transfer tensor g s p rrs  rs   rs  as  u s   g s     u s      u s  .           consequently  by performing this recursive decomposition  till the bottom of the binary tree  we can decompose the original n            nd  order tensor x   u d into the combination of the   order leaf frames and   order transfer tensors.  notice that here rs   as hierarchical rank  is an important parameter that determines the decomposition effect.     .     ht based rnn models    this subsection describes the details of compressing the  large size rnn models to the compact ones using ht decomposition  including the key steps as well as the important  ht based forward and backward propagation schemes.  tensorization. in general  the key idea of building htrnn is to transform the weight matrix w p rm  n to the  ht based format. considering w is a   d matrix  while  ht decomposition is mainly performed on high order tensor  we first need to reshape w as well as its affiliated input    vector x p rn and output vector y p rm to tensor for       nd  mat as w p rm       md  n       nd   x p rn   and  d  m       md  y p r    respectively  where m   i   mi and   d  n   j   nj .  decomposing w . given a tensorized w as w p  rm       md  n       nd   we can now leverage ht decomposition to represent the large size w using a set of small size  matrices and tensors. in general  following equation      w  can be decomposed as  w pi        id  j        jd q      d  rd   rd r           pg d qpk p qq  k   p   q              pu d  qpp  d  pi jqq pu d  qpq  d  pi jqq    where  s pi  jq is a mapping function that produces the correct indices i   pi            id q and j   pj            jd q for a specified frame u s with the given s and d. for instance  with  d     and s   t    u  the output of  s pi  jq is pi    i    j    j  q.  in addition  u d  and u d  can be recursively computed as  r s   r s            pu s qpk  s pi jqq      pg s qpk p qq  p   q              pu s  qpp  s  pi jqq pu s  qpq  s  pi jqq    where d   t              du  d    t           td  uu and d     trd  s          du are associated with left and right child nodes  of the root node.  ht layer. with the ht decomposed weight matrix  the  component ht layer of ht based rnn models can now be  developed. specifically  the ht format matrix vector multiplication  as the kernel computation in the forward propagation procedure  is performed as follows   y piq      d  rd   rd r            pg d qpk p qq  j k   p   q              pu d  qpp  d  pi jqq pu d  qpq  d  pi jqq x pjq .  considering the desired output y of ht layer is a vector   the calculated y needs to be re shaped again to the   d format. consequently  we denote the entire forward computing     model  rnn fp  rnn bp  tt rnn fp  tt rnn bp  tr rnn fp  tr rnn bp  bt rnn fp  bt rnn bp  ht rnn fp  ht rnn bp    space    time    opn m q  opn m q  opdmr  n q  opdmnr  q  opd  mr  n q  opdr  n   dr  m q  opdmnr  q  opd  r  n   nd  r  m q  opdmrd n cq  opdmnr   rd q  opd  mrd n cq  opdmr  n   dr  n q  opdmnr   dr  q  opd  mr  n   d  r  n q  opn m q    table    comparison of complexity with different tensor  decomposition based rnns. here fp and bp mean forward and  backward propagation  respectively. c is the cp rank value defined  in bt decomposition  and r   maxs d rs   m   maxkpd mk    n   maxkpd nk      propagation should also be accordingly reformulated to htbased format. in general  considering for ht layer w   u d  by    x   assuming s is associated with a left node  as  and bu  d  we denote that f psq and bpsq are the sets associated with the  father and brother nodes of the s associated node in the binary  tree  respectively  and define  s   minpsq   s   maxpsq   the partial derivative of output tensor with respect to frames  can be calculated in the following recursive way until f psq is  equal to d   by   g f psq     u bpsq  bu s                 urts   pht lpwu   xrtsq   vu hrt    s   bu q  f rts   pht lpwf   xrtsq   vf hrt    s   bf q  orts   pht lpwo   xrtsq   vo hrt    s   bo q  crts  f rts d crt    s   urtsd  tanhpht lpwc   xrtsq   vc hrt    s   bc q  hrts  orts d tanhpcrtsq            where    tanh and d are the sigmoid function  hyperbolic  function and element wise product  respectively.  ht based gradient calculation. to ensure the valid  training on ht rnn  the gradient calculation in the backward    by  .  bu f psq           based on equation      the gradients for leaf frames and transfer tensors can be computed as follows   bl  by  s   s         d    bl              s     s         d     bu s bu s  by  bl  y          s    s            us  bg s bu s         s    s                 remark    hierarchical structure in ht layer. in tensor theory both the tensors and their computations can be  graphically represented using tensor diagram  see figure    a  . for ht decomposition  its inherent hierarchical characteristics make the corresponding ht layer exhibit strong  multi level hierarchical structure  which is visualized in the  specific tensor diagram of ht layer  see figure   b  . considering that a well known feature and advantage of deep  neural network is its strong ability of capturing hierarchical  pattern and representation via its multi layer structure  the  existence of such hierarchical structure in ht layer can effectively improve the representation capability of the overall  neural network models. in section    empirical experiments  on different datasets demonstrate that ht based rnn models  indeed outperform many other types of rnn models in terms  of accuracy. besides  it is worth noting that some recent advances in learning theory  nadav et al.        also indicates  the strong connection and correlation between ht modeling  and expressive power of deep neural networks.  ht lstm. with the ht based component layers  a htbased rnn model can be simply constructed by replacing the  original uncompressed layer with the ht layer. considering  lstm is the most popular and advanced variant of rnns  we  develop the corresponding ht lstm model as follows                 f psq   f psq   bpsq   bpsq       procedure from input x to output y as  y   ht lpw   xq.           bpsq           bpsqbpsq    bpsq     f psq   f psq                                 d             ss    ss     u s                 d        .                 bl  .  by           complexity analysis    to better understand the impact of ht decomposition on  rnns  we analyze the theoretical complexity of ht rnn   and compare it with the vanilla uncompressed rnn as well as  other tensor decomposition based rnn models. table   summarizes the space complexity and time complexity of different rnn models. it is seen that compared with the other compressed rnn models using tensor decomposition  ht rnn  has the lowest space complexity to store the model parameters. meanwhile  ht rnn also enjoys less time complexity  than most listed models for both forward and backward propagation. it is worth noting that though tt rnn has even less  time complexity than ht rnn  it has weaker representation  capability  which translates to the lower accuracy  as demonstrated via the experimental results in section  .  remark  . besides theoretical complexity analysis  we  also verify the low cost benefits of ht based compression  via empirical experiments. figure   a  shows the number of  parameters to store a compressed weight matrix  and figure    b  shows the number of needed operations for multiplication between the compressed weight matrix and vector. here  we adopt the size               weight matrix used in  yang  et al.         ye et al.         pan et al.        for evaluation. from figure   it is seen that ht based approach indeed  achieves lower costs  especially on storage requirement  than  other tensor decomposition based methods. next  the experiments in section   further show the advantages of ht based  rnn on compression ratios over various datasets.         experiments    in this section  we evaluate the performance of ht based  rnn on different datasets  and compare them with the stateof the art with respect to compression ratio and test accuracy.     figure    top  comparison on number of required parameters of  weight matrix. bottom  comparison on number of required operations of matrix vector multiplications. all the tensor decomposition methods use the same setting d      pn            n  q    p               q  pm            m  q   p             q  and r is the rank.    considering lstm is the current most commonly used rnn  variant in both academia and industry  our experiments focus  on ht lstm  and compare it with the vanilla uncompressed  lstm and recent advances in compressed rnn such as ttlstm  bt lstm and tr lstm. besides  we also compare  ht lstm with other reported models that can be evaluated  on the same datasets.  training strategy. following the similar setting in prior  work  we adopt two types of training strategy  end to end  direct training and training with pre trained cnn. in the endto end direct training the input of lstm is the raw data  e.g.  video clips  while training with pre trained cnns means the  back end lstm receives the compact features extracted by  a front end pre trained cnn. next we describe our experiments belonging to these two categories  respectively.     .     end to end direct training    hyperparameter setting. we train the models using adam  optimizer with l  regularization of coefficient  .   . also   dropout rate is set as  .   and batch size is   .  ucf   dataset. the ucf   dataset  liu et al.         consists of    class human action  e.g. biking  diving  basketball  videos with totally       video clips. each class is  assembled by    video groups  where each group contains at  least   action clips. for each clip  the resolution is          .  at data pre processing stage we choose the same settings  used in the related work  ye et al.         pan et al.         for fair comparison. specifically  the resolution of video clips  is first scaled down to            and then   frames from  each clip are randomly sampled to form the sequential input  for our ht lstm model.  for the baseline vanilla uncompressed lstm model  it  contains   input to hidden layers  where the size of its input  vector is                          and the number of hidden  states in each layer is    . for our proposed ht lstm  the  input vector is reshaped to a tensor of shape               and the output tensor of the input to hidden layer is of shape                   . all leaf ranks are set as    and all non leaf  ranks are set as  .    model    cr      param.    accuracy        lstm  tt lstm   icml      bt lstm   cvpr      tr lstm   aaai      ht lstm   ours            m      .                           .                           .                           .                           .     table    performance of different rnn compression work on  ucf   dataset using end to end direct training. cr stands for compression ratios. results of tt lstm  bt lstm and tr lstm are  reported in  yang et al.          ye et al.        and  pan et al.          respectively.    table   summarizes the performance of our ht lstm on  ucf   dataset and compare it with the related work. it is  seen that compared with vanilla lstm using    million parameters  ht lstm only needs         fewer parameters  with   .   accuracy increase. compared with the recent  advances on compressing rnns using other tensor decomposition methods  including tt lstm  bt lstm and trlstm  our proposed ht lstm requires at least  .    fewer  parameters with at least  .   increase in accuracy.  youtube celebrities face dataset. youtube dataset  kim  et al.        contains       video clips from    subjects   where each of it is a celebrated individual such as movie  star. also  the resolutions of the frames vary for different  video clips. being consistent with prior work  for data preprocessing the resolution of the input data to ht lstm is  re scaled as          . also    frames in each video clips are  randomly sampled to form the input sequence.  in this experiment we build a ht lstm with the similar  setting with the one used in ucf   dataset. to be specific     input to hidden layers are equipped  and the shapes of tensorized input and output vectors are                     and                     respectively. a slight difference is in this  ht lstm all leaf ranks are set as    and all non leaf ranks  are set as  .  table   compares the performance of ht lstm with  prior work on youtube celebrities face dataset. considering among the state of the art work  only tt rnn  yang et  al.         including tt gru and tt lstm  reports both the  accuracy and compression ratio on this dataset  the comparison in table   is mainly between ht lstm and tt based  solutions. from this table it is seen that ht lstm achieves          compression ratio over the original uncompressed  lstm with much higher accuracy. compared with the existing high compression model tt gru  ht lstm has  .    fewer parameters but offering  .   accuracy increase.  besides  on the same youtube dataset we also compare htlstm with several other reported works without using tensor  decomposition method. as shown in table    among those  works the state of the art model is  ortiz et al.         which  has the highest reported accuracy    .   . compared with  that model  ht lstm achieves  .   higher test accuracy.     model    cr      param.    accuracy        lstm  tt gru  tt lstm  ht lstm  ours                                        m                         .     .     .     .     model   wang et al.          sharma et al.          cho et al.          gammulle et al.         cnn   lstm  pan et al.         cnn   tr lstm  pan et al.         cnn   ht lstm  ours     table    performance of different rnn compression work on  youtube celebrities face dataset using end to end direct training. cr  stands for compression ratios. results of tt lstm and tt gru  are reported in  yang et al.       .    model    accuracy         kim et al.          harandi et al.          ortiz et al.          lu et al.         ht lstm  ours       .     .     .     .     .     table    comparison between ht lstm using end to end direct  training and other models without using tensor decomposition on  youtube celebrities face dataset.     .     training with pre trained cnns    another set of our experiments is based on training strategy  using pre trained cnns. to be specific  the pre trained cnn  first extracts the useful and compact features from the largesize raw input data  and then sends those captured features  to rnn  as indicated in  donahue et al.         using the  front end cnn can not only reduce the required input vector  size of rnn  but can also significantly improve the overall  performance of the entire cnn rnn model.  hyperparameter setting. in this part of experiments  dropout rate is set as  . . the l  regularization with coefficient  .     is used  and the entire ht lstm model is  trained using adam optimizer with batch size   .  ucf   dataset. consistent with  pan et al.          inception v  is selected as the the front end cnn feature extractor  whose output is a flattened size       feature vector.  for our proposed ht lstm  this feature vector  as the input  to the model  is reshaped to a tensor of size              .  similarly  the output vector is reshaped to a tensor of size               . in addition  the ranks for all the nodes are set  as  .  table   summarizes the test accuracy of different models  over ucf   dataset. it is seen that with pre trained cnn  model as front end feature extractor  ht lstm achieves    .   accuracy  which is  .   higher than the best reported  accuracy from the state of the art. compared with the trlstm using the same front end cnn  ht lstm achieves   .   accuracy increase. meanwhile  being compressed from  the same vanilla lstm  ht lstm model brings very high  compression ratio as         while the compression ratio of  tr lstm is only   .  hmdb   dataset. hmdb   dataset  kuehne et al.         contains       video clips that belong to    action categories  where each of them consists of more than     clips.  again  for the experiment on this dataset we use inceptionv  as the pre trained cnn model  whose extracted feature is  flattened to a length       vector. reshaped from this vector     accuracy        .     .     .     .     .     .     .     table    comparison between ht lstm using front end pretrained cnn and other related work on ucf   dataset.    the input tensor has size of              . we also reshape the  output vector to a tensor of size              . meanwhile   the ranks of all the nodes are set as  .  table   summarizes the performance of cnn aided htlstm and other related work on this dataset. it is seen that  ht lstm achieves   .   accuracy  which obtains  .   increase than the recent tr lstm. meanwhile  the compression ratio brought by ht lstm is         which is much  higher than the     parameter reduction in tr lstm.  it is worth noting that the state of the art work  carreira  and zisserman        achieves a higher accuracy    .     than ht lstm. this performance is based on using two  streams of input  rgb images and optical flow   while htlstm does not utilize optical flow information of the video.  when only rgb information of the video is sent to the models  ht lstm can achieve very competitive classification  performance as compared to the prior work.  model   wang et al.          feichtenhofer et al.          carreira and zisserman         cnn   lstm  pan et al.         cnn   tr lstm  pan et al.         cnn   ht lstm  ours     accuracy        .     .   rgb   flow    .   rgb    .     .     .     .     table    comparison between ht lstm using front end pretrained cnn and other related work on hmdb   dataset.         conclusion    in this paper  we propose a new rnn compression approach  using hierarchical tucker  ht  decomposition. the htbased rnn models exhibit strong hierarchical structure as  well as low storage and computational costs. our experiments  on different datasets show that  our proposed ht lstm models significantly outperform the state of the art compressed  rnn models in terms of both compression ratio and test accuracy.     