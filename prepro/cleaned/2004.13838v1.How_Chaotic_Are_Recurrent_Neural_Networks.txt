introduction    recurrent neural networks  rnns   e.g.  vanilla rnn and long short term memory  lstm   hochreiter   schmidhuber         are important architectures for sequential data processing. in  natural language processing  nlp   for example  an rnn is used to not only learn the representation of text but also generate a sentence in a certain task  ilya sutskever       .  an rnn works in the following way  the rnn keeps a fixed dimensional hidden state. at each  time step  it reads in an input signal  updates its state accordingly  and makes a prediction if needed.  we are particularly interested in rnns applied to text generation  where at each step  the previously  generated word is fed as input and the rnn predicts the next word.  mathematically speaking  such rnns are actually an iterative map  or a map  strogatz         from  the space of the hidden state to itself. this means that the hidden state is iteratively updated at every  step  based on the previous hidden state  by the rnn transition.  it is noted that  for a typical rnn like vanilla transition and lstm  the map is non linear  and   a potential known problem of non linear iterative maps is its chaotic behavior  t. alligood et al.        . usually  a chaotic map is not periodic in the limit of the map being iterated for infinitely  many times. also  a chaotic map prevents an accurate prediction of its state for future steps  in  which case we say that the map is attracted to an strange attractor.  analyzing the chaotic behavior of an rnn becomes a fundamental scientific question for its applications  as chaos behavior might affect how stable an rnn is or how well the rnn performs. there  have been a few studies on the chaotic behavior of rnns.  both vanilla and lstm rnns are shown to be chaotic with certain parameters  bertschinger    natschlager        laurent   von brecht         which are usually assigned by humans in a low dimensional  e.g.   d  hidden state. based on such results  a few regularization methods are proposed  to make rnns less chaotic  laurent   von brecht        chang et al.       . for simplicity  such  studies also ignore the input to the rnn at every step.  in this paper  we analyze the chaos of rnns  vanilla transition and lstm  in a more realistic setting  in a practical text generation task. we show empirically that along the entire training process from  random weights to a well trained model the rnn does not exhibit chaotic behavior. moreover  we  experimented the settings of with and without input  empirical results show that  if an rnn is not  fed with input  its hidden state is almost always attracted to a single fixed point.        iclr      workshop deepdiffeq    our unexpected results also imply that  for future work  we should adopt a realistic setting to understand the typical behavior of an rnn in real world applications.         f ormulation    an iterative map  or a map  is a function f   s   s from one space to the same space. an rnn can  be formulated as ht   rnn ht     xt    where ht is the hidden state for the tth step  h  being the  initial hidden state  and xt is the input. in text generation  for example  we feed in the previously  generated word as input  which is usually the most probable word  i.e.  xt   arg max p xt    ht    .  this means that such rnn is an iterative map because ht   rnn ht     arg max p   ht     . in  previous work where researchers ignore input of rnn  we have ht   rnn ht         indicating  that in this setting the rnn is also an iterative map. these are two scenarios we would explore in  this paper  and we refer to them as  with input  and  without input   respectively.   an important notion in the study of iterative maps is the orbit  which is the sequence of function  values of a map  given an initial input. in the context of an rnn with a given initial hidden state h     the orbit is  h    h    h           .  an orbit may exhibit different behaviors. if f k  p    p  where f k means f           f for k times  we  say the orbit containing p is periodic with a period of k  or a period k orbit. especially  if k       the point p is called a sink. if an orbit  h    h    ....  ht   gets closer to a periodic orbit as t      we  say the orbit is asymptotically periodic.  an orbit may also be chaotic. while different researchers do not agree with a common definition  strogatz        t. alligood et al.         a chaotic orbit usually means that the orbit   h    h            is sensitive to h    and  h    h            must not be asymptotically period.  in text generation in our experiments  the hidden state of an rnn is usually a high dimensional  space  and it may be difficult to detect periodicity based on rnn s hidden state. we instead  approximate it by the rnn s output words. that is  p yt  ht     softmax wo ht   bo    where  wo and bo are the output parameters. we choose the most probable word as the predicted word  yt   arg max p yt  ht  . in this case  we call  y    y            an orbit in the output space.  it is noted that the orbit  y    y            being periodic is a necessary condition of  h    h            being  periodic. however  we will show that such approximation would be fairly accurate by plotting the  latent space with dimensionality reduction. thus  we will compute the period  if existing  of an  rnn map by the period of output words in our experiments.  for example  if an rnn generates a sequence  i like it but   i like it but           for a fairly large  number of steps  e.g.   k   k   we would empirically estimate the period of this orbit as    and we  will show that the hidden states are indeed trapped in   points.         e xperiments    setup. we use the anna kerenina textbook as our training corpus.  the corpus has    k words   and we split the dataset by     for training and validation  mainly for early stop . the test phase is  to generate sentences following the trained language model  which does not involve input.  to analyze different types of rnns  we experimented with the vanilla and the lstm transitions.  the size of the hidden state and embedding size  in both cases  were     and      respectively. we  used adam as the training algorithm with the initial learning rate  .    and other default hyperparameters.  our analysis of how chaotic an rnn is involves running the rnn for a large number of times with  different initial hidden states h  . the treatment was slightly different. for the rnn without input   we randomly sampled h  from a gaussian distribution centered at  . for rnn with input  we  experimented with different initial words x  fed as input for the first step. in this case  the rnn     in other usages of rnn  for example  sentence encoding   the input xt is a word in a given sentence. such  rnn is not well defined as an iterative map  because the function from ht   to ht is subject to xt .     https   www.kaggle.com wanderdust anna karenina book          iclr      workshop deepdiffeq    period   orbit    period   orbit    period   orbit    period   orbit    period    orbit    period    orbit    period   orbit    period    orbit     a  orbits of vanilla rnn     b  orbits of lstm    figure    hidden state of a  vanilla rnn and b  lstm with input traps in a periodic orbit.    vanilla rnn  lstm    average period  non periodic  average period  non periodic    epoch                    epoch                       epoch                          epoch                          epoch                          table    vanilla rnn and lstm trained with input are used to generate words  using the entire  words in the vocabulary as input. this table lists the average period of the periodic orbits that the  hidden states trap into and also the percentage of the initial words of the vocabulary that perform  non periodic behavior.    could be thought of as an iterative map  starting from the second step  with different initial hidden  states h  . in total  we had   k trajectories  different runnings of rnn  in our experiments for each  setting.  we detect the period of rnns by its output words. we ran each trajectory by  k   k steps  and  then verified the period by another  k   k steps. for the setting with longest periods  table     a  periodic orbit is verified by roughly    times.  results of rnns with input. we first analyze the behavior of an rnn with input  which is the  more realistic usage in applications. we train the model on the training set while monitoring the  validation perplexity. we obtained     perplexity for vanilla rnn and     perplexity for lstm.  we compared our results to previous work  mikolov et al.         and although the corpora are  different  we obtained a reasonable perplexity of english.  unexpectedly  we observe that a well trained rnn  with input  does not exhibit any chaotic behavior. this is shown in figure    where we randomly select a few trajectories and plot the highdimensional hidden state into a  d space with principle component analysis  pca  bishop       .  in each plot  we have more than  k points  but in fact  they are periodic with a length of     .  we are curious if the training process would affect the periodic or chaotic behavior of an rnn. thus   we quantitatively report with different words the average period of an rnn and the percentage of  non periodic orbits in table  . it is seen that all the orbits we obtained are attracted by periodic  orbits  showing that they are not chaotic.  it is also seen that  for both vanilla rnn and lstm  the average period at the beginning of training  is short and then the period becomes longer up to epoch   . for vanilla rnn  the average period  decreases if we train it more  but we do not observe such decrease for lstm. also  the average  period of lstm is significantly longer than that of a vanilla rnn after    epochs.        iclr      workshop deepdiffeq    period         non periodic    vanilla rnn                  lstm                  table    vanilla rnn and lstm trained without input are used to generate words  with different  initialization of the hidden state sampled from a normal distribution between   and  . this table lists  the percentage of the initial hidden states that resulted in the specified periodic outputs.    period   orbit    period   orbit    period   orbit    period   orbit    period   orbit    period   orbit    period   orbit    period   orbit     a  hidden state of vanilla rnn     b  hidden state of lstm    figure    hidden state of a  vanilla rnn and b  lstm without input is attracted to a sink.    results of rnns without input. we also analyzed the behavior of an rnn without input. this  is the typical setting of previous work on analyzing rnn s chaotic behavior  bertschinger    natschlager        laurent   von brecht       .  we show the period of such rnns in table  . again  we do not observe any chaotic behavior for  both vanilla rnn and lstm. more surprisingly  all these orbits are attracted by a sink  i.e.  the  period is  . this is further confirmed by the pca plot of rnn s hidden states  figure   . as shown   all the hidden states are monotonically approaching a single point  which is a sink of the rnn.  in comparison to rnn with input  we see that the period of rnn without input is much shorter  and  in fact  all orbits are period  . our conjecture is that  if a word is fed to rnn as an input  it is taken  from the argmax of the predicted probability in the previous step. such input word is a discrete  token chosen from the vocabulary. this  in turn  may drag the rnn s hidden state and make the  period longer.  it is also noted that a few previous papers report the chaotic behavior of rnn  which seemingly  contradicts the observation of our paper. we contacted by personal email  and with the help of  them  we replicated the chaos with certain rnn weights. considering the evidence in previous  work and our paper  we conclude that vanilla rnn or lstm could be chaotic with certain weights   but in a realistic setting where the rnn weights are either randomly initialized or well trained  the  rnn does not exhibit chaotic behavior.         c onclusion and f uture w ork    in this paper  we show that an rnn with either vanilla or lstm transition is not chaotic along the  training process in real applications. our results contradict previous belief that rnns are chaotic  non linear dynamic systems  although previous work indeed shows that rnns could be chaotic with  certain magic weights.        iclr      workshop deepdiffeq    our findings also suggest that  in future work  it is unnecessary to encourage non chaos for an rnn   because it does not exhibit chaos in real applications. on the contrary  we observe that a better  rnn typically exhibits a longer period  e.g.  lstm vs. vanilla rnn  well trained vs. randomly  initialized . we conjecture that  in real applications of rnns  chaos should be encouraged  rather  than discouraged  so that the rnn is not attracted to a periodic orbit.    acknowledgments  this work is partially supported by the natural sciences and engineering research council of  canada  nserc  under grant no. rgpin           . lili mou is also supported by altaml   the amii fellow program  and the canadian cifar ai chair program.    