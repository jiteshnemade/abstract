introduction      we introduce a principled way to capture link information  for linked sequence mathematically     we propose a novel rnn framework linkedrnn  which can  model sequential and link information coherently for linked  sequences  and     woodstock     july       el paso  texas usa    zhiwei wang  yao ma  dawei yin  and jiliang tang    s     s     s     s     where a i  j      if there is a link between the sequence s i and  s j and a i  j       otherwise. in this work  we following the transductive learning setting. in detail  we assume that a part of the  sequences from s   to s k are labeled where k   n . we denote  the labeled sequences as sl    s     s     . . .   s k  . for a sequence  s j   sl   we use y j to denote its label where y j is a continuous  number for the regression problem and y j is one symbol for the  classification problem. note that in this work  we focus on the unweighted and undirected links among sequences. however  it is  straightforward to extend the proposed framework for weighted  and directed links. we would like to leave it as one future work.  although the proposed framework is designed for transductive  learning  we also can use it for inductive learning  which will be discussed when we introduce the proposed framework in the following  section.  with the above notations and definitions  we formally define the  problem we target in this work as follows   given a set of sequences s with sequential information s i    i   and link information a  and a subset of labeled   x  i   x  i           x n  i  k    sequences  sl    y j  j      we aim to build a rnn model by leveraging  k  s  a and  sl    y j  j      which can learn representations for sequences    figure    an illustration of linked sequences. s     s     s   and  s   denote four sequences and they are connected via four  links.    we validate the effectiveness of the proposed framework on  real world datasets across different domains.  the rest of the paper is organized as follows. section   gives a  formal definition of the problem we aim to investigate. in section     we motivate and detail the framework linkedrnn. the experiment  design  results and the datasets are described in section  . section    briefly reviews the related work in literature. finally  we conclude  our work and discuss the future work in section  .         problem statement    before we give a formal definition of the problem  we want firstly  give notations that will be used throughout the paper. we denote  scalars by lower case letters such as i and j  vectors are denoted by  bold lower case letters such as x and h  and matrices are represented  by bold upper case letters such as w and u. for a matrix a  we  denote the entry at the i t h row and j t h column of it as a i  j   the  i th row as a i     and j t h column as a    j . in addition  let           represent set where the order of the elements does not matter and  the superscripts are used to denote indexes of elements such as   s     s     s      which is equivalent to  s     s     s    . in contrast           is  used to denote a set of sequential events where the order matters  and we use subscripts to indicate the order indexes of the events in  sequences such as  x     x     x    .  let s    s     s             s n   be the set of n sequences. for linked  sequences  two types of information are available. one is the sequential information for each sequence. we denote the sequential  i   where n i is the length of  information of s i as    x  i   x  i           x n  i  s i . the other is the link information. we use an adjacent matrix  a   rn  n to denote the link information of linked sequences    to predict the labels of the unlabeled sequences in s .         the proposed framework    in addition to sequential information  link information is available  for linked sequences as shown in figure  . as aforementioned  the  major challenges to model linked sequences are how to capture link  information and how to combine sequential and link information  coherently. to tackle these two challenges  we propose a novel  recurrent neural networks linkedrnn. an illustrate of the proposed framework on the toy example of figure   is demonstrated  in figure  . it mainly consists of two layers. the rnn layer is to  capture the sequential information. the output of the rnn layer is  the input of the link layer where link information is captured. next   we first detail each layer and then present the overall framework  of linkedrnn.     .     capturing sequential information    i   the rnn layer aims to learn  given a sequence s i   x  i   x  i         x n  i  a representation vector that can capture its complex sequential  patterns via recurrent neural networks. in deep learning community  recurrent neural networks  rnns          have been very  successful to capture sequential patterns in many fields        .  specifically  rnn consists of recurrent units that take the previous  state hit    and current event xit as input and output a current state  hit containing the sequential information seen so far as     hit   f  uhit      wxit             where u and w are the learnable parameters and f     is a activation function which enables the non linearity. however  one  major limitation of the vanilla rnn in equation   is that it suffers from gradients vanishing or exploding issues  which fail the  learning procedure as it cannot capture the error signals during  back propagation process   .     linked recurrent neural networks    woodstock     july       el paso  texas usa    output layer    link layer    rnn layer    rnn    rnn    rnn    rnn    s     s     s     s     input layer    figure    an illustrate of the proposed framework linkedrnn on the toy example as shown in figure  . it consists of two  major layers where rnn layer is to capture sequential information and the link layer is to capture link information.  more advanced recurrent units such as long short term memory   lstm  model      and the gated recurrent unit  gru      have  been proposed to solve the gradient vanishing problem. different  from vallina rnn  these variants employ gating mechanism to decide when and how much the state should be updated with the  current information. in this work  due to its simplicity and effectiveness  we choose gru as our rnn unit. specifically  in the gru   current state hit is a linear interpolation between previous state  hit    and a candidate state h it    hit   zti   hit           zti     h it           where   is the element wise multiplication and zt is called update  gate which is introduced to control how much current state should  be updated. it is obtained through the following equation   zti      wz xit   uz hit                where wz and uz are the parameters and       is the sigmoid function  that is     x      e   x . in addition  the newly introduced candidate state h it is computed by the equation     h it     wxit   u r ti   hit        x          x    where      is the tanh function that   x    ee x  e   e  x and w and u  are model parameters. r t is the reset gate which determines the  contribution of previous state to the candidate state and is obtained  as follows   r ti            wr xit      ur hit                the output of the rnn layer will be the input of the link layer.  for a sequence s i   the rnn layer will learn a sequence of latent  representations  hi    hi    . . .   hin i  . there are various ways to obtain    the final output h i of s i from  hi    hi    . . .   hin i  . in this work  we  investigate two popular ways     as the last latent representation hin i is able to capture information from previous states  we can just use it as the  representation of the whole sequence. we denote this way of  aggregation as a  re ation    . specifically  we let h i   hin i .    the attention mechanism can help the model automatically  focus on relevant parts of the sequence to better capture  the long range structure and it has shown effectiveness in  many tasks           . thus  we define our second way of  aggregation based on the attention mechanism as follows   h i      ni       a j hij           j      where a j is the attention score  which can be obtained as  i    aj        e a hj      me    i    a hm           where a hij   is a feedforward layer   a hij     vta tanh wa hij             note that different attention mechanisms can be used  we  will leave it as one future work. we denote the aggregation  way described above as a  re ation    .     woodstock     july       el paso  texas usa    zhiwei wang  yao ma  dawei yin  and jiliang tang    for the general purpose  we will use rnn to denote gru in the  rest of the paper.     .     capturing link information    the rnn layer is able to capture the sequential information. however  in linked sequences  sequences are naturally related. the  homophily theory suggests that linked entities tend to have similar attributes       which have been validated in many real world  networks such as social networks       web networks       and  biological networks    . as indicated by homophily  a node is likely  to share similar attributes and properties with nodes with connections. in other words  a node is similar to its neighbors. with this  intuition  we propose the link layer to capture link information in  linked sequences.  as shown in figure    to capture link information  for a node   the link layer not only includes information from its sequential  information but also aggregates information from its neighbors.  the link layer can contain multiple hidden layers. in other words   for one node  we can aggregate information from itself and its  neighbors multiple times. let vki be the hidden representations of  the sequence s i after k aggregations. note that when k      vi  is  the input of the link layer  i.e.  vi    h i . then vki    can be updated  as         vki      act    vki    vkj           n  i        j  s  n i     where act   is an element wise activation function  n  i  is the set  of neighbors who are linked with s i   i.e.  n  i     s j  a i  j         and  n  i   is the number of neighbors of s i . we define vk     vk    vk    . . .   vkn   as the matrix form of representations of all sequences at the k th layer. we modify the original adjacency matrix  a by allowing a i  i     . the aggregation in the eq.     can be  written in the matrix form as   vk     act ad   vk              where vk    is the embedding matrix after k     step aggregation   and d is the diagonal matrix where d i  i  is defined as   d i  i       n       a i  j             j       .     linked recurrent neural networks    with the model components to capture sequential and link information  the procedure of the proposed framework linkedrnn is  presented below    hi    hi    . . .   hin i        rn n  s      to learn the parameters of the proposed framework linkedrnn   we need to define a loss function that depends on the specific task.  in this work  we investigate linkedrnn in two tasks   classification  and regression.  classification. the final output of a sequence s i is zi . we can  consider zi as features and build the classifier. in particular  the  predicted class labels can be obtained through a softmax function  as   p i   so f tmax wc zi   bc              where wc and bc are the coefficients and the bias parameters   respectively. p i is the predicted label of the sequence s i . the corresponding loss function used in this paper is the cross entropy  loss.  regression. for the regression problem  we choose linear regression in this work. in other words  the regression label of the  sequence s i is predicted as   p i   wr zi   br            where wr and br are the regression coefficients and the bias parameters  respectively. then square loss is adopted in this work as  the loss function as     vi    h i    zi        as vim is the output of the last layer  we can define the  final representation as  zi   vim   and we denote this way as  a  re ation    .    although the new representation vm incorporates all the  neighbor information  the signal in the representation of  itself may be overwhelmed during the aggregation process.  this is especially likely to happen when there are a large  number of neighbors. thus  to make the new representation  to focus more on itself  we propose to use a feed forward  neural network to perform the combination. we concatenate  representations from the last two layers as the input of the  feed forward network. we refer this aggregation method as  a  re ation    .  j    each representation vi could contain its unique information   which cannot be carried in the later part. thus  similarly  we  use a feed forward neural network to perform the combination of  vi    vi            vim  . we refer this aggregation method  as a  re ation    .    i    h i   a  re ation  hi    hi    . . .   hin i    vki      act      hi    hi    . . .   hin i  . the sequence of latent representations will be  aggregated to obtain the output of the rnn layer  which serves as  the input of the link layer. after m layers  link layer produces a  sequence of latent representations  vi    vi    . . .   vim    which will be  aggregated to the final representation.  the final representation zi for the sequence s i is to aggregate  the sequence  vi    vi    . . .   vim   from the link layer. in this work  we  investigate several ways to obtain the final representation zi as         vk     n  i       i         s j  n i   a  re ation  vi    vi    . . .   vim      vkj       l           where the input of the rnn layer is the sequential information and  the rnn layer will produce the sequence of latent representations    k      i   y   p i     k i              note that there are other ways to define loss functions for classification and regression. we would like to leave the investigation of  other formats of loss functions as one future work.     linked recurrent neural networks    woodstock     july       el paso  texas usa    table    statistics of the datasets.  description    of sequences  network density      avg length of sequences  max length of sequences    dblp           .     .         boohee           .       .         prediction. for an unlabeled sequence s j under the classification problem  its label is predicted as the one corresponding to the  entity with the highest probability in so f tmax wc zj   bc  .  for an unlabeled sequence s j under the regression problem  its  label is predicted as wr zi   br .  although the framework is designed for transductive learning   it can be naturally used for inductive learning. for a sequence s k    which is unseen in the given linked sequences s  according to its  sequential information and its neighbors n  k   it is easy to obtain  its representation zk via eq.     . then based on zk   its label can be  predicted as the normal prediction step described above.         experiment    in this section  we present experimental details to verify the effectiveness of the proposed framework. specifically  we validate the  proposed framework on datasets from two different domains. next   we firstly describe the datasets we used in the experiments and  then compare the performance of the proposed framework with  representative baselines. lastly  we analyze the key components of  linkedrnn.     .     datasets    in this study  we collect two types of linked sequences. one is from  dblp where data contains textual sequences of papers. the other  is from a weight loss website boohee where data includes weight  sequences of users. some statistics of the datasets are demonstrated  in table  . next we introduce more details.  dblp dataset. we constructed a paper citation network from  the public available dblp data set      . this dataset contains information for millions of paper from a variety of research fields.  specifically  each paper contains the following relevant information  paper id  publication venue  the id references of it and abstract.  following the similar practice in       we only select papers from  conferences in    largest computer science domains including vcg   acl  ip  tc  wc  ccs  cvpr  pds   nips  kdd  www  icse  bioinformatics  tcs. we construct a sequence for each paper from their  abstracts and regard their citation relationships as the link information between sequences. specifically  we first split the abstract into  sentences and tokenize each sentence using python nltk package.  then  we use word vec      to embed each word into euclidean  space and for each sentence  we treat the mean of its word vectors  as the sentence embedding. thus  the abstract of each paper can be  represented by a sequence of sentence embeddings. we will conduct the classification task on this dataset  i.e.  paper classification.    https   aminer.org citation.    thus  the label of each sequence is the corresponding publication  venue.  boohee dataset. this dataset is collected from one of the most  popular weight management mobile applications  boohee   . it  contains million of users who self track their weights and interact with each other in the internal social network provided by the  application. specifically  they can follow friends  make comment  to friends  post and mention     friends in comments or posts.  the recored weights by users form sequences which contain the  weight dynamic information and the social networking behaviors  result in three networks that correspond to following  commenting  and mentioning interactions  respectively. previous work       has shown a social correlation on the users  weight loss. thus  we  use these social networks as the link information for the weight  sequence data. we preprocess the dataset to filter out the sequences  from suspicious spam users. moreover  we change the time granularity of weight sequence from days to weeks to remove the daily  fluctuation noise. specifically  we compute the mean value of all the  recorded weights in one week and use it as the weight for that week.  for networks  we combine three networks into one by adding them  together and filter out weak ties. in this dataset  we will conduct a  regression task of weight prediction. we choose the most recent  weight in a weight sequence as the weight we aim to predict  or the  groundtruth of the regression problem . note that for a user  we  remove all social interactions that form after the most recent weight  where we want to avoid the issue of using future link information  for weight prediction.     .     representative baselines    to validate the effectiveness of the proposed framework  we construct three groups of representative baselines. the first group includes the state of the art network embedding methods  i.e.  node vec       and gcn       which only capture the link information. the second group is the gru rnn model       which is the basic model  we used in our model to capture sequential information. baselines  in the third group is to combine models in the first and second  groups  which captures both sequential and link information. next   we present more details about these baselines.    node vec     . node vec is one state of the art network embedding method. it learns the representation of sequences  only capturing the link information in a random walk perspective.    gcn      it is the traditional graph convolutional graph  algorithm. it is trained with both link and label information.  hence  it is different from node vec  which is learnt with  only link information and is totally independent on the task.    rnn     . rnns have been widely used for modeling sequential data and achieved great success in a variety of domains. however  they tend to ignore the correlation between  sequences and only focus on sequential information. we construct this baseline to show the importance of correlation  information. to make the comparison fair  we employ the  same recurrent unit  gru  in both the proposed framework  and this baseline.    https www.boohee.com     woodstock     july       el paso  texas usa    zhiwei wang  yao ma  dawei yin  and jiliang tang    table    performance comparison in the dblp dataset  measurement    micro f     macro f     method    training ratio                                node vec     .         .         .         .        gcn  rnn     .       .         .       .         .       .         .       .        rnn node vec  rnn gcn  linkedrnn     .       .       .         .       .       .         .       .       .         .       .       .        node vec     .         .         .         .        gcn  rnn     .       .         .       .         .       .         .       .        rnn node vec  rnn gcn  linkedrnn     .       .       .         .       .       .         .       .       .         .       .       .        table    performance comparision in the boohee dataset  method  node vec  gcn  rnn  rnn node vec  rnn gcn  linkedrnn           .       .       .       .       .       .        training ratio         .       .       .       .       .       .          rnn node vec. the node vec method is able to learn representation from the link information and the rnn can do so  from the sequential information. thus  to obtain the representation of sequences that contains both link and sequential  information  we concatenate the two sets of embeddings obtained from node vec and rnn via a feed forward neural  network.    rnn gcn. rnn gcn applies a similar strategy of combining rnn and node vec to combine rnn and gcn.    there are several notes about the baselines. first  node vec does  not use label information and it is unsupervised   rnn and rnnnode vec utilize label information and they are supervised  and  gcn and rnn gcn use both label information and unlabeled data  and they are semi supervised. second  some sequences may not  have link information and baselines only capture link information  cannot learn representations for these sequences  hence  in this  work  when representations from link information are unavailable   we will use the representations from the sequential information  via rnn instead. third  we do not choose lstm and its variants as  baselines since our current model is based on gru and we also can  choose lstm and its variants as the base models.     .           .       .       .       .       .       .              .       .       .       .       .       .        experimental settings    data split  for both datasets  we randomly select     for test.  then we fix the test set and choose x  of the remaining     data  for training and     x  for validation to select parameters for  baselines and the proposed framework. in this work  we vary x as                  .  parameter selection  in our experiments  we set the dimension  of representation vectors of sequences to    . for node vec  we  use the validation data to select the best value for p and q from    .     .             as suggested by the authors      and use the default values for the remaining parameters. in addition  the learning  rate for all of the methods are selected through validation set.  evaluation metrics  since we will perform classification in the  dblp data  we use micro and macro f  scores as the metrics for  dblp  which are widely used for classification problems         .  the higher value means better performance. we perform the regression problem weight prediction in the boohee data. therefore the  performance in boohee data is evaluated by mean squared error   mse  score. the lower value of mse indicates higher prediction  performance.     linked recurrent neural networks     .       linkedrnn    this variant utilizes a  re ation    and a  re ation       linkedrnn    it is the variant which chooses a  re ation     and a  re ation       linkedrnn    we construct the variant by adopting a  re ation     and a  re ation       experimental results    we first present the results in dblp data. the results are shown  in table  . for the proposed framework  we choose m     for the  link layer and more details about discussions about the choices of  its aggregation functions will be discussed in the following section.  from the table  we make the following observations     as we can see in table    in most cases  the performance  tends to improve as the number of training samples increases.    the random guess can obtain  .  for both micro f  and  macro f . we note that the network embedding methods  perform much better than the random guess  which clearly  shows that the link information is indeed helpful for the  prediction.    gcn achieves much better performance than node vec. as  we mentioned before  gcn uses label information and the  learnt representations are optimal for the given task. while  node vec learns representations independent on the given  task  the representations may be not optimal.    the rnn approach has higher performance than gcn. both  of them use the label information. this observation suggests  that the content and sequential information is very helpful.    most of the time  rnn node vec and rnn gcn outperform  the individual models. this observation indicates that both  sequential and link information are important and they contain complementary information.    the proposed framework linkedrnn consistently outperforms baselines. this strongly demonstrates the effectiveness  of linkedrnn. in addition  comparing to rnn node vec and  rnn gcn  the proposed framework is able to jointly capture  the sequential and link information coherently  which leads  to significant performance gain.  we present the performance on boohee in table  . overall  we  make similar observations as these on dblp as       the performance improves with the increase of number of training samples       the combined models outperform individual ones most of the  time and     the proposed framework linkedrnn obtains the best  performance.  via the comparison  we can conclude that both sequential and  link information in the linked sequences are important and they  contain complementary information. meanwhile  the consistent  impressive performance of linkedrnn on datasets from different  domains demonstrate its effectiveness in capturing the sequential  and link information presented in the sequences.    the results are demonstrated in figure  . note that we only show  results on dblp with     as training since we can have similar  observations with other settings. it can be observed     generally  the variants of linkedrnn with a  re ation    obtain better performance than a  re ation    . it demonstrates  that aggregating the sequence of the latent presentations  with the help of the attention mechanism can boost the performance.    aggregating representations from more layers in the link  layer typically can result in better performance.     .      .      .       macro f      .     woodstock     july       el paso  texas usa     .      .      .      .      .                               rnn edrnn edrnn edrnn edrnn edrnn  k  k  k  k  k  lin  lin  lin  lin  lin    ked    lin    aggregation functions  figure    the impact of aggregation functions on the performance of the proposed framework.    component analysis    in the proposed framework linkedrnn  we have investigate several  ways to define the two aggregation functions. in this subsection   we investigate the impact of the aggregation functions on the performance of the proposed framework linkedrnn by defining the  following variants.    linkedrnn    it is the variant which chooses a  re ation     and a  re ation       linkedrnn    we define the variant by using a  re ation     and a  re ation       linkedrnn    this variant is made by applying a  re ation     and a  re ation        .     parameter analysis    linkedrnn uses the link layer to capture link information. the  link layer can have multiple layers. in this subsection  we study the  impact of the number of layers on the performance of linkedrnn.  the performance changes with the number of layers are shown  in figure  . similar to the component analysis  we only report the  results with one setting in dblp since we have similar observations.  in general  the performance first dramatically increases and then  slowly decreases. one layer is not sufficient to capture the link  information while more layers may result in overfitting.     woodstock     july       el paso  texas usa    zhiwei wang  yao ma  dawei yin  and jiliang tang     .       macro f      .        .        .        .                         number of layers  figure    the performance variance with the number of layers of the link layer.         related work    in this section  we briefly review the rnn based deep methods that  have been proposed to learn the sequential data effectively     .  although it has been designed to model arbitrarily long sequences   there are tremendous challenges to prevent it from effectively capturing the long term dependencies. for example  the gradient vanishing and exploding issues make it very difficult to back propagate  error signals and the dependencies in sequences are complex. in  addition  it is also time consuming to train these models as the  training procedure is hard to parallelize. thus  many researchers  have attempted to develop advanced architectures to overcome  aforementioned challenges. one of the most successful attempts  is to add gate mechanism to the recurrent unit. the two representative works are long short term memory  lstm       and  gated recurrent units  gru       where sophisticated activation  function is introduced to capture long term dependencies in sequences. for example  in lstm  a recurrent unit maintains two  states and three gates which decide how much the new memory  should added  how much exiting memory should be forgotten  the  amount of memory context exposure  respectively. the rnns that  are equipped with such gating mechanism can effectively mitigate  the gradient exploding and vanishing issues and have demonstrated  extraordinary performance in a variety of tasks  such as machine  translation       speech recognition       and medical events detection     . moreover  several works have introduced additional  gates into lstm unit to deal with the situation where irregularly  sampled data presents        . these time aware models can largely  improve the training efficiency and effectiveness of rnns.    beside gating mechanism  other directions of extending rnn for  better performance are also heavily explored. schuster et al       described a new architecture called bidirectional recurrent neural  networks brnns   where two hidden layers that process the input  from opposite directions were proposed. although brnns are able  to use all available input sequential information and effectively  boost the prediction performance       the limitation of it is quite  obvious as it requires the information from the future     . recently   koutnik et al.      presented a clockwork rnn which modifies standard rnn architecture and partitions the hidden layer into separate  modules. in this way  each individual can process the sequence at  its own temporal granularity. one recent work proposed by change  et al.     tries to tackle those major challenges together. in doing  so  they introduced a dilated recurrent skip connection which can  largely reduce the model parameters and therefore enhances the  computational efficiency. in addition  such layers can be stacked  so that the dependencies of different scales are learned effectively  at different layers. while a large body of research has focused on  modeling the dependencies within the sequences  limited efforts  have been made to model the dependencies between sequences. in  this paper  we devote to tackling this novel challenge brought by  the links among sequences and propose an effective model which  has shown promising results.         conclusion    rnns have been proven to be powerful in modeling sequences in  many domains. most of existing rnn methods have been designed  for sequences which are assumed to be i.i.d. however  in many  real world applications  sequences are inherently linked and linked  sequences present both challenges and opportunities to existing  rnn methods  which calls for novel rnn methods. in this paper  we  study the problem of designing rnn models for linked sequences.  suggested by homophily  we introduce a principled method to  capture link information and propose a novel rnn framework  linkedrnn  which can jointly model sequential and link information. experimental results on datasets from different domains  demonstrate that     the proposed framework can outperform a variety of representative baselines  and     link information is helpful  to boost the rnn performance.  there are several interesting directions to investigate in the  future. first  our current model focuses on unweighted and undirected links and we will study weighted and directed links and  the corresponding rnn models. second  in current work  we focus  on classification and regression problems with certain loss functions. we will investigate other types of loss functions to learn the  parameters of the proposed framework and also investigate more  applications of the proposed framework. third  since our model  can be naturally extended for inductive learning  we will further  validate the effectiveness of the proposed framework for inductive  learning. finally  in some applications  the link information may be  evolving  thus we plan to study rnn models  which can capture  the dynamics of links as well.    