introduction  the classic task of image recognition is beginning to approach a solved problem with the latest inception resnet       achieving a top   error rate of  .    on the ilsvrc         dataset  surpassing humans. interest is therefore growing in generating richer descriptions of image properties  rather than simple categorisations  including multi label  classification tagging                  and image captioning                         .  in multi label classification the aim is to describe rather  than merely recognise an image by annotating all visual  concepts that appear in the image. the label space is thus  richer than in the single label recognition case   labels can  refer to scene properties  objects  attributes  actions  aesthetics etc. such labels have richer relationships  e.g.  a policeman is a person  car and sky co exist more often than car  and sea. image captioning has a related aim  with the dif     using such layers as the input to the rnn has a number  of adverse effects on learning an end to end recurrent image annotation model. first  since the cnn output feature  is not explicitly semantically meaningful  both the label prediction and label correlation grammar modelling tasks now  need to be shouldered by the rnn model alone. this exacerbates the already challenging task of rnn training  since  the number of visual concepts words is often vast  there are  more than        words in the ms coco training cap      i    f  w     c   w    word embedding    ie    w    w    t      t       b     t           ie    je    i    cnn    lstm  output layer    f    lstm    w  t      cnn    cnn    f s  s     d   cnn layers    recurrent units    image joint embedding    lstm     a     i    lstm    ie         lstm    f    lstm    cnn    lstm    i    w    w    t      t           semantic embedding    figure  . cnn rnn architectures for image annotation  multi label classification and captioning . in all models  lstm is used as the  rnn model.  a  cnn encodes an image  i  to a feature representation  f . the image embedding ie and word representation go through  the same word embedding layer before being fed into the lstm     .  b  the image cnn output features f set the lstm hidden states      .  c  the image cnn output feature layer is integrated with the lstm output via late fusion         .  d  the proposed semantically  regularised model. the cnn model is regularised by the ground truth semantic concepts s  which serve as strong deep supervision to  guide the learning of the cnn layers. the cnn prediction layer s  is used as image embedding which is used to set the lstm initial states.  best viewed in colour.    tions  and their correlation is rich. second  a connected  cnn rnn model is effectively rather deep considering the  rnn unrolling  existing cnn rnn models apply supervision only at the final rnn output and propagate the supervision back to the earlier  cnn  layers. this leads to training  difficulties in the form of  vanishing  gradients     . in addition  joint training of cnn and rnn has to be carried out  very carefully to prevent noisy gradients back propagated  from the rnn from corrupting the cnn model. as a result  model convergence is often extremely slow     .  in this paper we propose to change the image embedding layer and introduce semantic regularisation to a cnnrnn model in order to produce significantly more accurate results and make model training more stable and faster.  specifically  we perform multi task learning where the auxiliary task  besides tagging sentence generation  is to regularise the image embedding interface layer to encode semantically meaningful visual concepts which are directly  related to the label prediction task  fig.   d  . this can  be understood from several perspectives   i  as splitting  up the system into a model for generating unary potentials   the cnn  by predicting the label individually  and modelling their relations  rnn  for structured prediction. with  the unary cnn taking the responsibility of concept prediction  the relational rnn model is better able to focus on  learning concept correlations sentence generation. in the  multi label classification case  where the label space of the  semantic regularisation and the rnn output space are the  same  this can be seen as analogous to crf decoding of  a joint distribution     .  ii  as a deeply supervised net     work       providing auxiliary supervision to the middle of  what is effectively a very deep network. such deep supervision improves accuracy and convergence speed         .  in our case specifically  it largely eliminates the problem of  noisy rnn gradients back propagating to corrupt the cnn  encoder     . it thus allows for better and more efficient  fine tuning of the cnn module  as well as fast convergence  in end to end training of the full cnn rnn model.  iii   as pursuing an encoder decoder model with prior bias of  preferring semantically meaningful codes     .  the contributions of this paper are as follows      we  propose a novel cnn rnn image annotation model which  differs from the existing models in the selection of the image embedding layer and in the introduction of deeplysupervised semantic regularisation to the embedding layer.      our proposed semantic regularisation enables reliable  fine tuning of the cnn image encoder as well as the fast  convergence of end to end cnn rnn training.     we  demonstrate through extensive experiments that on both  multi label classification and image captioning  we achieve  the state of the art performance.     . related work  deep multi label classification  many earlier studies       treat the multi label classification problem as multiple single label classification problems and ignore the rich  correlations in the label space. in order to model label  correlation  a structured output model is required. deng  et al.     propose a hierarchy and exclusion graph  hex   to model the structure of labels  however  they only fo      cus on single label classification. deep structured learning  is widely employed in object segmentation. for instance   zheng et al.      present an end to end structured model  that combines the cnn model with a crf. it allows for  fast inference and learning of a deep model with gaussian  edge potentials. this was extended by chen et al.     to  a deep model which combines mrfs and cnn to model  output correlations  and is applied to multi label classification. multi label structure was also effectively modelled by  conditional graph lasso       but for shallow models.  these cnn crf mrf models work well for image segmentation. however  for multi label classification  the large  label space  seriously imbalanced label distribution  and the  need for variable length prediction challenge the application  of these models     . recently  the cnn rnn          pattern has been applied to multi label classification to capture  label correlations  as well as address label imbalance and  variable length prediction. since rnn requires sequential  input  before training the unordered label set is converted  to an ordered list  e.g.  frequent first      or rare first     .  small classes can be promoted by using the rare first order.  for structured prediction  it is more computationally efficient than cnn crf  as it only iterates until the required  number of labels are output. furthermore  it is an end toend predictive model as it outputs labels directly  rather than  prediction scores  thus eliminating tricky prediction score  thresholding heuristics. our model is related to           in that it follows the cnn rnn design pattern  however   it uses a semantically regularised image embedding layer as  the interface layer rather than an unregularised cnn feature  layer.  another line of work is to incorporate side information  in multi label classification  since side information could  be complementary to the image data. the side information  could be user tags or groups from image metadata         .  johnson et al.      uses a non parametric approach to find  image neighbours according to the metadata  and then aggregates visual information of the image and its neighbours  with a deep network to improve classification. in      tags   groups  and labels are modelled by different concept layers   which corresponds to different level of abstractions. messages can be passed top down and bottom up by leveraging a bidirectional structured network. side information can  also be exploited in our model  but we show that even using  less side information  e.g.  tags only  our model can outperform those in          significantly.  neural network based image captioning a number of  recent captioning studies take a bottom up approach  where  words or phrases are first detected and then composed to  sentence with a language model. fang et al.     propose a  caption model that first detects keywords using a multiple  instance learning  and then uses the keywords to generate  sentences. a similar model is proposed in      with the    main difference being that lstm is used as the language  model. compared with these model  our model is an endto end cnn rnn model which jointly learns the image encoding and language decoding modules.  cnn rnn based image captioning models have become  popular. vinyals et al.          follow an encoder decoder  scheme  and feed image features as the initial input to the  rnn decoder  so that sentences are generated according to  the image. a similar approach is employed in     . our  work is related to       but we use semantic concepts to  regularise the representation of the cnn rnn interface  layer  which leads to significantly improved performance  and much easier model training. recently  visual attention  has been incorporated to improve captioning accuracy. xu  et al.      propose a model capable of sequentially attending to discriminative regions to improve the caption generation. you et al.      propose to combine visual attributes  and image features. an attention mechanism is introduced  to reweight attribute predictions and merged with both the  input and output of the rnn. image features are fed at the  first step as an external guide. such attention models could  easily be integrated into our model to further improve performance.  semantic regularisation in deep encoder decoders  the idea of introducing semantic regularisation to an  encoder decoder model has been exploited in the context  of image synthesis. yan et al.      extend the variational  autoencoder      by introducing attribute induced semantic regularisation to the middle embedding layer. a similar  model based on generative adversarial networks is also proposed     . despite the similar strategy to ours  the objective is very different  we use the encoder decoder architecture to align the text and image modalities and middle layer  supervision is employed to achieve more effective and efficient training of both the encoder and decoder.     . methodology  we first give an overview of existing cnn rnn models before introducing our semantically regularised cnnrnn. its application to multi label classification and image  captioning are detailed in sec.   and sec.   respectively.     . . cnn rnn  a cnn rnn model is composed of two parts  a visual  encoder perceives the visual content of an image and encodes it to an image embedding  and a decoder takes the embedding as input and generates sequences of labels  words .  given an image i  a visual encoder will encode it to a  fixed length vector ie   rd   called image embedding   ie   fenc  i             where fenc is the encoder  which could be a pretrained cnn  optionally with some additional transformation layers. so     s    captions     f    s     lr          s      lstm    cnn    little    lstm    i    a          start     a     a  end to end training of the whole model    s  s     b  pretraining of the unary model  a    s    little    boy eating    lstm    donut  dining table   chair  person    lstm    labels   binary    f    lstm    or    lu  s  s      cnn    i          start     a    little    lstm    a little boy eating a  chocolate doughnut  with sprinkles.    bow         boy     c  pretraining of the relational model    figure  . the full pipeline of the proposed semantically regularised annotation model. the ground truth semantic concepts serve as strong  supervision in the middle to regularise the training of the unary model  a . due to the use of semantic concepts as the interface between  cnn and rnn  the unary model and relational models can be pretrained in parallel  as shown in  b    c .    ie could either be a feature layer               e.g.  fc   layer of vgg         or its linear transform         . in  this paper  we enforce it to be a semantic representation to  better interact with the rnn.  the rnn decoder will then take ie as a condition  and  generate a predictive path      a    a    ...  ans    where for  multi label classification  ai is semantic label  and ns is the  number of labels predicted for image i  while for image  captioning ai is the word token  and ns is the length of the  sentence. the path is an ordered sequence  so in multi label  classification  a priority of the labels has to be defined to  convert labels to a sequence. we take a rare first order so as  to give rare classes more importance during the prediction   therefore countering the label imbalance problem.  many different cnns have been considered for the encoder  but for the rnn decoder  the long short term memory  lstm  model      has been chosen by almost all existing models. this is because it controls message passing between times steps with gates in order to alleviate the vanishing exploding gradient problem which plagued the training  of prior rnn models. the model has two types of states   cell state c and hidden state h. following       a forward  pass at time t with input xt is computed as follows.  it     wi h   ht     wi c   ct     wi x   xt   bi      ft     wf h   ht     wf c   ct     wf x   xt   bf      ot     wo h   ht     wo c   ct     wo x   xt   bo      gt     wg h   ht     wg c   ct     wg x   xt   bg    ct   ft    ct     it    ht   ot      ct      gt         where ct and ht are the model s cell and hidden states   it   ft   ot are the activation of input gate  forget gate     and output gate respectively  w  h   w  c are the recurrent  weights  and w  x is the input weight  and b  are the biases.       is the sigmoid function  and   is the output activation  function.  at time step t  the model uses its last prediction at   as  input  and computes a distribution over possible outputs   xt   e   at        ht   lst m  xt   ht     ct                yt   sof tmax w   ht   b    where e is the word embedding matrix  ht   is the hidden  state of the recurrent units at t      w   b are the weight and  bias of the output layer  at   is the one hot coding of last  prediction at     and lst m     is a forward step of the unit.  the output yt defines a distribution over possible actions   from which the next action at   is sampled.  to generate image conditioned sequences  the decoder  has to take advantage of the image embedding ie   and existing models achieve this in multiple ways. vinyals et al.        fig.   a   propose to feed ie as step zero input to the lstm  model  that is   h    c      lst m  ie          where   is  a zero vector. in this case the weights of the word embedding are shared with image embedding  which is a questionable assumption  as the two embeddings have very different  meanings and their dimensions have not been aligned. instead of treating ie as an lstm input  wang et al.      and  mao et al.      combine word embedding and image features via output fusion  fig.   c  . in contrast  jin et al.       use the image embedding to initialise the lstm  fig.   b    by setting hidden state h    wi   ie   bi   where wi   bi  are image input weights and biases.  despite these differences  existing cnn rnn models  have a key common characteristic  the image embedding     ie that acts as the interface between the cnn and rnn  models is taken to be a layer of weak and implicit semantics  e.g.  cnn feature layer  or its transform. this means  that the rnn has to simultaneously learn to predict semantic concepts from the provided features  as well as model  the correlation of those concepts. learning to predict the  concepts is harder for the rnn because gradients are back  propagated from relatively  far  supervision away  the rnn  outputs at future time steps . moreover fine tuning the cnn  becomes tricky because noisy gradients propagated from  the rnn can easily degrade rather than improve performance     .     . . semantically regularised cnn rnn  to reduce the burden on the rnn  we propose a divideand conquer strategy to separate two tasks  semantic concept learning and relational modelling. specifically  semantic concept learning is now performed by the unary cnn  model which takes as input images  and associated side information if any   and produces a probabilistic estimate of  the semantic concepts. relational modelling is handled by  the rnn model which takes in the concept probability estimates and models their correlations to generate label word  sequences. concretely  instead of using a cnn feature layer  as embedding ie   we use the cnn label prediction layer   e.g.  concept prediction layer of an inception net     . since  the chosen embedding is trained under direct supervision  of ground truth labels visual concepts  it has clear semantic  meaning  each unit corresponds to a semantic concept.  as shown in fig.    in our semantically regularised  cnn rnn  s cnn rnn   the cnn part takes an image  i as input  and predicts the likelihood of the semantic concepts s    rk   where k is the number of semantic concepts  . the rnn model takes s  as input  and generates  sequences  . the key implication is that supervision can  now be added at both the rnn output layer and the embedding layer s . this results in two losses  a loss for concept prediction lu  s  s   and a loss for relational modelling  lr          s  . formally  we have  x  lu  s  s       u  si   s i    i         lr        s    l         x  i     r   i    i   s i        lu  s  s     lr          s              where si is the ground truth concept labels for the i th training image and s i is the corresponding prediction  for the  rnn loss lr          s     i  is the ground truth path   i is the  predicted path  which is a sequence of word tokens or list of  tags. the specific form of the losses will be discussed next.    k is the size of label space in multi label classification. for image  captioning  k is the number of visual concepts  which is typically smaller  than the vocabulary size as not all words are visual.     . . training and inference  the introduction of semantic regularisation in the middle of cnn rnn allows for more effective and efficient  model training. it facilitates a two staged training strategy  illustrated in fig.  . in the first stage  we pretrain the cnn  model and rnn model in parallel and in the second stage   they are fine tuned together.  cnn for pretraining of the cnn model  fig.   b    the  ground truth semantic concepts si are used as the learning  target in a standard cross entropy loss for k visual concepts      u  si   s i        k  x  j    sij  log s ij      sij   log    s ij           lstm for the lstm pretraining  fig.   c    the concept  input s i is first connected to a fully connected  fc  layer before being used to set the initial hidden state of the lstm  .  the lstm model learns to maximise the likelihood of generating the target sequences conditioned on the semantic input  and the loss lr          s   is simply the sum of the negative log likelihood over all time steps. by feeding s   rather  than s  the lstm can be pre trained independently of the  cnn.  joint cnn lstm after the cnn and rnn models are  pretrained  the whole model can be jointly trained by simultaneously optimising the deeply supervised joint loss l. for  inference  we condition on the image by setting the initial  state  then feed a start signal and recurrently sample model  predictions of the previous step as input until an end signal  is generated. for multi label classification  we just greedily  take the maximum model output  whilst beam search with a  width of three is employed for image captioning     .     . application to multi label classification   . . formulation  to apply our s cnn rnn to multi label classification   we first rank the training labels according to their frequency  in the training set and generate a ordered label list with the  rare labels first. we also explore the use of side information            exploiting the noisy user provided tags available  with each image. in this case the model in fig.   is slightly  modified. specifically  we pretrain a multiple layer perception  mlp   single     neuron hidden layer and relu activation  to predict the true tags given the noisy metadata.  then we combine the image model with the pretrained tag  model by summing their predictions as the final embedding  s    and train them together with a cross entropy loss     .    this    is to allow for the flexibility of using arbitrary lstm unit size.      . . datasets and settings  datasets  two widely used large scale benchmark  datasets are selected to evaluate our model. nus wide      dataset contains         images. originally coming from  flickr  there are       unique user tags released along with  the images. of them     tags are manually selected and  refined as the ground truth      covering different aspects  including object classes  scenes  and attributes. the ground  truth labels are highly imbalanced  the most frequent tag   sky appears        times while the rarest one map appears     times. in addition  the user provided tags are extremely  noisy and sparse    .   noisy tags per image on average.  following           we consider two settings  multi label  classification with only imagery data and with both images  and noisy tags as side information. the most popular        noisy user tags are kept and we remove the images without  any ground truth tags. as in many flicker based studies   the numbers of images used by different works vary as they  download the images at different times. for fair comparison  we use the same train test split ratio as           as a  result         images are used for training and        for  testing. microsoft coco      is popular for tasks such as  object detection  segmentation and image captioning. following       we also use it for multi label classification by  treating the    object classes as labels. since there are normally many types of objects in each image  it is naturally a  multi label classification problem. because the label space  contains objects only and some objects are rather small  it  is perhaps more suitable than nus wide for evaluating  a structured prediction model  as modelling label correlation becomes more important to detect visually similar and  small objects. we also download the original user tags from  flickr via the provided urls  and the most frequent        tags are used as side information. we keep the original  train validation split      for training and evaluation.  implementation details for fair comparisons with previous work  in our s cnn rnn model  we use the caffe  reference net     as our unary cnn subnet on the nuswide dataset      and vgg   on ms coco. both models are pretrained on the ilsvrc   dataset     . for pretraining the cnn subnet  the learning rate is set to  e   for  nus wide and  e   for ms coco. for the rnn subnet   we use     lstm cells and a     dimensional word embedding. the output vocabulary size is set to    for nuswide and    for ms coco  including all labels and an  end token. we use the basiclstmcell in tensorflow  as lstm cells and employ relu as activation function.  the relational model is trained using a rms prop optimiser  with a learning rate of  e  . both the code and trained models will be made available at the first author s website.  evaluation metrics as in           both per class and  per image metrics including mean precision and mean recall are used. for each class image  the precision is de     fined as  p y   y     y   y    y    and recall is defined as   r y   y     y   y    y   where y and y  are the set of ground  truth labels and predicted labels  and       is the cardinality  of a set. the overall precision  o p  recall  o r  is computed by taking the average precision recall over all samples  while the per class precision  c p  recall  c r  is averaged over all classes. f  score is also computed by computing the harmonic mean of precision and recall. as in  existing cnn rnn models           we let the model to  decide its own prediction length           whilst for other  compared fixed length predictive models               we  use the top   ranked predictions.     . . experimental results  competitors we compare with the following models. in  all compared models  the same cnn and rnn modules are  used. cnn logistic  this model treats each label independently by fitting a logistic regression classifier for each  label. the results are reported in     . cnn softmax  a  cnn model that uses softmax as classifier  and the cross entropy between prediction and ground truth is used as the loss  function. the results reported in      for nus wide and       for ms coco are used. cnn warp  same cnn  model as above  but uses a weighted approximate ranking  loss function for training to promote the prec k metric.  we use the results reported in      for nus wide and       for ms coco. cnn rnn  a cnn rnn model which  uses output fusion  fig.   c   to merge cnn output features and rnn outputs     . ria  in this cnn rnn model        the cnn output features are used to set the lstm hidden state  fig.   b  . note that only smaller datasets were  used in      and no code is available  we thus use our own  carefully trained implementation in the experiments. tagneighbour  it uses a non parametric approach to find image neighbours according to metadata  and then aggregates  image features for classification. tag neighbour with  k  tags gives the best performance     . it uses more side information than ours and is also transductive requiring access  to the whole test set at once. sinn  it      uses different  concept layers of tags  groups  and labels to model the semantic correlation between concepts of different abstraction  levels. a bidirectional rnn like algorithm is adopted to integrate information for prediction.  k noisy tags and      query words are used as side information  which is more  than what our model uses. variants of our model  our scnn rnn with and without the side information are called  ours and ours tag k respectively. since the results reported by sinn      and tagneighbour      were based  on imagenet pretrained cnn models  for direct comparison we train a variant of our model that fixes the weights of  the cnn subnet without finetuning  ours tag k fix .  results on nus wide we make the following observations from the results shown in table  .     the proposed s      algorithms    c r    c p    c f     o r    o p    o f     algorithms    c r    c p    c f     o r    o p    o f     cnn logistic       cnn softmax       cnn warp       cnn rnn       ria       tagneighboor        sinn            .      .      .      .      .      .      .        .      .      .      .      .      .      .        .      .      .      .      .      .      .        .      .      .      .      .      .      .        .      .      .      .      .      .      .        .      .      .      .      .      .      .      cnn logistic       cnn softmax       cnn warp       cnn rnn       ria           .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .      ours  ours tag k fix   ours tag k       .      .      .        .      .      .        .      .      .        .      .      .        .      .      .        .      .      .      ours  ours tag k       .      .        .      .        .      .        .      .        .      .        .      .      table  . multi label classification results on nus wide. results  that use side information are marked with superscript  .    cnn rnn performs consistently better than all alternatives  in terms of the f  score  both with  ours tag k  and without side information  ours .     looking at the precision  and recall metrics  our model is more impressive on precision than recall. this is expected because compared to the  non cnn rnn based models that predict a fixed number  of   labels  a cnn rnn model tends to makes less predictions for this dataset with on average  .  ground truth tags  per image.     the gaps between ours and cnn rnn       and ria      show clearly the importance of adding semantic regularisation to the cnn embedding layer.     comparing ours tag k fix with tagneighboor      and sinn        we can see that significant improvements are obtained  even with less side information. this is due to the ability of  the rnn decoder in our cnn rnn model to model highorder label correlations.     our full model  ours tag k   further improves over ours tag k fix on both per class  and per image metric. this shows the importance of having an end to end cnn rnn that can be trained effectively  with the introduced deeply supervised semantic regularisation. qualitative results can be found in the supplementary  material.  results on ms coco similar conclusions can be drawn  from the results in table  . comparing with the results on  nus wide  it is noted that the performance gain obtained  by using the  k noisy tags as side information is smaller.  this is because that the number of user provided tags on  coco is smaller   .   vs.  .   per image with  k unique  tags .     . application to image captioning   . . datasets and settings  datasets and metrics we use the popular microsoft  coco dataset      for evaluation. the dataset contains         training images and        validation images. each  image is manually annotated with   captions. the comparison against the state of the art is conducted using the actual  ms coco test set comprising        images. note that    table  . multi label classification results on microsoft coco.    the annotation of the test set is not publicly available  so  the results are obtained from the coco evaluation server.  for an ablation study  we also follow the setting of           by a held out set of       images from the validation set as  the test set. the widely used bleu  cider  meteor  and  rouge scores are employed to measure the quality of generated captions. for the ablation study  they are computed  using the coco evaluation code    .  implementation details for our s cnn rnn  we use  inception v       as the cnn subnet  and an lstm network  is used as rnn subnet. the number of lstm cells is       equalling to the dimension of the word embedding. the  output vocabulary size for sentence generation is       .  note that all these are exactly the same as the nic v        model ensuring a fair comparison. for semantic regularisation by deep supervision of image embedding layer  we  need to extract a set of semantic concepts training labels  from the vocubulary. to this end  we follow     and simply  use the       most frequent words in the captions  which  cover     of word occurrences. the ground truth labels for  a training image is defined as the words that appear at least  once in the   captions. for the cnn pretraining  we initially  just learn the prediction layer  and then tune all the parameters for        iterations with a batch size of    and learning rate of  e  . in parallel  the rnn model is pretrained  for           iterations with the ground truth semantic labels as image embedding. after both models are pretrained   the full model is fine tuned for         iterations.     . . experimental results  competitors five state of the art models are selected for  comparison  msrcap  the microsoft captivator     combines the bottom up based word generation model     with a  gated recurrent neural network      grnn  for image captioning. mrnn  the multimodal recurrent neural network       uses a multimodal layer to combine the cnn and rnn.  nicv   the nicv       is an improved version of the neural image caption generator     . it uses a better image  encoder inception v . in addition  scheduled sampling      and an ensemble of    models are used  both improved the  accuracy of captioning. neither is used in our model. v l   the v l model      use a cnn based attribute detector to     metric    c     b      c      c     b      c      c     b      c      c     b      c      meteor  c   c      rouge  c   c      c     cider  c      msrcap      mrnn       v l       nicv        att          .        .        .        .        .         .       .        .        .        .          .        .        .        .        .         .       .        .        .        .          .        .        .        .        .         .        .        .        .        .          .        .        .        .        .         .        .        .        .        .          .        .        .        .       .          .        .        .        .       .          .        .        .        .        .         .        .        .        .        .          .        .        .        .        .          .        .        .        .        .         ours     .         .         .         .         .         .         .         .         .         .         .         .         .         .        table  . results from the official ms coco testing server  https   www.codalab.org competitions      results .  the subscript indicates the ranking as on the submission date w.r.t. each metric.    firstly generate     attributes  and then feed as initial input to a lstm model to generate captions. att  the semantic attention model      uses both image features and  visual attributes  and introduces an attention mechanism to  reweight the attribute context to improve captioning accuracy. all five models use a cnn and a rnn  but only  nicv  does end to end training. in contrast  att does attention model and rnn joint training  and uses a   model  ensemble. there is no joint training for the other three.  results we submit our results to the official evaluation  server to compare with the five baselines which also appear  in the official ranking. the evaluation is done with both    and    reference captions  c  and c   . it can be seen from  table   that our model beats all five competitors on all     metrics  often by a significant margin. among the    submitted models  our model is ranked the  th and we could  not find references for the four higher ranked models. note  that our performance across all metrics is very consistent.  in contrast  the   competitors often do well on some metrics but very badly on others. it is worth pointing out that  our result is obtained without a model ensemble  a practice  commonly used in this type of benchmarking exercise  e.g.   both nicv  and att use ensembles . in addition  no auxiliary captioning data is used for training. this result thus  represents the state of the art. for qualitative results please  see the supplementary material.  ablation study we compare our full model with two  stripped down versions. nic f  removing the semantic  regularisation and use the cnn output feature layer as the  inference ie to rnn. this gives us the standard nic model       with the same inception v  as cnn subnet. the model  is finetuned end to end on coco. nic deeply  this model  is closer to ours   it uses the same deeply supervised semantic regularisation as our model  but the penultimate feature layer is taken as the embedding  rather than the prediction layer s . as a result  the cnn feature representation  benefits from the deep supervision  rather than distal supervision via the rnn   but the specific embedding used as  the rnn interface is not directly semantically meaningful.  the results on the validation set split are shown in table  .  it can be seen that      semantic regularisation is critical     e.g.  it brings about    on cider comparing nic f and  our full model.     the deep supervision is the most crucial  contributor to the good performance of our model. even  when the embedding layer is not semantically explicit as in  nic deeply  the benefit is evident. the smaller gap between  nic deeply and ours is due to the use of the semantically  explicit prediction layer as the embedding at the cnn rnn  interface.  metric    cider    meteor    rouge    b      nic f  nic deeply  ours     .      .      .        .      .      .        .      .      .        .      .      .       table  . ablation study results on the coco validation set split.    computational cost thanks to the semantic regularisation  the proposed model can be trained very efficiently. the  total training takes two days on a single nvidia titan x  gpu. in contrast training one of nic s    model ensemble members takes more than    days on the same gpu.  in particular  the deep supervision allows the model to converge very fast. for example  pretraining our inception v        cnn only needs        iterations with a batch size of    . the pretraining of the rnn model is also fast since its  inputs are ground truth labels. after the pretraining  the full  model fine tuning converges much faster than nicv .     . conclusion  we proposed a semantically regularised cnn rnn  model for image annotation. the semantic regularisation  makes the cnn rnn interface semantically meaningful   distributes the label prediction and correlation tasks between the cnn and rnn models  and importantly the deep  supervision makes training the full model more stable and  efficient. extensive evaluations on nus wide and mscoco demonstrate the efficacy of the proposed model on  both multi label classification and image captioning.     references      s. bengio  o. vinyals  n. jaitly  and n. shazeer. scheduled  sampling for sequence prediction with recurrent neural networks. in nips      .        l. c. chen  a. g. schwing  a. l. yuille  and r. urtasun.  learning deep structured models. in icml      .        x. chen  h. fang  t. lin  r. vedantam  s. gupta  p. dolla r   and c. l. zitnick. microsoft coco captions  data collection and evaluation server. corr  abs     .           .        k. cho  b. van merrie nboer  c. gulcehre  d. bahdanau   f. bougares  h. schwenk  and y. bengio. learning phrase  representations using rnn encoder decoder for statistical  machine translation. arxiv preprint arxiv     .          .         t. s. chua  j. tang  r. hong  h. li  z. luo  and y. t.  zheng. nus wide  a real world web image database from  national university of singapore. in civr      .        j. deng  n. ding  y. jia  a. frome  k. murphy  s. bengio   y. li  h. neven  and h. adam. large scale object classification using label relation graphs. in eccv      .        j. devlin  h. cheng  h. fang  s. gupta  l. deng  x. he   g. zweig  and m. mitchell. language models for image captioning  the quirks and what works. in acl      .           j. donahue  y. jia  o. vinyals  j. hoffman  n. zhang   e. tzeng  and t. darrell. decaf  a deep convolutional activation feature for generic visual recognition. in icml      .         h. fang  s. gupta  f. iandola  r. k. srivastava  l. deng   p. dolla r  j. gao  x. he  m. mitchell  j. c. platt  et al. from  captions to visual concepts and back. in cvpr      .               y. gong  y. jia  t. leung  a. toshev  and s. ioffe. deep  convolutional ranking for multilabel image annotation. arxiv  preprint arxiv     .          .            a. graves  a. mohamed  and g. hinton. speech recognition  with deep recurrent neural networks. in icassp      .         s. hochreiter and j. schmidhuber. long short term memory.  neural computation      .         h. hu  g. t. zhou  z. deng  z. liao  and g. mori. learning  structured inference neural networks with label relations. in  cvpr      .                     j. jin and h. nakayama. annotation order matters  recurrent image annotator for arbitrary length image tagging. in  icpr      .                        j. johnson  l. ballan  and l. fei fei. love thy neighbors   image annotation by exploiting image metadata. in iccv       .                        a. karpathy and l. fei fei. deep visual semantic alignments for generating image descriptions. in cvpr      .             d. p. kingma and m. welling. auto encoding variational  bayes. corr  abs     .          .         a. krizhevsky  i. sutskever  and g. e. hinton. imagenet  classification with deep convolutional neural networks. in  nips      .         c. y. lee  s. xie  p. gallagher  z. zhang  and z. tu. deeplysupervised nets. in aistats      .           q. li  m. qiao  w. bian  and d. tao. conditional graphical  lasso for multi label image classification. in cvpr      .         t. y. lin  m. maire  s. belongie  j. hays  p. perona  d. ramanan  p. dolla r  and c. l. zitnick. microsoft coco  common objects in context. in eccv      .            j. mao  w. xu  y. yang  j. wang  z. huang  and a. yuille.  deep captioning with multimodal recurrent neural networks   m rnn . in iclr      .                        s. reed  z. akata  x. yan  l. logeswaran  b. schiele  and  h. lee. generative adversarial text to image synthesis. in  icml      .         o. russakovsky  j. deng  h. su  j. krause  s. satheesh   s. ma  z. huang  a. karpathy  a. khosla  m. bernstein   a. c. berg  and l. fei fei. imagenet large scale visual  recognition challenge. ijcv      .            k. simonyan and a. zisserman. very deep convolutional  networks for large scale image recognition. in iclr      .          c. szegedy  s. ioffe  and v. vanhoucke. inception v    inception resnet and the impact of residual connections on  learning. arxiv preprint arxiv     .           .         c. szegedy  w. liu  y. jia  p. sermanet  s. reed   d. anguelov  d. erhan  v. vanhoucke  and a. rabinovich.  going deeper with convolutions. in cvpr      .            c. szegedy  v. vanhoucke  s. ioffe  j. shlens  and z. wojna.  rethinking the inception architecture for computer vision. in  cvpr      .               o. vinyals  a. toshev  s. bengio  and d. erhan. show and  tell  a neural image caption generator. in cvpr      .                         o. vinyals  a. toshev  s. bengio  and d. erhan. show and  tell  lessons learned from the      mscoco image captioning challenge. tpami      .                           j. wang  y. yang  j. mao  z. huang  c. huang  and w. xu.  cnn rnn  a unified framework for multi label image classification. in cvpr      .                        q. wu  c. shen  l. liu  a. dick  and a. van den hengel.  what value do explicit high level concepts have in vision to  language problems  in cvpr      .                  k. xu  j. ba  r. kiros  k. cho  a. courville  r. salakhutdinov  r. zemel  and y. bengio. show  attend and tell  neural  image caption generation with visual attention. in icml       .            x. yan  j. yang  k. sohn  and h. lee. attribute image  conditional image generation from visual attributes. in eccv       .            q. you  h. jin  z. wang  c. fang  and j. luo. image captioning with semantic attention. in cvpr      .               s. zheng  s. jayasumana  b. romera paredes  v. vineet   z. su  d. du  c. huang  and p. h. torr. conditional random fields as recurrent neural networks. in cvpr      .             b. zhou  y. tian  s. sukhbaatar  a. szlam  and r. fergus. simple baseline for visual question answering. arxiv  preprint arxiv     .           .       supplementary material for  semantic regularisation for recurrent image annotation    arxiv     .     v   cs.cv     nov          . qualitative results of multi label classification  qualitative results of multi label classification are shown in fig.  . the human row shows the ground truth annotation   we organise them in a rare first order  where rare classes are presented earlier than the frequent classes. the cnn tag k use  the model in      the prediction are sorted according to their prediction scores in a descending order. the last row shows the  results of our model  where the prediction order of rnn is preserved.    human  reflection  lake  water  clouds  sky  cnn tag k  sky  clouds  reflection  ours tag k  reflection  lake  clouds  water  sky    human  sun  beach  sunset  ocean  lake  water  clouds  sky  cnn tag k  sky  clouds  sun  ours tag k  sun  beach  sunset  ocean  lake  clouds  water    human  leaf  plants  cnn tag k  plants  flowers  sky  ours tag k  flowers  plants    human  person  cnn tag k  person  military  vehicle  ours tag k  person  military    human  clouds  sky  cnn tag k  sky  clouds  buildings  ours tag k  nighttime  clouds  sky    human  house  vehicle  window  water  cnn tag k  water  window  house  ours tag k  house  window  boats  water    human  umbrella  cup  diningtable  chair  person  cnn tag k  person  umbrella  chair  ours tag k  umbrella  cup  diningtable  chair  person    human  baseballbat  baseballglove  cellphone  person  cnn tag k  baseballbat  person  sportsball  ours tag k  baseballbat  baseballglove  person    human  parkingmeter  umbrella  truck  handbag    car  person  cnn tag k  person  umbrella  car  ours tag k  umbrella  handbag  car  person    human  sandwich  backpack  diningtable  chair    person  cnn tag k  person  chair  hotdog  ours tag k  sandwich  diningtable  person    human  skis  backpack  person  cnn tag k  person  skis  snowboard  ours tag k  skis  backpack  person    human  pizza  fork  knife  bottle  cup  diningtable    person  cnn tag k  diningtable  pizza  cup  ours tag k  pizza  fork  knife  cup  diningtable    figure  . qualitative results of multi label classification. the top   images are from the nus wide dataset  and the bottom   are from  ms coco.          the results show that our algorithm mostly make predictions follow the desired rare first order  thereby small classes are  promoted. it tends to give more specific results rather than focus on large general concepts as does cnn tag k. note that  for some images  our prediction is even more accurate than ground truth  due to the missed tagging in manual labelling.     . qualitative results of image captioning  in this section  we show some example captions of our model and the nic model    . the generated captions are shown  in fig  . compared with the nic model  our model is more accurate in recognising concepts  e.g.  objects  colour  status   counts etc.  thus being able to capture object interactions and describe an image with more detailed nouns and adjectives.  however  when the visual cue is compromised  our algorithm will also fail  as in the failure cases shown in fig  . novel  concept can also influence captioning. the last example in fig.   shows that novel object life guard station is beyond the  recognition ability of the algorithm  but it still manages to give a somewhat meaningful description.    nic   a bus that is sitting in the street .  ours  a red and white bus driving down a street .    nic  a group of people standing on top of a sandy  beach.  ours  a group of people standing on a beach with  surfboards.    nic  a group of giraffes standing in a field.  ours  a giraffe standing in a fenced in area.    nic  a man standing next to a brown horse.  ours  a man riding a horse in a field .    nic  a close up of a toaster on a wall.  ours  a close up of a pair of scissors .    nic  a white plate topped with a cut in half sandwich.  ours  a white plate topped with a sandwich and salad.    nic  a city street filled with lots of traffic.  ours  a bus driving down a street next to a  traffic light.    nic  a person laying on a bed with a laptop.  ours  a dog laying on a bed in a bedroom.    nic  a train traveling down tracks next to a forest.  ours  a train is traveling down the tracks in the snow.    nic  a man in a field with a frisbee.  ours  a couple of men playing frisbee in a field.    nic  a group of people riding bikes down a  street .  ours  a man riding a bike down a busy street .    nic  a black and white dog laying on a grass  covered field.  ours  a black and white dog playing with a frisbee.    figure  . qualitative results of image captioning on the ms coco dataset. the errors in captions are hightlighted in red  while the  fine grained detials are hightlighted in green.    