introduction              recurrent neural networks          .     standard rnn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          .     long short term memory . . . . . . . . . . . . . . . . . . . . . . . .          .     stacked rnns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .           .     multiplicative rnns . . . . . . . . . . . . . . . . . . . . . . . . . .           .     character prediction rnns . . . . . . . . . . . . . . . . . . . . . . .                    optimization           .     first order approximations . . . . . . . . . . . . . . . . . . . . . . .           .     deriving newton s method . . . . . . . . . . . . . . . . . . . . . . .           .     hessian free optimization . . . . . . . . . . . . . . . . . . . . . . . .           . .     conjugate gradient . . . . . . . . . . . . . . . . . . . . . . .           . .     gauss newton matrix . . . . . . . . . . . . . . . . . . . . .           . .     damping . . . . . . . . . . . . . . . . . . . . . . . . . . . .           . .     overview of hessian free optimization . . . . . . . . . . . .           . .     batch sizes . . . . . . . . . . . . . . . . . . . . . . . . . . .          preliminary experiments           .     overview of methods . . . . . . . . . . . . . . . . . . . . . . . . . .           .     damping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .           .     lstm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .           .     stacked mrnns . . . . . . . . . . . . . . . . . . . . . . . . . . . . .           .     multiplicative lstm . . . . . . . . . . . . . . . . . . . . . . . . . .           .     overview of results . . . . . . . . . . . . . . . . . . . . . . . . . . .          vii          full scale experiments           .     penn treebank  full set  experiments . . . . . . . . . . . . . . . . . .           . .     methods    . . . . . . . . . . . . . . . . . . . . . . . . . . . .           . .     results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .           . .     samples from model . . . . . . . . . . . . . . . . . . . . . .          wikipedia experiments . . . . . . . . . . . . . . . . . . . . . . . . .           . .     methods    . . . . . . . . . . . . . . . . . . . . . . . . . . . .           . .     results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .           . .     samples from model . . . . . . . . . . . . . . . . . . . . . .           . .     time lag experiment . . . . . . . . . . . . . . . . . . . . . .           .          conclusion               appendix           .     lstm r forward pass . . . . . . . . . . . . . . . . . . . . . . . . .           .     lstm backward pass . . . . . . . . . . . . . . . . . . . . . . . . . .           .     multiplicative lstm r forward pass . . . . . . . . . . . . . . . . . .           .     multiplicative lstm backward pass . . . . . . . . . . . . . . . . . .          bibliography          viii     chapter    introduction  feedforward neural networks are a powerful method for modelling static functions   that with enough hidden units can in theory model any function with arbitrary accuracy  hornik et al.       . however  when modelling sequences  a simple feedforward neural network must have a fixed context of past inputs that it can use to make  predictions. in order to use large contexts to make predictions  feed forward neural  networks must use many inputs  and therefore must be much larger. a much more  efficient and powerful way of using long time contexts is to give feedforward neural  networks recursive connections  making them into recurrent neural networks  rnns .  rnns have shown great potential to complex non linear sequences that require long  time range dependencies. however  there are many difficulties associated with training  rnns that make it hard to fully realize this potential. additionally  as neural networks  and datasets both become larger and highly parallelizable hardware such as gpus improve  it is becoming increasingly important for training to become more efficient via  parallelization than currently used methods. for these reasons  this study takes a deep  look at training recurrent neural networks with hessian free optimization  martens and  sutskever         which has shown potential to be both more effective and much more  efficient via parallelization than commonly used training methods. there are many  possible rnn architectures  some of which appear to be more expressive than others.  often  when one architecture outperforms another  it is difficult to know if the higher  performing architecture is truly more expressive  or if the other architecture simply underperformed because the fitting algorithm was not powerful enough. for this reason   this study compares several different architectural ideas using hessian free optimization for training  which is thought to be much less prone to under fitting than other  commonly used methods  martens       . some of these architectures have never             chapter  . introduction    been trained with hessian free optimization  while others are novel architectures that  have never been used  but likely would not be practical without the use of a powerful optimizer. for benchmarking these models  character level sequence modelling is  used  a task that requires using highly non linear transitions as well as long range dependencies to be successful. the goals of this study were to compare different ways of  implementing hessian free optimization as well as the expressiveness of several rnn  architectures  and compare the best combinations to state of the art results for rnn  character modelling.     chapter    recurrent neural networks  recurrent neural networks  rnns  are a powerful class of sequence modellers that  use recursive connections to store information about the past. the recursive nature of  the hidden states of rnns allows them to use a potentially unlimited context to make  predictions. while the theoretical potential of recurrent neural networks to solve sequence modelling tasks has long been known  recently  improvements in optimization  as well as computer hardware have allowed rnns to achieve successful results at many  supervised learning tasks. these achievements include state of the art results at speech  recognition  graves et al.         language modelling  mikolov and zweig         and  handwriting recognition  graves and schmidhuber        among many others.     .     standard rnn    the standard rnn architecture consists of one input vector i  one hidden vector h  and  one output vector o. it contains   weights matrices of connections  input to hidden   whi    hidden to output  woh    and recurrent hidden to hidden  whh    and contains a  vector of hidden state biases  bh  . additionally  the output units typically have the  softmax function applied to them to model a normalized probability distribution over  the output units  and the hidden units have a non linear squashing function such as the  hyperbolic tangent  tanh  applied. the rnn operates over a sequence of t timesteps   and predicts o t  using i t  as well as passed context as shown in the equations below.  h t    tanh bh  whi i t   whh h t            .      o t    softmax woh h t        .                 chapter  . recurrent neural networks    figure  .   a standard rnn    pseudocode for a recurrent neural network is also given in algorithm  .  for t  ...t do  h t    bh    h t    h t   whi i t    if t     then  h t    h t   whh h t        end  h t    tanh h t     o t    woh h t    o t    softmax o t     end  algorithm    recurrent neural network  an rnn can also be interpreted graphically with nodes to represent the state vectors  and edges to represent weight matrices  figure  .  .  the performance of a recurrent neural network in modelling probabilistic outputs  is evaluated by the cross entropy error between the rnns outputs o  and the target  outputs  .  e t       t  log o t        .        . . standard rnn         the partial derivatives of the cross entropy error with respect to the parameters  of the network      e          also known as the gradient or direction of steepest descent  can    be computed using the chain rule with the back propagation through time algorithm   which is given for a standard rnn with the following equations below  werbos       .    note that the     e    o t      t    o t        .      t   e   e      hout  t t   woh t    o t       .       e   e  t  e  t    woh   whh   hout  t    o t    hin  t           .       e   e      hin  t   hout  t       .           h t     h t      t   e   e      h t     t   whh t    hin  t       .      t   e   e      i t t   whi t    hin  t       .      operator is the hadamard product for element wise matrix multi     plication.  the most basic learning algorithm for an rnn would be to iteratively update the  weights by a small negative factor of the gradient during training  an algorithm known  as gradient descent. this can be done on single training cases of sequences segmented  into fixed lengths  as is the case with stochastic gradient decent. alternatively  training  can become much more computationally efficient by using mini batches  in which the  gradient is computed on several training cases simultaneously in parallel. it is also  possible to use full batch gradient descent  in which the gradients are computed on  the full dataset at once. while computationally efficient  this is generally impractical  because the change to the sum of the error surfaces over all examples usually cannot be  accurately approximated with a straight line  which is what gradient descent attempts to  do. furthermore  having stochasticity in training orders and updates provides training  advantages  and this becomes no longer possible.  in general  training an rnn with gradient descent can result in a training difficulty  known as the vanishing exploding gradient problem  hochreiter et al.         where  the gradient tends to decay or explode exponentially as it is back propagated through  time. it can be seen why this problem arises by considering the matrix of derivatives          chapter  . recurrent neural networks    of the hidden states at a given time point with respect to hidden states n time steps in  the past     hout  t    hout  t n  .  n     hout  t   t      whh  diag     h t   k    hout  t   n  k      h t   k        .       for a large n  this matrix of derivatives will tend to either explode or decay exponentially because it is the product of many matrices. this result makes it difficult for  rnns to learn to use long term dependencies in their predictions. when the gradient  vanishes  the updates to the weights will not help with learning long time lags because  this contribution to the gradient will be exponentially small  and when the gradient explodes learning becomes unstable. for this reason  more advanced architectures and or  learning algorithms are usually needed to train rnns on difficult problems.     .     long short term memory    long short term memory  lstm  is an rnn architecture designed to address some  of the gradient based learning problems with the standard rnn architecture  hochreiter and schmidhuber       . it addresses this by having memory cells that use soft  multiplicative gates to control information flow. in its original formulation  lstm had  input gates to control how much of the memory cell s total input is written to its internal state  and output gates to control how much of the memory cell s internal state is  output. the memory cell had a self recurrent weight of   preserving the internal state.  like in a standard recurrent neural network  the hidden state receives in inputs from  the input layer and the previous hidden state  hin  t    whi i t   whh hout  t           .       sometimes  a non linear squashing function is applied to hin  t . the input and  output gates    and    both also receive their own inputs  and typically use a sigmoid  function to squash their input between   and    allowing them to act as soft  differentiable gates.      t    sigmoid w i i t   w h hout  t            .         t    sigmoid w i i t   w h hout  t             .         . . long short term memory         the input gate controls how much of hin  t  is written to the lstm cell s internal  state  which has a self recurrent weight of  . the input gate allows the cell to ignore  irrelevant inputs.  hstate  t    hstate  t          t     hin  t       .       the output gate then controls how much of the memory cells internal state is output   to be squashed with a non linear function such as a hyperbolic tangent. the output gate  allows the lstm cell to keep information that is not relevant to the current output  but  may be relevant later.  hout  t    tanh hstate  t       .         t       o t    softmax woh hout  t        .       the linear connections between successive internal states allow hidden states to  be partially linear functions of their passed states  whereas in a standard rnn they  become highly non linear functions of their past states very quickly. additionally   because the internal state has a self recurrent weight of        hstate  t    hstate  t       contains a term    of    protecting against the vanishing gradient problem. in lstm s original learning  algorithm  exploding gradients were dealt with a type of derivative truncation in which   e   hout  t     was approximated as     e t    hout  t       thereby only allowing error to back propagate    though the lstm s internal state. by including this truncation  it was assured that   hstate  t    hstate  t    trunc         and the approximated gradients would never vanish or explode.    this also allowed for learning to occur completely online in real time by making it  possible to store     hstate  t       trunc    for all parameters of the network. this would be memory    intensive to do for a standard recurrent neural network  but due to the approximations  made to the gradient that force     hstate  t    hstate  t    trunc          it holds that     hstate  t    hstate  t        hstate  t            trunc         t   trunc      .       this recursion allows just one derivative value to be stored for every parameter in     allowing for online approximations to the gradient to be computed in a memory  efficient manner.  forget gates     were later added to lstm control how much of the memory cell s  previous internal state is remembered  in place of using a self recurrent weight of     gers et al.       . forget gates receive input and are squashed non linearly with a          chapter  . recurrent neural networks    sigmoid in the same fashion as other gates. their role in controlling information flow  in the memory cell is given below.    hstate  t      t     now     hstate  t    hstate  t       hstate  t          t     hin t       .       includes a term equal to   t  instead of    so the gradients once    again can vanish  but will usually do so very slowly if   is usually close to  . forget  gates proved to be helpful for learning  and made lstm more similar to standard  rnns. a diagram of information flow through an lstm memory cell  now including  forget gates  is presented in figure  . . note that in this diagram  passing through a  square represents going through a multiplicative gate  and the edges simply represent  the direction of information flow.    figure  .   lstm cell    there are several variants of lstm  and one of the simpler implementations based      . . long short term memory         on the equations explained above is given in pseudocode below.  for t  ...t do  hin  t    whi i t      t    w i i t      t    w i i t      t    w i i t    if t     then  hin  t    hin  t   whh hout  t          t      t   w h hout  t          t      t   w h hout  t          t      t   w h hout  t        end    t    sigmoid   t       t    sigmoid   t       t    sigmoid   t     hstate  t      t     hin  t      if t     then  hstate  t    hstate  t    hstate  t           t     end  hout  t    tanh hstate  t       t       o t    woh hout  t    o t    softmax o t     end  algorithm    long short term memory    when lstm was first invented  it was tested on a number of synthetic tasks that  had previously been unsolvable by rnns  designed to benchmark time lag capabilities. an example of one of these tasks is the marked addition task  hochreiter and  schmidhuber         where the rnn receives a boolean and a linear input at every  time step  and gives one linear output at the end of the sequence. the boolean input  will be set to   twice  and   for every other time step. the rnn must learn to ignore  the linear inputs when the boolean flag is set to    and at the final time step must output the sum of the   marked linear inputs when the boolean flag was on. lstm was  able to solve this task even when the sequence was    s of time steps long  meaning  that it was able to learn to store the relevant inputs for hundreds of time steps while           chapter  . recurrent neural networks    ignoring irrelevant inputs. this was significant because it demonstrated that this architecture was able to use information from a very long time context. lstm has also  been very successful at natural tasks  achieving state of the art results at many supervising learning problems including handwriting recognition  graves and schmidhuber          speech recognition  graves et al.         language modelling  zaremba et al.          and rnn character prediction  graves       .     .     stacked rnns    a commonly used strategy to increase the expressiveness of an rnn architecture is  to stack rnns sequentially  forming a sort of deep recurrent neural network hybrid   hermans and schrauwen          pascanu et al.       . this allows for a greater deal  of non linear processing to occur between seeing input i t  and outputting output o t .  it could also potentially allow for the rnn to store different time scales of information  at each layer. the basic architecture of a stacked rnn is given in figure  . .    figure  .   a stacked rnn    one drawback of this architecture is that gradients now have to be back propagated  through feed forward layers as well as through time  magnifying exploding and vanishing gradient problems. layer by layer training similar to what is commonly seen  in deep neural networks has been helpful in training stacked rnns  hermans and      . . multiplicative rnns          schrauwen       . another commonly used strategy to circumvent this is to give all  the stacked rnn layers direct input and output connections  pascanu et al.       . this  gives the rnn the flexibility of using varying degrees of non linear processing when  predicting output  intending to make training easier. the concept of stacked rnns can  be used with any rnn architecture  and is often found to improve results  hermans  and schrauwen          graves          graves et al.       .     .     multiplicative rnns    the multiplicative rnn is an architecture that was invented to allow varying transitions between hidden states depending on the input  sutskever et al.       . this is  accomplished through a matrix factorization that allows the weights in the hidden transition to vary with a great degree depending on the input. the hidden to hidden weight  matrix whh in a standard rnn is replaced by a factorization with intermediate state m.  whh   whm diag wmi i t  wmh      .       the effective hidden to hidden weight matrix that results from multiplying this out  can be very different for each input i t . for instance  the signs of effective hidden to  hidden weights can be positive with some inputs and negative with others. the full  equations for the architecture are given below.  h t    tanh bh  whi i t   whm diag wmi i t  wmh h t            .       o t    softmax woh h t        .       a diagram of a multiplicative rnn is given in figure  . .  multiplicative rnns are especially difficult to train with gradient descent because  of the high degree of curvature in their error surface  see more on this in next chapter  on optimization . for this reason  it is usually necessary to use  nd order methods to  train this architecture.     .     character prediction rnns    one of the more challenging sequence modelling tasks for an rnn is character prediction  which entails generating a probability distribution over a series of text characters.  the rnn can start with the first character of a string c    and predict p c     followed           chapter  . recurrent neural networks    figure  .   a multiplicative rnn    by p c   c     p c   c   c     p ct    c  ...ct    effectively generating a probability distribution over all possible sequences of characters. typically  the rnn will receive ct as  an input and try to predict ct   at each time step. performance is generally evaluated  by the cross entropy of some unseen text in units of bits per character  computed as     t         t  log  o t        .       t    this is representative of the theoretical minimum number of bits per character that  the sequence of text could be compressed to using the rnn as a probabilistic model.  while modelling language at the word level is certainly easier because the model starts  out with knowledge of the possible words  modelling language at the character level  presents the potential to use sub word segments to determine word meaning  as well as  to correctly handle punctuation. character level models are challenging to optimize   and push the limits of fitting algorithms  making them a good option for benchmarking  advanced optimization methods.  several experiments have been performed using rnn character level models. two  of the most widely benchmarked corpora have been the penn treebank corpus and  several variations of the wikipedia corpus. there have been a number of different  rnn architectures tested on both of these corpora  and some of the main results are  presented in tables  .  and  . .      . . character prediction rnns          architecture    test error number of parameters    rnn      .                mrnn      .                 stacked lstm      .                 stacked rnn      .                table  .   rnn experiments on penn treebank corpus with test error given in bits char     rnn  pascanu et al.           mrnn  mikolov et al.           stacked lstm using    weight noise regularization and dynamic evaluation  graves           stacked rnn   pascanu et al.           architecture    test error number of parameters    mrnn      .                 mrnn      .                 stacked rnn      .                 stacked lstm      .                  stacked lstm      .                  table  .   rnn experiments on wikipedia corpus  with test error given in bits char     mrnn using split of     million character subset for both training and testing  with xml    cut out of dataset  sutskever et al.           mrnn using full  .  billion character training set     million character subset for testing  with xml cut out of dataset  sutskever  et al.           stacked rnn using full  .  billion character training set     million character subset for testing  with xml cut out of dataset  hermans and schrauwen            stacked lstm using split of     million character subset for both training and testing     with xml included in dataset  graves           stacked lstm using split of     million character subset for both training and testing  with xml included in dataset  using  dynamic evaluation  graves             chapter    optimization  as stated in the previous chapter  there are certain difficulties in optimizing rnns due  to exponentially exploding and decaying gradients. a sensible solution to this problem  is to either directly or indirectly use second order information in the optimization process. this will increase the step size in directions of low curvature  and decrease the  step size in directions of high curvature  allowing for a much finer convergence. one  reason that this is a sensible way to deal with the vanishing and exploding gradient  problems is that the second derivatives are very likely to decay or explode at a similar  rate to the first derivatives  martens         so the step sizes taken by a second order  algorithm should naturally amplify decaying gradients and reduce exploding gradients  appropriately.     .     first order approximations    some simpler methods that indirectly use second order information involve slight modifications to first order algorithms to better control the step size in the direction of the  gradient. one straightforward way of doing this is momentum  which is designed to  accelerate convergence along valleys in the error surface by using a factor of previous  updates as part of the current update. there are a number of ways to use momentum   but the classical way is given by the following equations  polyak         vt      vt      f   t        .       t      t   vt        .      where   is the momentum constant    is the learning rate  and   f   t   is the gradient.  when the gradient is small but consistent  meaning the second derivatives are low  us             chapter  . optimization    ing momentum will amplify updates to account for this. another way to indirectly  use second order information that has been found to work especially well for rnns  is gradient norm clipping. in this method  when the norm of the gradient exceeds a  predefined threshold  the gradient is normalized so that the norm equals this threshold  mikolov       . the idea behind this method is that if the gradient of an rnn  is too high  it is likely the result of an exploding gradient and high second derivatives. therefore in these cases  it makes sense to reduce the updates to account for  the high curvature. first order methods that indirectly account for curvature can work  well when the curvature is not too extreme. however  there are certain cases where  modifying the step size in the direction of the gradient is not enough to find a good  solution in a reasonable amount of time. to illustrate this  consider the multiplicative rnn  mrnn  architecture from the previous chapter  where the hidden to hidden  transitions are given by  whh   whm diag wmi i t  wmh      .      with this hidden to hidden transition  many weights are being multiplied together  and  a small change to one weight could greatly affect the gradients of many other weights  in varying ways. this phenomenon is known as the pathological curvature problem  of neural networks  martens         and it is especially severe in mrnns. first order algorithms rely on the gradient direction being constant enough that it is possible  to make reasonably significant step sizes in the direction of the gradient without the  gradient completely changing. however  in an mrnn  even taking a very small step  size in the direction of the gradient may cause the direction of steepest descent to completely change  making the original gradient no longer a viable search direction. while  gradient descent could in theory train an mrnn with very small step sizes  this could  take an impractically large amount of time. in order to deal with this  it is advisable  to use second order methods that account for the full curvature matrix  or atleast some  approximation to it  when computing updates. this allows the local error surface to be  approximated with a quadratic bowl rather than a line  allowing for the approximation  to be valid for a much larger radius when the curvature is high.     .     deriving newton s method    second order methods are derived from the second order taylor series local approximation of the error as a function of the change in parameters.      . . hessian free optimization      t h f        f                t  f      f                  .      where   is a vector of all the parameters of the network  f is the error as a function  of these parameters   f    is the gradient  and h f     is the hessian matrix. solving  for the minimum by taking the derivative with respect to    and setting this equal to    yields equation  . .        h f        f         .      the most straightforward optimization method that can be derived from this equation is newton s method  which iteratively uses equation  .  as an update rule until  the function has converged.          h f         f         .       .      hessian free optimization    while simple and straightforward  newton s method would not be computationally  feasible to train large neural networks. for a network with n weights  storing the hessian would be o n     memory  and inverting it would be o n     computation. large  neural networks often have millions of weights  making this training method impractical. a solution to this problem is to use an iterative truncated newton s methods that  can compute  a   b for some matrix a and some vector b using only matrix vector  products  a technique that has been well studied in the optimization community  nocedal and wright         and recently introduced to machine learning. in the case of  neural networks  methods have been developed to efficiently compute hessian vector  products exactly with out ever having to store h  pearlmutter       . these methods  generally would require n iterations to compute  a   b exactly when a is a dense  nxn matrix and b is an nx  vector. however  when a is sparse or low rank  which  is generally the case for the hessian of neural networks   a   b can be approximated  well in far fewer than n steps.     . .     conjugate gradient    one of the most commonly used of such iterative methods is the conjugate gradient  method. the conjugate gradient method can be contrasted to a simple steepest descent           chapter  . optimization    based method for solving a quadratic in the same form as the second order taylor  expansion  xt ax    bt x   c    .       a steepest descent optimizer of this function would always step in the direction of  q x       the negative gradient  and would have an update rule of   x   x    ax   b       .      however  due to the off diagonal elements in a  the update directions will partially  oppose previous updates  resulting in slower convergence. to prevent this  conjugate  gradient ensures that each search direction si is conjugate with respect to the previous  search direction si   by adding an additional term to si . conjugacy would be achieved  by setting search directions to satisfy this condition   sit asi            .      to enforce this  si can be set equal to the negative gradient of the quadratic plus a  scalar factor   of the previous search direction that can be solved for to ensure conjugacy. the search direction will be in the form  si     ax   b     si        .       solving for   to ensure conjugacy    t     ax   b t asi      si    asi            .       t   si    asi      ax   b t asi     ax   b t asi        t as  si    i        .       .       for each search direction s  the step size   that minimizes the quadratic q must  also be solved for.   x    s t a x    s     bt  x    s      expanding this and eliminating terms without   yields the following term  q x    s        st as             st ax   bt s        .         . . hessian free optimization          differentiating with respecting   and setting to   yields     st as      st ax   bt s         st  ax   b         st as       .       .       it can be shown that combining updates with these step sizes and directions will  continually produce updates that are conjugate to all previous updates  shewchuk        . the full conjugate gradient algorithm is given below.  x   some initial guess    s     ax   b    initial search direction equal to negative derivative of quadratic  for i  ...n do         st  ax b      st as    x   x    s          ax b t as     st as    s     ax   b     s   end  algorithm    conjugate gradient    note that the above derivation of conjugate gradient   gibiansky         is somewhat different from  but mathematically equivalent to the form that is typically presented. the conjugate gradient method has several advantages over other iterative  methods for solving linear systems  including being able to make use of a good initial  guess  and not requiring memory storage from previous iterations.  another important detail of applying conjugate gradient to neural networks is deciding when to terminate. the conjugate gradient algorithm in theory could take n  iterations to fully converge for a network with n parameters  however  because the  curvature matrix will almost always be sparse and or low rank  most conjugate gradient methods applied to neural networks are limited to        iterations. additionally  various heuristics can be used that terminate conjugate gradient early if progress  in optimizing the quadratic approximation and or neural network error objective has  nearly stopped. a commonly used heuristic is to terminate when the progress made  in optimizing the quadratic after k iterations is less than some tolerance   per iteration           chapter  . optimization     martens       .   q i    q i   k       k  q i      . .       .       gauss newton matrix    one potential limitation of the conjugate gradient method is that the curvature matrix  must be positive semi definite  or else the algorithm may converge to a maximum or a  saddle point rather than a minimum. for this reason  an approximation to the hessian  matrix that only includes positive curvature known as the gauss newton matrix is used  in place of the hessian. note that from this point on  h  will refer to the hessian of  the loss function  and any other 