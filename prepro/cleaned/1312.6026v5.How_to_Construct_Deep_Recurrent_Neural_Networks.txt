introduction    recurrent neural networks  rnn  see  e.g.  rumelhart et al.        have recently become a popular  choice for modeling variable length sequences. rnns have been successfully used for various task  such as language modeling  see  e.g.  graves        pascanu et al.      a  mikolov        sutskever  et al.         learning word embeddings  see  e.g.  mikolov et al.      a   online handwritten recognition  graves et al.        and speech recognition  graves et al.       .  in this work  we explore deep extensions of the basic rnn. depth for feedforward models can  lead to more expressive models  pascanu et al.      b   and we believe the same should hold for  recurrent models. we claim that  unlike in the case of feedforward neural networks  the depth of an  rnn is ambiguous. in one sense  if we consider the existence of a composition of several nonlinear  computational layers in a neural network being deep  rnns are already deep  since any rnn can  be expressed as a composition of multiple nonlinear layers when unfolded in time.  schmidhuber         el hihi and bengio        earlier proposed another way of building a deep  rnn by stacking multiple recurrent hidden states on top of each other. this approach potentially allows the hidden state at each level to operate at different timescale  see  e.g.  hermans and  schrauwen       . nonetheless  we notice that there are some other aspects of the model that may  still be considered shallow. for instance  the transition between two consecutive hidden states at  a single level is shallow  when viewed separately.this has implications on what kind of transitions  this model can represent as discussed in section  . . .  based on this observation  in this paper  we investigate possible approaches to extending an rnn  into a deep rnn. we begin by studying which parts of an rnn may be considered shallow. then         for each shallow part  we propose an alternative deeper design  which leads to a number of deeper  variants of an rnn. the proposed deeper variants are then empirically evaluated on two sequence  modeling tasks.  the layout of the paper is as follows. in section   we briefly introduce the concept of an rnn. in  section   we explore different concepts of depth in rnns. in particular  in section  . .   . .  we  propose two novel variants of deep rnns and evaluate them empirically in section   on two tasks   polyphonic music prediction  boulanger lewandowski et al.        and language modeling. finally  we discuss the shortcomings and advantages of the proposed models in section  .         recurrent neural networks    a recurrent neural network  rnn  is a neural network that simulates a discrete time dynamical  system that has an input xt   an output yt and a hidden state ht . in our notation the subscript t  represents time. the dynamical system is defined by  ht   fh  xt   ht      yt   fo  ht                   where fh and fo are a state transition function and an output function  respectively. each function is  parameterized by a set of parameters    h and   o .  n    on   n    n    n    n   given a set of n training sequences d     x    y     . . .    xtn   ytn      the parameters  n    of an rnn can be estimated by minimizing the following cost function   j        n      n      n     n tn    xx   n    n   d yt   fo  ht      n n   t              n     where ht   fh  xt   ht     and h     . d a  b  is a predefined divergence measure between a  and b  such as euclidean distance or cross entropy.   .     conventional recurrent neural networks    a conventional rnn is constructed by defining the transition function and the output function as     ht   fh  xt   ht        h w  ht     u  xt             yt   fo  ht   xt      o v ht         where w  u and v are respectively the transition  input and output matrices  and  h and  o are  element wise nonlinear functions. it is usual to use a saturating nonlinear function such as a logistic  sigmoid function or a hyperbolic tangent function for  h . an illustration of this rnn is in fig.     a .  the parameters of the conventional rnn can be estimated by  for instance  stochastic gradient descent  sgd  algorithm with the gradient of the cost function in eq.     computed by backpropagation  through time  rumelhart et al.       .        .     deep recurrent neural networks  why deep recurrent neural networks     deep learning is built around a hypothesis that a deep  hierarchical model can be exponentially  more efficient at representing some functions than a shallow one  bengio       . a number of  recent theoretical results support this hypothesis  see  e.g.  le roux and bengio        delalleau  and bengio        pascanu et al.      b . for instance  it has been shown by delalleau and bengio         that a deep sum product network may require exponentially less units to represent the same  function compared to a shallow sum product network. furthermore  there is a wealth of empirical  evidences supporting this hypothesis  see  e.g.  goodfellow et al.        hinton et al.      b a .  these findings make us suspect that the same argument should apply to recurrent neural networks.         .     depth of a recurrent neural network  yt      yt    ht      xt      ht    xt    yt      ht      xt      figure    a conventional recurrent neural network unfolded in time.  the depth is defined in the case of feedforward neural networks as having multiple nonlinear layers  between input and output. unfortunately this definition does not apply trivially to a recurrent neural  network  rnn  because of its temporal structure. for instance  any rnn when unfolded in time as  in fig.   is deep  because a computational path between the input at time k   t to the output at time  t crosses several nonlinear layers.  a close analysis of the computation carried out by an rnn  see fig.    a   at each time step individually  however  shows that certain transitions are not deep  but are only results of a linear projection  followed by an element wise nonlinearity. it is clear that the hidden to hidden  ht     ht    hiddento output  ht   yt   and input to hidden  xt   ht   functions are all shallow in the sense that there  exists no intermediate  nonlinear hidden layer.  we can now consider different types of depth of an rnn by considering those transitions separately.  we may make the hidden to hidden transition deeper by having one or more intermediate nonlinear  layers between two consecutive hidden states  ht   and ht  . at the same time  the hidden tooutput function can be made deeper  as described previously  by plugging  multiple intermediate  nonlinear layers between the hidden state ht and the output yt . each of these choices has a different  implication.   . .     deep input to hidden function    a model can exploit more non temporal structure from the input by making the input to hidden  function deep. previous work has shown that higher level representations of deep networks tend to  better disentangle the underlying factors of variation than the original input  goodfellow et al.         glorot et al.      b  and flatten the manifolds near which the data concentrate  bengio et al.       .  we hypothesize that such higher level representations should make it easier to learn the temporal  structure between successive time steps because the relationship between abstract features can generally be expressed more easily. this has been  for instance  illustrated by the recent work  mikolov  et al.      b  showing that word embeddings from neural language models tend to be related to their  temporal neighbors by simple algebraic relationships  with the same type of relationship  adding a  vector  holding over very different regions of the space  allowing a form of analogical reasoning.  this approach of making the input to hidden function deeper is in the line with the standard practice  of replacing input with extracted features in order to improve the performance of a machine learning  model  see  e.g.  bengio       . recently  chen and deng        reported that a better speech  recognition performance could be achieved by employing this strategy  although they did not jointly  train the deep input to hidden function together with other parameters of an rnn.   . .     deep hidden to output function    a deep hidden to output function can be useful to disentangle the factors of variations in the hidden  state  making it easier to predict the output. this allows the hidden state of the model to be more  compact and may result in the model being able to summarize the history of previous inputs more  efficiently. let us denote an rnn with this deep hidden to output function a deep output rnn   do rnn .  instead of having feedforward  intermediate layers between the hidden state and the output   boulanger lewandowski et al.        proposed to replace the output layer with a conditional gen      yt    yt    yt    yt    yt    z t      ht      ht  xt     a  rnn    ht    ht      ht    ht      xt     b  dt rnn    xt     b   dt s  rnn    ht      ht  xt     c  dot rnn    ht      zt    ht  xt     d  stacked rnn    figure    illustrations of four different recurrent neural networks  rnn .  a  a conventional rnn.   b  deep transition  dt  rnn.  b   dt rnn with shortcut connections  c  deep transition  deep  output  dot  rnn.  d  stacked rnn  erative model such as restricted boltzmann machines or neural autoregressive distribution estimator  larochelle and murray       . in this paper we only consider feedforward intermediate layers.   . .     deep hidden to hidden transition    the third knob we can play with is the depth of the hidden to hidden transition. the state transition  between the consecutive hidden states effectively adds a new input to the summary of the previous  inputs represented by the fixed length hidden state. previous work with rnns has generally limited  the architecture to a shallow operation  affine transformation followed by an element wise nonlinearity. instead  we argue that this procedure of constructing a new summary  or a hidden state  from  the combination of the previous one and the new input should be highly nonlinear. this nonlinear  transition could allow  for instance  the hidden state of an rnn to rapidly adapt to quickly changing  modes of the input  while still preserving a useful summary of the past. this may be impossible to  be modeled by a function from the family of generalized linear models. however  this highly nonlinear transition can be modeled by an mlp with one or more hidden layers which has an universal  approximator property  see  e.g.  hornik et al.       .  an rnn with this deep transition will be called a deep transition rnn  dt rnn  throughout remainder of this paper. this model is shown in fig.    b .  this approach of having a deep transition  however  introduces a potential problem. as the introduction of deep transition increases the number of nonlinear steps the gradient has to traverse  when propagated back in time  it might become more difficult to train the model to capture longterm dependencies  bengio et al.       . one possible way to address this difficulty is to introduce  shortcut connections  see  e.g.  raiko et al.        in the deep transition  where the added shortcut  connections provide shorter paths  skipping the intermediate layers  through which the gradient is  propagated back in time. we refer to an rnn having deep transition with shortcut connections by  dt s  rnn  see fig.    b   .  furthermore  we will call an rnn having both a deep hidden to output function and a deep transition a deep output  deep transition rnn  dot rnn . see fig.    c  for the illustration of dot rnn.  if we consider shortcut connections as well in the hidden to hidden transition  we call the resulting  model dot s  rnn.  an approach similar to the deep hidden to hidden transition has been proposed recently by pinheiro  and collobert        in the context of parsing a static scene. they introduced a recurrent convolutional neural network  rcnn  which can be understood as a recurrent network whose the transition  between consecutive hidden states  and input to hidden state  is modeled by a convolutional neural  network. the rcnn was shown to speed up scene parsing and obtained the state of the art result  in stanford background and sift flow datasets. ko and dieter        proposed deep transitions  for gaussian process models. earlier  valpola and karhunen        used a deep neural network to  model the state transition in a nonlinear  dynamical state space model.   . .     stack of hidden states    an rnn may be extended deeper in yet another way by stacking multiple recurrent hidden layers  on top of each other  schmidhuber        el hihi and bengio        jaeger        graves       .        we call this model a stacked rnn  srnn  to distinguish it from the other proposed variants. the  goal of a such model is to encourage each recurrent level to operate at a different timescale.  it should be noticed that the dt rnn and the srnn extend the conventional  shallow rnn in  different aspects. if we look at each recurrent level of the srnn separately  it is easy to see that  the transition between the consecutive hidden states is still shallow. as we have argued above  this  limits the family of functions it can represent. for example  if the structure of the data is sufficiently  complex  incorporating a new input frame into the summary of what had been seen up to now might  be an arbitrarily complex function. in such a case we would like to model this function by something  that has universal approximator properties  as an mlp. the model can not rely on the higher layers  to do so  because the higher layers do not feed back into the lower layer. on the other hand  the  srnn can deal with multiple time scales in the input sequence  which is not an obvious feature of  the dt rnn. the dt rnn and the srnn are  however  orthogonal in the sense that it is possible to  have both features of the dt rnn and the srnn by stacking multiple levels of dt rnns to build  a stacked dt rnn which we do not explore more in this paper.   .     formal descriptions of deep rnns    here we give a more formal description on how the deep transition recurrent neural network  dtrnn  and the deep output rnn  do rnn  as well as the stacked rnn are implemented.   . .     deep transition rnn    we noticed from the state transition equation of the dynamical system simulated by rnns in eq.      that there is no restriction on the form of fh . hence  we propose here to use a multilayer perceptron  to approximate fh instead.  in this case  we can implement fh by l intermediate layers such that        ht   fh  xt   ht        h wl   l   wl     l            w   ht     u  xt                 where  l and wl are the element wise nonlinear function and the weight matrix for the l th layer.  this rnn with a multilayered transition function is a deep transition rnn  dt rnn .  an illustration of building an rnn with the deep state transition function is shown in fig.    b .  in the illustration the state transition function is implemented with a neural network with a single  intermediate layer.  this formulation allows the rnn to learn a non trivial  highly nonlinear transition between the  consecutive hidden states.   . .     deep output rnn    similarly  we can use a multilayer perceptron with l intermediate layers to model the output function  fo in eq.     such that              yt   fo  ht      o vl   l   vl     l            v   ht     where  l and vl are the element wise nonlinear function and the weight matrix for the l th layer.  an rnn implementing this kind of multilayered output function is a deep output recurrent neural  network  do rnn .  fig.    c  draws a deep output  deep transition rnn  dot rnn  implemented using both the deep  transition and the deep output with a single intermediate layer each.   . .     stacked rnn    the stacked rnn  schmidhuber        el hihi and bengio        has multiple levels of transition  functions defined by         l    l    l      l    l    l     ht   fh  ht    ht        h wl  ht     u      l ht           l     where ht is the hidden state of the l th level at time t. when l      the state is computed using xt   l     instead of ht  . the hidden states of all the levels are recursively computed from the bottom level  l    .  once the top level hidden state is computed  the output can be obtained using the usual formulation in eq.    . alternatively  one may use all the hidden states to compute the output  hermans  and schrauwen       . each hidden state at each level may also be made to depend on the input  as well  graves       . both of them can be considered approaches using shortcut connections  discussed earlier.  the illustration of this stacked rnn is in fig.    d .         another perspective  neural operators    in this section  we briefly introduce a novel approach with which the already discussed deep transition  dt  and or deep output  do  recurrent neural networks  rnn  may be built. we call this  approach which is based on building an rnn with a set of predefined neural operators  an operatorbased framework.  in the operator based framework  one first defines a set of operators of which each is implemented  by a multilayer perceptron  mlp . for instance  a plus operator   may be defined as a function  receiving two vectors x and h and returning the summary h  of them   h    x   h   where we may constrain that the dimensionality of h and h  are identical. additionally  we can  define another operator b which predicts the most likely output symbol x  given a summary h  such  that  x    bh  it is possible to define many other operators  but in this paper  we stick to these two operators which  are sufficient to express all the proposed types of rnns.  yt       ht      ht  xt    figure    a view of an rnn under the  operator based framework    and b are  the plus and predict operators  respectively.    it is clear to see that the plus operator   and the predict  operator b correspond to the transition function and the  output function in eqs.        . thus  at each step  an  rnn can be thought as performing the plus operator to  update the hidden state given an input  ht   xt   ht      and then the predict operator to compute the output  yt    bht   b xt   ht     . see fig.   for the illustration of  how an rnn can be understood from the operator based  framework.  each operator can be parameterized as an mlp with one  or more hidden layers  hence a neural operator  since we  cannot simply expect the operation will be linear with  respect to the input vector s . by using an mlp to implement the operators  the proposed deep transition  deep  output rnn  dot rnn  naturally arises.    this framework provides us an insight on how the constructed rnn be regularized. for instance  one may regularize the model such that the plus operator    is commutative. however  in this paper  we do not explore further on this approach.  note that this is different from  mikolov et al.      a  where the learned embeddings of words  happened to be suitable for algebraic operators. the operator based framework proposed here is  rather geared toward learning these operators directly.         experiments    we train four types of rnns described in this paper on a number of benchmark datasets to evaluate  their performance. for each benchmark dataset  we try the task of predicting the next symbol.        the task of predicting the next symbol is equivalent to the task of modeling the distribution over a  sequence. for each sequence  x    . . .   xt    we decompose it into  p x    . . .   xt     p x       t  y    p xt   x    . . .   xt         t      and each term on the right hand side will be replaced with a single timestep of an rnn. in this  setting  the rnn predicts the probability of the next symbol xt in the sequence given the all previous  symbols x    . . . xt   . then  we train the rnn by maximizing the log likelihood.  we try this task of modeling the joint distribution on three different tasks  polyphonic music prediction  character level and word level language modeling.  we test the rnns on the task of polyphonic music prediction using three datasets which are nottingham  jsb chorales and musedata  boulanger lewandowski et al.       . on the task of characterlevel and word level language modeling  we use penn treebank corpus  marcus et al.       .   .     model descriptions    we compare the conventional recurrent neural network  rnn   deep transition rnn with shortcut  connections in the transition mlp  dt s  rnn   deep output transition rnn with shortcut connections in the hidden to hidden transition mlp  dot s  rnn  and stacked rnn  srnn . see fig.     a   d  for the illustrations of these models.    notthingam  music    jsb chorales  musedata  char level    language    word level      units    parameters    units    parameters    units    parameters    units    parameters    units    parameters    rnn    dt s  rnn    dot s  rnn            k         k          k          k        .  m                k              k              k              k            .  m                    k                  k                  k                  k                .  m    srnn    layers          k          k           k          k        .  m    table    the sizes of the trained models. we provide the number of hidden units as well as the total  number of parameters. for dt s  rnn  the two numbers provided for the number of units mean  the size of the hidden state and that of the intermediate layer  respectively. for dot s  rnn  the  three numbers are the size of the hidden state  that of the intermediate layer between the consecutive  hidden states and that of the intermediate layer between the hidden state and the output layer. for  srnn  the number corresponds to the size of the hidden state at each level  the size of each model is chosen from a limited set                           to minimize the validation error for each polyphonic music task  see table.   for the final models . in the case of  language modeling tasks  we chose the size of the models from            and            for  word level and character level tasks  respectively. in all cases  we use a logistic sigmoid function as  an element wise nonlinearity of each hidden unit. only for the character level language modeling  we used rectified linear units  glorot et al.      a  for the intermediate layers of the output function   which gave lower validation error.   .     training    we use stochastic gradient descent  sgd  and employ the strategy of clipping the gradient proposed  by pascanu et al.      a . training stops when the validation cost stops decreasing.  polyphonic music prediction  for nottingham and musedata datasets we compute each gradient  step on subsequences of at most     steps  while we use subsequences of    steps for jsb chorales.        we do not reset the hidden state for each subsequence  unless the subsequence belongs to a different  song than the previous subsequence.  the cutoff threshold for the gradients is set to  . the hyperparameter for the learning rate schedule   is tuned manually for each dataset. we set the hyperparameter   to      for nottingham       for  musedata and     for jsb chroales. they correspond to two epochs  a single epoch and a third of  an epoch  respectively.  the weights of the connections between any pair of hidden layers are sparse  having only    nonzero incoming connections per unit  see  e.g.  sutskever et al.       . each weight matrix is rescaled  to have a unit largest singular value  pascanu et al.      a . the weights of the connections between  the input layer and the hidden state as well as between the hidden state and the output layer are  initialized randomly from the white gaussian distribution with its standard deviation fixed to  .   and  .    respectively. in the case of deep output functions  dot s  rnn   the weights of the  connections between the hidden state and the intermediate layer are sampled initially from the white  gaussian distribution of standard deviation  .  . in all cases  the biases are initialized to  .  to regularize the models  we add white gaussian noise of standard deviation  .    to each weight  parameter every time the gradient is computed  graves       .  language modeling  we used the same strategy for initializing the parameters in the case of language modeling. for character level modeling  the standard deviations of the white gaussian distributions for the input to hidden weights and the hidden to output weights  we used  .   and  .      respectively  while those hyperparameters were both  .  for word level modeling. in the case of  dot s  rnn  we sample the weights of between the hidden state and the rectifier intermediate  layer of the output function from the white gaussian distribution of standard deviation  .  . when  using rectifier units  character based language modeling  we fix the biases to  . .  in language modeling  the learning rate starts from an initial value and is halved each time the validation cost does not decrease significantly  mikolov et al.       . we do not use any regularization  for the character level modeling  but for the word level modeling we use the same strategy of adding  weight noise as we do with the polyphonic music prediction.  for all the tasks  polyphonic music prediction  character level and word level language modeling    the stacked rnn and the dot s  rnn were initialized with the weights of the conventional rnn  and the dt s  rnn  which is similar to layer wise pretraining of a feedforward neural network  see   e.g.  hinton and salakhutdinov       . we use a ten times smaller learning rate for each parameter  that was pretrained as either rnn or dt s  rnn.    notthingam  jsb chorales  musedata    rnn   .      .      .       dt s  rnn   .      .      .       dot s  rnn   .      .      .       srnn   .      .      .       dot s  rnn    .     .     .      table    the performances of the four types of rnns on the polyphonic music prediction. the  numbers represent negative log probabilities on test sequences.     we obtained these results using  dot s  rnn with lp units in the deep transition  maxout units in the deep output function and  dropout  gulcehre et al.       .   .     result and analysis     . .     polyphonic music prediction    the log probabilities on the test set of each data are presented in the first four columns of tab.  . we  were able to observe that in all cases one of the proposed deep rnns outperformed the conventional   shallow rnn. though  the suitability of each deep rnn depended on the data it was trained on.  the best results obtained by the dt s  rnns on notthingam and jsb chorales are close to  but       we use at each update     the following learning rate                  max                   where    and   indicate respec     tively when the learning rate starts decreasing and how quickly the learning rate decreases. in the experiment   we set    to coincide with the time when the validation error starts increasing for the first time.          worse than the result obtained by rnns trained with the technique of fast dropout  fd  which are   .   and  .    respectively  bayer et al.       .  in order to quickly investigate whether the proposed deeper variants of rnns may also benefit from  the recent advances in feedforward neural networks  such as the use of non saturating activation  functions  and the method of dropout. we have built another set of dot s  rnns that have the  recently proposed lp units  gulcehre et al.        in deep transition and maxout units  goodfellow  et al.        in deep output function. furthermore  we used the method of dropout  hinton et al.       b  instead of weight noise during training. similarly to the previously trained models  we  searched for the size of the models as well as other learning hyperparameters that minimize the  validation performance. we  however  did not pretrain these models.  the results obtained by the dot s  rnns having lp and maxout units trained with dropout are  shown in the last column of tab.  . on every music dataset the performance by this model is significantly better than those achieved by all the other models as well as the best results reported with  recurrent neural networks in  bayer et al.       . this suggests us that the proposed variants of deep  rnns also benefit from having non saturating activations and using dropout  just like feedforward  neural networks. we reported these results and more details on the experiment in  gulcehre et al.        .  we  however  acknowledge that the model free state of the art results for the both datasets were  obtained using an rnn combined with a conditional generative model  such as restricted boltzmann machines or neural autoregressive distribution estimator  larochelle and murray         in  the output  boulanger lewandowski et al.       .    character level  word level    rnn   .        .     dt s  rnn   .        .     dot s  rnn   .        .     srnn   .        .         .                 .             table    the performances of the four types of rnns on the tasks of language modeling. the  numbers represent bit per character and perplexity computed on test sequence  respectively  for  the character level and word level modeling tasks.   the previous current state of the art results  obtained with shallow rnns.   the previous current state of the art results obtained with rnns  having long short term memory units.   . .     language modeling    on tab.    we can see the perplexities on the test set achieved by the all four models. we can clearly  see that the deep rnns  dt s  rnn  dot s  rnn and srnn  outperform the conventional  shallow rnn significantly. on these tasks dot s  rnn outperformed all the other models  which  suggests that it is important to have highly nonlinear mapping from the hidden state to the output in  the case of language modeling.  the results by both the dot s  rnn and the srnn for word level modeling surpassed the previous  best performance achieved by an rnn with      long short term memory  lstm  units  graves         as well as that by a shallow rnn with a larger hidden state  mikolov et al.         even when  both of them used dynamic evaluation  . the results we report here are without dynamic evaluation.  for character level modeling the state of the art results were obtained using an optimization method  hessian free with a specific type of rnn architecture called mrnn  mikolov et al.      a  or  a regularization technique called adaptive weight noise  graves       . our result  however  is  better than the performance achieved by conventional  shallow rnns without any of those advanced     note that it is not trivial to use non saturating activation functions in conventional rnns  as this may cause  the explosion of the activations of hidden states. however  it is perfectly safe to use non saturating activation  functions at the intermediate layers of a deep rnn with deep transition.     reported by mikolov et al.      a  using mrnn with hessian free optimization technique.     reported by mikolov et al.        using the dynamic evaluation.     reported by graves        using the dynamic evaluation and weight noise.     dynamic evaluation refers to an approach where the parameters of a model are updated as the validation test data is predicted.          regularization methods  mikolov et al.      b   where they reported the best performance of  .    using an rnn trained with the hessian free learning algorithm  martens and sutskever       .         discussion    in this paper  we have explored a novel approach to building a deep recurrent neural network  rnn .  we considered the structure of an rnn at each timestep  which revealed that the relationship between the consecutive hidden states and that between the hidden state and output are shallow. based  on this observation  we proposed two alternative designs of deep rnn that make those shallow relationships be modeled by deep neural networks. furthermore  we proposed to make use of shortcut  connections in these deep rnns to alleviate a problem of difficult learning potentially introduced  by the increasing depth.  we empirically evaluated the proposed designs against the conventional rnn which has only a  single hidden layer and against another approach of building a deep rnn  stacked rnn  graves          on the task of polyphonic music prediction and language modeling.  the experiments revealed that the rnn with the proposed deep transition and deep output  dot s rnn  outperformed both the conventional rnn and the stacked rnn on the task of language  modeling  achieving the state of the art result on the task of word level language modeling. for  polyphonic music prediction  a different deeper variant of an rnn achieved the best performance  for each dataset. importantly  however  in all the cases  the conventional  shallow rnn was not able  to outperform the deeper variants. these results strongly support our claim that an rnn benefits  from having a deeper architecture  just like feedforward neural networks.  the observation that there is no clear winner in the task of polyphonic music prediction suggests  us that each of the proposed deep rnns has a distinct characteristic that makes it more  or less   suitable for certain types of datasets. we suspect that in the future it will be possible to design and  train yet another deeper variant of an rnn that combines the proposed models together to be more  robust to the characteristics of datasets. for instance  a stacked dt s  rnn may be constructed by  combining the dt s  rnn and the srnn.  in a quick additional experiment where we have trained dot s  rnn constructed using nonsaturating nonlinear activation functions and trained with the method of dropout  we were able to  improve the performance of the deep recurrent neural networks on the polyphonic music prediction  tasks significantly. this suggests us that it is important to investigate the possibility of applying  recent advances in feedforward neural networks  such as novel  non saturating activation functions  and the method of dropout  to recurrent neural networks as well. however  we leave this as future  research.  one practical issue we ran into during the experiments was the difficulty of training deep rnns. we  were able to train the conventional rnn as well as the dt s  rnn easily  but it was not trivial to  train the dot s  rnn and the stacked rnn. in this paper  we proposed to use shortcut connections  as well as to pretrain them either with the conventional rnn or with the dt s  rnn. we  however   believe that learning may become even more problematic as the size and the depth of a model  increase. in the future  it will be important to investigate the root causes of this difficulty and to  explore potential solutions. we find some of the recently introduced approaches  such as advanced  regularization methods  pascanu et al.      a  and advanced optimization algorithms  see  e.g.   pascanu and bengio        martens         to be promising candidates.  acknowledgments  we would like to thank the developers of theano  bergstra et al.        bastien et al.       . we  also thank justin bayer for his insightful comments on the paper. we would like to thank nserc   compute canada  and calcul que bec for providing computational resources. razvan pascanu is  supported by a deepmind fellowship. kyunghyun cho is supported by fics  finnish doctoral  programme in computational sciences  and  the academy of finland  finnish centre of excellence  in computational inference research coin          .           