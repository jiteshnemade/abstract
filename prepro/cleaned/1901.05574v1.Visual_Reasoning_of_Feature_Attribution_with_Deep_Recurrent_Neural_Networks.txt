introduction  sequence classification is a fundamental problem in big  data analysis. recent advances in recurrent neural networks  rnns  have achieved convincing results for such  classification tasks. during the training process  rnn models  capture the discriminating patterns that distinguishing them  from different categories. it is thus increasingly used in many  real world domains such as funnel optimization for digital  marketing  click stream analysis for online purchase prediction  and patient treatment data analysis for medical recovery  predictions.  despite the popularity of rnn techniques  it can be challenging to understand what and how features are interpreted in  learning with a high performing model. as shown in figure     one can encode multidimensional features as one hot vectors   with categorical or numerical values vary at each element  along the temporal  horizontal  axis and the data instance   vertical  axis. it is important to understand which features  contribute more and how they contribute to the learning of  rnn models in different scenarios. in practice  analysts have  difficulties in feature selection. knowing which features contribute to high performance helps analysts to select meaningful  attributes during training. also  the reasoning of important  features can provide guidelines to business goals. for example   in sales performance management  analysts want to understand  what kind of behavior  such as visiting or emailing  can help  improve sales performance    .  in this work  we focus on the visual reasoning of feature  attribution for rnn models  motivated by the practical need in  customer churn prediction and prevention. analysts try to understand what customer service behaviors and their associated  factors may lead to a high risk of churning for maintaining  good customer relationship and increase customer loyalty.    ai      ai      ai      h     h     h     lstm    lstm    lstm    .  .    x     .    x     .    d dimensional  vector .xt  .  temporal sequence    softmax    x     training data  ...    class  .  .    ...  ...    fig.  . training data instances  sequences of one hot vectors and associated  classes. each dimension in the one hot vectors represents a feature type.    distilled from our interactions with the analysts  here are  three challenges when building a visual analytics solution.  pattern discovery from multivariate temporal sequences.  the training data is both complex in the feature dimensionality  and in their temporal variance. comparing the importance  of different attributes helps in determining what aspects to  address for achieving a certain analytics goal  and clarifying  their value level contributions explains how. analysts try to  discover the effect of each attribute as well as the combined  attributes. the value levels of these attributes change over  time. because rnns are known to be able to learn the temporal patterns  the discovery of the contribution of temporal  sequences becomes essential.  mixture of attribute types. real world dataset often contains  three attribute types  numerical  categorical and ordinal. from  the visual encoding perspective  it is challenging to unify the  design for comparing across multiple types.  multidimensional sequence. sequence data are often associated with metadata describing the result or consequence of  the change in attributes over time. merely visualizing the  sequences themselves is insufficient in revealing the patterns.  we present attributionheatmap to address the challenges  and surface the attribution of training data to the classification  kernel modeling. specifically  we provide             the design and implementation of a visual analytics  system that helps domain experts to understand the  feature attribution in prediction tasks based on rnns.  case studies validating our hybrid design which combines a multimatrix view and a multipartite graph view  to reveal salient features  their contributing values  and  temporal patterns.     ii. r elated w ork  visualization for deep neural networks  dnn  many  visualization techniques have been developed to facilitate the  dnn model building process  covering domains such as image  understanding          and natural language processing    .  techniques such as hierarchical correlation matrices      edgebundled dag      parallel coordinates     and co clustering      were introduced for interpreting model specifics. systems  like activis     and      reveal the links between filters  and patterns at data instance level. on the rnn side  many  literatures focus on model s attention mechanism to evaluate  how well the inputs are related to the outputs. xu et. al.       introduced an attention based model and visualized the  caption word corresponding saliency over images. bahdanau  et. al.      visualized heatmap like word attentions extracted  by rnn models for natural language analysis. yang et. al.       visualize word level and sentence level attentions over texts.  our work falls into this category in the spirit of analyzing  attentions from rnns and visualizes the saliency of data  instances. besides  our approach is capable of visualizing patterns from the training dataset instead of a few data instances  to derive meaningful conclusions.  multivariate data visualization multivariate data visualization have been developed in numerous fields of analysis    .  we summarize the related work based on the visualization  layout. grid based methods arrange variables in matrices to  benefit the pairwise comparison of attribute relationships.  gplom      extends scatterplot matrix      to generalized  plot matrices. ensemblematrix      enables the interaction  in matrices that help understand the classifiers in ensemble  learning. yuan et. al.      insert multidimensional scaling  plot into neighboring parallel coordinate axes. glyph based  methods encode multidimensional data with value interpolated  geometries     . dicon      uses a treemap like icon to encode data cluster that depicts multiple attributes and quality of  cluster. irimia et. al.      adopted connectograms to visualize  relationships between multidimensional neuron connectivities.  since matrix and parallel coordinate plots  pcp  based methods are more scalable to larger datasets  we investigate both  and further discuss the trade offs in later chapters.  temporal sequence visualization we summarize temporal  sequence visualizations into the following categories  juxtaposed representation features in the visualization of events  transfer. alluvial diagrams      reveal how network structures  change over time. outflow      visualizes temporal event  sequences in pathways that are similar to parallel coordinates.  matrix representation emphasizes links between events. for  example  matrixwave      aligns and compares the differences  in the occurrence of clickstreams augmented by event states.  liu et. al.      presented an analytic pipeline for pattern  mining  pattern pruning  and coordinated exploration between  patterns and sequences. vidx      extends marey s graph  with a time aware outlier preserving design to support faults  detection and troubleshooting on assembly lines. the temporal  sequence design in our work attempts to visualization and    table i  attributes used in c ustomer c hurn p rediction  feature  usage  support    attribute  usage level    usage level      of interaction  maintenance type  operation type    statistical type    range example    numerical  numerical  numerical  categorical    types   categorical    types                                       scheduled unscheduled  inspection repair    compare the temporal patterns of all data instances from  multiple predicted classes. we also adopt a juxtaposed design  to depict the temporal changes within the user specified range.  iii. background and m odeling  a. customer churn prediction  predicting customer s likelihood of canceling a subscription  to a service or a product is among the most studied big data  problems. data scientists build binary classification models  to predict churning  in which feature engineering is key in  identifying what may impact the churning behavior.  for example  analysts may consider two types of features   the usage features that characterizes customer s service or  product usage  and the support features that characterize  customer s interaction with customer engineers  ces   as  depicted in shown in i. real world datasets with such features  are often collected over time and may suffer data quality  issues. it is crucial to mine the temporal dependencies from  such input sequences  where analysts can find answers to  whether or what ce behavior or service  product  usage would  help decrease the customer churn rate. motivated by this use  case  we put our focus in this work to the attribution reasoning  of which feature and value subspace among the piled up  dimensions has contributed to the classification modeling.  b. rnn and attention  rnn is a family of connectionist models that capture the  dynamics of sequences via cycles in the network. recently   rnn based approaches have demonstrated ground breaking  performance on sequence data analysis such as speech synthesis and time series prediction.  we use the long short term memory  lstm       recurrent  network to capture the features at every time step conditioned  on the previous hidden state. refer to figure    xt is a vector  that encapsulates all the information at one time step. we feed  lstm with a sequence of such vectors x    x    . . .   xt    t  being the length of the sequence  and predict the churning  behavior given some hidden states. a hidden state ht is a  function ht   f  w x xt   w h ht    . the weight matrices w x  and recurrent weight matrix w h are updated through the  optimization in the back propagation process through time.  the hidden state vector at the final time step is fed into a  binary softmax classifier where it is multiplied by another  weight matrix and put through a softmax function that outputs  values between   and    effectively giving us the probabilities  of positive and negative customer churn.     the training process with attention mechanism      plugs  a vector representation c in between the input layers and the  output. the sequence representation takes the form   c     t  x  t       t ht    where    exp a ht   w c      t   pt  c  k   exp a ht   w              where a ht   w c   is an attention network  and w c is the  parameter set of the attention network. we feed the vector  c to a softmax classifier to predict the customer churn distribution of the target sequence. we train the attention enabled  model by minimizing the cross entropy between the predicted  distribution and the ground truth. a well trained lstm model  then assigns higher attention scores to the temporal events that  are more relevant to the classification task and lower attention  scores to the less contributing events.  during feature encoding  we convert the features in i into  vector xt . we use one hot vectors vc to represent the categorical attributes  concatenated with the vectors vn that represent  the numerical attributes to form xt . after successfully training  an lstm model  the output attention for each time step of  each training instance is associated with the vector xt . as  a result  the lstm model assigns the importance for each  event in the temporal sequences for the classification task.  by tracing back to the attribute levels at each event  we  achieve the importance of combined feature levels for the  whole dataset. we can understand how lstm is using these  features to distinguish the  loyal  from the  leaving  customers  by visualizing them in an effective way.  iv. d esign p rinciples  to visually support the feature attribution reasoning  we  consider the following design and development principles.  the design should facilitate the comparison between all  features and feature value levels for the temporal events in  all training instances.  dp . the design should support the attribution visualization under combined conditions. attribute combinations  may be more contributive than individual attributes. for example  customer interaction type can be less contributive for the  entire customer cohort but can be important for certain types  of customers. therefore  analyzing the combined condition of  customer type and customer interaction is more meaningful.  the design should visualize the contribution of combined  conditions in addition to individual variables.  dp . the design should support the visualization of  rnn attentions over time. customer churn can be the  consequence of a series of temporal events. the system design  should capture features  contribution from the events with  high low attentions over timesteps. on the other hand  from  the learned temporal patterns  we expect a better understanding  of lstm s temporal learning features.  dp . the visualization should facilitate the comparison  of training instances between different classes  and the  comparison of feature attributions. to understand how  lstm uses the features in distinguishing different classes  the    visualization should allow users to compare the contributing  instances between these classes. the visualization should also  support comparing different attributes of data instances.  dp . the visualization and interactions should facilitate the exploration of feature attribution for predictive  reasoning. the system should allow users to explore feature  attributions with the flexibility in specifying conditions  such  as a particular temporal or attention range.  v. v isual e ncoding  we closely work with data analysts and receive feedback  during the design iterations. our visual encoding design and  the interactions are as follows.  a. multivariate feature visualization  for dp  and dp   we visually encode multivariate data  that are assigned with model specified attention weights. in  an early stage design  we visualize feature attribution with  pcp where axes represent attribute types and coordinates on  the axes represent the attribute levels. however  the comparisons between lines of different directions increase the mental  burden. we then switch to chaining matrices  where matrices  represent the saliency of two combined attribute values. users  then have trouble in deciding matrix chaining orders.  we settle with a concise yet informative design   matrix  grid. as shown in figure    area a shows a matrix grid  representation of attribute values. each matrix  excluding  diagonal  corresponds to a combination of two attributes. the  rows and columns represent attributes  value levels  and each  cell reflects a combination of the unique value levels from two  attributes. to the top and left labels in the matrix grid represent  attribute names  a  b  ...  n  . users can further reference the  attribute value levels by the labels associated with matrix cells.  the value levels are arranged in the ascending order from left  to right and from top to bottom. in this paper  we refer matrix  p q to the matrix that in column p and row q.  the system automatically detects unique value number for  categorical attributes on data loading. color intensity represents the value in a cell  with the upper and lower triangular  having different meanings. cool and warm colors represent  positive and negative values  respectively. a user can select the  meaning of each cell from  positive class instances    negative  class instances    both classes   and their difference.  the matrix grid in figure   shows the difference between  positive  blue  and negative  red  instances. high color intensities indicate the instances strongly support their belonging  class. as in correspondence with dp   we use explicit encoding      to visualize the difference between two classes  for feature attribution comparison. users make sense of the  highly contributive attributes by locating the matrices of high  intensities. the visualization also shows the contribution at a  finer granularity  the value level. users can make sense of contributing values by the color and intensity of the corresponding  cells. the visualization result in area a tends to have more  salient values on the top left areas in many matrices. because  the dataset contains features such as the time of particular     d    a    b    e    d    c    e    fig.  . system overview of attributionheatmaps. a is the matrix grid view showing the feature attribution saliences for all training data attributes. the  difference from two training classes is shown here. b is the t partite graph showing the temporal patterns for each feature from two training classes. c is the  menu area where users can dynamically analyze feature contributions by viewing and slicing data from different facets. d  e are explained in section  .  i    g    b  g    i    h     a     fig.  .     b     a    a     c     matrices for combined attributes  a  g i   b  i g and  c  b h.  i    i     a  low saliency matrices    f    b  b    f     b  high saliency matrices    fig.  . matrices for individual attributes.  a  attribute a and i exhibit low  saliences on the diagonal cells   b  attribute b and f exhibit high saliences  on the diagonal cells.    activities  for which the number of instances exponentially  decays while the number of activities increases. however  the  visualization is designed for general purposes  and the trends  may differ for different datasets.  the matrices in the lower matrix grid triangle represent the  feature attribution saliences. the color intensity represents the  absolute value. the logarithmic scaled colormap is shown on  the top of the menu bar. for example in figure    a   matrix  g i represents the feature contribution for two ce activity  types g and i. the value levels corresponding to each cell  represent the number of the activity per month. the saliences  concentrate in the upper triangle  which indicates that the  number of i is greater than or equal to g for all current  data instances. the cells on the diagonal exhibit negative  contributions as shown in the red cells. the intensity decrease    from the top left to the bottom right. this phenomenon  indicates that the instances turn to be predicted by lstm as   negative  when g and i have an equal number of activities in  a month  especially for small numbers of activities. the dark  red cell on the top left corner indicates that the data instances   where neither g nor i happens in one month  contribute  significantly to the negative class. also  the cells near the upper  diagonal cells  where g values are slightly larger than i  show  positive contributions in blue colors  which indicates such data  instances contribute more to the positive class.  the diagonal cells on the diagonal matrices p  p show  the contributions of value levels for attribute p. as shown  in figure    matrices b b and f  f show higher color  intensities which indicates a higher contribution to the lstm  classification  while a a and i i behave conversely.  the matrices in the upper triangle illustrate the temporal variance of corresponding attributes. for cell  pi   qj    in  matrix p  q   we  compute the  variance over time as  p  p  p  q  p  q  t trange  heatt    i    j         t trange    heatt i    j      trange          reflected by  trange  the color intensity. similarly  blue and red colors represent the  positive and negative class  respectively. the upper triangular  matrices are a reflection of whether the attribute values behave  differently over time for all training instances. it exhibits an  abstraction of temporal information for higher level comparisons between attribute combinations. each matrix works as a  button that leads to a finer level visualization that is introduced  in the following sections. for example  figure    b  shows high  saliences by the red pi    pj cells  which indicates that when  the value from g and i equal to each other  instances from  the negative class exhibit a higher temporal variance.      churn  class    b. t partite graph for temporal sequences  in correspondence with dp   we design a visualization that  extends the matrix grid  identifying the difference between  training data sequences  and reveals the feature level changes  over time. we use the name t partite graph because the events  are partitioned into independent subsets by time.     visualization for individual attributes  pcp is proved  highly effective by many approaches for visualizing temporal sequential patterns in dnn    . in these cases  the axes  often represent the time steps  and the events for all sequences  are continuous through the time dimension  so the polylines  connecting the coordinates on the axes stops at each axis.  in our scenario  because the analysis would involve data  filtering with attention weights for temporal events  the filtered  event sequences can jump over time steps. by keeping the  advantages of pcp  we visualize the temporal changes in  training instances with t partite graphs  where t equals the  maximum number of time steps. we illustrate the difference  between pcp and t partite graphs in figure  .    t     t     t      a  parallel coordinates    t     t     t      b  tripartite graph     loyal  class            t     t     t      c  difference    fig.  . the comparison between parallel coordinates and t partite graphs.  both  a  parallel coordinates and  b  tripartite graph visualize the temporal  sequences in time steps t    t    t    with         coordinates on each axes   respectively.  a  visualize all possible event sequences.  b  is a complete  tripartite graph that also visualizes all possible event sequences including  consecutive temporal sequences and the sequences that across over time steps.   c  the difference between  a  and  b .    in figure   b area  we visualize the  churn  and  loyal   temporal sequences in the left and right columns  respectively.  each row in the t partite graphs shares the same label as the  matrix grid on the left. we use the juxtaposition encoding        as shown in figure   and figure    to show the difference  in temporal pattern from two classes. nodes on the axes  represent single events from the sequences. we connect events  with line segments if there are two or more events selected  from a sequence. purples represent the positive  loyal  class   while oranges represent the negative  churn  class. the color  representation is consistent with the color coding of the matrix  grid view where warm colors represent negatives  and cool  colors represent positives. we encode the number of events  with color intensities for nodes and encode the lines with a  transparency that is computed based on the maximum number  of event frequency. therefore  the significant patterns form  saliences due to the highly overlapped lines.  figure   and figure   shows the example    partite visualization for attribute number of maintenance and usage  level  respectively. they both exhibit different patterns from  two classes  and the patterns distinguish each other for the two  attributes. details are in the explanation under the figures.         k     k                fig.  . the temporal patterns of  the number of maintenance  for the churn  and loyal classes  shown in    partite graphs. the events from the  churn   class show an increasing pattern over time  while the events from the  loyal   class are evenly distributed.     churn  class     loyal  class         k   k     k      k     k    k     k    k     k    k     k    k     k    k     k    k     k    k     k    fig.  . the temporal patterns of  usage level  for the churn and loyal classes   shown in    partite graphs. events from the  loyal  class have fewer saliences   especially on the top areas  comparing to the  churn  class as shown in the  rectangles.       visualization for combined conditions  it s challenging  to design the visualization in correspondence with dp  and  dp  due to the complexity of visualizing both multidimensional data and their temporal patterns.  figure   shows our design for visualizing two attributes  g and a. in this t partite graph  axes remain the same  meaning   time steps. on each axis  we show the attribute  levels in two hierarchies. the value levels of the first attribute  g are top down arranged to the left. the number of values  is automatically extracted in real time  and the positions are  computed so that the groups use the vertical space efficiently.  the value levels of the second attribute a are top down  listed within the groups of g s levels. we arrange the vertical  positions by calculating the maximum number of values and  evenly distributing them within each primary attribute value  group along the vertical space. our design also benefits from  the mental easiness of referencing the same value levels at the  same vertical positions for different time steps. however  the  edges will overlap each other when the edges connecting nodes  with the same value levels. therefore  we use bezier curves  for such cases so that edges between farther nodes share longer  curving edges. to show the saliency clearly  we sort the edges  by their frequency and render the lower value edges earlier  and higher value edges later. we use explicit encoding for the  comparison between two classes. the number of value levels  for combined attributes is greater than the individual attributes.  therefore the comparison in a juxtaposition manner will cause  mental burden when viewing back and forth between graphs.  the visualization in figure   contains three patterns  i  the  lines connecting the nodes on the top left row and the nodes  on the lower right half  ii  the lines connecting the nodes on  the left half area and the nodes on the top row on the right  half area  and iii  the curves connecting the nodes on the top  row. we see the majority of i  are in orange colors and the  majority of ii  are in purple colors. this phenomenon shows  that the decreasing of g values along time has a negative  contribution and vise versa. within each g value group  we  can see that the orange lines mostly connect to large a values.     . . .                                                                 fig.  . temporal patterns of combined attributes g a.    this indicates that small g values with large a values have  more negative contributions. the orange curves on the right  half area in iii  shows the highest saliency which indicates  that the instances turn to be negative when both g and a  are zeros for months     . also  longer curves have relatively  lower intensities  which indicating that long term events have  lower influence comparing to short term consecutive events.  c. feature contribution exploration and reasoning  in correspondence with dp   we design the multi aspect  data slicing functionality for the free exploration of feature  attribution through the training set. our system employs a datadriven architecture where the data controls the flow by offering  different data slices chosen by the users. we pre compute the  attention values from the lstm model in the backend and  computations triggered by interaction are handled in the frontend to ensure interactivity for the matrix grid and t partite  graphs. the triggers are in three aspects. from the menu area   c in figure    users can select data of interest by updating   the attention range. as introduced in the background   each temporal event from a sequence instance is associated  with an attention value that reflects the importance for lstm  to differentiate the characteristics of sequences that lead to  customer churn from preserving. the analysis benefits from  attention based data slicing in two facets. first  comparisons  under high and low attention patterns help understand how  lstm make sense of data in the classification  which also  helps users solve real world problems by showing what behavioral attributes assist customer preserving. second  it helps the  attribution reasoning under noises. slight change of filtering  conditions may cause significant change due to a blob of  highly noisy instances. users make sense of contribution s  changing pattern while sliding the attention range monotonically. even if there is a sudden change in the process due to  noises  the method still can help discover the overall trend.  the temporal focus. as a routine practice  analysts often  divide time into stages. in customer churn prediction  data covers over a    months span is often explored on a quarterly basis    because the attributes are related to ces for whom customer  interactions are organized in such a unit. also  separating the  temporal stages help focus on short term patterns.  the attribute list. in practice  scientists often train lstm  models with all available attributes when there is no prior  knowledge for feature selections. however  not all attributes  are contributing to the classification. after generating the  visualization and acquiring saliency information  users can  remove attributes of low attribution and focus on a subset. in  customer churn analysis  the training dataset is initially merged  from two separate datasets. for examples  from matrix c l  in figure    we can observe a high correlation between these  two attributes and remove l from the beginning. users can  adjust the sequence of displayed attributes in a certain order  for hypothesis verification  or arrange similar attributes for  easy locating of common patterns from multiple attributes.  a typical exploration starts from the matrix grid visualization. by default  the matrix grid shows the saliences in  contributing features for all attributes. based on the visualization results  users then explore the data slice of interest  with the above listed interactions. when some attributes of  interest are found  they further look into their temporal patterns  with the upper triangular matrices and the graphs. due to the  complexity of temporal patterns for combined attributes  their  visualization requires large canvas space. clicking the corresponding matrix will trigger the t partite graph visualization  for combined attributes. clicking the symmetric matrix will  exchange the primary and secondary attributes.  vi. e xperiment and e valuation  the dataset contains   k events from      customers. each  temporal event sequence from a customer is labeled with  churn or loyal. every temporal event contains    dimensional  features as shown in i.   shows the visualization of all  attributes. the lstm model was trained using a     traintest split. the real world dataset is noisy but we achieved an  accuracy of  .   around the    th epochs. we normalize the  attention vectors to        for comparison.     change over the epochs  figure   illustrates the feature attribution change over the  learning process. the left most and right most figures in low  and high attention groups represent the first training epoch and  the epoch where the model reaches the optimum  respectively.  the low attention group shows that almost all sequences are  considered unimportant at the beginning  as the graph is  nearly complete. the high attention group shows only a few  sequences are considered important at the beginning. while  the number of epochs increases  lstm gradually updates  parameters and trims from a large number of sequences for  the unimportant sequence patterns and picks up the important  ones. this explains that the lstm training process tries to  narrow down the conditions that distinguish one class from  the other. besides  the difference between the positive and  negative classes becomes more salient while the number of  epochs increases.     temporal attributions  high attentions    temporal attributions   low attentions    epoch        epoch      epoch        epoch      epoch        epoch        fig.  . lstm s learning progress  the contribution for combined attributes for high attentions    .    .    and low attention       .    at different epochs.       case studies  we continue to work with data scientists and domain experts  who have decades of domain experiences as we develop attributionheatmap. we showcase a few case studies as follows   importance in interactively maintaining customer relationship. almost all matrices related to maintenance and  operation show red on the top left corner in the matrix grid.  this indicates that not conducting maintenance and operation  greatly contributes to customer churn. domain experts explain  that maintenances and operations play an essential role in  maintaining a healthy relationship with customers. for some  customers that are difficult to find a contact person  it is  reasonable that their churn rate is higher. in addition  more  cells are in red tones in the upper triangular matrix grid.  this phenomenon indicates that the  churn  class have a larger  temporal variance than the  loyal  class. the temporal patterns  for  interaction frequency  shown by the t partite graphs also  reveal that the  churn  class tend to have higher frequencies  in the first time step and the frequencies decrease in later  time steps. meanwhile  the  loyal  class have more evenly  distributed frequencies among all time steps. the visualization  indicates that it is easier to retain a customer from churning  if interacting with the customer at a stable frequency. the  visualization also reveals churn contributing patterns where  interactions are highly infrequent in the first half and increase  greatly in the second half temporal period.    interaction  frequency    sm    um                                                                                    interaction  frequency     a                                                  b     fig.   . the attributionheatmaps for  a  sm if  b  um if.    customer interaction  scheduled or unscheduled   maintenance can be scheduled maintenance  sm  or unscheduled maintenance  um . ums are for unexpected issues   and sms are routine and periodic. interaction frequency  if   covers all maintenance types including sm and um. common  sense is that um has negative contributions because um    indicates some issues in the products. however  attributionheatmap interprets contrarily from several perspectives.  figure     a  and  b  show the heatmap for sm if and umif. in  a   the diagonal red cells show that interacting with  customers only in sms contributes to customer churn. the  blue cells under the red line show that other interaction types  contribute otherwise. in contrast  the first red column in  b   shows that not conducting um has negative impacts. besides   the dark blue     and     cells indicate that having once  um per month have a greater positive impact. these two  matrices together reveal that um has a higher contribution  in customer retention and sm highly contributes to customer  churn. explanations are when customer issues are not solved  at the first interaction  the next interaction becomes sm. thus  one um in a month  in a high chance  indicates that ces solved  the problems from customers at one interaction. therefore  the  observation verifies that quick problem solving is important in  customer retention. the t partite graphs of if  primary  and  um  secondary  show bottom left to top right purple edges  and top left to bottom right orange edges  which means more  interactions in the first half year and fewer interactions in the  second half year contributes to the customer retention. the  visualization results also show that one time um in the second  year  especially month     have negative contributions  which  provide hints to guide ce interactions.     accuracy improvement  from the case studies  we found the usage features e   f  and the support features b  h  j have strong attribution   where the usage features are more important than the support  features. the found important features guide data engineers  to source more fine grained data for the support features.  during the second experimental iteration  the customer churn  prediction accuracy is improved by  .   after including such  data. although the improvement is small  scientists give highly  positive feedbacks since they have tried to train the model  with many attributes but the model fails in getting a better  accuracy. this proves our visual reasoning approach helps  reveal feature attribution and suggest fine grained factors for  the improvement of prediction classification accuracy.     vii. d iscussion  we discuss the extensibility and limitation in this section.  generality the use of attributionheatmap is not limited to  the feature attribution for customer churn prediction. the datadriven approach we adopt does not depend on any applicationspecific condition and can be generalized to more applications.  our approach also handles different statistical data types  automatically. besides  the t partite graph depends on the  matrix grid  but not contrariwise. the matrix grid view alone  works as a visualization view for feature attribution without  digging into the temporal changes. therefore  the matrix grid  can be potentially extended to the feature attribution analysis  for convolutional neural networks.  scalability attributionheatmap uses webgl based rendering and therefore supports big data analysis. we demonstrate in  this paper that the dataset contains    attributes  each of which  has   value levels on average. and each data instance contains     temporal events. the ability to handle such a complex  dataset demonstrate the scalability of our system. also  the  matrix grid view expands while the number of attributes and  the corresponding value levels increase  but the ui lets users  trim and rearrange rows and columns during the exploration.  limitations attributionheatmap works with fine trained  rnn models. our approach does not apply for the datasets  that fail in learning a convergent model. besides  the higher the  prediction accuracy is  the better the conclusion can be made.  without an accurate prediction model  the visualization result  would suffer from lacking reasonable evidence. moreover   attributionheatmap works best for balanced datasets because  it compares the data instances  contributions from different  classes. for imbalanced datasets  conclusions can be made  based on oversampled or undersampled data.  viii. c onclusion  in this paper  we present an effective solution for value level  feature attribution analysis with labeled multivariate temporal sequences. we implement a functional tool attributionheatmap with an easy to interpret design. attributionheatmap  facilitates the understanding of contributing features and their  temporal patterns for predictions. we deploy and test attributionheatmap with a churn prediction dataset collected over     months from an international corporation. the experimental  results help to understand the learning progress of attention  mechanism in rnns. the case study results demonstrate  that the approach can help domain experts reason feature  attribution and give suggestion to achieve their goals.  acknowledgment  this work was supported in part by the u.s. national  science foundation with grant iis        .  