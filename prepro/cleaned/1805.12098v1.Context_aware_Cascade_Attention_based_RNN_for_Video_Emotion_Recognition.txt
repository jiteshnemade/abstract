introduction  understanding human emotion has attracted a lot of attention recently. and it also plays an important role in many  applications such as human computer interaction  advertising   social media communication and cognitive science. however   emotion recognition is still a challenging task. it s very  difficult to find an effective model for emotion and facial  expressions  let alone combining of multimodal data of visual   vocal and even text to emotion recognition. in this work   a novel model is proposed to consider facial and context  information concurrently  which leads to superior performance  in emotion recognition.  one of the key problems of emotion recognition is emotion  representation or emotion model. there are many researches  about emotion representation  such as six discrete basic emotion classes proposed by ekman      continuous dimensional  models  e.g. valence and arousal in       and facial action  coding system  facs  in      which describes facial movement  in action units  au . combination of facial action units can  be classified into different emotions.    many emotion datasets and challenges have also been published using aforementioned emotion representation models  such as aff wild      which uses valence and arousal space  to model facial expression. for the emotion classification  representation  challenges such as emotion recognition challenge in the wild  emotiw      and multimodal emotion  recognition challenge  mec       which is the challenge  of this work attending  have been held for recent years.  furthermore  some challenges such as          also includes  vocal features for emotion recognition.  in psychological researches           it has been discussed  that other than facial expressions  contextual information  such as body  pose and surrounding environment can also  provide important clues for emotion perception. evidence  and experiments are also provided in          to show that  emotion perception can be influenced by context. moreover   in some cases  context is even indispensable for emotion  communication. similar results are also proposed in computer  vision literatures. experiments in     show that when using  both context and body information  performance of emotion  recognition outperforms that of using only body image or  only context image. a dataset  emotions in context database    emotic  has also been published recently in    .  most of recent emotion recognition methods focus on  exploring facial features based on deep neural network. convolutional neural network  cnn  has been used to extract  face features in some works           . some researches  incorporate  d convolutional networks  c d  and recurrent  neural network  rnn  to model spatial and temporal clues  of faces           . also some works           combine audio  models to perform multimodal emotion recognition.  this work focuses on video emotion classification using  sequence of both facial and context information. a contextaware cascade attention based rnn is proposed here to  leverage both facial and context features to perform video  emotion recognition. to further evaluate the influence of context information  multiple rnn based networks are designed  to compare different methods of fusing face and context  features.     ii. r elated w orks  many works proposed video recognition methods based on  neural networks. c d networks were utilized to learn temporal  information from sequential images for action recognition             and video emotion recognition     . recurrent neural network  which has temporal recurrence of latent variables   was proposed in     . long short term memory  lstm  was  proposed in      to handle long range sequence learning.   sequence to sequence  model       which models sequence encoding and generation using lstm rnns  is one  of the popular architectures of learning from sequential data.  it has achieved the state of the art results in neural machine  translation. the sequence to sequence model consists of a  rnn based encoder and a decoder to learn temporal structures  and generate output sequence. furthermore  temporal attention  mechanism has also been proposed in      as a soft alignment  method in machine translation to learn relevant temporal  segments from encoder and decoder.  the encoder decoder framework in sequence to sequence  has also been used to describe videos with cnn and rnn  to generate captions     . the temporal attention mechanism  has also been incorporated in      to learn relevant temporal  intervals from video sequences.  iii. p roposed m ethod  in this section  the effect of context on improving emotion  perception is first shown in section. iii a. then  inspired  by sequence to sequence model and attention mechanism for  neural machine translation  a novel architecture for sequence  classification will be proposed. sequence to sequence model  and attention mechanism will be briefly described in section.  iii b and iii c respectively. then the proposed model will be  introduced in detail in section. iii d.    b. sequence to sequence framework  sequence to sequence  which is an encoder decoder framework  was proposed in      to perform machine translation.  the encoder reads a sequence of input vectors over input time    . . . t as x    x    . . .   xt    into a  context vector  at encoder ce t . the context vector here represents the stored state  and information of the encoder. when the encoder is a rnn   the hidden states are represented as he t   f  xe t   he t      and ce t   g he     he     . . .   he t    where f     and g    are  nonlinear functions.  the decoder then generates one prediction at output time i   yi   from context vector ce t and all the previous predicted output. with a rnn decoder  its conditional probability models  as  p yi   y    . . .   yi     ce t     h yi     ce t   hd i              where hd i is the hidden states of the decoder rnn at  output time i  hd i   f  hd i     yi     ce t    and f      h     are nonlinear functions.         variance of labeled intensity    without context information. the dataset consists of        images with bonding box of each face. each image is labeled  by intensity of   emotions   angry    disgust    happy    sad     surprise    fear    neutral    contempt    confused  and intensity is in score from   to   by each labeler. each image is  labeled by at least   labelers. then the disagreement of each  emotion is calculated by averaging the variance of labeled  intensity of each image.  in fig.    it is shown that when providing the labelers with  original images of both face and context  more consensus  can be observed in the obtained labels. furthermore  emotion  class  happy  and  neutral  have the least disagreement  which  means happy and neutral can be recognized easily  while other  emotions of higher disagreement shows more confusion in  labelers.    original image  cropped face   .     c. attention mechanism     .      .      .          an    gr    y    dis    gu    st    ha    pp  y    sa    d    su    rp    ris    e    fea    r    ne    ut    co    ra    l    nt    co    em    pt    nfu    se    ov  d    er    all    fig.    visualization of human emotion perception.  the dataset described in section. iii a is labeled twice   original images are the label obtained providing the labelers  with original images  cropped face are the labels when only  lablers are provided with cropped face.  a. effect of context information on emotion perception  to visualize the effect of context information to emotion  perception for human  an experiment was designed to show  disagreement of human label of the same dataset with and    attention mechanism has been proposed to improve the  performance of english to french machine translation in       as a sequence to sequence model. the mechanism makes  decoder not only depend the context vector from encoder but  allow the decoder model to learn relevant parts over source  sequence to predict target.  the soft dot global attention in      is adapted here. an  alignment vector ai    ai     . . .   ai t   is a sequence over input  time  where ai t is derived by encoder s hidden states he t at  input time t and current decoder s hidden state hd i at output  time i  which is  exp score hd i   he t     .  ai t   pt  t   exp score hd i   he t              the score function implemented here is dot operation from  hd i and he t   where score hd i   he t     htd i he t and noted  that the hidden size of encoder and decoder should be equal.     attention    lstm    lstm    lstm    lstm    lstm    lstm    lstm    lstm    cnn    cnn    cnn    cnn    cnn    cnn    cnn    cnn    t      t      t      t      t      t      t      t      face stream    context stream    fig.    context aware cascade attention based rnn  caca rnn . a video clip is preprocessed into a face stream  sequence  of cropped faces  and a context stream  sequence of original frames . then  the extracted features of the two sequences are  fed into two lstms  the context rnn learns contextual temporal information and the face rnn learns facial information.  the two lstms are in a cascaded architecture with attention mechanism. the context rnn stores context clue in lstm cells  and initiates the first state of face rnn at the first time step. the face rnn then learns from face features and processed  context information from attention mechanism. then the output of face rnn of the last frame of clip is considered to be the  prediction of the clip.    then the context vector ci at output time i is derived as  weighted sum of all source he and ai t   which is computed as  ci      tx  x    ai t   he t .           t      to summarize     and      the context vector with attention  mechanism is a function of hidden states of both encoder and  decoder. based on both decoder and encoder s hidden states   the decoder can pay attention to relevant input sequence of  encoder and generate aligned output sequence.  d. context aware cascade attention based rnn  context aware cascade attention based rnn  cacarnn  is proposed to leverage both face and context features to  perform emotion recognition. the caca rnn consists of two  lstms in a hierarchical cascade architecture with attention  mechanism  fig.   .  unlike encoder decoder framework  which the decoder is a  generative model  there are two encoder like rnns in cacarnn   context rnn  and  face rnn   which they read face  feature and context feature respectively. the two rnns are  cascaded with attention mechanism in between. the attention  mechanism of the caca rnn can locate relevant context  information from the context rnn when processing the face  sequence in the face rnn. after the context rnn reads whole  context features  the face rnn encodes face features and  its lstm context vector is derived from attention operation  of face and context rnn s hidden states. then the face  rnn outputs the final prediction class after forwarding whole  sequence.    the context rnn reads the context feature sequence from  time step   to t as xcontext    xcontext    . . .   xcontextt   into  a lstm context vector ccontextt   where its hidden states at  time t being denoted as hcontextt   f  xcontextt   hcontextt    .  similarily  the face rnn reads face feature sequences xface     xface    . . .   xfacet   and its conditional probability is modeled  as  p yi   xface    . . .   xfacei   cfacei      h xfacei   cfacei   hfacei              where hfacei is the hidden state of the face rnn  denoted  as hfacei   f  hfacei     xfacei   cfacei  . and the lstm context  vector cfacei is derived from eq.     and eq.      which is    cfacei      t  x  t      exp score hfacei   hcontextt       hcontextt .      pt  t   exp score hfacei   hcontextt       iv. c omparison architectures  to investigate performance of rnn based models fusing  face and context features on emotion recognition  various  architectures of combining both features or using only one  of the features are introduced as the following   context rnn performs emotion classification using video  frames as input  depicted in fig.  a. the model is similar to a  model in      for action recognition  where a lstm processes  sequential features from cnn and learns to predict human  action. the context rnn reads context features which are  extracted from a cnn feature extractor  and last prediction is  taken as output.     v. e xperiments and e valuation    lstm    cnn    lstm    cnn    lstm    cnn    cnn    cnn    lstm    lstm    lstm    lstm    lstm    lstm    cnn    cnn    cnn    a. datasets    fc    cnn     a  face rnn and contextrnn     b  parallel rnn  attention    lstm    cnn    cnn    lstm    cnn    cnn    lstm    cnn    cnn     c  concatenated rnn    lstm    lstm    lstm    lstm    lstm    lstm    cnn    cnn    cnn    cnn    cnn    cnn     d  caca rnn    fig.    comparison of rnn based emotion recognition architectures using face and or context streams as input. both  streams are processed by a cnn feature extractor into corresponding feature stream as lstm s input.  a  a single lstm  performs emotion prediction from a feature stream.  b  two  lstms reading two streams individually. hidden states of the  two lstms are fused together by fully connected network  for final prediction.  c  two feature streams are concatenated  together as input of single lstm.  d  a cascaded rnn  architecture of two lstms processing two feature streams   the left lstm reads one feature stream and store a context  vector in its hidden states. and the right lstm reads the  other feature stream with its hidden states is initiated by the  left lstm s context vector. the right lstm s hidden states  are generated from its input and attention from the left lstm.    chinese natural audio visual emotion database   cheavd   .  is used by multimodal emotion recognition  challenge  mec       challenge. cheavd  .  includes       training clips      validation clips and      testing  clips from chinese movies and tv programs. each clip is  labeled with one emotion category according to both video  and audio content in   classes   happy    sad    angry     surprise    disgust    worried    anxious   and  neutral .  to enlarge the training set  the submitted model was trained  with additional private dataset. the dataset includes       video clips   happy         sad         angry         surprise         disgust         worried        anxious        and   neutral        and training data clips are trimmed from the  videos.  b. feature extraction  for each video clip  two streams are generated in preprocessing stage   face stream  and  context stream . face  stream contains sequence of each detected faces from each  frame  cropped and scaled to          . with each detected  face  corresponding context stream contains the original frame   center cropped and scaled to          .  then  two pre trained cnns are used to extract features  from the face and context streams as the input of cacarnn s face rnn and context rnn. the face feature extractor  is a classifier trained with a private dataset for image emotion  classification without last layer. and the context feature extractor is a pre trained model from squeezenet      with imagenet  classification       which the output of the last convolution  layer is used as context feature.  c. training details    face rnn  on the contrary to the context rnn  is only using face feature sequence to predict emotion. the architecture  is depicted in fig.  a.  parallel rnn  illustrated in fig.  b  consists of two rnns  processing face and context features individually. then the  hidden states of the two rnns are fused for final prediction  at latest time step.  concatenated rnn contains a rnn taking concatenated  face and context features as input. then its output at the last  time step is taken to be the prediction of the video clip.  fig.   c   caca rnn a  context aware cascade attention based  rnn  consists of two rnn in a cascade architecture with  attention mechanism  fig.   . the left rnn reads context  stream into a context vector. and the right rnn takes  attention processed context vector and face stream as input   then predicts output at the latest time.  caca rnn b has the same structure as caca rnn a   fig.    except for its left rnn reads face stream and the right  rnn reads context stream.    videos are downsampled to   fps in both training and  inference phases. in training phase  each video is sampled from  a random initial frame for data augmentation. in evaluation  phase  the initial frame is fixed to be the first frame of video  clip.  for the submitted result  adam      was used with learning  rate      with mini batch size   . the weights of feature  extractor cnns are fixed while training. there is a pooling  layer after the context cnn to down sample feature size to size        as the input of context rnn. also  a fully connected  layer is added after the face cnn to encode face features with  vector of size    . additionally  the face rnn is a two layer  lstm with     hidden states. the context rnn is also a  lstm with     hidden states and attention mechanism is on  face rnn.  d. quantitative results  to explore performance of the architectures described in  section. iv  the validation results are summarized in table. i   with only cheavd  .  was used for training and validation  in the experiments.     model  face rnn  context rnn  parallel rnn  concatenated rnn  caca rnn a  caca rnn b     params   .  m   .  m   .  m   .  m   .  m   .  m    map        .      .      .      .      .      .      acc        .      .      .      .      .      .       d  cnn    table i  experimental results on mec     validation set.    to make comparison fair  the models are adjusted such  that number of parameters are close. for each experiment   training was repeated five times with different random seeds  and median performance metrics are selected among them. the  feature sizes of face feature and context feature are encoded to      by fully connected layers. face rnn  context rnn and  concatenated rnn has a two layer lstm with     hidden  nodes. in parallel rnn  there are two two layer lstms of      hidden nodes. and the first lstm of caca rnn also  has two layers with     hidden states  the second rnn is a  lstm with     hidden nodes.  notably  the models fusing both context and face clues  outperform the models using only one of the features. it  shows that the context information is helpful but not enough  to perform emotion recognition without cropped face.  for models fusing face and context features  the results show  that parallel rnn outperforms concatenated rnn. learning  from multiple different feature sequences can be more effective  for networks with multiple branches than a single branch  model. the performance of caca rnn a and caca rnn  b is higher than the others in evaluation in map  mean average  precision  and accuracy. it shows that cascade attention based  rnn can improve the performance of handling two kinds of  features.  vi. r esults on mec       five submissions are allowed to submit to video based  emotion recognition sub challenge in mec    . evaluation  on mean average precision  map  and accuracy are both  considered in mec  two  of five  evaluation results of submissions are shown in table. ii       caca rnn   described  in previous sections  the one with highest map       cacarnn  d  d cnn   the one with highest best accuracy. their  confusion matrices are shown in fig.  .  caca rnn  d  d cnn is an ensemble network of  caca rnn and a  d  d cnn network using linear  weighted output from each model as the output prediction.  the  d  d cnn network is a spatiotemporal convolutional  network  depicted in fig.    learning temporal information  from high level face feature maps from the same face feature  extractor cnn in caca rnn.     d  cnn     d  cnn     d  cnn     d  cnn     d  cnn     d  cnn    t      t      t      t      shared  parameters    fig.    a  d  d cnn model for emotion recognition. the  video stream is preprocessed into face stream of cropped faces.  each face of face stream is first encoded by a pre trained cnn  face feature extractor to high level feature maps. then a  d  convolution neural network learns spatiaotemporal kernel from  the feature map stream. there is an adaptive pooling before  classifier to handle various length of input.  method  caca rnn  caca rnn    d  d cnn  baseline        validation set  map      acc        .      .      .      .     testing set  map      acc        .      .        .      .       .      .       .      .     table ii  submitted results to mec    .    vii. c onclusion  in this work  context information is first shown to be helpful  in emotion perception. then  different architectures using facial and or context information for video emotion recognition  was implemented and evaluation results were compared. it  shows that the models using both information can achieve the  highest accuracy.  among them  a novel architecture  caca rnn  was proposed with a cascaded lstm attention based architecture  to leverage both face and context information from video.  caca rnn has the best performance in the compared models. caca rnn consists of two lstms  context rnn and  face rnn  processing context and face features respectively   and attention mechanism in the face rnn enables it to learn  relationship to the context rnn and fuse information from  two sequences. compared to a multi branch rnn  parallelrnn  and a single rnn handling concatenated features   concatenated rnn   caca rnn outperforms on evaluation  mec     validation dataset. the experiments also shows that  context information improves for video emotion recognition.  the models with additional context features perform better  than the models using only face features.  caca rnn may be further extended to fusing more kinds  of inputs for multi modal emotion recognition or be extended  to other video based tasks such as action recognition.      a  caca rnn     b  caca rnn  d  d cnn    fig.    confusion matrices of mec     testing set.  a   the model has the highest map among five submissions in  mec    .  b  the model has the highest accuracy among five  submissions in mec    .    