introduction  paper     describes an automatic vehicle classifier  avc  for toll roads  based on video classification and installed  on most of russian toll roads. vehicle passage detector  vpd  is one of the most important parts of the avc system.  vpd uses as input several binary signals from other avc subsystems  binary detectors   and makes decisions about a  vehicle passage  whether a vehicle is located in the checkpoint area . vpd is based on a  voting scheme   a vehicle  passage is detected if most of the binary detectors provide a positive answer. this logic is augmented by a set of empirical  rules  provided by a human expert to quantify and take into account time delays between switches of the binary signals   properties of a sequence of these switches and other information. these rules were extended and modified during avc  test deployment based on an analysis of encountered errors.  the previous paper  devoted to vpd in avc      states that the vpd accuracy is   .   . since then the test dataset  has been extended by new detection and classification error cases. it should be noted that in the current paper we use  tests which run with a disabled trailer coupler detector. avc version  described in the previous paper  provides   .     accuracy on the new dataset if the coupler detector is disabled. at the same time the current classifier version provides    .    accuracy without the coupler detector and   .    with it. comparing to the previous version the new classifier  has optimized algorithms for a shield and a trailer couplers detection  a correlational detector and an updated fusion method  for binary detectors aggregation.  creating rules of this type is a painstaking job requiring creative approach. it would be interesting to develop an  automatic method where a machine learning algorithm could replace a human expert. this approach potentially could also  produce a higher classification quality. therefore  in this paper we solve the problem of creating a method for automating  an avc synthesis and minimizing a human involvement.     . data description  the input data consists of avc log records. each file contains one or several vehicles passages. all the numerical  experiments are conducted on a dataset consisting of      log files. the system log is filled with three dimensional signal  samples xt   each component of which is binarized and produced by one of the following sensors  a correlational detector   sensitive to changes in video stream images   an induction loop  mounted inside a lane and sensitive to a metal   a shield  detector  detects occlusions of a shield  located opposite to a camera . also the records contain a frame sequence number   manually created reference signal  labels  and predictions of a basic classifier  which is based on human tweaked rules    .  a file record is created and saved in a database if and only if at least one of the input signals has changed.  table   shows a log sample. notations  shield   a shield detector  loop   an induction loop  cor   a correlational  detector  basic clf   a basic classifier prediction  ref. pass   a reference passage.     frame  no.                                                 shield    loop    cor    basic clf    ref. pass                                                                                                                table    a log and a labeled sample                                  . quality evaluation  the problem specifics dictates that a special quality passage evaluation metric should be used  which is equal to the  standard pointwise two class classification metric accuracy only in extreme cases. the reason is that the standard metrics  using pointwise difference between a reference signal and a predicted signal are not able to estimate a passage classification  quality from a physically sensible viewpoint. the metric used in this paper is a pass quality  pq   pq   r  r err   where  r is a number of correctly detected passages  while   err is a sum of classification error costs on a whole test dataset.  calculation of   err for various error types  missed passage  merged passages  etc.  is a complicated procedure described  in      see also table  . here l denotes the true number of passages in the analysed test signal  k denotes the number of  detected passages  see details in     .  ref. pass.           l     l    detected pass.  err. desc.     no error     missed passage     false passage     merged passages  k  split passage  k  multiple error  table    classification error costs    err. weight           l  k  max l  k     this quality metric does not take into account how far the detected passage is shifted from the ideal one. however  the conducted experiments show that this is not needed for applications  since the sequence of correct passages and their  intersections with real passages at least in one instant are important  not the delays themselves.     . exploratory analysis  in this section we compare results obtained with various machine learning methods  gradient tree boosting xgb  see  description of the algorithm in      the implementation from     was used   logistic regression lr from the scikit learn  package      fully connected neural network nn with a one hidden layer consisting of    neurons  see approaches to  training such networks in          simple recurrent neural network simplernn from the keras package    .  due to an atypical task partitioning of the data into the training and the control sets were organized as follows  logs for  a respective set are randomly selected in a random order  but the order of frames inside every log remains unchanged.  results for training on the source signal xt without accounting for past values are provided in table    see lines xgb     lr    nn  . the obtained results imply that due to the autocorrelation of xt   classification using standard methods without  taking into account the dependency of xt on xt     . . . xt k   . . .   provides low pq values. the situation that pq values are  comparable can be easily explained by the fact that at each moment of time the three dimensional vector xt can take only  eight distinct values  thus in the considered case all methods can easily provide similar decision rules.  the first idea to increase classification quality w.r.t. the pq metric is to extend the feature space xt by using previous  values  xt     . . .   xt w  . the results of these experiments are provided in table    see lines xgb    lr    nn  . the optimal  window size w for each classifier type is selected using the cross validation procedure. the threshold  producing a binary     signal from a classifier output probability  provided by the logistic regression and the neural network   is selected by  maximizing the objective function on the training set. one may see that this extension provides significant increase in  quality. the nn  gives the best result. this result  however  is not better than that of the basic classifier.  the second natural idea is to use recurrent neural networks  which proved to be productive in labelling of sequences  and time series classification. we will use the standard recurrent network simplernn with a one recurrent layer and a  one hidden layer     . it turns out that this model allows to obtain a higher quality which is pretty close to that of the basic  classifier.  classifier  r  pq    err  xgb       .      .   .     lr       .      .   .     nn       .      .   .     xgb       .     .   .     lr       .     .   .     nn       .     .   .     simplernn      .     .   .     basic classifier     .     .   .     table    comparison of classifier models    note  the basic classifier uses an additional feature from the trailer coupler detector. this feature allows to distinguish  between vehicles moving at a close distance and vehicles with trailers thus increasing the basic method accuracy pq from   .    to  .   . in our experiments we do not use this feature  although this additional information could potentially  increase classification quality.  the primary reason for a low prediction accuracy is that all algorithms optimize not the target quality metric but a  different value   the mean squared prediction error mse  which is not in a good correspondence with the target metric  pq. hereinafter by an error we mean pqe       pq.     . classification based on rnns  from the results of section   we can notice that only the rnn model provides results with the accuracy comparable to  that of the baseline classifier  constructed manually by collecting and ensembling rules  distributed in time. thus  rnns  is a promising approach to automate construction of classifiers and further improve the accuracy of vpd.     .  rnn architecture  the original recurrent neural network model simplernn contains only one hidden layer  the output signal of each  neuron is used as an input to the same neuron at the next moment of time. in case an input signal has a long duration   the exploding and the vanishing gradient problem appears when training the model and calculating gradients of a neural  network performance function  see      for more details. this effect stems from the fact that the gradients of the rnn s  performance function depend on the product of the gradients of neurons in the hidden layer  calculated for all successive  values of the training signal  see fig.    as a consequence  this product can take big absolute values as well as tends to  zero.  one approach to avoid this effect is to use the long short term memory neural network architecture  lstm   which  allows effective modelling of long range dependences in signals.    figure    long range dependence in rnn  in             the authors proposed the neural network architecture called gated recurrent unit  gru   based on the     same principles as the lstm  but it uses a more economical parametrization and fewer operations to compute an output  signal. a more extensive overviews of rnn architectures can be found in         .     .  selection of rnn architecture  to use rnn in practice  it is necessary to search for its optimal architecture. this section describes results on this  matter. in experiments we use the following types of rnn  lstm  gru and simplernn. all experiments are performed  using the keras framework      which is a wrapper of the python libraries theano      and tensorflow     .  for each of the rnn types we conduct a series of experiments  we play with network hyper parameters  activation  function types and a window length w. since the training time of an rnn with several layers is rather big  we fix the  number of hidden layers to be equal to one  number of neurons in each layer is limited from above by eight  the maximum  number of learning epochs is limited from above by   . in table   we provide results  averaged over    experiments  of  optimal architecture selection for each rnn type. one may see that selection of hyper parameters  even in a limited space  significantly improves the quality of simplernn model  cf. with the first experiments presented in table  . according to  these results we decide to continue to use architecture containing lstm layers  since it provides the highest performance.  model  r  pq    err  simplernn      .     .   .     lstm      .     .   .     gru      .     .   .     basic classifier     .     .   .     table    comparison of different rnn models     .  further improvements  since when training a neural network we optimize a mean square point wise error  which is not appropriate for the  considered problem  in this section we consider various additional approaches to improve further the detection accuracy   evaluated by the pq value. in particular  we consider the following tweaks  weighting of the mean square error  used as  a performance function when training a neural network  smoothing the input signal by a morphological filter       adding  a penalty on a derivative of the neural network output  used when calculating the performance function  optimization of a  threshold value  used to binarize the output signal  according to the target quality criterion pq on the training set  applying  the morphological filter to the neural network output signal.  it turned out that improvement of the detection accuracy can be obtained when expanding significantly the structure  of the lstm neural network. in fig.   we provide the modified architecture we use  one input lstm layer and two  hidden dense layers. however  if we do not use dropout transformation  despite the fact that the standard error mse  decreases on the validation sample up to  .      .    the target error pqe increases on the test set. in turn  if we use  dropout transformation before each dense layer  then mse increases up to  .    but at the same time the target error  pqe decreases for about  .       .   .  also we can achieve additional significant improvement of the detection accuracy by selecting the threshold value of  the output signal via the cross validation procedure on the training set and the subsequent application of the morphological  filter to the neural network output  binarized using the selected threshold value.  also we evaluate an importance of each input signal component by estimating its influence on the accuracy of the final  model. in table   we provide values of the performance criterion for models  constructed using all possible combinations  of input features. we can see that the best set of features is a pair  shield  cor .     . conclusions  we can achieve significantly better quality of classification equal to  .    using only two input features  whereas in  order to achieve the detection performance pq equal to  .     the original  handcrafted  classifier takes as input additional  fourth feature from the trailer coupler detector  without which the performance drops to  .   . thus  in this study we  developed the automated approach for constructing and training a classifier which is superior in terms of vpd performance  to the previously constructed classifiers. at this stage  further research is possible in several directions.  first  we can increase the classification accuracy by a direct optimization of the pq criterion when training the neural network. the implementation of such learning algorithm is possible through the use of gradient free optimization  algorithms.  second  we can create an aggregating mechanism for calculating a final decision from outputs of different detectors.     figure    final lstm rnn model  and finally  we can implement an integrated solution in order to eliminate the initial input data pre processing  provided  by classical image recognition methods and other additional steps of data processing  which happen between the event  a  vehicle is shot with a camera  and the event  a binarized input signal xt is produced . in other words  we propose to  use the following neural network structure  which realizes all vpd subsystems by a single stack of convolutional neural  networks and rnns     on the first level we use a set of convolutional neural networks  processing images from all available cameras to  extract features     on the next levels features  extracted by the set of convolutional neural networks  are combined through the rnn  architecture with signals  obtained from the induction loop and other devices of the avc system     finally  the rnn type model with the structure similar to the one  shown in fig.    is used for passage detection.  acknowledgements  the work of the first author was supported by the rfbr grants             a and              ofi m. the work of the other authors was conducted in iitp ras and supported solely by the russian science foundation  grant  project             .    