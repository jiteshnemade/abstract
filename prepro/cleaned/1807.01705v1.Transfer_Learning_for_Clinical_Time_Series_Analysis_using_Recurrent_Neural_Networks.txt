introduction    electronic health records  ehr  consisting of a patient s medical  history can be leveraged for various clinical applications such as diagnosis  recommending medicine  etc. traditional machine learning  techniques often require careful domain specific feature engineering before building the prediction models. on the other hand  deep  learning approaches enable end to end learning without the need  of hand crafted and domain specific features  and have recently  produced promising results for various clinical prediction tasks          . applications of such approaches include medical diagnosis           predicting future clinical events       etc.  deep recurrent neural networks  rnns  have been successfully  explored for various time series and sequential modeling applications of ehr data such as diagnoses             mortality prediction  and estimating length of stay            . however  training rnns  is compute intensive due to sequential nature of computations  and  requires large amount of labeled data. transfer learning            has been used to overcome these challenges. it enables knowledge  transfer from neural networks trained on a source task with sufficient training instances to a related target task with few training  instances. moreover  fine tuning a pre trained network for target  task is often faster and easier than constructing and training a new  network from scratch        . another advantage of learning in  such a manner is that the pre trained network has already learned  to extract a rich set of generic features that can then be applied to  a wide range of other similar tasks        .  transfer learning via fine tuning parameters of pre trained models for end tasks has been recently considered for medical applications  e.g.        . however  fine tuning a large number of parameters with a small labeled dataset may cause overfitting. if the  parameters to be tuned for target task can be reduced to a small  number  then the pre trained deep models can be leveraged in a  better way    . in this work  we evaluate an approach to transfer the  learning from a set of tasks to another related task for clinical time  series by means of an rnn. considering phenotype detection from  time series of physiological parameters as a binary classification  task  we train an rnn classifier on a diverse set of such binary  classification tasks  one task per phenotype  simultaneously using  a large labeled dataset  so that the rnn thus obtained provides  general purpose features for time series. the features extracted  using this rnn are then transferred to train a simple logistic regression model     for target tasks  i.e. identifying a new phenotype  and predicting in hospital mortality  with few labeled instances   detailed in section   .  through empirical evaluation on mimic iii dataset      as detailed in section     we demonstrate that     it is possible to leverage  deep rnns for clinical time series classification tasks in scarcelylabeled scenarios via transfer learning     a deep model trained on  multiple diverse tasks on a large labeled dataset provides features  that are generic enough to build models for new tasks from clinical  time series data. further  our approach provides a computationallyefficient way to use deep models for new phenotypes once an rnn  has been trained to classify a diverse enough set of phenotypes.         related work    unsupervised pre training has been shown to be effective in capturing the generic patterns and distribution from ehr data     . further  rnns for time series classification from ehr data have been  successfully explored  e.g. in        . however  these approaches  do not address the challenge posed by limited labeled data  which  is the focus of this work. transfer learning using deep neural networks has been recently explored for medical applications  a model  learned from one hospital could be adapted to another hospital for  same task via recurrent neural networks    . a deep neural network  was used to transfer knowledge from one dataset to another while     mlmh workshop  kdd  august              london    gupta  malhotra  vig  shroff    the source and target tasks  named entity recognition from medical records  are the same in     . however  in both these transfer  learning approaches  the source and target tasks are the same while  only the dataset changes.  features extracted from a pre trained off the shelf rnn based  feature extractor  timenet       have been shown to be useful  for patient phenotyping and mortality prediction tasks    . in this  work  we provide an approach to transfer the model trained on  several healthcare specific tasks to a different  although related   classification task using rnns for clinical time series. training a  deep rnn for multiple related tasks simultaneously on clinical time  series has been shown to improve the performance for all tasks     . in this work  we additionally demonstrate that a model trained  in this manner serves as a reasonable starting point for building  models for new related tasks.          .      i    i   z    f e  x i    we    y  i       wc z    l   bc     i      i     ys by y for simplicity of notation  we have x   x  x  . . . x  denote  a time series of length     where xt   rn is an n dimensional vector  corresponding to n parameters such as glucose level  heart rate   etc. further  y    y    . . .   yk           k   where k is the number of  binary classification tasks. for example  for k     binary classification tasks corresponding to presence or absence of   phenotypes   y                   indicates that phenotypes       and   are present   i   i  nt  while phenotypes   and   are absent. dt     xt   yt   i    such   i   yt    that nt   n s   and           such that the target task is a  binary classification task. we assume that the time series in dt  belongs to same n parameters as in ds . we first train the deep rnn  on k source tasks using ds   and then train the simpler logistic  regression  lr  classifier for target task using dt and the features  obtained via the deep rnn  as shown in figure  . we next provide  details of training rnn and lr models.    pre trained rnn    logistic regression for target task    gru    z       z       z       z     gru    x     sparse weigths  z     z     gru    x     yt    ys    sigmoid layer    z       z       features for target task    x     figure    inference in the proposed transfer learning approach. rnn with l     hidden layers is shown unrolled  over       time steps.     i      i      i      i      i     c yk   y k     yk   lo  y k          yk     lo       y k     ns    k         i   i   l    c yk   y k  .  n s   k i      proposed approach    consider sets ds and dt of labeled time series instances corresponding to source  s  and target  t   tasks  respectively. ds     i   i  n s    where n s is the number of time series instances    xs   ys   i    corresponding to n s patients  in our experiments  we consider each  episode of hospital stay for a patient as a separate data instance .   i   denoting time series xs by x and the corresponding target label    supervised pre training of rnn    training an rnn on k binary classification tasks simultaneously  can be considered as a multi label classification problem. we train a  multi layered rnn with l recurrent layers having gated recurrent  units  grus      to map x i    ds to y i  . let zt l   rh denote the  output of recurrent units in l th hidden layer at time t  and zt     zt     . . .   zt  l     rm denote the hidden state at time t obtained as  concatenation of hidden states of all layers  where h is the number  of gru units in a hidden layer and m   h  l. the parameters of the  network are obtained by minimizing the cross entropy loss given  by l via stochastic gradient descent            k            e  x        here    x     is the sigmoid activation function  y  i  is   i   the estimate for target y   we are parameters of recurrent layers   and wc and bc are parameters of the classification layer.     .     using features from pre trained rnn   i     for input x i    dt   the hidden state z  at last time step   is  used as input feature vector for training the lr model. we obtain  probability of the positive class for the binary classification task   i   as y   i       wc  z    bc     where wc    bc  are parameters of lr. the  parameters are obtained by minimizing the negative log likelihood  loss l      l         nt     i      c y  i    y   i        w  c                 where   wc        m  j    w j   is the l  regularizer with   controlling  the extent of sparsity   with higher   implying more sparsity  i.e.  fewer features from the representation vector are selected for the  final classifier. it is to be noted that this way of training the lr  model on pre trained rnn features is equivalent to freezing the  parameters of all the hidden layers of the pre trained rnn while  tuning the parameters of a new final classification layer. the sparsity constraint ensures that only a small number of parameters are  to be tuned which is useful to avoid overfitting when labeled data  is small.         experimental evaluation    we evaluate the proposed approach on binary classification tasks  as defined in      i  estimating the presence  class    or absence   class    of a phenotype  e.g. cardiac dysrhythmia  chronic kidney  disease  etc.  from time series of parameters such as heart rate and  respiratory rate  and ii  in hospital mortality prediction where the  goal is to predict whether the patient will survive or not given time  series observations after icu admission  class    patient dies  class     patient survives .  dataset details  we use mimic iii  v .   clinical database     which consists of over     transfer learning for clinical time series using rnns         icu stays across        critical care patients. we use benchmark data from     with same data splits for train  validation and  test datasets  . train  validation and test sets for various scenarios  considered in our experiments are subsets of the respective original  datasets  as described later . the data contains multivariate time  series for multiple physiological parameters with    real valued   e.g. blood glucose level  systolic blood pressure  etc.  and   categorical parameters  e.g. glascow coma scale motor response  glascow  coma scale verbal  etc.   sampled at   hour interval. the categorical  variables are converted to one hot vectors such that final multivariate time series has dimension n     . we use time series from  only up to first    hours of icu stay for all predictions  such that          to imitate the practical scenario where early predictions  are important.  the benchmark dataset contains label information for presence absence of    phenotypes common in adult icus  e.g. acute  cerebrovascular disease  diabetes mellitus with complications  gastrointestinal hemorrhage  etc. . we consider k      phenotypes to  obtain the pre trained rnn which we refer to as mimic net  mn    and test the transferability of the features from mn to remaining    phenotype  binary  classification tasks with varying labeled data  sizes. since more than one phenotypes may be present in a patient  at a time  we remove all patients with any of the   test phenotypes  from the original train and validate sets  despite of them having one  of the    train phenotypes also  to avoid any information leakage.  we report average results in terms of weighted auroc  as in       on two random splits of    train phenotypes and   test phenotypes   such that we have    test phenotypes  tested one at a time . we  also test transferability of mn features to in hospital mortality  prediction task.  we consider number of hidden layers l      batch size of       regularization using dropout factor      of  .   and adam optimizer       with initial learning rate      for training rnns. the number  of hidden units h with minimum l  eq.    on the validation set  is chosen from                     . best mn model was obtained  for h       such that total number of features is m      . the l   parameter   is tuned on   .    .  . . .        on a logarithmic scale   to minimize l    eq.    on the validation set.     .      auroc     .     .     .     .             lr  rnn c  mn lr    mn lr                 training data size     a  phenotyping     .     .     .     .     .     .      auroc     .                lr  rnn c  mn lr    mn lr                 training data size     b  in hospital mortality prediction    figure    auroc with varying labeled data size. models using transfer learning  mn lr   and mn lr    perform significantly better for smaller training data sizes.      refer        and https   github.com yerevann mimic  benchmarks for dataset sizes and  other details.    mlmh workshop  kdd  august              london  table    fraction of features with weight    .  task  phenotyping   in hospital mortality    lr   .       .      .       mn lr     .       .      .       mn lr     .       .      .       results and observations  we refer to the lr model learned using mn features as mn lr   and consider two baselines for comparison     logistic regression   lr  using statistical features  including mean  standard deviation   etc.  from raw time series as used in         rnn classifier  rnn c   learned using training data for the target task. to test the robustness  of the models for small labeled training sets  we consider subsets  of training and validation datasets  while the test set remains the  same. further  we also evaluate the relevance of layer wise features  z   l from the l     hidden layers. mn lr   and mn lr   refer  to models trained using z      the topmost hidden layer only  and  z     z       z        from both hidden layers   respectively.  robustness to training data size  phenotyping results in figure   a  suggest that   i  mn lr and rnn c perform equally well  when using      training data  and are better than lr. this implies  that the transfer learning based models are as effective as models  trained specifically for the target task on large labeled datasets.  ii   mn lr consistently outperforms rnn c and lr models as training dataset is reduced. as the size of labeled training set reduces   the performance of rnn c as well as mn lr degrades. however   importantly  we observe that mn lr degrades more gracefully and  performs better than rnn c. the performance gains from transfer  learning are greater when the training set of the target task is small.  therefore  with transfer learning  fewer labeled instances are needed to  achieve the same level of performance as model trained on target data  alone.  iii  as labeled training set is reduced  lr performs better  than rnn c confirming that deep networks are prone to overfitting  on small datasets.  from figure   b   we interestingly observe that mn lr results  are at least as good as rnn c and lr on the seemingly unrelated  task of mortality prediction  suggesting that the features learned are  generic enough and transfer well.  importance of features from different hidden layers  we  observe that mn lr   and mn lr   perform equally well for phenotyping task  figure   a    suggesting that adding features z      from lower hidden layer do not improve the performance given  higher layer features z     . for the mortality prediction task  we  observe slight improvement in mn lr   over mn lr    i.e. adding  lower layer features helps. a possible explanation for this behavior  is as follows  since training was done on phenotyping tasks  features from top most layer suffice for new phenotypes as well  on  the other hand  the more generic features from the lower layer are  useful for the unrelated task of mortality prediction.  number of relevant features for a task  we observe that only  a small number of features are actually relevant for a target classification task out of large number of input features to lr models       for lr      for mn lr    and     for mn lr     as shown in table          of features have weight      absolute value    .     for    the    average and standard deviation over    phenotypes is reported.     mlmh workshop  kdd  august              london  p   p   p   p   p   mortality                   gupta  malhotra  vig  shroff                                                                                                          .    .    .    .    .    .     figure    feature weights  absolute  for mn lr  . here pi  i      . . .       denotes i th phenotype identification task. x axis   feature number  y axis  task.  mn lr models corresponding to phenotyping tasks due to sparsity  constraint  eq.     i.e. most features do not contribute to the classification decision. the weights of features that are non zero for  at least one of target tasks for mn lr   are shown in figure  . we  observe that  for example  for mn lr   model only     features   out of      are relevant across the    phenotype classification tasks  and the mortality prediction task. this suggests that mn provides  several generic features while lr learns to select the most relevant  ones given a small labeled dataset. table   and figure   also suggest  that mn lr models use larger number of features for mortality  prediction task  possibly because concise features for mortality prediction are not available in the learned set of features as mn was  pre trained for phenotype identification tasks.         conclusion    we have proposed an approach to leverage deep rnns for small  labeled datasets via transfer learning. we trained an rnn model  to identify several phenotypes via multi label classification. this  model is found to be generalize well for new tasks including identification of new phenotypes  and interestingly  for mortality prediction. we found that transfer learning performs better than the  models trained specifically for the end task. such transfer learning  approaches can be a good starting point when building models  with limited labeled datasets. transferability and generalization  capability of rnns trained simultaneously on diverse tasks  such  as length of stay  mortality prediction  phenotyping  etc.          to  new tasks is an interesting future direction.    