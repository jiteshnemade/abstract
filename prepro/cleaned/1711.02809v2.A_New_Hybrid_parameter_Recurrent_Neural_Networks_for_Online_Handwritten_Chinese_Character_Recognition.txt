introduction  decades ago  rnn was proposed as a perception tool for  sequence processing. with the widespread use of rnns   many improvements have been obtained  particularly in avoiding vanishing gradient problem  bengio  simard  and frasconi      . lstm  hochreiter and schmidhuber       and  gru  cho et al.       are two famous and popular improvements of rnns  hidden layer function and have been applied  in many tasks  such as speech recognition  hinton et al.         natural language processing  nlp   ruales        character recognition  messina and j.louradour        machine  translation  cho et al.      . alex graves  graves        proposed a sequence generator based on the rnns equipped  with lstm. he and tang  he et al.       applied lstmrnn to scene text recognition. zhang et al.  xu yao zhang        proposed a chinese character recognizer based on  rnns equipped with lstm and gru. gru was first proposed for machine translation by bengio  cho et al.      .  compared with lstm  gru has a simpler structure and  obtains competitive training results.    figure    the example of in air handwritten with the leap  motion sensor.  interaction way. with some sensors  e.g.  the leap motion  sensor  and computers  people write in air and computer  recognizes what you write quickly. this kind of humancomputer interaction way is shown in fig.  . generally  speaking  in air handwritten chinese characters recognition  is more challenge than traditional handwritten chinese character recognition  hccr   liu  jaeger  and nakagawa        bai and huo       okamoto and yamamoto      . first   each character has only one stroke without any mask of pen  up and down. second  in air handwritten strokes would be  more squiggly lines than handwritten strokes on touch screen.  fig.   gives some examples to distinguish the difference  between iahcc and hcc on touch screen. since in air  handwritten is such an amazing way of human computer interaction  many researchers explore in this filed. qu et al.  qu  et al.       presented a multi stage classifier for iahccr      generated as    i      i      i      i      i     xi   x    x            x t       x t         x t           i      i      i      i                    xt   x    x            x t      .     a  in air handwritten chi   b  handwritten chinese  nese characters  characters on touch screen    figure    comparison of hcc and iahcc  ren et al.      .    and their system achieved a relatively low accuracy. qu et  al.  qu et al.       presented a new feature representation  to extend the power of   direction feature and applied it  in iahcc recognition.   direction feature had been shown  to be a discriminative feature in many works and achieved  a good performance  bai and huo       liu et al.        jin et al.      . ren et al.  ren et al.       proposed an  end to end recognizer for oiahccr based on a new rnn.  in ren s work  it is the first time for rnn to be used for  oiahccr and to obtain a high recognition accuracy.  in the proposed system  three contributions are proposed  to increase the recognition accuracy or the calculation speed  of the in air handwritten chinese character rnn recognizer.    first  a new hidden layer function mpu is proposed. compared with lstm and gru respectively  the proposed  mpu has fewer parameters and a straightforward workflow. a series of experiments were carried out on iahccucas     dataset. it is proved that the proposed hidden  layer function  mpu  obtains a high recognition accuracy.    second  a hybrid parameter rnns architecture is proposed.  compared with general rnns and bidirectional rnns  correspondingly  the proposed hybrid parameter rnns  architecture obtains competitive recognition accuracy with  fewer parameters and faster calculation speed.    third  when all the size of hidden layers of a rnn are  same  we make a suggestion that all the outputs of each  layer are stacked as the output of network. by synthesizing  all the outputs of each layer  stacked method increases the  recognition accuracy without increasing the parameters.    basic rnn system  hybrid parameter rnns  calculation speed and parameter quantity are the two significant performance index for all the neural networks. however   designing an efficiency neural network with fewer parameters and faster calculation speed is a challenging research.  fig.   shows the proposed hybrid parameter rnns system  and input data processing of input sample i. the left part  of fig.   shows all the input character location sequence is  divided into two parts. then the new sequence of sample i is    as shown in eq.      the new sequence of sample i is represented as a sequence with  t       t location dots. the  network calculation process is shown in the right part of  fig.  . to begin with  two sets of rnn parameters     and       parameters in    are initialized randomly and parameters in     is initialized as zeros  are initialized with the same scale.  given a character sample i  the new system input is a sequence of location dots  as shown in eq.    . when the rnn  iterating on temperal dimension  the parameters are changed  as follows. during the first  t     time step s iteration  the  rnn calculates with parameters    . then the parameters  change to         when the rnn iterates the time steps from   t     to t . the rnn calculates the iteration of the last  t      time step with the only parameters    . for each time step of  first  t     time  the hidden layer states are computed by  h t   h   xt   h t            hnt   hn  xt   hn      hnt       n     t  where t                  t    . n                 n           where h  and hn denote the first and nth hidden layer hidden  layer function respectively.     and   n denote the corresponding network parameters. n denotes that there are n hidden  layers. for each time step of second t    t     time  the  hidden layers states are computed by  h t   h   xt   h t                  hnt   hn  xt   hn      hnt       n     n     t  where t    t           t                  t. n                 n        where   n     n denotes the network parameters of the nth  layer during the second t    t     time step. for each time  step of the last  t     time  the hidden layers states are computed by  h t   h   xt   h t            hnt   hn  xt   hn      hnt       n     t  where t   t      t              t    t    . n                 n        where   n denotes the network parameters of nth layer during  the last  t     time steps. during the whole temporal iteration   both     and     are participated in the rnn calculation with  all the location dots of the original sample i. after all the t     t     time step iterations  t    t     hidden layer states are  n  n  n  generated at the n th layer  e.g.  hn      h    ...ht ...ht   t     .  then the final output of the rnn is computed by  y   by    whu  y u    whu  y u     u       t  x  t      hn  t                 figure    hybrid parameter rnns.  t   t        u       x    hn  t    kinds of rnn could be represented as          t  t          where y denotes the output vector  whu  y and whu  y denote the weight matrix from u  and u  to the fully connected  layer  output layer . by denotes the fully connected layer  bias vector. the sum operation  sum pooling in eq.     and  eq.      proposed by ren et al.  ren et al.      . then a  softmax regression is used on the fully connected layer outputs for computing the class probability distribution. since an  in air chinese handwritten character generally corresponds  to a long sequence of dot locations and the proposed network architecture involves five hidden layers  we add the skip  connections  graves       from the input layer to all hidden  layers  and from all hidden layers to the fully connected layer   to alleviate the vanishing gradient problems  bengio  simard   and frasconi      .  computing speed and parameters quantity general  rnns are computed with only one set of parameters during all the iteration at temperal dimension. the parameters  are changed during iterating at temporal dimension in the  proposed rnn with hybrid parameters. experimental results  show the proposed rnn structure obtains a higher recognition accuracy with a smaller hidden layer size. the smaller  hidden layer size means the fewer parameters  e.g.  a general  rnn with five     size hidden layers could obtain a recognition accuracy of   .  . the recognition accuracy can not  be increased when the hidden layer size increases. however   recognition accuracy would be decreased when the hidden  layer size decreases. the proposed rnn with hybrid parameters could obtain a higher recognition accuracy of   .    with     size. the numerical relationship between the two    q       d    d    d    d            dn      dn        z     w                           d   d   d   d            dn   dn        z              w     q            d    d    d    d            dn      dn     z        w                                d   d   d   d           dn   dn         z              w     where q denotes the parameter quantity of the general rnn.  dn denotes the hidden layer size of nth hidden layer. w   denotes the parameters of all the weighted matrix of each  layer inputs. w  denotes the parameters of all the weighted  matrix from the hidden layer state ht   to ht . and q  dn    w    w  have the similar definition for the proposed hybridparameter rnn. the multiplicator   in eq.     and eq.      represents the number of state vectors in the hidden layer  function. the hidden layer function used in rnn is gru.  therefore  the number of state vectors is  . the multiplicator    in eq.     represents there are two set of parameters with  the same scale. from the eq.     and eq.     we can see that  q is as twice as q the hybrid parameter rnn and the general  rnn have same hidden layer size.  compared with the bidirectional rnn  the hybridparameter rnn is calculated with only t    t     time steps   iteration  while the bidirectional rnn needs to be calculated  with     t time steps  iteration. therefore  the proposed  hybrid parameter rnn has a faster computing speed than the  bidirectional rnn and obtains a competitive result.     learning parameters  at the top of the rnn  a softmax layer is used to generate  the probability distribution corresponding to      character classes. to train the neural network  the following loss  function is widely used and minimized  i.e.     it     wxi xt   whi ht     bi       j                   m x  k  x          m i      the memory pool stores a state vector  which is updated by  the inputs with the limitation of input gate. with the control  of output gate  the new hidden layer state is generated by  using the updated memory pool state. the proposed mpu  could be described as follows       c  i       l       i   exp yl                 l log pk   i   j   exp yj                           c  i    l            c  i    l             c  i     l.  where m is the total number of training patterns  and the        i   function selects the true class of the example. yj denotes the  corresponding output s jth element of the rnn given a pattern x i  . the minimization of the loss function corresponds  to maximizing the probability of correct classification of patterns in essence. then the optimal parameters of the proposed  system are obtained by constantly updating parameters based  on the rmsprop  tieleman and hinton       method  a form  of stochastic gradient descent.    memory pool unit    figure    memory pool unit.  hidden layer function is significant for rnns. lstm and  gru are the two popular hidden layer functions used for  many tasks. as a hidden layer function with a simpler architecture and fewer parameters  gru obtains similar performance with lstm. to make computation and implementation much simpler  memory state is wiped out in the gru.  however  hidden layer function with memory state make the  whole hidden layer state calculation process more reasonable and convictive. we propose a new type of hidden layer  function with memory state that has been motivated by the  lstm and gru but is simpler than lstm. in this section   we focus on describing the proposed hidden layer function  memory pool unit  mpu . mpu is based on a simple and  more straightforward hidden layer state calculation process.  as shown in fig.    the core of the proposed hidden layer  function is a memory pool. there are two gates in the architecture of mpu. one is input gate and the other is output gate.    mt   tanh wxm  it    xt     whm  it ht             it   mt                    ot     wxo xt   who ht     bo           ht   ot mt        where i and o denote input gates and output gates. m denotes  the memory pool state. eq.      shows that all the inputs  of memory pool are restricted by the same input gate. the  inputs include the last layer s outputs  the network inputs  and the hidden layer state of last time step  ht    . the last  layer s outputs and the network inputs are represented by xt  in equations from eq.      to eq.     . as shown in eq.        when the input gate is close to zero  the memory pool state  is forced to ignore the previous inputs. therefore  mt keeps  its former state. for other situations  the memory pool state  is updated by the inputs and its former state. the current  hidden layer state is calculated using the memory pool state  under the control of the output gate. the whole processing  mechanism is like a pool with two valves and a self renewal  capacity. the input gate allows useful information to pour  into the pool  then the pool state self renews  and the useful  information drains out through the output gate at last. the  self renewal capacity of memory pool state  as shown in  eq.       is the most important part of the mpu. on one side   the input gate restricts not only the last time step hidden layer  state but also the current inputs. by restricting the hidden  state of last time step  significant information is taken out  for later computation and irrelevant information is ignored.  the restriction to the current input is also important for our  memory pool. without the restriction  a great deal of useless  information will be stacked in the pool when the rnns are  deep  to be exactly  without the restriction to current input  the  training process of rnns becomes difficult if there are more  than two hidden layers . on the other side  the information  of pool input and former memory pool state used to renew  the memory pool state is complementary in a way. this kind  of complementary makes better use of all the information  and avoids irrespective information being stacked in the pool.  because all the inputs are restricted by the input gate  it is  significant that the network inputs are used for compensating  the information loss of the vertical direction. wxi denotes  the weighted matrix from input vector to input gate. whi    who   wxo   wxm and whm have similar meanings. bi and  bo are the biases of the corresponding gates.    memory pool unit with input compensation  as network input compensation is significant for the proposed  mpu  another method for compensating the information loss  of the vertical direction is proposed. we change the eq.      of last subsection into  ht   tanh ot    mt   relu wxc xt                where wxc denotes the compensation weighted matrix of  last layer s outputs. with the compensation item in eq.       network input is no longer necessary for each layer except  the first layer.    computed as   y   by   whu  y u    whu  y u     and the network output y could be represented as     stacked different hidden layer states  in the general rnns  the neural network outputs are calculated by using only the last hidden layer state  xu yao zhang        or all the hidden layers states  graves      . it is better  to calculate the outputs by using all the hidden layers states.  generally  the calculation of all the hidden layers states is  always weighted sum  such as   n  x         whnu y u n      n  x         n      whnu y u n                   n      and the network output y could be represented as   by      n  x    whnu y u n       by      n  x         n    n  x    whnu y u n       n       whnu    t  x       y    n      t   t        hnt   whnu    x       n x  t  x   by       whnu y hnt    n x  t  x    t  t        x    w    hn  u  y    hnt      t  t            xt   hn      hnt         n     n   whnu y     t       n   t           n  x    t   t        x     whu  y     by      t  x    t   t        t      n x  t  x    n x  t  x    x    hnt   whu  y    whu  y hnt      hnt      t  t        n  x    t   t        x    whu  y hnt    n   t  t            xt   hn      hnt         n     n   whu  y     t    n   t           n t   t  x  x         xt   hn      hnt         n     n   whu  y     t    n   t  t                as shown in eq.      the network output y is a function expression of rnn parameter    n     n   and full layer weighted  matrix parameter  whu  y   whu  y  . the full layer weighted  matrix parameter are mainly used for the integration of feature vectors learned by each layer. y calculated by weighted  sum means more parameters. difficulties are brought by the  parameter increase. in a sense  eq.      and eq.      are mathematically equivalent due to parameter variation. the sum  calculation has obvious advantages during training our system. although the sum and weighted sum calculation could  ideally achieve the same training effect  fewer parameters  of sum calculation bring faster training process and higher  recognition accuracy. compared with the outputs calculated  by using only the last hidden layer state  outputs calculated by  using sum calculation could even obtain a higher recognition  accuracy without increasing the amount of parameters. the  proposed sum calculation is an optimal hidden layers state  process method.    experimental results    t  t                by      hnt      y    t      n   t      n  x    n       by      where whnu y  n       ...n  denotes the weight matrix from     nth hidden layer to the full connection layer  output layer .  the weight matrix brings a great amount of parameters   which have a bad effect on neural network perception. when  all the hidden layers  sizes are same  we make a adaptation  that all the hidden layer states are stacked as the output of  network. the calculation process could be described as eq.     and eq.   . the hidden layer neurons are calculated by the  neural network parameters  the parameters change during  training process. therefore  the attributes and function of  neurons are relative rather than absolute. due to neurons   relativity  the sum calculation without weight matrix could  achieve the similar goals with weighted sum calculation   which are proved as follows. given the rnn output y with  weighted sum calculation    n  x     by              n      y   by      by   whu  y u    whu  y u     n   t      whnu y u n                xt   hn      hnt         n     n   whnu y     t       n   t  t                as shown in eq.      the network output y is a function expression of rnn parameter    n     n   and full layer weighted  matrix parameter  whnu y    whnu y   . and the network out      put y of rnn with stacked different hidden layer states is    all the experiments are carried out on iahcc ucas      dataset and casia olhwdb .  dataset. iahccucas     is a dataset of in air handwritten chinese  characters containing      character classes in total. concretely  it contains      chinese characters     case sensitive  english letters  and    digits. each character class has      samples. for each class  we choose    samples as  training set  and the remaining    samples are used as testing  set.    samples in the training set are randomly sampled  to form the validation set. for further evaluation of the  proposed methods  experiments are also carried out on  casia olhwdb .  dataset. casia olhwdb .  dataset  is a handwritten chinese character dataset which contains       characters from gb     written by     writers. the  data from     persons are used for training  and the data  from the remaining    persons are used for testing.     table    recognition accuracy of different methods    method      .      .    general  rnns         bidirectional  rnns         hybrid parameter  rnns         general  rnns       paras    acc.    paras    speed  sec sample    acc.    paras    speed  sec sample    acc.    acc.      h layers     .  mil      .       .  mil     .           .       .  mil     .           .        .        h layers     .  mil      .       .  mil     .           .       .  mil     .           .        .        h layers     .  mil      .       .  mil     .           .       .  mil     .           .        .        h layers     .  mil      .       .  mil     .           .       .  mil     .           .        .        h layers     .  mil      .       .  mil     .           .       .  mil     .           .      n a      h layers     .  mil      .       .  mil     .           .       .  mil     .           .      n a            denotes the hidden layer size of corresponding rnns.  h layers denotes hidden layers.    . and   . denote experiments carried out on the iahcc ucas     dataset and casia olhwdb .  dataset respectively.    data preprocessing  the location sequence of a handwritten character is denoted   i    i    i    i    i   by  xi    x    x            xt           xt    where the xt de i   notes tth location dot vector  the xt could be represented   i    i   as xit    mit   nit    t                 t   where mt and nt  denote tth location dot s coordinate values respectively. before being fed into our system  the data is preprocessed by   i    i    i   two steps. concretely      m i   m    m            mt   and   i    i    i   ni  n    n            nt   of all the locations are scaled to a  range of   to         the dot locations are further normalized so that their mean equals to zero  i.e.  the new coordinate  m  i    n  i    is obtained by m  i    m i    m  i     n  i    n i    n  i    where m  i  and n  i  denote the means  of the corresponding coordinates of all the dot locations.    network configuration  in our experiments  the inputs of the rnn networks are    dimensional or   dimensional temporal sequences  corresponding to coordinates of dot locations on writing trajectory  of in air handwritten character and handwritten character  with pen up and down mask on touching scren. the number of neurons in the hidden layers are all     except some  systems in table  . the dropout values are all set to  .  and  mini batch size is    . all the computation is performed by  gpus on tesla k  .    performance evaluation of hybrid parameters  rnns  the parameters of the proposed hybrid parameter rnns are  changed during the iteration at temperal dimension. the proposed hybrid parameter rnns system could obtain a higher  recognition accuracy with fewer parameters than the general rnn. compared with bidirectional rnns  the proposed    hybrid parameter rnn has a faster calculation speed. the  experiments are carried out on iahcc ucas     dataset.  as shown in table    the proposed hybrid parameter rnns  with five     size hidden layers obtain a higher recognition accuracy than the genral rnn with five     size hidden layers. however  the parameter quantity of proposed  hybrid parameter rnns are as half as the general rnns.  compared with bidirectional rnns with calculation speed  of  .      second sample   .      second sample   .       second sample  and  .      second sample  the proposed  hybrid parameter rnn obtains competitive recognition accuracy with faster calculation speed of  .      second sample    .      second sample   .      second sample and  .       second sample corresponding four kinds of hidden layer size.  the calculation time is shortened by about    .  for further evaluations  experiments are also carried out  on casia olhwdb .  dataset. in the two different architectures of rnns containing   or   hidden layers  compared  with the general rnns  the hybrid parameter rnns bring  parameter decrease of     from  .  mil to  .  mil   and      from  .  mil to  .  mil respectively and obtain high  recognition results. compared with the bidirectional rnns   the the hybrid parameter rnns bring calculation speed increase of     from  .      second sample to  .       second sample  and     from  .      second sample to   .      second sample respectively and obtain competitive  results.    performance evaluation of memory pool unit  the proposed mpu is applied to in air handwritten chinese  character recognition. the main part of the proposed method  is a memory pool which could learn the useful temporal information. and the core of the memory pool is the renewal  function. input compensation is important for network training and two methods are proposed for input compensation.     table    recognition accuracy of different methods  methods    gru    lstm    mpu    mpu c      h layers      .        .        .        .        h layers      .        .        .        .        h layers      .        .        .        .        h layers      .        .        .        .      one is that the network inputs are contained in the inputs  of each layer. the other method is using the memory pool  outputs and last layer outputs to compute the current hidden  layer state.  to evaluate the performance of the mpu  experiments  are carried out on iahcc ucas     dataset. the rnn  structures used in these experiments are the general rnns  equipped with gru  lstm  mpu and mpu with input compensation respectively. as shown in table    the mpu and  mpu with input compensation methods obtain a competitive  recognition accuracy. compared with lstm  the proposed  method has fewer parameters and a simpler structure. compared with gru  the whole hidden layer state calculation  process are more reasonable and convictive.  for further performance evaluation of the mpu  another experiment was carried out on casia olhwdb .   dataset  liu et al.      . the data from    persons in training set are randomly sampled to form the validation set. as  shown in table    in the two different architectures of rnns  containing   or   hidden layers  compared with the exsiting  structure lstm  the rnns equipped with mpu obtains a  higher recognition results.  table    recognition accuracy of different methods  methods    mpu c    gru    lstm      h layers    h layers      .      .        .      .        .      .      performance evaluation of stacked different  hidden layer states  for rnns with same hidden layer size in each layer  we make  a suggestion that all the outputs of each layer are stacked as  the output of network. by synthesizing all the outputs of each  layer  stacked method increases the recognition accuracy  without increasing the parameters.  to evaluate the performance of our proposed stack method   experiments are carried out on iahcc ucas     dataset.  the rnns structure used in the four experiments are the  general rnns equipped with gru. the experimental results  are shown in table  . the experimental results show that the  proposed stack method could bring a recognition accuracy  increase.    table    recognition accuracy of different methods  methods    with  stacked    without  stacked    synthesize with  weighted matrix     h   h   h   h      .      .      .      .        .      .      .      .        .      .      .      .      layers  layers  layers  layers    recognition accuracy comparison between ours  and the state of arts method  based on the constructed rnns in table   and table    we  construct an ensemble classifier containing all the networks.  concretely  the output of the ensemble classifier y is the  pn  sum of outputs of child rnns  i.e.  y   j   yj   where yj  denotes the output of a network in table   and table    n  denotes n networks in table   and table  .  since the accuracy in  qu et al.       is obtained only for  the      chinese character classes  the experimental results  in table   are also reported on the      chinese characters  for the sake of fair comparison. from table    we can see the  ensemble classifier can achieve the recognition accuracy of    .  . the proposed classifiers outperform the state of theart results  qu et al.       ren et al.      .  table    comparison of recognition accuracy between ours  and the state of the art method  qu et al.        method    ours ensemble    method  .    method  .    acc.      .        .        .      method  .  ren et al.       method  .  qu et al.          conclusion  this paper presents an end to end recognizer for online inair handwritten chinese characters by using recurrent neural  networks  rnn  and it has obtained competitive performance  compared with the state of the art methods  ren et al.        qu et al.      . the merit of the proposed method is that it  does not need the explicit feature representation in modeling  the classifier. to make the classic rnn work better for online  iahccr  three mechanisms are proposed  i.e.  the menory  pool method  hybrid parameters rnns  stacked different  hidden layer states. the experimental results show that all  the mechanisms can effectively promote the classification  performance of rnn.    acknowledgments  this paper is under consideration at pattern recognition  letters.     