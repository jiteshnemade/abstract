introduction  best known for the sequence to sequence learning  the  recurrent neural networks  rnns  belong to a class of  neural architectures designed to capture the dynamic temporal behaviors of data. the vanilla fully connected rnn  utilizes a feedback loop to memorize previous information   while it is inept to handle long sequences as the gradient exponentially vanishes along the time        . unlike  the vanilla rnns passing information between layers with  direct matrix vector multiplications  the long short term    figure    architecture of bt lstm. the redundant dense  connections between input and hidden state is replaced by  low rank bt representation.  memory  lstm  introduces a number of gates and passes  information with element wise operations     . this improvement drastically alleviates the gradient vanishing issue  therefore lstm and its variants  e.g. gated recurrent  unit  gru       are widely used in various computer vision  cv  tasks             to model the long term correlations in sequences.  the current formulation of lstm  however  suffers from  an excess of parameters  making it notoriously difficult to  train and susceptible to overfitting. the formulation of  lstm can be described by the following equations   ft  it  ot  c t  ct  ht        wf   xt   uf   ht     bf        wi   xt   ui   ht     bi        wo   xt   uo   ht     bo      tanh wc   xt   uc   ht     bc      ft ct     it c t    ot tanh ct                                       where  denotes the element wise product       denotes  the sigmoid function and tanh    is the hyperbolic tangent  function. the weight matrices w  and u  transform the  input xt and the hidden state ht     respectively  to cell up      date c t and three gates ft   it   and ot . please note that given  an image feature vector xt fetch from a convolutional neural network  cnn  network  the shape of xt will raise to  i        and i                 w.r.t vgg        and inception v      . if the number of hidden states is j         the total number of parameters in calculating the four w   is     i   j  which can up to  .        and  .           respectively. therefore  the giant matrix vector multiplication  i.e.  w    xt   leads to the major inefficiency   the current parameter intensive design not only subjects the model  difficult to train  but also lead to high computation complexity and memory usage.  in addition  each w    xt essentially represents a fully  connected operation that transforms the input vector xt  into the hidden state vector. however  extensive research  on cnns has proven that the dense connection is significantly inefficient at extracting the spatially latent local  structures and local correlations naturally exhibited in the  image         . recent leading cnns architectures  e.g.   densenet       resnet      and inception v        also  try to circumvent one huge cumbersome dense layer     .  but the discussions of improving the dense connections in  rnns are still quite limited         . it is imperative to seek  a more efficient design to replace w    xt .  in this work  we propose to design a sparsely connected  tensor representation  i.e.  the block term decomposition   btd       to replace the redundant and densely connected  operation in lstm   . the block term decomposition is a  low rank approximation method that decomposes a highorder tensor into a sum of multiple tucker decomposition  models                 . in detail  we represent the four  weight matrices  i.e.  w    and the input data xt into a various order of tensor. in the process of rnns training  the  btd layer automatically learns inter parameter correlations  to implicitly prune redundant dense connections rendered  by w   x. by plugging the new btd layer into current  rnns formulations  we present a new bt rnn model with  a similar representation power but several orders of fewer  parameters. the refined lstm model with the block term  representation is illustrated in fig.  .  the major merits of bt rnn are shown as follows     the low rank btd can compress the dense connections in the input to hidden transformation  while still  retaining the current design philosophy of lstm.  by reducing several orders of model parameters  btlstm has better convergence rate than the traditional  lstm architecture  significantly enhancing the training speed.    each dimension in the input data can share weights  with all the other dimensions as the existence of core  tensors  thus bt representation has the strong connection between different dimensions  enhancing the    we focus on lstm in this paper  but the proposed approach also applies for other variants such as gru.    ability to capture sufficient local correlations. empirical results show that  compared with the tensor train  model       the bt model has a better representation  power with the same amount of model parameters.    the design of multiple tucker models can significantly  reduce the sensitivity to noisy input data and widen  network  leading to a more robust rnn model. in  contrary to the tensor train based tensor approaches            the bt model does not suffer from the difficulty of ranks setting  releasing researchers from intolerable work in choosing hyper parameters.  in order to demonstrate the performance of the btlstm model  we design three challenging computer vision tasks   action recognition in videos  image caption  and image generation   to quantitatively and qualitatively  evaluate the proposed bt lstm against the baseline lstm  and other low rank variants such as the tensor train lstm   tt lstm . experimental results have demonstrated the  promising performance of the bt lstm model.     . related work  the poor image modeling efficiency of full connections  in the perception architecture  i.e.  w   x       has been  widely recognized by the computer vision  cv  community. the most prominent example is the great success made  by convolutional neural networks  cnns  for the general  image recognition. instead of using the dense connections  in multi layer perceptions  cnns relies on sparsely connected convolutional kernels to extract the latent regional  features in an image. hence  going sparse on connections is  the key to the success of cnns                    . though  extensive discussions toward the efficient cnns design  the  discussions of improving the dense connections in rnns  are still quite limited         .  compared with aforementioned explicit structure  changes  the low rank method is one orthogonal approach  to implicitly prune the dense connections. low rank  tensor methods have been successfully applied to  address the redundant dense connection problem in  cnns                    . since the key operation in one  perception is w   x  sainath et al.      decompose w  with singular value decomposition  svd   reducing up to      parameters in w  but also demonstrates up to      accuracy loss     . the accuracy loss majorly results from  losing the high order spatial information  as intermediate  data after image convolutions are intrinsically in  d.  in order to capture the high order spatial correlations   recently  tensor methods were introduced into neural networks to approximate w   x. for example  tensor train   tt  method was employed to alleviate the large computation w x and reduce the number of parameters             .  yu et al.      also used a tensor train representation to forecast long term information. since this approach targets in  long historic states  it increases additional parameters  leading to a difficulty in training. other tensor decomposition     methods also applied in deep neural networks  dnns  for  various purposes             .  although tt decomposition has obtained a great success  in addressing dense connections problem  there are some  limitations which block tt method to achieve better performance     the optimal setting of tt ranks is that they are  small in the border cores and large in middle cores  e.g.  like  an olive     . however  in most applications  tt ranks are  set equally  which will hinder tt s representation ability.     tt ranks has a strong constraint that the rank in border tensors must set to    r    rd         leading to a seriously  limited representation ability and flexibility         .  instead of difficultly finding the optimal tt ranks setting  btd has these advantages     tucker decomposition  introduces a core tensor to represent the correlations between different dimensions  achieving better weight sharing.    ranks in core tensor can be set to equal  avoiding unbalance weight sharing in different dimensions  leading to a  robust model toward different permutations of input data.     btd uses a sum of multiple tucker models to approximate  a high order tensor  breaking a large tucker decomposition  to several smaller models  widening network and increasing  representation ability. meanwhile  multiple tucker models  also lead to a more robust rnn model to noisy input data.     . tensorizing recurrent neural networks  the core concept of this work is to approximate w   x  with much fewer parameters  while still preserving the  memorization mechanism in existing rnn formulations.  the technique we use for the approximation is block term  decomposition  btd   which represents w   x as a series  of light weighted small tensor products. in the process of  rnn training  the btd layer automatically learns interparameter correlations to implicitly prune redundant dense  connections rendered by w   x. by plugging the new btd  layer into current rnn formulations  we present a new btrnn model with several orders of magnitude fewer parameters while maintaining the representation power.  this section elaborates the details of the proposed  methodology. it starts with exploring the background of  tensor representations and btd  before delving into the  transformation of a regular rnn model to the bt rnn   then we present the back propagation procedures for the btrnn  finally  we analyze the time and memory complexity  of the bt rnn compared with the regular one.    figure    block term decomposition for a   order case tensor. a   order tensor x   ri   i   i  can be approximated  by n tucker decompositions. we call the n the cp rank   r    r    r  the tucker rank and d the core order.  let s denote  k as the tensor tensor product on kth order     . given two d order tensor a   ri       id and  b   rj       jd   the tensor product on kth order is    a  k b i   i   j    j      k    k    k    k    ik  x    ai   p i  bj    p j   .  k    p      k    k           k    to simplify  we use i   k denotes indices  i    . . .   ik       while i   denotes   i     k   . . .   id  . the whole indices can be  k        denoted as i     ik   ik   i   k  . as we can see that each tensor product will be calculated along ik dimension  which is  consistent with matrix product.  contraction is an extension of tensor product      it conducts tensor products on multiple orders at the same time.  for example  if ik   jk   ik     jk     we can conduct a  tensor product according the kth and  k     th order    a  k k   b i   i   k    k                 jk  jk      ik ix  k    x  p   q           ai   p q i  bj    p q j   .  k    k      k           k      block term decomposition  btd  block term decomposition is a combination of cp decomposition     and  tucker decomposition     . given a d order tensor x    ri       id   btd decomposes it into n block terms  and  each term conducts  k between a core tensor gn    ik  rk  rr       rd and d factor matrices a k   on gn  s  n   r  kth dimension  where n       n   and k       d     . the  formulation of btd is as follows   x     n  x          d   gn    a     n    an           d an .           n       . . preliminaries and background  tensor representation we use the boldface euler script  letter  e.g.  x  to denote a tensor. a d order tensor represents a d dimensional multiway array  thereby a vector and a  matrix is a   order tensor and a   order tensor  respectively.  an element in a d order tensor is denoted as xi   ... id .  tensor product and contraction two tensors can perform product on a kth order if their kth dimension matches.    we call the n the cp rank  r    r    r  the tucker rank and  d the core order. fig.   demonstrates an example of how    order tensor x being decomposed into n block terms.     . . bt rnn model  this section demonstrates the core steps of bt rnn  model.    we transform w and x into tensor representations  w and x     then we decompose w into several  low rank core tensors gn and their corresponding factor      a  vector to tensor     b  matrix to tensor    figure    tensorization operation in a case of   order tensors.  a  tensorizing a vector with shape i   i    i    i  to  a tensor with shape i    i    i     b  tensorizing a matrix  with shape i     i    i    to a tensor with shape i    i    i  .  tensors a d   n using btd     subsequently  the original product w x is approximated by the tensor contraction between  decomposed weight tensor w and input tensor x     finally   we present the gradient calculations amid back propagation  through time  bptt           to demonstrate the learning  procedures of bt rnn model.  tensorizing w and x we tensorize the input vector x  to a high order tensor x to capture spatial information of  the input data  while we tensorize the weight matrix w to  decomposed weight tensor w with btd.  formally  given an input vector xt   ri   we define the  notation   to denote the tensorization operation. it can be  either a stack operation or a reshape operation. we use reshape operation for tensorization as it does not need to duplicate the element of the data. essentially reshaping is regrouping the data. fig.   outlines how we reshape a vector  and a matrix into   order tensors.  decomposing w with btd given a   dimensions  weight matrix w   rj i   we can tensorize it as a  d dimensions tensor w   rj   i   j       jd  id   where i    i  i        id and j   j  j        jd . following btd in eq.       we can decomposes w into   bt d w       n  x     d   gn    a     n            d a n              n      where gn   rr       rd denotes the core tensor  an d     rid  jd  rd denotes the factor tensor  n is the cp rank and  d is the core order. from the mathematical property of  bt s ranks       we have rk   ik  and jk    k      . . .   d.  if rk   ik  or jk    it is difficult for the model to obtain  bonus in performance. what s more  to obtain a robust  model  in practice  we set each tucker rank to be equal   e.g.  ri   r  i       d   to avoid unbalanced weight sharing in different dimensions and to alleviate the difficulty in  hyper parameters setting.  computation between w and x after substituting the  matrix vector product by bt representation and tensorized  input vector  we replace the input to hidden matrix vector  product w   xt with the following form     w  xt     bt d w       ... d xt              figure    diagrams of bt representation for matrix vector  product y   wx  w   rj i . we substitute the weight  matrix w by the bt representation  then tensorize the input  vector x to a tensor with shape i    i    i  . after operating  the tensor contraction between bt representation and input  tensor  we get the result tensor in shape j    j    j  . with  the reverse tensorize operation  we get the output vector y    rj   j   j  .  where the tensor contraction operation      ... d will be computed along all ik dimensions in w and x  yielding the  same size in the element wise form as the original one. fig.    demonstrates the substitution intuitively.  training bt rnn the gradient of rnn is computed by  back propagation through time  bptt      . we derive  the gradients amid the framework of bptt for the proposed  bt rnn model.  following the regular lstm back propagation procedure  the gradient  l   y can be computed by the original  bptt algorithm  where y   wxt . using the tensorization  operation same to y  we can obtain the tensorized gradient   l   y . for a more intuitive understanding  we rewrite eq.       in element wise case   y j       ... rd i x   ... id y  d  n r x  x  n         r     i     k     xt  i an ik  jk  rk gn  r .         k        to denote the  here  for simplified writing  we use  i   j and r  indices  i    . . .   id     j    . . .   jd   and  r    . . .   rd    respectively. since the right hand side of eq.      is a scalar   the element wise gradient for parameters in bt rnn is as  follows    l   k    an ik  jk  rk         d  x x x y    gn  r an ik   jk   rk  xt  i           rk  i  ik    r  j  jk k   k     l      y j            d    xx y   l   l     an ik  jk  rk xt  i  .   gn  r   y j   i       j    k                             r      r      r      logarithmically. however  this will result in the loss of important spatial information in an extremely high order bt  model. in practice  core order d          is recommended.    r        params                                                                                                                core order d    figure    the number of parameters w.r.t core order d and  tucker rank r  in the setting of i         j        n     . while the vanilla rnn contains i   j           parameters. refer to eq.       when d is small  the first part  pd    ik jk r does the main contribution to parameters. while  d is large  the second part rd does. so we can see the number of parameters will go down sharply at first  but rise up  gradually as d grows up  except for the case of r     .     . . hyper parameters and complexity analysis   . .     hyper parameters analysis    total  params btd decomposes w into n block terms  and each block term is a tucker representation            therefore the total amount of parameters is as follows   pbt d   n      d  x    ik jk r   rd  .            k      by comparison  the original weight matrix w contains  qd  prn n   i   j   k   ik jk parameters  which is several  orders of magnitude larger than it in the btd representation.   params w.r.t core order  d  core order d is the most  significant factor affecting the total amount of parameters  as rd term in eq.     . it determines the total dimensions  of core tensors  the number of factor tensors  and the total  dimensions of input and output tensors. if we set d       the model degenerates to the original matrix vector product with the largest number of parameters and the highest  complexity. fig.   demonstrates how total amount of parameters vary w.r.t different core order d. if the tuckerrank r      the total amount of parameters first decreases  with d increasing until reaches the minimum  then starts increasing afterwards. this mainly results from the non linear  characteristic of d in eq.     .  hence  a proper choice of d is particularly important. enlarging the parameter d is the simplest way to reduce the  number of parameters. but due to the second term rd in  eq.       enlarging d will also increase the amount of parameters in the core tensors  resulting in the high computational complexity and memory usage. with the core order  d increasing  each dimensions of the input tensor decreases     params w.r.t tucker rank  r  the tucker rank r controls the complexity of tucker decomposition. this hyperparameter is conceptually similar to the number of singular  values in singular value decomposition  svd . eq.       and fig.   also suggest the total amount of parameters is  sensitive to r. particularly  btd degenerates to a cp decomposition if we set it as r    . since r   ik  and jk     the choice of r is limited in a small value range  releasing  researchers from heavily hyper parameters setting.   params w.r.t cp rank  n   the cp rank n controls the  number of block terms. if n      btd degenerates to a  tucker decomposition. as we can see from table   that n  does not affect the memory usage in forward and backward  passes  so if we need a more memory saving model  we can  enlarge n while decreasing r and d at the same time.   . .     computational complexity analysis    complexity in forward process eq.      raises the  computation peak  o ijr   at the last tensor product  d    according to left to right computational order. however   we can reorder the computations to further reduce the total model complexity o n dijr . the reordering is     w  xt        n  x     d   xt    a     n           d an      ... d gn .    n            the main difference is each tensor product will be first computed along all r dimensions in eq.       while in eq.       along all ik dimensions. since btd is a low rank decomposition method  e.g.  r   jk and jmax r d      j  the new  computation order can significantly reduce the complexity  of the last tensor product from o ijr  to o ijmax rd     where j   j  j        jd   jmax   maxk  jk    k       d .  and then the total complexity of our model reduces from  o n dijr  to o n dijmax rd  . if we decrease tuckerrank r  the computation complexity decreases logarithmically in eq.      while linearly in eq.     .  complexity in backward process to derive the computational complexity in the backward process  we present  gradients in the tensor product form. the gradients of factor  tensors and core tensors are    l   a k   n          l   k xt    a     n    . . .   y             k   a k      k   a k      k   . . .  n  n   d a d   n      ... d gn   l   l   d    xt    a     n           d an      ... d   gn   y             time  o ij   o ij   o dir  jmax    o d  ir  jmax    o n dird jmax    o n d  ird jmax      memory  o ij   o ij   o ri   o r  i   o rd i   o rd i        lstm params    . m top   .     btlstm r    cr       x top   .     btlstm r    cr       x top   .     btlstm r    cr       x top   .     ttlstm r    cr       x top   .        .     train loss    method  rnn forward  rnn backward  tt rnn forward  tt rnn backward  bt rnn forward  bt rnn backward        .       .                                                                  epoch    since eq.      and eq.      follow the same form of eq.        the backward computational complexity is same as the  forward pass o dijmax rd  . therefore  the n  d factor tensors demonstrate a total complexity of o n d  ijmax rd  .  complexity comparisons we analyze the time complexity and memory usage of rnn  tensor train rnn  and btrnn. the statistics are shown in table  . in our observation  both tt rnn and bt rnn hold lower computation  complexity and memory usage than the vanilla rnn  since  the extra hyper parameters are several orders smaller than i  or j. as we claim that the suggested choice of core order is  d           the complexity of tt rnn and bt rnn should  be comparable.     a  training loss of baseline lstm  tt lstm and bt lstm.    validation accuracy    table    comparison of complexity and memory usage of  vanilla rnn  tensor train representation rnn  tt rnn            and our bt representation rnn  bt rnn . in this  table  the weight matrix s shape is i   j. the input and  hidden tensors  shapes are i   i            id and j   j             jd   respectively. here  jmax   maxk  jk    k       d .  both tt rnn and bt rnn are set in same rank r.     .    .    .    .    .    .    .    .    .        lstm params    . m top   .     btlstm r    cr       x top   .     btlstm r    cr       x top   .     btlstm r    cr       x top   .     ttlstm r    cr       x top   .                   . . implementations  since operations in ft   it   ot and c t follow the same computation pattern  we merge them together by concatenating  wf   wi   wo and wc into one giant w  and so does u.  this observation leads to the following simplified lstm  formulations      ft     it     c  t   ot     w   xt   u   ht     b             ft   it   c t   ot       ft        it      tanh c  t      ot     .       we implemented bt lstm on the top of simplified lstm  formulation with keras and tensorflow. the initialization  of baseline lstm models use the default settings in keras                                              epoch     b  validation accuracy of baseline lstm  tt lstm and bt lstm.    figure    performance of different rnn models on the action recognition task trained with ucf  . cr stands for  compression ratio  r is tucker rank  and top is the highest  validation accuracy observed in the training. though btlstm utilizes       times less parameters than the vanilla  lstm    .  millons   bt lstm demonstrates a   .    higher accuracy improvement than lstm. bt lstm also  demonstrates and extra  .   improvement over the ttlstm with comparable parameters.     . experiments  rnn is a versatile and powerful modeling tool widely  used in various computer vision tasks. we design three  challenging computer vision tasks action recognition in  videos  image caption and image generation to quantitatively and qualitatively evaluate proposed bt lstm  against baseline lstm and other low rank variants such as  tensor train lstm  tt lstm . finally  we design a control experiment to elucidate the effects of different hyperparameters.           orthogonal  approaches  rnn  approaches    method  original       spatial temporal       visual attention       lstm  tt lstm       bt lstm    accuracy   .      .      .      .      .      .       table    state of the art results on ucf   dataset reported  in literature  in comparison with our best model.  and tensorflow  while we use adam optimizer with the  same learning rate  lr  across different tasks.     . . quantitative evaluations of bt lstm on the  task of action recognition in videos  we use ucf   youtube action dataset      for action  recognition in videos. the dataset contains      video  clips  falling into    action categories. each category contains    video groups  within each contains at least   clips.  all video clips are converted to   .  fps mpg  . we scale  down original frames from               to                   http   crcv.ucf.edu data ucf   updated mpg.rar     then we sample   random frames in ascending order from  each video clip as the input data. for more details on the  preprocessing  please refer to     .  we use a single lstm cell as the model architecture to  evaluate bt lstm against lstm and tt lstm in fig.  .  please note there are other orthogonal approaches aiming  at improving the model such as visual attention      and  spatial temporal     . since our discussion is limited to a  single lstm cell  we can always replace the lstm cells  in those high level models with bt lstm to acquire better  accuracies. we set the hyper parameters of bt lstm and  tt lstm as follows  the factor tensor counts is d      the  shape of input tensor is i       i        i        i         and the hidden shape is j    j    j    j       the rank  of tt lstm is r    r       r    r    r       while  bt lstm is set to various tucker ranks.  fig.   demonstrates the training loss and validation accuracy of bt lstm against lstm and tt lstm under  different settings. table   demonstrates the top accuracies  of different models. from these experiments  we claim that               times parameter reductions  the vanilla  lstm has   .  millons parameters in w  while bt lstm  deliveries better accuracies even with several orders of less  parameters. the total parameters in bt lstm follows eq.      . at tucker rank          bt lstm uses             and      parameters  demonstrating compression ratios of       x       x and      x  respectively.     faster convergence  bt lstm demonstrates significant convergence improvement over the vanilla lstm  based on training losses and validation accuracies in fig.    a  and fig.   b . in terms of validation accuracies  btlstm reaches     accuracies at epoch    while lstm  takes     epochs. the data demonstrates   x convergence  speedup. it is widely acknowledged that the model with few  parameters is easier to train. therefore  the convergence  speedup majorly results from the drastic parameter reductions. at nearly same parameters  the training loss of btlstm   also decreases faster than ttlstm     epoches            substantiating that bt model captures better spatial  information than the tensor train model.     better model efficiency  though several orders of parameter reductions  bt lstm demonstrates extra   .    accuracies than lstm. in addition  bt lstm also demonstrates extra  .   accuraies than tt lstm with comparable parameters. in different tucker ranks  bt lstm converges to identical losses  but increasing tucker ranks also  improves the accuracy. this is consistent with the intuition  since the high rank models capture additional relevant information.     . . qualitative evaluations of bt lstm on tasks  of image generation and image captioning  we also conduct experiments on image generation and  image captioning to further substantiate the effciency of  bt lstm.                                                                                                                                                                                             a  lstm   params  . m     b  bt lstm   params         figure    image generation  generating mnist style digits  with lstm and bt lstm based model. the results are  merely identical  while the parameters of bt lstm is       times less.  task    image generation image generation intends to  learn latent representation from images  then it tires to generate new image of same style from the learned model.  the model for this task is deep recurrent attentive writer   draw     . it uses an encoder rnn network to encode  images into latent representations  then an decoder rnn  network decodes the latent representations to construct an  image. we substitute lstm in encoder network with our  bt lstm.  in this task  encoder network must capture sufficient correlations and visual features from raw images to generate  high quality of feature vectors. as shown in fig.    both  lstm and bt lstm model generate comparable images.  task    image captioning image captioning intends to  describe the content of an image. we use the model in neural image caption     to evaluate the performance of btlstm by replacing the lstm cells.  the training dataset is mscoco       a large scale  dataset for the object detection  segmentation  and captioning. each image is scaled to           in rgb channels  and subtract the channel means as the input to a pretrained  inception v  model.  fig.   demonstrates the image captions generated by  bt lstm and lstm. it is obvious that both bt lstm   tt lstm and lstm can generate proper sentences to describe the content of an image  but with little improvement  in bt lstm. since the input data of bt model is a compact feature vector merged with the embedding images features from inception v  and language features from a word  embedding network  our model demonstrates the qualitative  improvement in captioning. the results also demonstrate  that bt lstm captures local correlations missed by traditional lstm.     . . sensitivity analysis on hyper parameters  there are   key hyper parameters in bt lstm  which  are core order d  tucker rank r and cp rank n . in order to  scrutinize the impacts of these hyper parameters  we design  a control experiment illustrate their effects.      a  lstm  a train traveling down  tracks next to a forest.  tt lstm  a train traveling down  train tracks next to a forest.  bt lstm  a train traveling  through a lush green forest.     b     lstm  a group of people  standing next to each other.  tt lstm  a group of men standing next to each other.  bt lstm  a group of people posing for a photo.     c  lstm  a man and a dog are     d  lstm  a large elephant stand     standing in the snow.  tt lstm  a man and a dog are in  the snow.  bt lstm  a man and a dog playing with a frisbee.    ing next to a baby elephant.  tt lstm  an elephant walking  down a dirt road near trees.  bt lstm  a large elephant walking down a road with cars.    figure    results of image caption in mscoco dataset.     a  truth w     p          b  y   w x   p          c  d    r     n    p        cp rank  n    cp rank contributes to the number of  parameters linearly  playing an important role when r is  small. by comparing fig.   c  and fig.   e   we can see  that the latter result has less noise in figure  showing that  a proper cp rank setting will lead to a more robust model   since we use multiple tucker models to capture information  from input data.     . conclusion     d  d    r     n    p         e  d    r     n    p         f  d    r     n    p        figure    the trained w for different bt lstm settings.  the closer to  a   the better w is.  we try to sample y from the distribution of y   w    x   where x  y   r   . each x is generated from a gaussian  distribution n     .  . we also add a small noise into x  to avoid overfitting. y is generated by plugging x back to  y   w    x. given x and y  we randomly initlize w  and  start training. eventually  w should be similar to w  since  x and y drawn from the distribution of y   w    x. please  note that the purpose of this experiment is to evaluate the  impact of the bt model on different parameter settings  despite these are many other good methods such as l  regularization and lasso regularization  to recover the w weight  matrix.  core order  d   parameters goes down if d grows and  d    . parameters reduce about  .  times from fig.   d   to fig.   f   and d increase from   to  . with less parameters  the reconstructed w deteriorates quickly. we claim  that high core order d loses important spatial information   as tensor becomes too small to capture enough latent correlations. this result is consistent with our declaration.  tucker rank  r   the rank r take effectiveness exponentially to the parameters. by comparing fig.   c  and  fig.   d   when r increases from   to    bt model has  more parameters to capture sufficient information from input data  obtaining a more robust model.    we proposed a block term rnn architecture to address  the redundancy problem in rnns. by using a block term  tensor decomposition to prune connections in the input tohidden weight matrix of rnns  we provide a new rnn  model with a less number of parameters and stronger correlation modeling between feature dimensions  leading to  easy model training and improved performance. experiment results on a video action recognition data set show  that our bt rnn architecture can not only consume several  orders fewer parameters but also improve the model performance over standard traditional lstm and the tt lstm.  the next works are to    explore the sparsity in factor tensors and core tensors of bt model  further reducing the  number of model parameters     concatenate hidden states  and input data for a period of time  respectively  extracting  the temporal features via tensor methods     quantify factor  tensors and core tensors to reduce memory usage.    acknowledgment  this paper was in part supported by a grant  from the natural science foundation of china   no.                talent program startup funding   a                 g  qnqr     and a fundamental  research fund for the central universities of china  no.  a            . zenglin xu is the major corresponding  author.    