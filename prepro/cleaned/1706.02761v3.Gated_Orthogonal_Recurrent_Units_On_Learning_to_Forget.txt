introduction    recurrent neural networks with gating units   such  as long short term memory  lstms   hochreiter and  schmidhuber       gers       and gated recurrent units   grus   cho et al.     b    have led to rapid progress  in different areas of machine learning such as language  modeling  graves  wayne  and danihelka        neural  machine translation  cho et al.     b  sutskever  vinyals   and le        and speech recognition  chan et al.        chorowski et al.      . these works have proven the importance of gating units for recurrent neural networks.  the main advantage of using these gated units in rnns is  primarily due to the ease of optimization of the models using  them and to reduce the learning degeneracies such as vanishing gradients that can cripple conventional rnns  pascanu   mikolov  and bengio      . most importantly  by designing special gates  it is easier to impose a particular behavior  on the model  such as creating shortcut connections through  time by using input and forget gates in lstms and resetting  the memory via the reset gate of a gru. this feature also  brings modularity to the neural network design that seems  to make training of those models easier. gated rnns are  also empirically shown to achieve better results for a wide  variety of real world tasks.  recently  using unitary and orthogonal matrices  instead  of general matrices  in rnns  arjovsky  shah  and ben     gio       jing et al.       henaff  szlam  and lecun        have attracted an increasing amount of attention in the machine learning community. this trend was following the  demonstration that these matrices can be effective in solving tasks involving long term dependencies and gradients  vanishing exploding  bengio  simard  and frasconi        hochreiter       problem. thus a unitary orthogonal rnn  can capture long term dependencies more effectively in sequential data than a conventional rnn or lstm. as a result  this type of model has been shown to perform well  on tasks that would require rote memorization hochreiter        and simple reasoning  such as the copy task hochreiter and schmidhuber       and the sequential mnist le   jaitly  and hinton      . those models can just be viewed  as an extension to vanilla rnns jordan       that replaces  the transition matrices with either unitary or orthogonal matrices.  in this paper  we refer the ability of a model to omit  parts of the input sequence that contain redundant information and to filter out the noise input in general as the means  of a forgetting mechanism. previously  gers  schmidhuber   and cummins       have shown the importance of the forgetting mechanism for lstm networks and with very similar motivations  we discuss the utilization of a forgetting  mechanism for rnns with orthogonal transitions. the importance of forgetting for those networks is mainly due to  that unitary orthogonal rnns can backpropagate the gradients without vanishing through time  and it is very easy for  them to just have an output that depends on equal amounts  of all the elements of the whole input sequence. from this  perspective  learning to forget can be difficult with unitary orthogonal rnns and they can clog up the memory  with useless information. however  most real world applications and natural tasks require the model to filter out irrelevant or redundant information from the input sequence.  we argue that difficulties of forgetting can cause unitary and  orthogonal rnns to perform badly on many realistic tasks   and demonstrate this empirically with a toy task.  we propose a new architecture  the gated orthogonal recurrent unit  goru   which combines the advantages of the  above two frameworks  namely  i  the ability to capture long  term dependencies by using orthogonal matrices and  ii  the  ability to  forget  by using a gru structure. we demonstrate that goru is able to learn long term dependencies     effectively  even in complicated datasets which require a forgetting ability. in this work  we focus on implementation of  orthogonal transition matrices which is just a subset of the  unitary matrices.  goru outperforms a recent variation of unitary rnn  called eurnn  jing et al.       on language modeling  denoising  parenthesis and the question answering tasks. we  show that the unitary rnn fails catastrophically on a denoising task which requires the model to forget. on question answering  speech spectrum prediction  algorithmic  parenthesis and the denoising tasks  goru achieves better accuracy  on the test set over all other models that we compare against.  we have attempted to use gates on the unitary matrices with  complex numbers  but we encountered some training challenges of training gating mechanisms  thus we have decided  to just to focus on orthogonal matrices for this paper.         background  dx    given an input sequence xt   r   t                  t    a  vanilla rnn defines a sequence of hidden states ht   rdh  updated at each time step according to the rule  ht     wh ht     wx xt   b    dh  dh    dx  dh         dh    where wh   r    wx   r  and b   r are  model parameters and   is a nonlinear activation function.  rnns have proven to be effective for solving sequential  tasks due to their flexibility to . however  a well known  problem called gradient vanishing and gradient explosion  has prevented rnns from efficiently learning long term dependencies  bengio  simard  and frasconi      . several  approaches have been developed to solve this problem  with  lstms and grus being the most successful and widely  used.    gated recurrent unit  a big step forward from lstm is the gated recurrent unit   gru   proposed by cho et al   cho et al.     a   which  removed the extra memory state in lstm. specifically  the  hidden state ht in a gru is updated as follows   ht  zt  rt      zt ht          zt   tanh wx xt    rt wh ht     bh      sigmoid wz  ht     xt     bz       sigmoid wr  ht     xt     br                        where w z r    r dh  dx   dh   wx   rdx  dh   wh    rdh  dh and b z r h    rdh . figure   demonstrated the architecture of gru model.  although lstms and grus were proposed to solve the  exploding and vanishing gradient problem  hochreiter        bengio  simard  and frasconi       they can in practice still  suffer from this issue for long term tasks. as a result  gradient clipping  pascanu  mikolov  and bengio       is usually  required in the training process  although that only addresses  the gradient explosion.    unitary and orthogonal matrix rnn  a complex valued matrix u is unitary when it satisfies  uu t   i. a matrix u is orthogonal if it is both unitary    z      z  tanh    h    w    r    wx    in  out    figure    illustration of gru. r and z are reset and update  gates. h is the hidden state.  and real valued. therefore  any vector x that multiplies a  unitary or an orthogonal matrix satisfies     ux       x             thanks to this property  a unitary orthogonal matrix is able  to preserve the norm of vector flows through it and thus  allow for the gradient to propagate through longer time  steps. recent papers from arjovsky et al  arjovsky  shah   and bengio       henaff  szlam  and lecun       pointed  out that unitary orthogonal matrices can effectively prevent  the gradient vanishing explosion problem in conventional  rnns. after this work  several other unitary orthogonal  rnn models have been proposed  jing et al.       wisdom  et al.       hyland and rtsch       mhammedi et al.         all showing promising abilities in capturing long term dependencies in data.  a unitary orthogonal matrix rnn is simply defined as replacing the hidden to hidden matrices in a vanilla rnn by  unitary orthogonal matrices   ht     uht     wx xt   b .           for unitary matrices  the nonlinear activation function    needs to handle complex valued inputs. in this paper  we  use the generalizations of the popular real valued activation  function relu x    max    x  known as  zi  modrelu zi   bi      relu  zi     bi           zi    where b is a bias vector. this variant was found to perform  effectively on a suite of benchmarks in  arjovsky  shah  and  bengio       jing et al.      . even though modrelu was  developed for complex value models  it turns out this activation function fits unitary orthogonal matrices best.         the gated orthogonal recurrent unit    the problem of forgetting in orthogonal rnns  first  we argue for the advantage of an rnn which can forget some of its past inputs. this is desirable because we seek  a state representation which can capture the most important  elements of the past sequence  and can throw away irrelevant details or noise. this ability becomes particularly critical when the dimensionality of the rnn state is smaller than  the product of the sequence length with the input dimension   i.e.  when some form of compression is necessary. for this  compression to be most useful for further processing  it is     likely that it requires a non linear combination of the past  input values  allowing the network to forget and ignore unnecessary elements from the past.  now consider an rnn whose state is obtained as a sequence of orthogonal transformations  with each transformation being a function of the input at a given time step. let  us focus on class of orthogonal transformations that are basically rotation for the simplicity of our analysis  which  noncommutativity aside  are analogous to addition in the space  of angles. when we compose several orthogonal operators   we just add more angles together. so we forget in the mild  sense that we get in the state a combination of several rotations  like adding the angles  and we lose track of exactly  which individual rotations were applied. the advantage is  that  in the space of angles  the derivative of the final angle  to any of the individually added angle is    so there is no  vanishing gradient. however  we cannot have complete forgetting  e.g.  making the new state independent of the past  inputs  or of some of them which we wish to forget   for  a new rotation to cancel an old rotation  one would need  the new rotation to  know  about the old rotation to cancel  i.e.  it would need to be a function of the old rotation.  but this is not what happens  because each new rotation is  chosen before looking at the current state. instead  in a regular rnn  the state update depends in a non linear way on  the past state  so that for example when a particular value  of the state is reached  it can be reset to  . this would not  be possible with just the composition of orthogonal transformations. these considerations motivate an architecture  in which we combine orthogonal or unitary transformations  with non linearities which can be trained to forget when and  where it is appropriate.    goru architecture  this section introduces the gated orthogonal recurrent  unit  goru . in our architecture  we change the hidden  state loop matrix into an orthogonal matrix and change the  respective activation function to modrelu   ht  zt  rt      zt ht          zt   modrelu wx xt    rt  uht       bh            sigmoid wz ht     wz x xt   bz            sigmoid wr ht     wr x xt   br             where   is a suitable nonlinear activation function and wz   wr   rdh  dh   bz   br   bh   rdh    wz x   wr x   wx   rdx  dh . rt and zt are the reset and  update gates  respectively. u   rdh  dh is kept orthogonal.  in fact  we have only modified the main loop that absorbs  new information to orthogonal while leaving the gates  unchanged compared to the gru. figure   demonstrated  the architecture of goru model.  we enforce matrix u to be orthogonal by using  parametrization method purposed in  jing et al.      . u is  decomposed into a sequence of   by   rotation matrices as  shown in figure  . each   by   rotation contains one trainable rotation parameter.  the update gates of the goru help the model to filter out  irrelevant or noise information coming from the input. it can    z      z  modrelu    h    u    r    wx    in  out    figure    illustration of goru. h is the hidden state. for  goru  r and z are reset and update gates. it uses modrelu  activation function instead of tanh.    u    figure    orthogonal matrix parametrization by   by   rotation matrices. each row represents one neuron in the hidden  state. each junction represents a   by   rotation matrix on  corresponding two neurons.  be thought of as acting like a low pass filter. the orthogonal  transition matrices help the model to prevent the gradients  to vanish through time. however  the ways an orthogonal  transformation can interact with the hidden state of an rnn  is limited to reflections and rotations. the reset gate enables  the model to rescale the magnitude of the hidden state activations  ht  .         experiments    we compare goru with unitary rnns  using the eurnn  parameterization purposed by jing et al. jing et al.         and two other well known gatedrnns  lstms and grus .  previous research on unitary rnns has mainly focused on  memorization tasks  in contrast  we focus on more realistic  noisy tasks  which require the model to discard parts of the  input sequence to be able to use its capacity efficiently.  goru is implemented in tensorflow  available  from  https   github.com jingli       goru tensorflow    copying memory task  the first task we consider is the well known copying memory task. the copying task is a synthetic task that is commonly used to test the network s ability to remember information seen t time steps earlier.  specifically  the task is defined as follows. an alphabet  consists of symbols  ai    i                  n      n  n       the  first n of which represent data  and the remaining two representing  blank  and  marker   respectively. here we choose  n    . the input sequence contains    data steps  followed  by  blank . the rnn model is supposed to output  blank      and give the original sequence once it sees the  marker .  note that each instance has a different location for these     elements of data.  in this experiment  we use rmsprop optimization with a  learning rate of  .    and a decay rate of  .  for all models.  the batch size is set to    . all these models have roughly  same number of hidden to hidden parameters  despite not  having similar neuron layer sizes.    to    . all these models have roughly same number of hidden to hidden parameters.  denoise task     .     goru  gru  lstm  eurnn     .      .      goru  gru  lstm  eurnn     .     .      loss     .    .    .      .     .       .      .     .     .     .      loss     .     copying memory task                        training iterations                      figure    copying memory task with delay time t        on goru  gru  lstm and eurnn. hidden state sizes are  set to                    respectively to match total number  of hidden to hidden parameters. goru is the only gatedsystem to successfully solve this task while the gru and  lstm both get stuck at the baseline. eurnn is seen to converges within hundreds of iterations.  this task only requires the model to efficiently overcome  the gradient vanishing explosion problem and does not require a forgetting ability. the eurnn performs perfectly  and goes through to the baseline in no time   as previously  seen. the goru is the only gated system to successfully  solve this task while the gru and lstm get stuck at the  baseline as shown in figure  .    denoise task  we evaluate the forgetting ability of each rnn architecture  on a synthetic  denoise  task. a list of data points are located randomly in a long noisy sequence. the rnn model  is supposed to filter out the useless part   noise   and output  the remaining sequential labels.  similarly to the labels of copying memory task above   an alphabet consists of symbols  ai    i                  n       n  n       the first n of which represent data  and the remaining two represent  noise  and  marker   respectively.  the input sequence contains    randomly located data steps  and the rest are filled by  noise . the rnn model is supposed to output those    data in a sequence after it sees the   marker . just as in the previous experiment  we use rmsprop optimization algorithm with a learning rate of  .    and a decay rate of  .  for all models. the batch size is set                        training iterations                      figure    denoise task with sequence length t       on  goru  gru  lstm and eurnn. hidden state sizes are  set to                    respectively to match total number of hidden to hidden parameters. eurnn get stuck at  the baseline because of lacking forgetting mechanism  while  goru and gru successfully solve the task.  this task requires both the ability of learning long dependencies but also the ability to forget the noisy input. the  goru and gru both are able to successfully outperform  lstm in terms of both learning speed and final performances as shown in figure  . eurnn  however  gets stuck  at the baseline  just as we intuitively expected.    parenthesis task  the parenthesis task foerster et al.       requires the rnn  model to count the number of each type of unmatched parentheses at each time step  given that there are    types of  parentheses. the input data contains    pairs of different  parenthesis types   e.g.                          ...    mixed  with random noise characters between them. the neural network outputs how many unmatched parentheses there are.  for instance  given        the neural network would output       .note that there are never more than    unmatched  parentheses in any category.  in our experiment  the total input length is set to    . we  used batch size     and rmsprop optimizer with a learning  rate  .     decay rate  .  on all models. hidden state sizes  are set to match their total numbers of hidden to hidden parameters.  this task requires learning long term dependencies and  forgetting of the noisy data. the goru is able to successfully outperform gru  lstm and eurnn in terms of both  learning speed and final performances as shown in figure  .  we also analyzed the activations of the update gates for  goru and gru. according to the histogram of activations  shown in figure    both models behave very similarly  and  when the model receives noise as input  the activations of its     an input sequence and required to output the shortest path at  the end of the sequence. we have used the exact same setup  and use the data provided as in  li et al.      .  we used batch size    and hidden size     for all models. the rnns are trained with rmsprop optimizer with a  learning rate of  .    and decay rate of  . .  we summarized the test set results in table  . we found  that the goru performs averagely better than gru lstm  and eurnn.    parenthesis task         goru  gru  lstm  eurnn          loss  log scale                                                             training iterations                      figure    parenthesis tasks with total sequence length t        on goru  gru  lstm and eurnn. hidden state  sizes are set to                    respectively to match total number of hidden to hidden parameters. loss is shown in  log scale. goru is able to outperform all other models in  both learning speed and final performance.     .     .     .     .     .     .     .     .     .        .     .     .     .     .                                                                                                                                noise input    gru    goru    update gate peaks. this behavior helps the model to forget  and omit the noise input.    figure    the activations of the update gates for goru  and gru on the parenthesis task. in those bar charts we  visualize the number of activations that are greater than   .  normalized by the total number of units in the gate   pdh     ri   .   .  dh    i      as can be seen in the top two plots  the activations of the update gate peaks and becomes almost one when  the input is noise when the magnitude of the noise input in  the bottom plot is above the red bar.    algorithmic task  we tested the rnn models on algorithmic task as described  in  li et al.      . the model is fed with random graph as    model  eurnn  lstm  gru  goru    accuracy    .     .     .     .     .     .     .     .     table    algorithmic test on goru  gru  lstm and eurnn.    babi  episodic question answering  we tested the ability of our rnn models on a word level  episodic question answering task. the babi dataset  weston  et al.   examines rnn s ability to understand language and  perform basic logical reasoning. each training example is a  set of statements  that are logically related in some fashion.  for instance  one training example consists of these three  statements and a question  mary went to the bathroom. john  moved to the hallway. mary traveled to the office. where is  mary  answer  office.  there are twenty different types of questions that can be  asked   some requiring deduction between lines  some requiring association. the babi dataset is useful because it  contains a small sized vocabulary  short sentences and requires one word answers for each story. thus  it is a good  benchmarking test because the word mapping layers are not  the dominant sources of parameters.  we test each task with a uni directional rnn without any  attention mechanism. in detail  we word embed and then  feed one rnn the sequence of statements. another rnn  is fed the word embedded question. then  we concatenate  the outputs of the two rnn s into a single input for a third  rnn that then outputs the correct word.  we summarized the test set results as follows in table   . we found that the goru performs averagely better than  gru lstm and eurnn. we also show the big gains over  eurnn by introducing the gates.    language modeling  character level prediction  we test each rnn on character level language modeling.  the rnn is fed by one character each step from a real  context and supposed to output the prediction for the next  character. we used the penn treebank corpus  marcus   marcinkiewicz  and santorini      .  we use rmsprop with minibatch size of    and a learning rate of  .   . each training sequence is unfolded into     time steps. similar to most work in language modeling  at  the end of each sequence  the hidden state is saved and used     task      single supporting fact      two supporting facts      three supporting facts      two arg. relations      three arg. relations      yes no questions      counting      lists sets      simple negation       indefinite knowledge       basic coreference       conjunction       compound coref.       time reasoning       basic deduction       basic induction       positional reasoning       size reasoning       path finding       agent s motivations  mean performance    goru    .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .    .     .     .     gru    .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .    .     .     .     lstm    .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .    .     .     .     eurnn    .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .    .     .     .     baseline  weston et al.                                                                             .         .     table    question answering task on babi dataset. test accuracy     on goru  gru  lstm and eurnn. all models are  performed on one way rnn without extra memory or attention mechanism. goru achieves highest average accuracy.  to initialize the hidden state for the next sequence. this allows the neural network to give consistent predictions even  at the beginning of a sequence.  we show the final test performance in table   by comparing their performance in terms of bits per character. goru  is performing comparable to lstm and gru in our experiments and it performs significantly better than eurnn.  we have also done an ablation study with disabling reset  and update gates. since most of the relevant information  for character level prediction can be obtained by only using  the recent rather than distant past  karpathy  johnson  and  fei fei        the core of the character prediction challenge  does not involve the main strength of eurnn.    model  lstm  gru  eurnn  goru  goru  w o reset gate   goru  w o update gate     bpc   .      .      .      .      .      .         units                                   table    penn treebank character level modeling test on  goru  gru  lstm and eurnn. we only use single layer  models. we choose the size of the models to match the number of parameters. goru is able to outperform eurnn.  we also tested the performance of restricted goru which  shows the necessity of both reset and update gates.    speech spectrum prediction  we tested the ability of our rnn models on real world  speech spectrum prediction task in short time fourier transform  stft  wisdom et al.       jing et al.      . we used  timit dataset sampled in   khz. the audio .wav file is initially divided into different time frames and then fourier  transformed into the frequency domain and finally normalized for training testing. in our stft operation we uses a  hann analysis window of     samples     milliseconds  and  a window hop of     samples     milliseconds . in this task   the rnns are required to predict th log magnitude of the  stft frame at time t      given all the log magnitudes of  stft frames up to time t.  we used a training set with      utterances  a validation  set of     utterances and      utterances for test. we trained  all rnns for with the same batch size    using adam optimization with a learning rate of  .   .  we found goru significantly outperforms all other models with same hidden size as shown in table  .  model  lstm  gru  eurnn  goru     parameters    k    k    k    k    mse validation     .     .     .     .     mse test     .     .     .     .     table    speech spectrum prediction test on goru  gru   lstm and eurnn. we only use single layer models with  same hidden size    .          conclusion    we have built a novel rnn that brings the benefits of orthogonal matrices to gated architectures  the gated orthogonal  recurrent units  goru . by replacing the hidden to hidden  matrix in the reseting path of the gru to be an orthogonal matrix  and replacing the non linear activation to a modrelu  goru gains the advantage of unitary orthogonal  rnns since the gradient can pass through long time steps  without exploding. our empirical results showed that goru  is the only model we found that could solve both the synthetic copying task and the denoise task. moreover  goru  is able to outperform gru and lstm in several benchmark  tasks.  these results suggest that the goru is the first step in  bringing an explicit forgetting mechanism to the class of  unitary orthogonal rnns. our method demonstrates how to  incorporate orthogonal matrices into a variety of neural network architectures  and we are excited to open the gate the  next level of neural networks.    acknowledgment  this work was partially supported by the army research  office through the institute for soldier nanotechnologies under contract w   nf    d      the national science foundation under grant no. ccf          and by  the semiconductor research corporation under grant no.       ep      b.    