introduction    in the last few years recurrent neural networks  rnns            have proved very effective in several natural language processing  nlp  tasks such as part of speech tagging  pos tagging   chunking  named entity recognition  ner   spoken language  understanding  slu   machine translation and even more                       . these  models are particularly effective thanks to their recurrent architecture  which allows  neural models to keep in memory past information and re use it at the current processing step.        in the literature of rnns applied to nlp  several architectures have been proposed.  at first elman and jordan rnns  introduced in         and known also as simple rnns   have been adapted to nlp. the difference between these two models is in the type of  connection giving the recurrent character to these two architectures  in the elman rnn  the recursion is a loop at the hidden layer  while in the jordan rnn it relies the output  layer to the hidden layer. this last recursion allows to use at the current step labels  predicted for previous positions in a sequence.  these two recurrent models have shown limitations in learning relatively long contexts     . in order to overcome this limitation the rnns known as long short term  memory  lstm  have been proposed    . recently  a simplified and  apparently  more  effective variant of lstm has been proposed  using gated recurrent units and thus  named gru     .  despite outstanding performances on several nlp tasks  rnns have not been explicitly adapted to integrate effectively label dependency information in sequence labeling tasks. their sequence labeling decisions are based on intrinsically local functions  e.g. the softmax . in order to overcome this limitation  sophisticated hybrid  rnn crf models have been proposed               where the traditional output layer  is replaced by a crf neural layer. these models reach state of the art performances   their evaluation however is not clear. in particular it is not clear if performances derive from the model itself  or thanks to particular experimental conditions. in      for  example  the best result of pos tagging on the penn treebank corpus is an accuracy  of   .    which is reached using word embeddings trained using glove       on huge  amount of unlabeled data. the model of      without pre trained embeddings reaches  an accuracy of   .   which doesn t seem that outstanding if we consider that a crf  model dating from       trained from scratch  without using any external resource   reaches an accuracy of   .  on the same data     . we achieved the same result on  the same data with a crf model trained from scratch using the incremental procedure  described in     . moreover  the first version of the network proposed in this paper   but using a sigmoid activation function and only the l  regularization  tough with a  slightly different data preprocessing  achieves an accuracy on the penn treebank of    .      .  the intuition behind this paper is that embeddings allow a fine and effective modeling not only of words  but also of labels and label dependencies  which are crucial in  some tasks of sequence labeling. in this paper we propose  as alternative to rnn crf  models  a variant of rnn allowing this more effective modeling. surprisingly  a simple  modification to the rnn architecture results in a very effective model  in our variant of  rnn the recurrent connection connects the output layer to the input layer and  since the  first layer is just a look up table mapping discrete items into embeddings  labels predicted at the output layer are mapped into embeddings the same way as words. label  embeddings and word embeddings are combined at the hidden layer  allowing to learn  relations between these two types of information  which are used to predict the label at  current position in a sequence. our intuition is that using several label embeddings as  context  a rnn is able to model correctly label dependencies  the same way as more  sophisticated models explicitly designed for sequence labeling like crfs     .  this paper is a straight follow up of     . contributions with respect to that work  are as follows          a  elman     b  jordan     c  our variant    figure    high level schema of simple rnns  elman and jordan  and the variant  proposed in this paper.  i  an analysis of performances of forward  backward and bidirectional models. ii   the use of relu hidden layer and dropout regularization      at the hidden and embedding layers for improved regularized models. iii  the integration of a character level  convolution layer. iv  an in depth evaluation  showing the effect of different components and of different information level on the performance. v  a straightforward  comparison of the proposed variant of rnn to elman  jordan  lstm and gru rnns   showing that the new variant is at least as effective as the best rnn models  such as  lstm and gru. our variant is even more effective when taking label dependencies  into account is crucial in the task  proving that our intuition is correct.  an high level schema of simple rnns and of the variant proposed in this paper is  shown in figure    where w is the input word  y is the label  e  h  o and r are the  model parameters  which will be discussed in the following sections.  since evaluations on tasks like pos tagging on the penn treebank are basically  reaching perfection  state of the art is at   .   accuracy   any new model would probably provide little or no improvement. also  performances on this type of tasks seem to  have reached a plateau  as models achieving   .  accuracy or even better  were already  published starting from              . we propose instead to evaluate all the models  on two different and widely used tasks of spoken language understanding       which  provide more variate evaluation settings  atis      and media     .  atis is a relatively simple task and doesn t require a sophisticated modeling of  label dependencies. this task allows to evaluate models in similar settings as tasks like  pos tagging or named entity recognition as defined in the conll shared task        both widely used as benchmarks in nlp papers. media is a very challenging task   where the ability of models to keep label dependencies into account is crucial to obtain  good results.  results show that our new variant is as effective as the best rnn models on a simple task like atis  still having the advantage of being much simpler. on the media  task however  our variant outperforms all the other rnns by a large margin  and even  sophisticated crf models  providing the best absolute result ever achieved on this task.  the paper is organized as follows  in the next section we describe the rnns used  in the literature for nlp  starting from existing models to arrive at describing the new          variant we propose. in the section   we present the corpora used for evaluation  the  experimental settings and the results obtained in several experimental conditions. we  draw some conclusions in section  .         recurrent neural networks  rnns     in this section we describe the most popular rnns used for nlp  such as elman and  jordan rnns         and the most sophisticated rnns like lstm and gru        .  we also describe training and inference procedures  and the rnn variant we propose.     .     elman and jordan rnns    elman and jordan rnns are defined as follows   ht elman     r helman  t     h it    ht jordan     r yt     h it                  the difference between these two models is in the way of computing hidden activities   while the output is computed in the same way   yt   sof tmax o h t               ht and yt are respectively the hidden and output layer s activities     is an activation  function  h  o and r are the parameters at the hidden  output and recurrent layer   respectively  biases are omitted to keep equations lighter . helman  t   is the hidden layer  activity computed at previous time step and used as context in the elman rnn  while  yt   is the previous predicted labels  used as context in the jordan rnn. it is the  input  which is often the concatenation of word embeddings in a fixed window dw  for  window of words  around the current word wt to be labeled. we define as e wi   the  embedding of any word wi . it is then defined as   it    ew  wt dw  ...ew  wt  ...ew  wt dw          where     is the concatenation of vectors  or matrices in the following sections . the  sof tmax function  given a set s of m numerical values vi   associated to discrete elements i       m   computes the probability associated to each element as   vi   i       m  p i    pme evj  j    this function allows to compute the probability associated to each label and choose as  predicted label the one with the highest probability.     .  long short term memory  lstm  rnns  while lstm is often used as the name of the whole network  it just defines a different  way of computing the hidden layer activities. lstms use gate units to control how  past and present information affect the network s internal state  and a cell to store past  information that is going to be used as context at the current processing step. forget   input gates and cell state are computed as   ft           wf ht     uf it             it           wi ht     ui it             c t           wc ht     uc it               h means the hidden layer of any model  as the output layer is computed in the same way for all networks     described in this paper.            is used to indicate a different activation function from    . c t is actually an intermediate value used to update the cell state value as follows   ct   ft ct     it c t       is the element wise multiplication. once these quantities have been computed  the  output gate is computed and used to control the hidden layer activities at the current  time step t   ht    ot  lstm           wo ht     uo it        ot             ct              once again  and in the remainder of the paper   biases are omitted to keep equations  lighter. as we can see  each gate and the cell state have their own parameter matrices  w and u   used for the linear transformation of the previous hidden state  ht     and  the current input  it  . the evolution of the lstm layer named gru  gated recurrent  units        combines together forget and input gates  and the previous hidden layer  with the cell state     ht    zt           wz ht     uz it              rt  h t           wr ht     ur it                     w  rt    gru              zt      ht       u it    ht     zt    h t                  gru is thus a simplification of lstm  it uses less units and it has less parameters to  learn.     .     ld rnn   label dependencies aware recurrent neural networks    the variant of rnn that we propose in this paper can be thought of as having a recurrent connection from the output to the input layer. note that from a different perspective  this variant can just be seen as a feed forward neural network  ffnn  using  previous predicted labels as input. since jordan rnn has the same architecture  the  only difference being that in contrast to jordan models we embed labels  we still prefer  talking about recurrent network. this simple modification to the architecture of the  network has important consequences on the model.  the reason motivating this modification is that we want embeddings for labels and  use them the same way as word embeddings. like we mentioned in the introduction   the first layer is a look up table mapping discrete  or one hot    representations into  distributional representations.  such representations can encode very fine syntactic and semantic properties  as it  has already been proved by word vec      or glove     . we want similar properties to  be learned also for labels  so that to encode in label embeddings the label dependencies  needed for sequence labeling tasks. in this paper we learn label embeddings from the  sequences of labels associated to word sentences in annotated data. but this procedure    in    the literature   and   are the sigmoid and tanh  respectively  one hot representation of a token represented by an index i in a dictionary  is a vector v of the same  size as the dictionary and assigned zero everywhere  except at position i where it is  .    the          could be applied also when structured label information is available. we could thus  exploit syntactic parse trees  structured named entities or entity relations for learning  sophisticated label embeddings.  the idea of using label embeddings has been introduced in      for dependency  parsing  resulting in a very effective parser. in this paper we go ahead with respect  to      by using several label embeddings as context to predict the label at current  position in a sequence. also we pre train label embeddings like it is usually done for  words. as consequence  we learn first generic dependencies between labels without  their interactions with words. such interactions are then integrated and refined during  the learning phase of the target sequence labeling task. for this ability to learn labeldependencies  we name our variant ld rnn  standing for label dependencies aware  rnn.  using the same formalism as before  we define ew the matrix for word embeddings  while el is the matrix for label embeddings. the word level input to our rnn  is it as for the other rnns  while the label level input is   lt    el  yt dl      el  yt dl      . . . el  yt             which is the concatenation of vectors representing the dl previous predicted labels  dl  stands  for window of labels  . the hidden layer activities of our rnn variant are  computed as   ht ld rnn     h  it lt           we note that we could rewrite the equation above as   hw it   hl lt   with a similar  formalism as before  the two equations are equivalent if we define h    hw hl  .  thanks to the use of label embeddings and their combination at the hidden layer   our ld rnn variant learns very effectively label dependencies. since the other rnns  in general don t use explicitly the label information as context  they can predict incoherent label sequences. as we already mentioned  this limitation lead research toward  hybrid rnn crf models             .  another consequence of the modification introduced in our rnn variant is an improved robustness to prediction mistakes. since we use several label embeddings as  context  see lt above   once the model has learned label embeddings  in the test phase  it is unlikely that several prediction mistakes occur in the same context. even in that  case  thanks to properties encoded in the embeddings  mistaken labels have similar  representations to correct labels  allowing the model to possibly predict correct labels.  reusing an example from       if paris is replaced by rome in a text  this has no impact on several nlp tasks  as they are both proper nouns in pos tagging  localization in  named entity recognition etc. using label embeddings provides the ld rnn variant  with the same robustness on the label side.  while the traditional jordan rnn uses also previous labels as context information   it has not the same robustness because of the poor label representation used in adaptations of this model to nlp tasks. in jordan rnns used for nlp like             labels  are represented either with the probability distribution computed by the sof tmax  or  with the one hot representation computed from the probability distribution.  in the latter case it is clear that a prediction mistake can have a bad impact in  the context  as the only value being   in the one hot representation would be in the  wrong position. instead  using the probability distribution may seem a kind of fazzy          representation over several labels  but we have found empirically that the probability is  very sharp and picked on one or just few labels. in any case this representation doesn t  provide the desired robustness that can be achieved with label embeddings.  from another point of view  we can interpret the computation of the hidden activities in a jordan rnn as using label embeddings. in the equation    the multiplication  ryt     since yt   is a sparse vector  can be interpreted as the selection of an embedding from r.  even with this interpretation there is a substantial difference between a jordan rnn  and our variant. in the jordan rnn  once the label embedding has been computed with  ryt     the result is not involved in the linear transformation applied by the matrix h   which is only applied to the word level input it . the result of this multiplication is  added to ryt   and then the activation function is applied.  in our variant in contrast  labels are first mapped into embeddings with e yi    .  word and label inputs it and lt are then both transformed by multiplying by h  which  is correctly dimensioned to apply the linear transformation on both inputs. in our  variant thus  two different label transformations are always applied  i  the conversion  from sparse to embedding representation  ii  the linear transformation by multiplying  label embeddings by h.     .     learning and inference    we learn the ld rnn variant like all the other rnns  by minimizing the cross entropy  between the expected label lt and the predicted label yt at position t in the sequence   plus a l  regularization term               c    lt log yt               is a hyper parameter to be tuned    is a short notation for ew   el   h  o. lt is the  one hot representation of the expected label. since yt above is the probability distribution over the label set  we can see the output of the network as the probability  p  i it   lt    i       m   where it and lt are the input of the network  words and  labels   i is the index of one of the labels defined in the targeted task.  we can thus associate to the ld rnn model the following decision function   argmaxi    m  p  i it   lt          we note that this is still a local decision function  as the probability of each label is  normalized at each position of a sequence. despite this  the use of label embeddings  lt as context allows the ld rnn to effectively model label dependencies. since the  other rnns like elman and lstm don t use the label information in their context   their decision function can be defined as   argmaxi    m  p  i it          which can lead to incoherent predicted label sequences.  we use the traditional back propagation algorithm with momentum to learn our networks     . given the recurrent nature of the networks  the back propagation through  time  bptt  is often used     . this algorithm consists in unfolding the rnn for n  previous steps  n being a parameter to choose  and using thus the n previous inputs  and hidden states to update the model s parameters. the traditional back propagation    in    our case  yi is explicitly converted from probability distribution to one hot representation.          algorithm is then applied. this is equivalent to learn a feed froward network of depth  n . the bptt algorithm is supposed to allow the network to learn arbitrary long contexts. however     has shown that rnns for language modeling learn best with only  n     previous steps. this can be due to the fact that  at least in nlp  a longer context  does not lead necessarily to better performances  as a longer context is also more noisy.  since the bptt algorithm is quite expensive      chose to explicitly use the contextual information provided by the recurrent connection  and to use the traditional  back propagation algorithm  apparently without performance loss.  in this paper we use the same strategy. when the contextual information is used  explicitly in a jordan rnn  the hidden layer state is computed as follows   ht     r yt dl    yt dl    ... yt       h it          a similar modification can be applied also to elman  lstm and gru rnns to keep  into account explicitly the previous hidden states. to our knowledge however  these  networks are effectively learned using only one previous hidden state             .  from explanations above we can say that using explicit wide context of words and  labels like we do in ld rnn  can be seen as an approximation of the bptt algorithm.     .     toward more sophisticated networks  character level convolution    even if word embeddings provide a very fine encoding of word features  several works  such like                  have shown that more effective models can be obtained using a  convolution layer over characters of words. character level information is indeed very  useful to allow a model generalizing over rare inflected surface forms and even out ofvocabulary words in the test phase. word embeddings are in fact much less effective in  such cases. the convolution over word characters provide also the advantage of being  very general  it can be applied in the same way to different languages  allowing to  re use the same system on different languages and tasks.  in this paper we focus on a convolution layer similar to the one used in     for  words. for any word w of length  w   we define ech  w  i  the embedding of the character i of the word w. we define wch the matrix of parameters for the linear transformation applied by the convolution  once again we omit the associated bias . we  compute a convolution of window size  dc     over characters of a word w as follows      i        w   convi   wch  ech  w  i   dc    . . . ech  w  i   . . . ech  w  i   dc       convch    conv  . . . conv w       charw   m ax convch    the m ax function is the so called max pooling    . while it is not strictly necessary  mapping characters into embeddings  it would be probably less interesting applying the  convolution on discrete representations. the matrix convch is made of the concatenation of vectors returned from the application of the linear transformation wch . its  size is thus  c     w   where  c  is the size of the convolution layer. the max pooling  computes the maxima over the word length direction  thus the final output charw has  size  c   which is independent from the word length. charw can be interpreted as a        distributional representation of the word w encoding the information at w s character  level. this is a complementary information with respect to word embeddings  which  encode inter word information  and provide the model with an information similar to  what is provided by discrete lexical features like word prefixes  suffixes  capitalization information etc.  plus information about morphologically correct words of a given  language.     .     rnn complexities    the improved modeling of label dependencies in our ld rnn variant is achieved at the  cost of more parameters with respect to the simple rnn models. however the number  of parameters is still much less than sophisticated networks like lstm. in this section  we provide a comparison of rnns complexity in terms of the number of parameters.  we introduce the following symbols   h  and  o  are the size of the hidden and  output layers  respectively. the size of the output layer is the number of labels  n is the  embedding size  in ld rnn we use the same size for word and label embeddings  dw  is the window size used for context words  and dl is the number of label embeddings  we use as context in ld rnn. we analyze the hidden layer of all networks  and the  embedding layer for ld rnn. the other layers are exactly the same for all the networks  described in this paper.  for elman and jordan rnns  the hidden layer has the following number of parameters  respectively     h     h  r     h      dw     n  h elman    o     h  r     h      dw     n  h jordan  subscripts indicate from which matrix the parameters come. the factor   dw     n  comes from the   dw      words used as input context and then mapped into embeddings. the factor  o     h  in jordan rnn is due to the fact that the matrix r connects  output and hidden layers.  in ld rnn we have     o    n  el       dw       dl  n      h  h ld rnn  the factor  o  n is due to the use of the matrix el containing  o  label embeddings  of size n . since in this paper we chose n    h  and  o     h   and since in ldrnn we don t use any matrix r on the recurrent connection  the fact of using label  embeddings doesn t increase the number of parameters of the ld rnn variant.  the hidden layer of ld rnn however is dimensioned to connect all the word and  label embeddings to all the hidden neurons. as consequence in the matrix h we have  dl n more parameters than in the matrix h of elman and jordan rnns.  in lstm and gru rnns we have two extra matrices w and u for each gate and  for the cell state  used to connect the previous hidden layer and the current input  respectively. these two matrices contain thus  h   h  and   wd   n   h  parameters   respectively.  using the same notation and the same settings as above  in the hidden layer of  lstm and gru we have the following number of parameters       h     h     u       dw     n   h lstm      h     h     u       dw     n   h gru          the   for gru reflects the fact that this network uses only   gates and a cell state. it  should be pointed out  however  that while we have been testing lstm and gru with  a word window for a matter of fair comparison    these layers are applied on the current  word and the previous hidden layer only  without the need of a word window. this is  because this layer learns automatically how to use previous word information. in such  case the complexity of the lstm layer reduces to     h     h     u     n   h lstm .  if we choose  u      h   such complexity is comparable to that of ld rnn in terms  of number of parameters  slightly less actually . the lstm is still more complex  however because the hidden layer computation requires   gates and the cell state  c t    computations  each involving   matrix multiplications   the update of the new cell state  ct  involving also   matrix multiplications   and only after the hidden state can be  computed. ld rnn s hidden state  in contrast  requires only matrix rows selection  and concatenation to compute it and lt   which are very efficient operations  and then  the hidden state can already be computed.  as consequence  while the variant of rnn we propose in this paper is more complex than simple rnns  lstm and gru rnns are by far the most complex networks.     .  forward  backward and bidirectional networks  the rnns introduced in this paper are proposed as forward  backward and bidirectional models     . the forward model is what has been described so far. the architecture of the backward model is exactly the same  the only difference is that the backward  model processes data from the end to the begin of sequences. labels and hidden layers  computed by the backward model can thus be used as future context in a bidirectional  model.  bidirectional models are described in details in     . in this paper we utilize the  version using separate forward and backward models. the final output is computed as  the geometric mean of the output of the two  p individual models  that is   yt   ytf ytb  where ytf and ytb are the output of the forward and backward models  respectively.  in the development phase of our systems  we noticed no difference in terms of  performance between the two types of bidirectional models described in     . we chose  thus the version described above  since it allows to initialize all the parameters with the  forward and backward models previously trained. as consequence the bidirectional  model is very close to a very good optimum since the first learning iteration  and very  few iterations are needed to learn the final model.        .     evaluation  corpora for spoken language understanding    we evaluated our models on two tasks of spoken language understanding  slu           indeed we observed better performances when using a word window with respect to when using a single  word           the atis corpus  air travel information system       was collected for building  a spoken dialog system able to provide flight information in the united states.  atis is a simple task dating from     . training data are made of      sentences  chosen among dependency free sentences in the atis   and atis   corpora. the  test set is made of     sentences taken from the atis   nov   and dec   data.  since there are not official development data  we taken a part of the training set for this  purpose. the word and label dictionaries contain      and    items  respectively. we  use the version of the corpus published in       where some word classes are available   such as city names  airport names  time expressions etc. these classes can be used as  features to improve the generalization of the model on rare or unseen words. more  details about this corpus can be found in     .  an example of utterance transcription taken from this corpus is  i want all the  flights from boston to philadelphia today . the words boston  philadelphia and today  in the transcription are associated to the concepts departure.city  arrival.city  and departure.date  respectively. all the other words don t belong to any concept   they are associated to the void concept named o  for outside . this example show the  simplicity of this task  the annotation is sparse  only   words of the transcription are  associated to a non void concept  there is no segmentation problem  as each concept is  associate to one word. because of these two characteristics  the atis task is similar  on the one hand to a pos tagging task  where there is no segmentation of labels over  multiple words  on the other hand it is similar to a linear named entity recognition  task  where the annotation is sparse.  we are aware of the existence of two version of the atis corpus  the official version published starting from       and the version associated to the tutorial of deep  learning made available by the authors of    .  . this last version has been modified   some proper nouns have been re segmented  for example the token new york has been  replaced by two tokens new york   and a preprocessing has been applied to reduce the  word dictionary  numbers have been converted into the conventional token digit  and  singletons of the training data  as well as out of vocabulary words of the developpement and test data  have been converted into the token unk . following the tutorial of      we have been able to download the second version of the atis corpus. however  in this version word classes that are available in the first version are not given. we ran  some experiments with these data  using only words as input. the results we obtained  are comparable with those published in       in part from same authors of    . however without word classes we cannot fairly compare with works that are using them. in  this paper we thus compare only with published works that used the official version of  atis.  the french corpus media      was collected to create and evaluate spoken dialog systems providing touristic information about hotels in france. this corpus is  made of      dialogs collected with wizard of oz approach. the dialogs have been  manually transcribed and annotated following a rich concept ontology. simple semantic components can be combined to create complex semantic structures.  the rich  semantic annotation is a source of difficulties  but also the annotation of coreference    available    at http   deeplearning.net tutorial rnnslu.html  example the component localization can be combined with other components like city   relative distance  generic relative location  street etc.    for           words  oui  l   hotel  le  prix     moins  cinquante  cinq  euros    media  classes  labels  answer b  bdobject b  bdobject i  object b  object i  comp. payment b  relative  comp. payment i  tens  paym. amount b  units  paym. amount i  currency  paym. currency b    words  i d  like  to  fly  delta  between  boston  and  chicago    atis  classes  labels  o  o  o  o  airline  airline name  o  city  fromloc.city name  o  city  toloc.city name    table    an example of annotated utterance transcription taken from media  left   and atis  right . the translation in french is  yes  the one which price is less than     euros per night     sentences    mots    vocab.    oov     training          words  concepts                                     dev.         words  concepts                           .     .      test         words  concepts                              .     .      table    statistic of the corpus media  phenomena. some words cannot be correctly annotated without knowing a relatively  long context  often going beyond a single dialog turn. for example in the utterance  transcription  yes  the one which price is less than    euros per night   the one is a  mention of an hotel previously introduced in the dialog. statistics on the corpus media are shown in table  .  the task resulting from the corpus media can be modeled as a sequence labeling  task by chunking the concepts over several words using the traditional bio notation      .  thanks to the characteristics of these two corpora  together with their relatively  small size which allows training models in a reasonable time  these two tasks provide  ideal settings for the evaluation of models for sequence labeling. a comparative example of annotation  showing also the word classes available for the two tasks and  mentioned above  is shown in the table  .     .     settings    the rnn variant ld rnn has been implemented in octave  using openblas for  low level computations  .  ld rnn models are trained with the following procedure     neural network language models  nnlm   like the one described in       are    https   www.gnu.org software octave    our code is described at http   www.marcodinarelli.it software.php and available upon request.    http   www.openblas.net  this library allows a speed up of roughly      on a single matrix matrix  multiplication using    cores. this is very attractive with respect to the speed up of      that can be  reached with a gpu  keeping into account that both octave and openblas are available for free.           trained for words and labels to generate the embeddings  separately .    forward and backward models are trained using the word and label embeddings  trained at previous step.    the bidirectional model is trained using as starting point the forward and backward models trained at previous step.  we ran also some experiments using embeddings trained with word vec     . the  results obtained are not significantly different from those obtained following the procedure described above. this outcome is similar to the one obtained in     . since the  tasks addressed in this paper are made of small data  we believe that any embedding is  equally effective. in particular tools like word vec are designed to work on relatively  big amount of data. results obtained with word vec embeddings will not be described  in the following sections.  we roughly tuned the number of learning epochs for each model on the development data of the addressed tasks     epochs are used to train word embeddings      for label embeddings     for the forward and backward models    for the bidirectional  model  the optimum of this model is often reached at the first epoch on the atis task   between the  rd and the  th epoch on media . at the end of the training phase  we  keep the model giving the best prediction accuracy on the development data. we stop  training the model if the accuracy is not improved for   consecutive epochs  also known  as early stopping strategy      .  we initialize all the weights with the  so called  xavier initialization       theoretically motivated in      as keeping the standard deviation of the weights during the  training phase when using relu  which is the type of hidden layer unit we chose for  our variant of rnn.  we also tuned some of the hyper parameters on the development data  we found out  that the best initial learning rate is  .   this is linearly decreased with a value computed  as the ratio between the initial learning rate and the number of epochs  learing rate  decay . we combine dropout and l  regularization       the best value for the dropout  probability is  .  at the hidden layer   .  at the embedding layer on atis   .   on  media. the best coefficient     for the l  regularization is  .   for all the models   except for the bidirectional model where the best is  e   .  we ran also some experiments for optimizing the size of the different layers. in  order to minimize the time and the number of experiments  this optimization has been  based on the result provided by the forward model on the two tasks  and using only  words and labels as input  without word classes and character convolution  which were  optimized separately . the best size for the embeddings and the hidden layer is      for both tasks. the best size for the character convolution layer is    on atis     on  media. in both cases  the best size for the convolution window is    meaning that  characters are used individually as input to the convolution. a window of size    one  character on the left  one on the right  plus the current character  gives roughly the  same results  we thus prefer the simpler model. with a window of size    results starts  to slightly deteriorate.  we also optimized the size of the word and label context used in the ld rnn  variant. on atis the best word context size is       on the lest    on the right plus the         model  ld rnn  ld rnn  ld rnn  ld rnn    words  words cc  words classes  words classes cc    forward    .       .       .       .       f  measure  backward  bidirectional    .       .       .       .       .       .       .       .       table    results in terms of f  measure on atis  using different level of information  as input.  model  ld rnn  ld rnn  ld rnn  ld rnn    words  words cc  words classes  words classes cc    forward    .       .       .       .       f  measure  backward  bidirectional    .       .       .       .       .       .       .       .       table    results in terms of f  measure on media  using different level of information  as input.  current word   the best label context size is  . on media the best sizes are   and    respectively. these values are the same found in      and comparable to those of     .  the best parameters found in this phase has been used to obtain baseline models. the goal was to understand the behavior of the models with the different level  of information used  the word classes available for the tasks  and the character level  convolution. some parameters needed to be re tuned  as we will describe later on.  concerning training and testing time of our models  the overall time to train and test  forward  backward and bidirectional models  using only words and classes as input  is  roughly   hour    minutes on media     minutes on atis. these times go to   hours  for media and   hours    minutes for atis  using also word classes and character  convolution as input. all these times are measured on a intel xeon e       at  .  ghz   using    cores.     .     results    all the results shown in this section are averages over   runs. embeddings were learned  once for all experiments.   . .     incremental results with different level of information    in this section we describe results obtained with incremental levels of information given  as input to the models  i  only words  previous labels are always given as input    indicated with words in the tables  ii  words and classes words classes  iii  words and  character convolution words cc  iv  all possible inputs words classes cc.  the results obtained on the atis task are shown in the table    results on media  are in table  .  results in these tables show that models have a similar behavior on the two tasks.  in particular on atis  adding the different level of information results improve progressively and the best performance is obtained integrating words  labels and character  convolution  though some of the improvements do not seem statistically significant   taking into account the small size of this corpus.           this observation is confirmed by results obtained on media  where adding the  character level convolution leads to a slight degradation of performances. in order to  understand the reason of this behavior we analyzed the training phase on the two tasks.  we found out that the main problem was an hidden layer saturation  with the number  of hidden neurons chosen in the preliminary optimization phase using only words  and  labels   the hidden layer was not able to model the whole information richness provided  by all the inputs at the same time. we ran thus some experiments using a larger hidden  layer with size      which gave the results shown in the two tables with the model  ld rnn words classes cc. for lack of time we did not further optimized the size  of the hidden layer.  beyond all of that  results shown in the table   and   are very competitive  as we  will discuss in the next section.   . .     comparison with the state of the art    in this section we compare our results with the best results found in the literature. in  order to be fair  the comparison is made using the same input information  words and  classes. in the tables we use e rnn for elman rnn  j rnn for jordan rnn  i rnn  for the improved rnn proposed by     .    in order to give an idea of how our rnn variant compares to lstm crf models  like the one of       we ran an experiment on the penn treebank     . with a similar  data pre processing  exactly the same data split  using a sigmoid activation function   and using only words as input  the ld rnn variant achieves an accuracy of   .  .  this is comparable to the   .  achieved by the lstm crf model of      without  pre trained embeddings.    results on the atis task are shown in table  . on this task we compare to results  published in      and     .  the results in the table   show that all models obtain a good performance on this  task  always higher than   .  f  . this confirm what we anticipated in the previous  section concerning how easy is this task.  the gru rnns of      and our variant ld rnn obtain equivalent results    .      which is slightly better than all the other models  in particular with the bidirectional  models. this is a good outcome  as our variant of rnn obtains the same result as  gru while using much less parameters  see section  .  for rnns complexity . indeed  lstm and gru are considered very effective models for learning very long contexts.  the way they are used in      allows to learn long contexts on the input side  words    they are not adapted however to learn also long label contexts  which is what we do in  this paper with our variant. the fact that the best word context on this task is made of     words  show that this is the most important information to obtain good results on  this task. it is thus not surprising that the gru rnn achieves such good performance.  comparing our results on the atis task with those published in      with a jordan  rnn  which uses the same label context as our models  we can conclude that the     this is a publication in french  but results in the tables are easy to understand and directly comparable  to our results.     we did not run further experiments because without a gpu  experiments on the penn treebank are still  quite expensive.           model       lstm       gru       e rnn       j rnn       i rnn  ld rnn words classes    forward    .       .       .       .       .       .       f  measure  backward  bidirectional       .          .       .       .       .       .       .       .       .       .       table    comparison of our results on the atis task with the literature  in terms of f   measure.  model    forward       crf       e rnn       j rnn       lstm       gru       e rnn       j rnn       i rnn  ld rnn words classes      .       .       .       .       .       .       .       .       f  measure  backward  bidirectional    .                      .          .       .       .       .       .       .       .       .       .       table    comparison of our results on the media task with the literature  in terms of  f  measure.  model       crf       crf       crf  ld rnn words  ld rnn words classes  ld rnn words classes cc    cer    .      .      .      .       .       .       .       .       .       table    results on the media task in terms of concept error rate  cer   compared with the  best results published so far on this task.    advantage in the variant ld rnn is given by the use of label embeddings and their  combination at the hidden layer.  this conclusion is more evident if we compare results obtained with rnns using  label embeddings with the other rnns on the media task. this comparison is shown  in table  . as we mentioned in the section  .   this task is very challenging for several  reason  but in the context of this paper we focus on the label dependencies that we  claim we can effectively model with our rnn variant.  in this context we note that a traditional jordan rnn  the j rnn of       which  is the only traditional model to explicitly use previous label information as context   is more effective than the other traditional models  including lstm and gru    .    f  with j rnn    .   with gru  second best model among traditional rnns . we  note also that on media  crfs  which are models specifically designed for sequence  labeling  are by far more effective than the traditional rnns    .   f  with the crf  of      .  the only models outperforming crfs on the media task are the i rnn model of       and our ld rnn variant  both using label embeddings.  even if results on media discussed so far are very competitive  this task has been  designed for spoken language understanding  slu      . in slu the goal is to extract  a correct semantic representation of a sentence  allowing a correct interpretation of the           user will by the spoken dialog system. while the f  measure is strongly correlated  with slu evaluation metrics  the evaluation measure used most often in the literature  is the concept error rate  cer . cer is defined exactly in the same way as word  error rate in automatic speech recognition  where words are replaced by concepts.    in order to place our results on an absolute ranking among models designed for the  media task  we propose a comparison in terms of cer to the best models published  in the literature  namely            and     . this comparison is shown in table  .  the best individual models published by            and      are crfs  achieving  a cer of   .     .  and   .   respectively. these models use both word and classes   and a rich set of lexical features such like word prefixes  suffixes  word capitalization  information etc. we note that the large gap between these crf models is due to the  fact that the crf of      is trained with an improved margin criterion  similar to the  large margin principle of svm         . we note also that comparing significance tests  published in       a difference of  .  in cer is already statistically significant. since  results in this paper are higher  we hypothesize than even smaller gains are significant.  our best ld rnn model achieve a cer of   .  . to the best of our knowledge  this is the best cer obtained on the media task with an individual model. moreover   instead of taking the mean of cer of several experiments  following a strategy similar  to      one can run several experiments and keep the model obtaining the best cer on  the development data of the target task. results obtained using this strategy are shown  in table   between parenthesis. the best result obtained by our ld rnn is a cer of    .    the best absolute result on this task so far  even better than the rover model       used in       which combines   individual models  including the individual crf  model achieving   .  cer.     .     results discussion    in order to understand the high performances of the ld rnn variant on the media  task  we made some simple analyses on the model output  comparing them to the output  of a jordan rnn trained with our own system in the same conditions as ld rnn  models. the main difference between these two models is the general tendency of the  jordan rnn to split a single concept into two or more concepts  mainly for concepts  instantiated by long surface forms  such like command tache. this concept is used to  mean the general user will in a dialog turn  e.g. hotel reservation  price information  etc. . the jordan rnn often split this concept into several concepts by introducing a  void label  associated to a stop word. this is due to the limitation of this model to take  relatively long label context into account  even if it is the only traditional rnn using  explicitly previous labels as context information.  surprisingly  ld rnn never makes this mistake and in general never makes segmentation errors  concerning the bio formalism . this can be due to two reasons. the  first is that label embeddings learns similar representations for semantically similar labels. this allows the model to correctly predict start of concept  b  even if the target  word has been seen in the training set only as continuation of concept  i   or viceversa   as the two labels acquire very similar representations. the second reason  which is not     the errors made by the system are classified as insertions  i   deletions  d  and substitutions  s . the  sum of these errors is divided by the number of concepts in the reference annotation  r   cer   i d s  .  r           in mutual exclusion with the first  is that the model factorizes information acquired on  similar words seen associated to start of concept labels. thus if a word has not been  seen associated to start of concept labels  but similar words do  the model is still able  to provide the correct annotation. this second reason is what made neural networks  popular for learning word embeddings in earlier publications     . in any case  in our  experience  we never observed such precise behavior even with crf models tuned for  the media task. for this reason we believe ld rnn deserves the name of label  dependencies aware rnn.  still ld rnn makes mistakes  which means that once a label annotation starts for  a target word  even if the label is not the correct one  the same label is kept even if the  following words provide evidence that the correct label is another one. ld rnn tends  to be coherent with previous labeling decisions. this behavior is due to the use of a  local decision function which definitely relies heavily on the label embedding context   but it doesn t prevent the model from being very effective. interestingly  this behavior  suggests that ld rnn could still benefit from a crf neural layer like those used in              . we leave this as future work.         conclusion    in this paper we proposed a new variant of rnn for sequence labeling using a wide  context of label embeddings in addition to the word context to predict the next label  in a sequence. we motivated our variant as being more effective at modeling label dependencies. results on two spoken language understanding tasks show that i  on a  simple task like atis our variant achieves the same performance as much more complex models such as lstm and gru  which are claimed the most effective rnns  ii   on the media task  where modeling label dependencies is crucial  our variant outperforms by a large margin all the other rnns  including lstm and gru. when  compared to the best models of the literature in terms of concept error rate  cer    our rnn variant results to be more effective  achieving a state of the art cer of   .  .         acknowledgements    this work has been partially funded by the french anr project democrat anr   ce       .    