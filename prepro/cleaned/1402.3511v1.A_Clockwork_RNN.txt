introduction  recurrent neural networks  rnns  robinson   fallside         werbos        williams        are a class of connectionist models that possess internal state or short term  memory due to recurrent feed back connections  that make  them suitable for dealing with sequential problems  such as  speech classification  prediction and generation.  standard rnns trained with stochastic gradient descent  have difficulty learning long term dependencies  i.e. spanning more that    time steps  encoded in the input sequences    hkou   idsia . ch  klaus   idsia . ch  tino   idsia . ch  juergen   idsia . ch    due to the vanishing gradient  hochreiter        hochreiter et al.       . the problem has been addressed for example by using a specialized neuron structure  or cell  in  long short term memory  lstm  networks  hochreiter    schmidhuber        that maintains constant backward  flow in the error signal  second order optimization methods  martens   sutskever        preserve the gradient by  estimating its curvature  or using informed random initialization  sutskever et al.        which allows for training the  networks with momentum and stochastic gradient descent  only.  this paper presents a novel modification to the simple rnn   srn  elman        architecture and  mutatis mutandis   an associated error back propagation through time  rumelhart et al.        werbos        williams        training  algorithm  that show superior performance in the generation and classification of sequences that contain long term  dependencies. here  the long term dependency problem  is solved by having different parts  modules  of the rnn  hidden layer running at different clock speeds  timing their  computation with different  discrete clock periods  hence  the name clockwork recurrent neural network  cw rnn .  cw rnn train and evaluate faster since not all modules are  executed at every time step  and have a smaller number of  weights compared to srns  because slower modules are not  connected to faster ones.  cw rnns were tested on two supervised learning tasks   sequence generation where a target audio signal must be  output by a network using no input  and spoken word classification using the timit dataset. in these preliminary  experiments  cw rnn outperformed both srn and lstm  with the same number of weights by a significant margin.  the next section provides an overview of the related work   section   describes the cw rnn architecture in detail and  section   discusses the results of experiments in section    and future potential of clockwork recurrent neural networks.     a clockwork rnn    figure  . cw rnn architecture is similar to a simple rnn with an input  output and hidden layer. the hidden layer is partitioned into g  modules each with its own clock rate. within each module the neurons are fully interconnected. neurons in faster module i are connected  to neurons in a slower module j only if a clock period ti   tj .     . related work  contributions to the sequence modeling and recognition  that are relevant to cw rnn are introduced in this section.  the primary focus is on rnn extensions that deal with the  problem of bridging long time lags.  one model that is similar in spirit to our approach is the  narx rnn   lin et al.       . but instead of simplifying  the network  it introduces an additional sets of recurrent  connections with time lags of    ..k time steps. these additional connections help to bridge long time lags  but introduce many additional parameters that make narx rnn  training more difficult and run k times slower.  long short term memory  lstm  hochreiter   schmidhuber        uses a specialized architecture that allows information to be stored in a linear unit called a constant error  carousel  cec  indefinitely. the cell containing the cec  has a set of multiplicative units  gates  connected to other  cells that regulate when new information enters the cec   input gate   when the activation of the cec is output to the  rest of the network  output gate   and when the activation  decays or is  forgotten   forget gate . these networks have  been very successful recently in speech and handwriting  recognition  graves et al.              sak et al.       .  stacking lstms into several layers  fernandez et al.         graves   schmidhuber        aims for hierarchical sequence processing. such a hierarchy  equipped with connec   narx stands for non linear auto regressive model with  exogeneous inputs    tionist temporal classification  ctc  graves et al.          performs simultaneous segmentation and recognition of sequences. its deep variant currently holds the state of theart result in phoneme recognition on the timit database   graves et al.       .  temporal transition hierarchy  tth  ring        incrementally adds high order neurons in order to build a memory  that is used to disambiguate an input at the current time step.  this approach can  in principle  bridge time intervals of any  length  but with proportionally growing network size. the  model was recently improved by adding recurrent connections  ring        that prevent it from bloating by reusing  the high level nodes through the recurrent connections.  one of the earliest attempts to enable rnns to handle  long term dependencies is the reduced description network  mozer             . it uses leaky neurons whose  activation changes only a bit in response to its inputs. this  technique was recently picked up by echo state networks   esn  jaeger       .  a similar technique has been used by sutskever   hinton         to solve some serial recall tasks. these temporalkernel rnns add a connection from each neuron to itself  that has a weight that decays exponentially in time. this  is implemented in a way that can be computed efficiently   however  its performance is still inferior to lstm.  evolino  schmidhuber et al.              feeds the input  to an rnn  which can be e.g. lstm to cope with long  time lags  and then transforms the rnn outputs to the target  sequences via a optimal linear mapping  that is computed     a clockwork rnn    analytically by pseudo inverse. the rnn is trained by an  evolutionary algorithm  therefore it does not suffer from the  vanishing gradient problem. evolino outperformed lstm  on a set of synthetic problems and was used to perform  complex robotic manipulation  mayer et al.       .  a modern theory of why rnns fail to learn long term dependencies is that simple gradient descent fails to optimize them  correctly. one attempt to mitigate this problem is hessian  free  hf  optimization  martens   sutskever         an  adapted second order training method that has been demonstrated to work well with rnns. it allows rnns to solve  some long term lag problems that were impossible with  stochastic gradient descent. their performance on rather  synthetic  long term memory benchmarks is approaching  the performance of lstm  though the number of optimization steps in hf rnn is usually greater. training networks  by hf optimization is an orthogonal approach to the network architecture  so both lstm and cw rnn can still  benefit from it.  hf optimization allowed for training of multiplicative rnn   mrnn  sutskever et al.        that port the concept of  multiplicative gating units to srns. the gating units are  represented by a factored   way tensor in order to reduce the  number of parameters. extensive training of an mrnn for  a number of days on a graphics cards provided impressive  results in text generation tasks.  training rnns with kalman filters  williams        has  shown advantages in bridging long time lags as well  although this approach is computationally unfeasible for  larger networks.  the methods mentioned above are strictly synchronous   elements of the network clock at the same speed. the sequence chunker  neural history compressor or hierarchical temporal memory  schmidhuber              consists  of a hierarchy or stack of rnn that may run at different  time scales  but  unlike the simpler cw rnn  it requires  unsupervised event predictors  a higher level rnn receives  an input only when the lower level rnn below is unable  to predict it. hence the clock of the higher level may speed  up or slow down  depending on the current predictability  of the input stream. this contrasts the cw rnn  in which  the clocks always run at the same speed  some slower  some  faster.     . a clockwork recurrent neural network  clockwork recurrent neural networks  cw rnn  like  srns  consist of input  hidden and output layers. there  are forward connections from the input to hidden layer   and from the hidden to output layer  but  unlike the srn     figure  . calculation of the hidden unit activations at time step  t     in cw rnn according to equation    . input and recurrent  weight matrices are partitioned into blocks. each block row in  wh and wi corresponds to the weights of a particular module.  at time step t      the first two modules with periods t      and  t      get evaluated  highlighted parts of wh and wi are used   and the highlighted outputs are updated. note that  while using  exponential series of periods  the active parts of wh and wi are  always contiguous.    the neurons in the hidden layer are partitioned into g modules of size k. each of the modules is assigned a clock  period tn    t    . . .   tg  . each module is internally fullyinterconnected  but the recurrent connections from module  j to module i exists only if the period ti is smaller than  period tj . sorting the modules by increasing period  the  connections between modules propagate the hidden state  right to left  from slower modules to faster modules  see  figure  .   t     the standard rnn output  yo   at a time step t is calculated  using the following equations    t     yh   fh  wh   y t      wi   x t       t      t     yo   fo  wo   yh                   where wh   wi and wo are the hidden  input and output  weight matrices  xt is the input vector at time step t  vectors   t    t     yh and yh  represent the hidden neuron activations at  time steps t and t  . functions fh  .  and fo  .  are the nonlinear activation functions. for simplicity  neuron biases are  omitted in the equations.  the main difference between cw rnn and an rnn is that  at each cw rnn time step t  only the output of modules i  that satisfy  t mod ti       are executed. the choice of the  set of periods  t    . . .   tg   is arbitrary. in this paper  we  use the exponential series of periods  module i has clock  period of ti    i   .  matrices wh and wi are partitioned into g blocks rows               wh   w i               wh     ...    wi     ...         whg    wig    and wh is a block upper triangular matrix  where  each block row  whi   is partitioned into block columns     a clockwork rnn           rnn    lstm    cw rnn          error                               figure  . the normalized mean squared error for the sequence  generation task  divided into one column per method  with one boxwhisker  showing mean value      and     quantiles  minimum   maximum and outliers  for every tested size of the network. note  that the plot for the standard rnn has a different scale than the  other two.          . . .    i     whi i   . . .   whi g  . at each forward pass  time step  only the block rows of wh and wi that correspond to the executed modules are used for evaluation in  equation          whi for  t mod ti        w hi            otherwise  and the corresponding parts of the output vector  yh   are  updated. the other modules retain their output values from  the previous time step. calculation of the hidden activation  at time step t     is illustrated in figure  .  as a result  the low clock rate modules process  retain and  output the long term information obtained from the input  sequences  not being distracted by the high speed modules    whereas the high speed modules focus on the local  highfrequency information  having the context provided by the  low speed modules available .  the backward pass of the error propagation is similar to  rnn as well. the only difference is that the error propagates only from modules that were executed at time step t.  the error of non activated modules gets copied back in time   similarly to copying the activations of nodes not activated  at the time step t during the corresponding forward pass           k  . k  k   k      k  . k  k   k      k  . k  k   k    of parameters    figure  . the classification error for the word classification task   divided into three columns  one per method   with one box whisker  for every tested network size.    where it is added to the back propagated error.  cw rnn runs much faster than a simple rnn with the  same number of hidden nodes since not all modules are  evaluated at every time step. the lower bound for the cwrnn speedup compared to an rnn with the same number  of neurons is g   in the case of this exponential clock setup   see appendix for a detailed derivation.     . experiments  cw rnns were compared to the simple rnn  srn  and  lstm networks. all networks have one hidden layer with  the tanh activation function  and the number of nodes in the  hidden layer was chosen to obtain  approximately  the same  number of parameters for all three methods  in the case of  cw rnn  the clock periods were included in the parameter  count .  initial values for all the weights were drawn from a gaussian distribution with zero mean and standard deviation of   . . initial values of all internal state variables were set  to  . each setup was run     times with different random  initialization of parameters. all networks were trained using stochastic gradient descent  sgd  with nesterov style  momentum  sutskever et al.       .     a clockwork rnn    rnn    lstm    cw rnn    value     .    .    .     .    .    .    .     .    .    .    .     .    .    .    .     .    .    .    .     .                                                                          timestep                                                             figure  . output of the best performing network  solid  green  compared to the target signal  dotted  blue  for each method  column  and  each training sequence  row . rnn tends to learn the first few steps of the sequence and then generates the mean of the remaining portion   while the output of lstm resembles a sliding average  and cw rnn approximates the sequence much more accurately.     . . sequence generation  the goal of this task is to train a recurrent neural network   that receives no input  to generate a target sequence as accurately as possible. the weights of the network can be seen  as a  lossy  encoding of the whole sequence  which could  be used for compression.  five different target sequences were created by sampling a  piece of music  at   .  hz for   ms. the resulting sequences  of     data points each were scaled to the interval        .  in the following experiments we compare performance on  these five sequences.  all networks used the same architecture  no inputs  one  hidden layer and a single linear output neuron. each network  type was run with   different sizes                 and       parameters  see table   for the summary of number of  hidden nodes. the networks were trained over      epochs  to minimize the mean squared error. after that time the  error no longer decreased noticeably. momentum was set  to  .   while the learning rate was optimized separately for  every method  but kept the same for all network sizes.  a learning rate of          was found to be optimal for       taken from the beginning of the first track many rista of album  musica deposita by cuprum    rnn and cw rnn while for lstm          gave better  results. for lstm it was also crucial to initialize the bias of  the forget gates to a high value    in this case  to encourage  the long term memory. the hidden units of cw rnn were  divided into nine equally sized groups with exponential  clock timings           . . .       .  the results for the experiments are shown in figure  . it is  obvious that rnns fail to generate the target sequence  and  they do not seem to improve with network size. lstm does  much better  and shows an improvement as the networks  get bigger. cw rnns give by far the best results  with the  smallest one being roughly on par with the second biggest  lstm network. also  all but the smallest cw rnn have  significantly less variance than all the other methods. to get  an intuitive understanding of what is happening  figure    shows the output of the best network of each type on each  one of the five audio samples. the average error of the best  networks is summarized in table    row   .   . . spoken word classification  the second task is sequence classification instead of generation. each sequence contains an audio signal of one  spoken word from the timit speech recognition benchmark  garofolo et al.       . the dataset contains    dif      a clockwork rnn  table  . number of hidden neurons  cells in the case of lstm   for rnn  lstm and cw rnn for each network size specified  in terms of the number of parameters  weights  for the sequence  generation task.    table  . number of hidden neurons  cells in the case of lstm   for rnn  lstm and cw rnn for each network size specified in  terms of the number of parameters  weights  for the spoken word  classification task.      of parameters    rnn    lstm    cw rnn      of parameters    rnn    lstm    cw rnn                                                                                                                                                                                ferent words  classes  arranged in   clusters based on their  suffix. because of the suffix similarity the network needs to  learn long term dependencies in order to disambiguate the  words. the words are   cluster    making  walking  cooking  looking   working    cluster    biblical  cyclical  technical   classical  critical    table  . mean error and standard deviation  averaged over      runs  for the largest  best  lstm  rnn and cw rnn on both  tasks. cw rnn is  .   better than lstm on task  .   sequence  generation  and more than    better than lstm on task  .    spoken word classification.    task    rnn     .  nmse   .    .     .  error       .   .     lstm    cw rnn     .    .      .   .      .     .       .   .     cluster    tradition  addition  audition   recognition  competition    cluster    musicians  discussions   regulations  accusations  conditions    cluster    subway  leeway  freeway  highway   hallway    for every word there are   examples from different speakers   which were partitioned into   for training and   for testing  for a total of     sequences      train     test . each  sequence element consists of    dimensional mfcc vector  mermelstein        plus energy  sampled every    ms  over a    ms window with a pre emphasis coefficient of   .  . each of the    channels was then normalized to have  zero mean and unit variance over the whole training set.  all network types used the same architecture     inputs  a  single hidden and a softmax output layer with    units. five  hidden layer sizes were chosen such that the total number of  parameters for the whole network is roughly  . k   k   . k    k  and   k.  all networks used a learning rate of            a momentum  of  .   and were trained to minimize the multinomial cross  entropy error. every experiment was repeated     times  with different random initializations.  because the dataset is so small  gaussian noise with a standard deviation of  .  was added to the inputs during training  to guard against overfitting. training was stopped once the  error on the noise free training set did not decrease for    epochs. to obtain good results with lstm  it was again    important to initialize the forget gate bias to  . for the cwrnn the neurons were divided evenly into   groups with  exponentially increasing periods                          .  figure   shows the classification error of the different networks on the word classification task. here again  rnns  perform the worst  followed by lstms  which give substantially better results  especially with more parameters.  cw rnns beat both rnn and lstm networks by a considerable margin of       on average irrespective of the  number of parameters. the error of the largest networks is  summarized in table    row   .     . discussion  the experimental results show that the simple mechanism  of running subsets of neurons at different speeds allows an  rnn to efficiently learn the different dynamic time scales  inherent in complex signals.  other functions could be used to set the module periods   linear  fibonacci  logarithmic series  or even fixed random  periods. these were not considered in this paper because the  intuitive setup of using an exponential series worked well  in these preliminary experiments. another option would  be to learn the periods as well  which  to use error backpropagation would require a differentiable modulo function  for triggering the clocks. alternatively  one could train the  clocks  together with the weights  using evolutionary algorithms which do not require a closed form for the gradient.     a clockwork rnn    note that the lowest period in the network can be greater  than  . such a network would not be able to change its  output at every time step  which may be useful as a low pass  filter when the data contains noise.  also  the modules do not have to be all of the same size. one  could adjust them according to the expected information in  the input sequences  by e.g. using frequency analysis of the  data and setting up modules sizes and clocks proportional  to the spectrum.  grouping hidden neurons into modules is a partway to having each weight have its own clock. initial experiments  not  included in this paper  have shown that such networks are  hard to train and do not provide good results.  cw rnn showed superior performance on the speech data  classification among all three models tested. note that   unlike in the standard approach  in which the speech signal  frequency coefficients are first translated to phonemes which  are modeled with a standard approach like hidden markov  modes for complete words  cw rnn attempts to model  and recognize the complete words directly  where it benefits  from the modules running at multiple speeds.    for exponentially scaled periods  ti    i   the upper bound  for number of operations  oh   needed for wh per time  step is     oh   k         g    x  g i  i       i     k         g    g    x    x i     g   i i    i  i       z      z                               k   g         nk   because g     this is less than or equal to n  . recurrent  operations in cw rnn are faster than in an rnn with the  same number of neurons by a factor of at least g    which   for typical cw rnn sizes ends up being between   and   . similarly  upper bound for the number of input weight  evaluations  ei   is     oi         g    x  km  i      ti         km    g    x     t  i   i          km    therefore  the overall cw rnn speed up w.r.t rnn is   k   g     kgm   kg  n    nm   n          or   oi    n  k   g         km    kg  g kg   m       g  kg   m       g             k g        m   g     k g        m   g         z       future work will start by conducting a detailed analysis of  the internal dynamics taking place in the cw rnn to understand how the network is allocating resources for a given  type of input sequence. further testing on other classes of  problems  such as reinforcement learning  and comparison  to the larger set of connectionist models for sequential data  processing are also planned.    note that this is a conservative lower bound.    appendix    acknowledgments    cw rnn has fewer total parameters and even fewer operations per time step than a standard rnn with the same  number of neurons. assume cw rnn consists of g modules of size k for a total of n   kg neurons. because a  neuron is only connected to other neurons with the same or  larger period  the number of parameters nh for the recurrent matrix is     this research was supported by swiss national science  foundation grant           theory and practice of  reinforcement learning     and the eu fp  project   nanobiotouch   grant        .    nh      g x  k  x    k g   i        k      i   j      g    x     g   i       i      n  nk     .                  