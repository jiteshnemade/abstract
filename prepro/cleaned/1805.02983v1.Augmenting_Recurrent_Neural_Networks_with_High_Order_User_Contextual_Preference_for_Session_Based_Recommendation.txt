introduction    making recommendations based on session logs of user item interactions has been a major challenge for the recommender systems  community. utilizing session logs for recommendations has obvious advantages     they can be used to infer user preferences to  make recommendations to anonymous fresh users  and    they can  provide a more personalized recommendation that matches a user s  current interest.  since the pioneering work of hidasi et al.     recurrent neural networks  rnns  have been the de facto choice for modeling  session based recommender systems  as they are capable of effectively exploiting the sequential nature of user session data. inspired  by this success  numerous rnn based session models have been  proposed to accommodate diverse aspects of session based recommender systems e.g.  by incorporating item content features      modeling latent user intent      or merging information from past  and current user sessions        .  another interesting application of rnns has been their usage in  context aware sequential recommendations         which can be  also applied to session based recommendation settings. however  a  majority of the research in this line focused on exploiting dynamic  temporal contexts e.g.  time of the day   and there have been few    corresponding      related works   .  session based   context aware  recommendation with rnns  traditional session based recommendation algorithms have largely  been based on item to item approaches       . however  the recent  success of deep learning has led to the adoption of recurrent neural networks  rnns  to session based recommendations  starting  from gru rec   . the motivation of gru rec was to mitigate  the consistently cold starting problem that occurs frequently in  a typical e commerce environment  where most of the customers  do not log in and thus are anonymous. after gru rec  several  rnn based models were introduced to consider various aspects of  session based recommendations                .  rnns also started to gain momentum in the field of contextaware sequential modeling. some approaches used temporal contexts like time of the day or time difference between the previous  and current interaction       . there was also an attempt to use the    author         .     type of interaction events as a contextual information    . however   all those contexts were dynamic contexts dependent on specific  transaction instances  and no models have been introduced that  can equip the powerful rnn session models with abundant static  user side contexts like age  location  or current job title.      .     model overview    the proposed model  which we call the augmented rnn  arnn   model  augments an rnn session model with a pnn context encoder.  at the training stage  a pnn context encoder and an rnn session  model are first pretrained separately to optimize the likelihood       where the negative items are defined using the items in the  session parallel mini batches as in gru rec   . then  a merging  layer is trained on top of the pretrained pnn and rnn to integrate  the information from both networks. after training  the arnn  scores the items conditioning on both the pnn produced contextual preference c t and the rnn hidden state ht . the details of the  pnn rnn components and the merging network are explained in  the following sections.    pnn and fm    in terms of context aware recommendations  the factorization machine  fm      and its neural network  nn  variants        have  been widely successful  particularly in applications such as clickthrough rate  ctr  prediction. the fm and its variants build the latent embeddings for the categorical contexts in order to estimate the   nd order interactions between categorical features  thereby aiming to resolve the data sparsity inherent in recommender systems.  among the nn variants of fm  the product based neural network   pnn      has been known to effectively capture the higher order  interactions between the categorical variables by applying pairwise  product operations for each possible pair of embedded categorical  fields and passing the resulting vector as the input to a nn.     .     pnn context encoder    the foundational element of the arnn is the pnn context encoder  that models user contextual preference by capturing high order  interactions between user contexts and previous items. similar to  the original pnn      the pnn context encoder is composed of three  layers  embedding layer  product layer  and fc layer  as shown in  figure  .      model description   .  problem setup  the goal of our work is to capture the contribution of user context  information in determining a user s item navigation path during an  online session. therefore  our dataset can be expressed as follows     r t     softmax    i    session  si     x t   yt   tt       where yt is the item that the user  interacted with at t within the session  and x t is the user  contextual vector.    training set  dt r ain    s     . . .   s   dt r ain        test set  dt est    s      . . .   s   d          ct      ct      fc  layer    t    n       p r  yt     r  yn   y  t   x t         ct      contextual  preference    zt      zt      zt n         pt    pt      pt      flatten  embedding  layer    ct h         fully connected    zt    product  layer         relu   batch norm    t es t    notice that x t is included in every user session. x t   a user contextual  vector  is a concatenation of the one hot encodings for each categorical fields  as in rendle    . for instance  if we have the context information  gender female location u.s.  about a user  then each  field are first encoded separately as                      . . .      and concatenated to produce the final input x t                      . . .     . the  objective of the arnn is  given a new sequence of  context item   pairs in a user session su      x     y     . . .    x t      yt       included  in a test set  to predict the next item yt with which the user will  interact.  given the dataset  our problem reduces to a sequential ranking  task with implicit feedback  where the input consists of the item  indices yt  s and user contexts x t  s  and the output consists of the  ranking scores for the positive items r  yt   and negative items r  yn  .  now  following the approach by rendle et al.     and introducing  the time independence assumption for user contexts  we can set up  the following joint likelihood for a single session as the objective  function   l       item scores         pt m    pairwise product    ft      ft           ft n    ft      field embeddings    embedding   xt     yt                                                field   field      previous item    figure    architecture of the pnn context encoder.  first  x t     as explained in section  .   and the previous item  yt    are concatenated and enters the embedding layer  producing  the embeddings for each fields including the previous item . as the  original pnn  only one position remains active per field.  next  pairwise inner product operations are applied to each possible pair of field embeddings to produce the pairwise signal pt . at  the same time  all field embeddings are flattened and concatenated  to produce the linear signal zt .  then  pt and zt are concatenated and fed as an input to the fc  layer  followed by a rectified linear unit  relu    batch normalization    . from this nonlinear transformation of both linear pairwise           t    n      where y  t    yt      . . .   y   .    for    convenience  we ll use the term  user context  to refer to  static user context   throughout this paper.        combined and used to calculate the item scores as follows     signals  the pnn can effectively model high order interactions between the user contexts and the previous item  obtaining a vector  encoding the user contextual preference.  finally  after the batch normalization layer  the user contextual  preference c t goes through the softmax layer that calculates the  final item scores. the item scores are then used to calculate the  ranking loss and to pretrain the pnn  as mentioned in section  . .     .     r  yt  y  t   x t        so f tmax m  c t   ht      c t   pn n  x t      yt      ht   gru  yt      ht       in our model  m is a simple fc layer followed by a relu   batch  normalization layer.      experiments   .  dataset    rnn session model    for simplicity  rnn session models are set to follow the simple gated  recurrent unit  gru     architecture proposed in gru rec   .  more specifically  let yt be the one hot encoded item for the t th  transaction and ht be the corresponding gru hidden state. the  scores for the next items are then calculated by the gru as follows     to test our algorithm  we used two datasets  the xing dataset from  the recsys      challenge and the tmall dataset from the ijcai   competition. table   presents the profiles of the two datasets.  the xing dataset contains the user item interaction logs from  xing.com  a social networking service specialized for job searching.  rich user side categorical contexts such as job roles and career  levels are included in the xing dataset. among those contexts   we picked only    attributes  e.g.  job roles  career level  and country region  that are likely to be useful for predicting job preferences.  by contrast  the tmall dataset is a standard e commerce dataset in  which only three types of user side categorical information  user id   age  and gender  are available. we thus used all three attributes. the  purpose of employing the tmall dataset was to examine how the  degree of abundance in user side contexts impact the performance  of the proposed context augmented model.    r  yt  y  t     so f tmax ht   ht   gru  yt      ht       similar to section  .   the item scores are then used to pretrain the  gru for the session based ranking task. however  our model does  not restrict the rnn session model to take the proposed simple  gru form  so any existing rnn session models can be used instead  of the gru.     .     augmenting the rnn with the pnn    r t     table    dataset statistics.      item scores    relu   batch norm   softmax  merging  layer      of users    of items    of sessions    of transactions    of user side categorical contexts used    fc layer  concat  ct    intermediate  layer    input  layer    xt      tmall       k   m   . m   . m           k   . m   . m    . m       ht     .   pnn      user context    xing    rnn    yt      preprocessing     . .  train test split. for both datasets  we extracted the last  three days for testing purposes and trained on the preceding     days. transactions including the items not in the training set were  ignored as in hidasi et al.   .      previous item    figure    overall architecture of the arnn model.     . .  session id marking. since session ids were missing in  both xing and tmall  we manually set time thresholds for marking a sequence of transactions as a session. the threshold was    hour for xing  and   day for tmall  as in jannach et al.    .    figure   shows the overall architecture of the arnn which  consists of three types of layers  the input layer  the intermediate  layer  and the merging layer. the input layer is simply where the  model receives its input data. the intermediate layer consists of the  pnn rnn that we pretrained as decribed in sections  .  and  .  .  to use the pretrained pnn rnn as feature extractors  we remove  the final softmax layers from both the pnn rnn  and freeze the  parameters so that they are not further updated  . the merging layer  m is where the pnn output c t and the rnn hidden state ht are     . .  sampling on the items. due to scalability issues  only the  top items that covered     of the transactions were selected for  modeling. in other words  the items were sorted by popularity  and a minimum item popularity threshold was set such that the  transactions including only the items above the threshold accounted  for     of the entire dataset. after this sampling  we were left with         items for xing and        items for tmall.   . .  encoding the categorical attributes. all categorical features  were encoded as binary features that indicated the presence of the  corresponding features. for xing  there were multi valued categorical features that held numerous values  e.g.  jobroles            however     we found that retraining only the batch normalization layers for the pnn  slightly increases the model performance.             . making binary features for all the multi valued features resulted in prohibitively high dimensional input to the pnn  which  incurred large computational cost. thus  similar to the process we  described in section  . .   we created binary features for only the  most popular categories that covered     of the transactions. the  least popular     of the features were mapped to a single  unknown  attribute. after this encoding  the total number of input  fields required for the pnn was     for xing and   for tmall   including the field kept for embedding the previous items.     .      . .  baselines. we compared the arnn with the following  baselines     item knn     a simple yet powerful item to item approach  that recommends items that are similar to the previous item  based on cosine similarity.    gru rec     a widely used rnn based session model  as  described in section  . . unlike arnn  gru rec uses only  the information from the item ids.    pnn      a pnn context encoder  which is included to assess  how well it captures high order context information relevant  for ranking.    optimization and hyperparameters     . .  loss function. the top  loss    was used as a ranking  loss to maximize the objective    . let ns be the number of negative  samples in a session parallel mini batch. in addition  let r s i be the  calculated score for the i th item in the session s. then  the top   loss is defined as   ns               top s i        r s  j   r s i         r s      j  ns j           table   shows the recall    and mrr    of the arnn and the  baselines for the two datasets.  table    evaluation results.  xing    this loss function is intended to push the scores of the negative  items to zero  which prevents the overall item scores from exploding  during optimization.    itemknn  gru rec  pnn  arnn     . .  hyperparameters and optimizer. here  we provide information about the core hyperparameters and optimizer. the hyperparameters for the gru rec follows that of quadrana et al.      for xing  and jannach et al.    for tmall.    as an optimizer  we used adagrad    with different learning rates  and weight decays for each dataset and model. it turned out that  finding the right optimizer hyperparameters was crucial to the  performance of the model.        evaluation    nhit s  nr ecs    mrr k           recall       mrr       recall       mrr        .       .       .       .         .       .       .       .         .       .       .       .         .       .       .       .        discussion and conclusions    in this study  we proposed an augmented rnn model that can  easily boost an existing rnn session model by estimating highorder user contextual preference using the pnn. since pnn context  encoders can handle arbitrary user side contextual information and  build upon an existing pretrained rnn session model  we believe  that deploying our model for real world systems would be a handy  solution that can improve the recommendation quality of a system  without considerable effort.  however  one limitation of the arnn is that it ignores the itemside contexts e.g.  item metadata  item content   unlike the usual  factorization machines or pnns used for ctr prediction. we omitted the item side contexts because our intention was to measure the  effect of incorporating pure user side contexts e.g.  age  location   login platform  with existing rnn session models using a fm like     . .  measures. we employed two ranking measures to evaluate the model performance  recall k and mrr k. let nhit s  be the number of times that a user chose an item from the recommendation list and nr ecs be the number of top k recommendation  attempts. then  the measures can be calculated as follows   recall k      tmall    evaluation of our model on xing provided us with a clear evidence that capturing user contextual preference using a pnn is  indeed helpful for rnn session models when rich user side context  is available. although the performance of the pnn context encoder  in itself was poor compared to gru rec  arnn as a whole outperformed both itemknn and gru rec in both the recall     and mrr    measures.  however  the performance of the arnn was only marginally better than the gru rec baseline in case of tmall. considering that  only three user context variables were available for tmall user id   age  gender   our hypothesis is that the number of user context fields  was not enough to augment new piece of information that could  not be inferred by the gru rec. thus  we recommend that arnn  should be used only when the number of user side categorical fields  is sufficiently large.      gru rec    gru hidden size       xing         tmall     dropout   .   xing   none  tmall     pnn    field embedding dimension      xing  tmall     fc layer hidden size       xing        tmall     dropout  none  xing  tmall     arnn    merging layer hidden size       xing         tmall     dropout  none  xing  tmall      .     results    n   r ecs       nr ecs n   rank n     where rank n  denotes the rank of the chosen item within the n th  top k recommendation list.    for    more details  please visit our github repository. the repository will go public  after the review process.        approach. therefore  designing an arnn architecture that can also  handle item side contexts would be an interesting research topic  for a future work.    