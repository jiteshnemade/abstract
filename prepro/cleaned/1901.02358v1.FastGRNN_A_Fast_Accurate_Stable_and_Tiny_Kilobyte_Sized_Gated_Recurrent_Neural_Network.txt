introduction  objective  this paper develops the fastgrnn  an acronym for a fast  accurate  stable and tiny  gated recurrent neural network  algorithm to address the twin rnn limitations of inaccurate training and inefficient prediction. fastgrnn almost matches the accuracies and training times of stateof the art unitary and gated rnns but has significantly lower prediction costs with models ranging  from   to   kilobytes for real world applications.  rnn training and prediction  it is well recognized that rnn training is inaccurate and unstable  as non unitary hidden state transition matrices could lead to exploding and vanishing gradients for  long input sequences and time series. an equally important concern for resource constrained and  real time applications is the rnn s model size and prediction time. squeezing the rnn model and  code into a few kilobytes could allow rnns to be deployed on billions of internet of things  iot   endpoints having just   kb ram and    kb flash memory         . similarly  squeezing the rnn  model and code into a few kilobytes of the    kb l  cache of a raspberry pi or smartphone  could  significantly reduce the prediction time and energy consumption and make rnns feasible for real  nd conference on neural information processing systems  neurips        montr al  canada.     time applications such as wake word detection                       predictive maintenance           human activity recognition         etc.  unitary and gated rnns  a number of techniques have been proposed to stabilize rnn training  based on improved optimization algorithms           unitary rnns                             and  gated rnns             . while such approaches have increased the rnn prediction accuracy they  have also significantly increased the model size. unitary rnns have avoided gradients exploding  and vanishing by limiting the range of the singular values of the hidden state transition matrix. this  has led to only limited gains in prediction accuracy as the optimal transition matrix might often not  be close to unitary. unitary rnns have compensated by learning higher dimensional representations  but  unfortunately  this has led to larger model sizes. gated rnns              have stabilized  training by adding extra parameters leading to state of the art prediction accuracies but with models  that might sometimes be even larger than unitary rnns.  fastrnn  this paper demonstrates that standard rnn training could be stabilized with the addition  of a residual connection                 having just   additional scalar parameters. residual connections for rnns have been proposed in      and further studied in    . this paper proposes the  fastrnn architecture and establishes that a simple variant of         with learnt weighted residual  connections     can lead to provably stable training and near state of the art prediction accuracies  with lower prediction costs than all unitary and gated rnns. in particular  fastrnn s prediction  accuracies could be   a  up to     higher than a standard rnn   b  could often surpass the accuracies of all unitary rnns and  c  could be just shy of the accuracies of leading gated rnns.  fastrnn s empirical performance could be understood on the basis of theorems proving that for an  input sequence with t steps and appropriate setting of residual connection weights   a  fastrnn  converges to a stationary point within o        sgd iterations  see theorem  .    independent of  t   while the same analysis for a standard rnn reveals an upper bound of o  t   iterations and  b   fastrnn s generalization error bound is independent of t whereas the same proof technique reveals  an exponential bound for standard rnns.  fastgrnn  inspired by this analysis  this paper develops the novel fastgrnn architecture by converting the residual connection to a gate while reusing the rnn matrices. this allowed fastgrnn to  match  and sometimes exceed  state of the art prediction accuracies of lstm  gru  ugrnn and  other leading gated rnn techniques while having    x fewer parameters. enforcing fastgrnn s  matrices to be low rank  sparse and quantized led to a minor decrease in the prediction accuracy but  resulted in models that could be up to   x smaller and fit in     kilobytes for many applications. for  instance  using a   kb model  fastgrnn could match the prediction accuracies of all other rnns  at the task of recognizing the  hey cortana  wakeword. this allowed fastgrnn to be deployed  on iot endpoints  such as the arduino uno  which were too small to hold other rnn models. on  slightly larger endpoints  such as the arduino mkr     or due  fastgrnn was found to be      x  faster at making predictions than other leading rnn methods.  contributions  this paper makes two contributions. first  it rigorously studies the residual connection based fastrnn architecture which could often outperform unitary rnns in terms of training  time  prediction accuracy and prediction cost. second  inspired by fastrnn  it develops the fastgrnn architecture which could almost match state of the art accuracies and training times but with  prediction costs that could be lower by an order of magnitude. fastrnn and fastgrnn s code can  be downloaded from     .      related work  residual connections  residual connections have been studied extensively in cnns          as  well as rnns        . the leaky integration unit architecture      proposed residual connections  for rnns but were unable to learn the state transition matrix due to the problem of exploding and  vanishing gradients. they therefore sampled the state transition matrix from a hand crafted distribution with spectral radius less than one. this limitation was addressed in     where the state  transition matrix was learnt but the residual connections were applied to only a few hidden units and  with randomly sampled weights. unfortunately  the distribution from which the weights were sampled could lead to an ill conditioned optimization problem. in contrast  the fastrnn architecture  leads to provably stable training with just two learnt weights connected to all the hidden units.             ht      u    u            xt    ht    ht      ht         w  xt     a  fastrnn   residual connection    w    f zt          zt       tanh     b  fastgrnn   gate    figure    block diagrams for fastrnn  a  and fastgrnn  b . fastgrnn uses shared matrices w   u to compute both the hidden state ht as well as the gate zt .  unitary rnns  unitary rnns                         stabilize rnn training by learning only wellconditioned state transition matrices. this limits their expressive power and prediction accuracy  while increasing training time. for instance  spectralrnn      learns a transition matrix with singular values in      . unfortunately  the training algorithm converged only for small   thereby limiting  accuracy on most datasets. increasing the number of hidden units was found to increase accuracy  somewhat but at the cost of increased training time  prediction time and model size.  gated rnns  gated architectures                  achieve state of the art classification accuracies  by adding extra parameters but also increase model size and prediction time. this has resulted in a  trend to reduce the number of gates and parameters with ugrnn      simplifying gru      which  in turn simplifies lstm     . fastgrnn can be seen as a natural simplification of ugrnn where  the rnn matrices are reused within the gate and are made low rank  sparse and quantized so as to  compress the model.  efficient training and prediction  efficient prediction algorithms have often been obtained by  making sparsity and low rank assumptions. most unitary methods effectively utilize a low rank  representation of the state transition matrix to control prediction and training complexity         .  sparsity  low rank  and quantization were shown to be effective in rnns               cnns        trees      and nearest neighbour classifiers     . fastgrnn builds on these ideas to utilize low rank   sparse and quantized representations for learning kilobyte sized classifiers without compromising  on classification accuracy. other approaches to speed up rnn training and prediction are based on  replacing sequential hidden state transitions by parallelizable convolutions     or on learning skip  connections      so as to avoid evaluating all the hidden states. such techniques are complementary  to the ones proposed in this paper and can be used to further improve fastgrnn s performance.      fastrnn and fastgrnn  notation  throughout the paper  parameters of an rnn are denoted by matrices w   rd  d   u    rd  d  and bias vectors b   rd    often using subscripts if multiple vectors are required to specify  the architecture. a   b denotes the hadamard product between a and b  i.e.   a   b i   ai   bi . k   k   denotes the number of non zeros entries in a matrix or vector. k   kf   k   k  denotes the frobenius  and spectral  p norm of a matrix  respectively. unless specified  k   k denotes k   k  of a matrix or vector.  a  b   i ai bi denotes the inner product of a and b.  standard rnn architecture      is known to be unstable for training due to exploding or vanishing  gradients and hence is shunned for more expensive gated architectures.    this paper studies the fastrnn architecture that is inspired by weighted residual connections            and shows that fastrnn can be significantly more stable and accurate than the standard rnn  while preserving its prediction complexity. in particular  section  . .  demonstrates parameter settings for fastrnn that guarantee well conditioned gradients as well as faster convergence rate and  smaller generalization error than the standard rnn. this paper further strengthens fastrnn to develop the fastgrnn architecture that is more accurate than unitary methods         and provides  comparable accuracy to the state of the art gated rnns at   x less computational cost  see table   .         .  fastrnn  let x    x    . . .   xt   be the input data where xt   rd denotes the t th step feature vector. then   the goal of multi class rnns is to learn a function f   rd t       . . .   l  that predicts one of l  classes for the given data point x. standard rnn architecture has a provision to produce an output  at every time step  but we focus on the setting where each data point is associated with a single label  that is predicted at the end of the time horizon t . standard rnn maintains a vector of hidden state  ht   rd  which captures temporal dynamics in the input data  i.e.   ht   tanh wxt   uht     b .           as explained in the next section  learning u  w in the above architecture is difficult as the gradient  can have exponentially large  in t   condition number. unitary methods explicitly control the condition number of the gradient but their training time can be significantly larger or the generated model  can be less accurate.  instead  fastrnn uses a simple weighted residual connection to stabilize the training by generating  well conditioned gradients. in particular  fastrnn updates the hidden state ht as follows   h t     wxt   uht     b    ht    h t    ht               where              are trainable weights that are parameterized by the sigmoid function.     r    r is a non linear function such as tanh  sigmoid  or relu  and can vary across datasets. given ht    the label for a given point x is predicted by applying a standard classifier  e.g.  logistic regression  to ht .  typically        and            especially for problems with larger t . fastrnn updates hidden  state in a controlled manner with      limiting the extent to which the current feature vector xt  updates the hidden state. also  fastrnn has only   more parameters than rnn and require only  d  more computations  which is a tiny fraction of per step computation complexity of rnn. unlike  unitary methods              fastrnn does not introduce expensive structural constraints on u and  hence scales well to large datasets with standard optimization techniques     .   . .  analysis  this section shows how fastrnn addresses the issue of ill conditioned gradients  leading to stable  training and smaller generalization error. for simplicity  assume that the label decision function is  one dimensional and is given by f  x    v  ht . let l x  y       l f  x   y     be the logistic  loss function for the given labeled data point  x  y  and with parameters      w  u  v . then  the  gradient of l w.r.t. w  u  v is given by   t  x   l  dt       u  t      t  x   l      dt   w  t      ty                              u dk      i    ht l h   t        k t    ty          u dk      i    ht l x   t      k t            y exp   y   v  ht     l     ht     v      exp   y   v  ht                where  ht l    c      y   v  and c        exp  y v    h   . a critical term in the above expression is   t  qt        m  u    k t   u dk      i   whose condition number   m u    is bounded by      m u                                     maxk ku  dk   k t  t  maxk ku  dk   k t  t                where dk   diag      wxk   uhk     b   is the jacobian matrix of the pointwise nonlinearity.  also if       and        which corresponds to standard rnn  the condition number of m  u  can  ku  d    k    t  t  be as large as  maxk  min  u k    where  min  a  denotes the minimum singular value of a.  dk        hence  gradient s condition number for the standard rnn can be exponential in t . this implies that   relative to the average eigenvalue  the gradient can explode or vanish in certain directions  leading  to unstable training.          in contrast to the standard rnn  if       and        then the condition number   m u    for     fastrnn is bounded by a small term. for example  if           and     t maxk ku      d  k   k  then  m u    o   . existing unitary methods are also motivated by similar observation. but they  attempt to control the  m u  by restricting the condition number   u   of u which can still lead  to ill conditioned gradients as u  dk   might still be very small in certain directions. by using  residual connections  fastrnn is able to address this issue  and hence have faster training and more  accurate model than the state of the art unitary rnns.  finally  by using the above observations and a careful perturbation analysis  we can provide the  following convergence and generalization error bounds for fastrnn   theorem  .   convergence bound .  let   x    y     . . .    xn   yn    be the given labeled sequential  p  training data. let l      n  i l xi   yi      be the loss function with      w  u  v  be the  parameters of fastrnn architecture     with           and   such that                               min   t    dkuk        t   ru t    kuk        where d   sup  k kd k k  . then  randomized stochastic gradient descent       a minor variation  of sgd  when applied to the data for a maximum of m iteration outputs a solution  b such that         o  t  l        rw ru rv o  t       b     e k   l   k  k    bm       d           m  d   m    where rx   maxx kxkf for x    u  w  v   l      isnthe loss of theoinitial classifier  and  d         the step size of the k th sgd iteration is fixed as   k   min o  t      t m   k    m    d     .    maximum number of iterations is bounded by m   o   t       poly l       rw ru rv   d          .    theorem  .   generalization error bound .     let y  y           and let ft denote the class  of fastrnn with kukf   ru   kwkf   rw . let the final classifier be given by   v  ht     kvk    rv . let l   y   y        b  be any   lipschitz loss function. let d be any distribution on  x   y such that kxit k    rx a.s. let          . for all           and   such that                      min            t    dkuk        t   ru t    kuk        where d   sup  k kd k k    we have that with probability at least        all functions f   v   ft  satisfy   s  n  x  ln        o  t       l f  xi    yi     c    ed  l f  x   y          b  n i    n  n  where c   rw ru rx rv represents the boundedness of the parameter matrices and the data.    the convergence bound states that if     o   t   then the algorithm converges to a stationary  point in constant time with respect to t and polynomial time with respect to all the other problem  parameters. generalization bound states that for     o   t    the generalization error of fastrnn  is independent of t . in contrast  similar proof technique provide exponentially poor  in t   error  bound and convergence rate for standard rnn. but  this is an upper bound  so potentially significantly better error bounds for rnn might exist  matching lower bound results for standard rnn  is an interesting research direction. also  o t     generalization error bound can be argued using  vc dimension style arguments    . but such bounds hold for specific settings like binary y  and are  independent of problem hardness parameterized by the size of the weight matrices  rw   ru  .  finally  note that the above analysis fixes     o   t               but in practice fastrnn  learns       which is similar to performing cross validation on      . however  interestingly  across  datasets the learnt      values indeed display a similar scaling wrt t for large t  see figure   .   .  fastgrnn  while fastrnn controls the condition number of gradient reasonably well  its expressive power  might be limited for some datasets. this concern is addressed by a novel architecture  fastgrnn         that uses a scalar weighted residual connection for each and every coordinate of the hidden state ht .  that is   zt     wxt   uht     bz     h t   tanh wxt   uht     bh     ht          zt          h t   zt   ht               where              are trainable parameters that are parameterized by the sigmoid function  and      r   r is a non linear function such as tanh  sigmoid and can vary across datasets. note that  each coordinate of zt is similar to parameter   in     and       zt       s coordinates simulate    parameter  also if              then it satisfies the intuition that          . it was observed that  across all datasets  this gating mechanism outperformed the simple vector extension of fastrnn  where each coordinate of   and   is learnt  see appendix g .  fastgrnn computes each coordinate of gate zt using a non linear function of xt and ht   . to  minimize the number of parameters  fastgrnn reuses the matrices w  u for the vector valued  gating function as well. hence  fastgrnn s inference complexity is almost same as that of the  standard rnn but its accuracy and training stability is on par with expensive gated architectures like  gru and lstm.  sparse low rank representation  fastgrnn further compresses the model size by using a lowrank and a sparse representation of the parameter matrices w  u. that is   w   w   w       u   u   u       kwi k    siw   kui k    siu   i                     where w    rd  rw   w    rd rw   and u    u    rd  ru . hyperparameters rw   sw   ru   su  provide an efficient way to control the accuracy memory trade off for fastgrnn and are typically  set via fine grained validation. in particular  such compression is critical for fastgrnn model to fit  on resource constrained devices. second  this low rank representation brings down the prediction  time by reducing the cost at each time step from o d  d   d    to o rw  d   d     ru d  . this  enables fastgrnn to provide on device prediction in real time on battery constrained devices.   . .  training fastgrnn  the parameters for fastgrnn   fastgrnn    wi   ui   bh   bz         are trained jointly using projected batch stochastic gradient descent  b sgd   or other stochastic optimization methods  with  typical batch sizes ranging from         . in particular  the optimization problem is given by   min     fastgrnn  kwi k   siw  kui k   siu  i          j   fastgrnn         x  l xj   yj    fastgrnn    n j           where l denotes the appropriate loss function  typically softmax cross entropy . the training procedure for fastgrnn is divided into   stages    i  learning low rank representation  l   in the first stage of the training  fastgrnn is trained  for e  epochs with the model as specified by     using b sgd. this stage of optimization ignores  the sparsity constraints on the parameters and learns a low rank representation of the parameters.   ii  learning sparsity structure  s   fastgrnn is next trained for e  epochs using b sgd  projecting the parameters onto the space of sparse low rank matrices after every few batches while maintaining support between two consecutive projection steps. this stage  using b sgd with iterative  hard thresholding  iht   helps fastgrnn identify the correct support for parameters  wi   ui  .   iii  optimizing with fixed parameter support  in the last stage  fastgrnn is trained for e   epochs with b sgd while freezing the support set of the parameters.  in practice  it is observed that e    e    e        generally leads to the convergence of fastgrnn  to a good solution. early stopping is often deployed in stages  ii  and  iii  to obtain the best models.   .  byte quantization  q   fastgrnn further compresses the model by quantizing each element of wi   ui   restricting them to  at most one byte along with byte indexing for sparse models. however  simple integer quantization  of wi   ui leads to a large loss in accuracy due to gross approximation. moreover  while such a        quantization reduces the model size  the prediction time can still be large as non linearities will require all the hidden states to be floating point. fastgrnn overcomes these shortcomings by training  wi and ui using piecewise linear approximation of the non linear functions  thereby ensuring that  all the computations can be performed with integer arithmetic. during training  fastgrnn replaces  the non linear function in     with their respective approximations and uses the above mentioned  training procedure to obtain  fastgrnn . the floating point parameters are then jointly quantized  to ensure that all the relevant entities are integer valued and the entire inference computation can  be executed efficiently with integer arithmetic without a significant drop in accuracy. for instance   tables      show that on several datasets fastgrnn models are    x faster than their corresponding  fastgrnn q models on common iot boards with no floating point unit  fpu . fastgrnn lsq   fastgrnn  minus  the low rank  sparse and quantized components  is the base model with no  compression.      experiments  datasets  fastrnn and fastgrnn s performance was benchmarked on the following iot tasks  where having low model sizes and prediction times was critical to the success of the application    a  wakeword          detecting utterances of the  hey cortana  wakeword   b  google          and google      detection of utterances of    and    commands plus background noise and silence  and  c  har       and dsa          human activity recognition  har  from an accelerometer  and gyroscope on a samsung galaxy s  smartphone and daily and sports activity  dsa  detection  from a resource constrained iot wearable device with   xsens mtx sensors having accelerometers   gyroscopes and magnetometers on the torso and four limbs. traditional rnn tasks typically do  not have prediction constraints and are therefore not the focus of this paper. nevertheless  for the  sake of completeness  experiments were also carried out on benchmark rnn tasks such as language  modeling on the penn treebank  ptb  dataset       star rating prediction on a scale of   to   of yelp  reviews      and classification of mnist images on a pixel by pixel sequence         .  all datasets  apart from wakeword    are publicly available and their pre processing and feature  extraction details are provided in appendix b. the publicly provided training set for each dataset  was subdivided into     for training and     for validation. once the hyperparameters had been  fixed  the algorithms were trained on the full training set and results were reported on the publicly  available test set. table   lists the statistics of all datasets.  baseline algorithms and implementation  fastrnn and fastgrnn were compared to standard rnn       leading unitary rnn approaches such as spectralrnn       orthogonal rnn   ornn        efficient unitary recurrent neural networks  eurnn        factoredrnn      and  state of the art gated rnns including ugrnn       gru      and lstm     . details of these  methods are provided in section  . native tensorflow implementations were used for the lstm  and gru architectures. for all the other rnns  publicly available implementations provided by the  authors were used taking care to ensure that published results could be reproduced thereby verifying  the code and hyper parameter settings. all experiments were run on an nvidia tesla p   gpu with  cuda  .  and cudnn  .  on a machine with an intel xeon  .   ghz cpu with    cores.  hyper parameters  the hyper parameters of each algorithm were set by a fine grained validation  wherever possible or according to the settings recommended by the authors otherwise. adam  nesterov momentum and sgd were used to optimize each algorithm on each dataset and the optimizer  with the best validation performance was selected. the learning rate was initialized to      for all  architectures except for rnns where the learning rate was initialized to      to ensure stable training. each algorithm was run for     epochs after which the learning rate was decreased by a factor  table    dataset statistics  dataset  google     google     wakeword    yelp    har    pixel mnist     ptb        dsa       table    ptb language modeling     layer     train     features     time  steps     test                                                                                                                                                                                                                                   method  rnn  fastrnn  fastgrnn lsq  fastgrnn  spectralrnn  ugrnn  lstm         test  perplexity    train  perplexity    model  size  kb        .       .        .       .       .       .       .        .       .      .      .      .      .      .                                           train  time  min    .      .      .      .         .      .       of      and the algorithm run again for another     epochs. this procedure was carried out on all  datasets except for pixel mnist where the learning rate was decayed by    after each pass of      epochs. batch sizes between    and     training points were tried for most architectures and a batch  size of     was found to work well in general except for standard rnns which required a batch size  of    . fastrnn used tanh as the non linearity in most cases except for a few  indicated by      where relu gave slightly better results. table    in the appendix lists the non linearity  optimizer  and hyper parameter settings for fastgrnn on all datasets.  evaluation criteria  the emphasis in this paper is on designing rnn architectures which can run  on low memory iot devices and which are efficient at prediction time. as such  the model size of  each architecture is reported along with its training time and classification accuracy  f  score on the  wakeword   dataset and perplexity on the ptb dataset . prediction times on some of the popular  iot boards are also reported. note that  for nlp applications such as ptb and yelp  just the model  size of the various rnn architectures has been reported. in a real application  the size of the learnt  word vector embeddings     mb for fastrnn and fastgrnn  would also have to be considered.  results  tables   and   compare the performance of fastrnn  fastgrnn and fastgrnn lsq  to state of the art rnns. three points are worth noting about fastrnn s performance. first  fastrnn s prediction accuracy gains over a standard rnn ranged from  .    on the pixel mnist  dataset to     on the google    dataset. second  fastrnn s prediction accuracy could surpass  leading unitary rnns on   out of the   datasets with gains up to  .    and  .    over spectralrnn on the google    and dsa    datasets respectively. third  fastrnn s training speedups over  all unitary and gated rnns could range from  . x over ugrnn on the yelp   and dsa    datasets  to    x over eurnn on the google    dataset. this demonstrates that the vanishing and exploding  gradient problem could be overcome by the addition of a simple weighted residual connection to  the standard rnn architecture thereby allowing fastrnn to train efficiently and stablely. this also  demonstrates that the residual connection offers a theoretically principled architecture that can often  result in accuracy gains without limiting the expressive power of the hidden state transition matrix.  tables   and   also demonstrate that fastgrnn lsq could be more accurate and faster to train than  all unitary rnns. furthermore  fastgrnn lsq could match the accuracies and training times of  state of the art gated rnns while having models that could be  .    .  x smaller. this demonstrates that extending the residual connection to a gate which reuses the rnn matrices increased  accuracy with virtually no increase in model size over fastrnn in most cases. in fact  on google   and pixel mnist fastgrnn lsq s model size was lower than fastrnn s as it had a lower  hidden dimension indicating that the gate efficiently increased expressive power.  table    fastgrnn had up to   x smaller models than leading rnns with almost no loss in accuracy  dataset    google     accuracy         google     accuracy         model  size  kb     wakeword    train  time  hr     f   score    model  size  kb     train  time hr     rnn      .             .        .             .        .            .      unitary proposed    train  time  hr     fastrnn  fastgrnn lsq  fastgrnn      .       .      .               .      .     .     .        .       .      .               .       .     .     .        .      .      .                  .     .     .      spectralrnn  eurnn  ornn  factoredrnn      .      .      .      .                               .       .      .     .        .      .      .      .                               .      .      .     .        .      .                             .      .            gated    method    model  size  kb     ugrnn  gru  lstm      .      .      .                       .     .     .        .      .      .                        .     .     .        .      .      .                     .     .     .      dataset    yelp      har      rnn model  size  kb     train  time  hr     dsa       accuracy         model  size  kb     train  time  hr     accuracy         model  size  kb     pixel mnist     train  time  min     accuracy         model  size  kb     train  time  hr     rnn      .              .        .             .        .             .        .              .      unitary proposed    accuracy         fastrnn  fastgrnn lsq  fastgrnn      .      .      .                      .     .     .        .       .      .                    .     .     .        .      .      .                .       .     .     .        .      .      .                      .      .      .      spectralrnn  eurnn  ornn  factoredrnn      .      .                              .      .              .      .      .      .                         .     .     .     .        .         .      .                          .                 .      .      .      .                               .            gated    method    ugrnn  gru  lstm      .      .      .                        .     .     .        .      .      .                     .     .     .        .      .      .                        .     .     .        .      .      .                        .      .      .            table    prediction time in ms on the arduino mkr      google    har   wakeword    method  fastgrnn  fastgrnn q  rnn  ugrnn  spectralrnn                                                                 table    prediction time in ms on the arduino due  google    har   wakeword    method                                   fastgrnn  fastgrnn q  rnn  ugrnn  spectralrnn                                                                                            finally  tables   and   show that fastgrnn s accuracy was at most  .    worse than the best  rnn but its model could be up to   x smaller even as compared to low rank unitary methods such  as spectralrnn. figures   and   in the appendix also show that fastgrnn lsq and fastgrnn s  classification accuracies could be higher than those obtained by the best unitary and gated rnns for  any given model size in the       kb range. this demonstrates the effectiveness of making fastgrnn s parameters low rank  sparse and quantized and allows fastgrnn to fit on the arduino uno  having just   kb ram and    kb flash memory. in particular  fastgrnn was able to recognize  the  hey cortana  wakeword just as accurately as leading rnns but with a   kb model.  prediction on iot boards  unfortunately  most rnns were too large to fit on an arduino uno apart  from fastgrnn. on the slightly more powerful arduino mkr     having an arm cortex m    microcontroller operating at    mhz with    kb ram and     kb flash memory  table   shows  that fastgrnn could achieve the same prediction accuracy while being      x faster at prediction  than ugrnn and       x faster than spectralrnn. results on the even more powerful arduino  due are presented in table   while results on the raspberry pi are presented in table    of the  appendix.  ablations  extensions and parameter settings  enforcing that fastgrnn s matrices be low rank  led to a slight increase in prediction accuracy and reduction in prediction costs as shown in the  ablation experiments in tables      and    in the appendix. adding sparsity and quantization  led to a slight drop in accuracy but resulted in significantly smaller models. next  table    in  the appendix shows that regularization and layering techniques      that have been proposed to  increase the prediction accuracy of other gated rnns are also effective for fastgrnn and can  lead to reductions in perplexity on the ptb dataset. finally  figure   and table   of the appendix  measure the agreement between fastrnn s theoretical analysis and empirical observations. figure     a  shows that the   learnt on datasets with t time steps is decreasing function of t and figure     b  shows that the learnt   and   follow the relation       o   t   for large t which is one of  the settings in which fastrnn s gradients stabilize and training converges quickly as proved by  theorems  .  and  . . furthermore    can be seen to be close to       for large t in figure    c   as assumed in section  . .  for the convergence of long sequences. for instance  the relative error  between   and       for google    with    timesteps was  .     for har   with     timesteps  was  .    and for mnist    with     timesteps was  .   . however  for short sequences where  there was a lower likelihood of gradients exploding or vanishing    was found to deviate significantly  from       as this led to improved prediction accuracy. enforcing that           on short sequences  was found to drop accuracy by up to  .  .   .      .    .     google     har    mnist        .        google     har    mnist        .      .    .            .    .       .       t     a      .              google       har    mnist        .    .       .      .       .      .      .      .      .      .            t     b     figure    plots  a  and  b  show the variation of   and            c     t    of fastrnn with respect to  for three datasets.  plot  c  shows the relation between   and      . in accordance with theorem  .   the learnt values of   and      scale as o   t   while           for long sequences.      conclusions  this paper proposed the fastrnn and fastgrnn architectures for efficient rnn training and prediction. fastrnn could lead to provably stable training by incorporating a residual connection with  two scalar parameters into the standard rnn architecture. fastrnn was demonstrated to have lower        training times  lower prediction costs and higher prediction accuracies than leading unitary rnns  in most cases. fastgrnn extended the residual connection to a gate reusing the rnn matrices and  was able to match the accuracies of state of the art gated rnns but with significantly lower prediction costs. fastgrnn s model could be compressed to     kb without compromising accuracy in  many cases by enforcing that its parameters be low rank  sparse and quantized. this allowed fastgrnn to make accurate predictions efficiently on severely resource constrained iot devices too  tiny to hold other rnn models.    acknowledgements  we are grateful to ankit anand  niladri chatterji  kunal dahiya  don dennis  inderjit s. dhillon   dinesh khandelwal  shishir patil  adithya pratapa  harsha vardhan simhadri and raghav somani  for helpful discussions and feedback. kb acknowledges the support of the nsf through grant iis        and of the afosr through grant fa              .    