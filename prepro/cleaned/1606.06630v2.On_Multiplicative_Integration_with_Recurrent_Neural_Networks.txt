introduction    recently there has been a resurgence of new structural designs for recurrent neural networks  rnns            . most of these designs are derived from popular structures including vanilla rnns  long  short term memory networks  lstms      and gated recurrent units  grus     . despite of their  varying characteristics  most of them share a common computational building block  described by the  following equation     wx   uz   b         where x   rn and z   rm are state vectors coming from different information sources  w   rd n  and u   rd m are state to state transition matrices  and b is a bias vector. this computational  building block serves as a combinator for integrating information flow from the x and z by a sum  operation      followed by a nonlinearity  . we refer to it as the additive building block. additive  building blocks are widely implemented in various state computations in rnns  e.g. hidden state  computations for vanilla rnns  gate cell computations of lstms and grus.  in this work  we propose an alternative design for constructing the computational building block by  changing the procedure of information integration. specifically  instead of utilizing sum operation       we propose to use the hadamard product     to fuse wx and uz     wx    uz   b            the result of this modification changes the rnn from first order to second order      while introducing  no extra parameters. we call this kind of information integration design a form of multiplicative  integration. the effect of multiplication naturally results in a gating type structure  in which wx  and uz are the gates of each other. more specifically  one can think of the state to state computation  uz  where for example z represents the previous state  as dynamically rescaled by wx  where  for example x represents the input . such rescaling does not exist in the additive building block  in  which uz is independent of x. this relatively simple modification brings about advantages over the  additive building block as it alters rnn s gradient properties  which we discuss in detail in the next  section  as well as verify through extensive experiments.       equal contribution.      th conference on neural information processing systems  nips        barcelona  spain.     in the following sections  we first introduce a general formulation of multiplicative integration. we  then compare it to the additive building block on several sequence learning tasks  including character  level language modelling  speech recognition  large scale sentence representation learning using a  skip thought model  and teaching a machine to read and comprehend for a question answering  task. the experimental results  together with several existing state of the art models  show that  various rnn structures  including vanilla rnns  lstms  and grus  equipped with multiplicative  integration provide better generalization and easier optimization. its main advantages include      it  enjoys better gradient properties due to the gating effect. most of the hidden units are non saturated       the general formulation of multiplicative integration naturally includes the regular additive  building block as a special case  and introduces almost no extra parameters compared to the additive  building block  and     it is a drop in replacement for the additive building block in most of the  popular rnn models  including lstms and grus. it can also be combined with other rnn training  techniques such as recurrent batch normalization    . we further discuss its relationship to existing  models  including hidden markov models  hmms       second order rnns     and multiplicative  rnns    .         structure description and analysis     .     general formulation of multiplicative integration    the key idea behind multiplicative integration is to integrate different information flows wx and uz   by the hadamard product    . a more general formulation of multiplicative integration includes  two more bias vectors    and    added to wx and uz      wx            uz          b            d    where           r are bias vectors. notice that such formulation contains the first order terms as  in a additive building block  i.e.     uht        wxt . in order to make the multiplicative  integration more flexible  we introduce another bias vector     rd to gate  the term wx uz   obtaining the following formulation          wx    uz         uz         wx   b             note that the number of parameters of the multiplicative integration is about the same as that of the  additive building block  since the number of new parameters        and      are negligible compared  to total number of parameters. also  multiplicative integration can be easily extended to lstms  and grus    that adopt vanilla building blocks for computing gates and output states  where one can  directly replace them with the multiplicative integration. more generally  in any kind of structure  where k information flows  k      are involved  e.g. residual networks        one can implement  pairwise multiplicative integration for integrating all k information sources.   .  gradient properties  the multiplicative integration has different gradient properties compared to the additive building  block. for clarity of presentation  we first look at vanilla rnn and rnn with multiplicative  integration embedded  referred to as mi rnn. that is  ht     wxt   uht     b  versus   ht  ht     wxt uht     b . in a vanilla rnn  the gradient  h  can be computed as follows   t n  t  y   ht     ut diag   k           ht n  k t n        k         where       wxk   uhk     b . the equation above shows that the gradient flow through time  heavily depends on the hidden to hidden matrix u  but w and xk appear to play a limited role  they   ht  only come in the derivative of    mixed with uhk   . on the other hand  the gradient  h  of a  t n  mi rnn is     t  y   ht     ut diag wxk  diag   k           ht n  k t n           if        the multiplicative integration will degenerate to the vanilla additive building block.  see exact formulations in the appendix.     here we adopt the simplest formulation of multiplicative integration for illustration. in the more general  case  eq.     diag wxk   in eq.   will become diag   wxk       .             where   k       wxk uhk     b . by looking at the gradient  we see that the matrix w and  the current input xk is directly involved in the gradient computation by gating the matrix u  hence  more capable of altering the updates of the learning system. as we show in our experiments  with  wxk directly gating the gradient  the vanishing exploding problem is alleviated  wxk dynamically  reconciles u  making the gradient propagation easier compared to the regular rnns. for lstms  and grus with multiplicative integration  the gradient propagation properties are more complicated.  but in principle  the benefits of the gating effect also persists in these models.         experiments    in all of our experiments  we use the general form of multiplicative integration  eq.    for any hidden  state gate computations  unless otherwise specified.   .  exploratory experiments  to further understand the functionality of multiplicative integration  we take a simple rnn for  illustration  and perform several exploratory experiments on the character level language modeling  task using penn treebank dataset       following the data partition in     . the length of the  training sequence is   . all models have a single hidden layer of size       and we use adam  optimization algorithm      with learning rate  e   . weights are initialized to samples drawn from  uniform   .     .   . performance is evaluated by the bits per character  bpc  metric  which is  log  of perplexity.   . .  gradient properties  to analyze the gradient flow of the model  we divide the gradient in eq.   into two parts   . the  gated matrix products  ut diag wxk    and  . the derivative of the nonlinearity      we separately  analyze the properties of each term compared to the additive building block. we first focus on the  gating effect brought by diag wxk  . in order to separate out the effect of nonlinearity  we chose    to be the identity map  hence both vanilla rnn and mi rnn reduce to linear models  referred to as  lin rnn and lin mi rnn.  for each model we monitor the log l  norm of the gradient log    c  ht      averaged over the  training set  after every training epoch  where ht is the hidden state at time step t  and c is the  negative log likelihood of the single character prediction at the final time step  t      . figure.    shows the evolution of the gradient norms for small t  i.e.            as they better reflect the gradient  propagation behaviour. observe that the norms of lin mi rnn  orange  increase rapidly and soon  exceed the corresponding norms of lin rnn by a large margin. the norms of lin rnn stay close to  zero           and their changes over time are almost negligible. this observation implies that with  the help of diag wxk   term  the gradient vanishing of lin mi rnn can be alleviated compared to  lin rnn. the final test bpc  bits per character  of lin mi rnn is  .    which is comparable to a  vanilla rnn with stabilizing regularizer       while lin rnn performs rather poorly  achieving a test  bpc of over  .  next we look into the nonlinearity  . we chose     tanh for both vanilla rnn and mi rnn.  figure    c  and  d  shows a comparison of histograms of hidden activations over all time steps on  the validation set after training. interestingly  in  c  for vanilla rnn  most activations are saturated  with values around     whereas in  d  for mi rnn  most activations are non saturated with values  around  . this has a direct consequence in gradient propagation  non saturated activations imply  that diag   k       for     tanh  which can help gradients propagate  whereas saturated activations  imply that diag   k        resulting in gradients vanishing.   . .  scaling problem  when adding two numbers at different order of magnitude  the smaller one might be negligible for the  sum. however  when multiplying two numbers  the value of the product depends on both regardless  of the scales. this principle also applies when comparing multiplicative integration to the additive  building blocks. in this experiment  we test whether multiplicative integration is more robust to the  scales of weight values. following the same models as in section  . .   we first calculated the norms  of wxk and uhk   for both vanilla rnn and mi rnn for different k after training. we found that  in both structures  wxk is a lot smaller than uhk   in magnitude. this might be due to the fact that  xk is a one hot vector  making the number of updates for  columns of  w be smaller than u. as a  result  in vanilla rnn  the pre activation term wxk   uhk   is largely controlled by the value of  uhk     while wxk becomes rather small. in mi rnn  on the other hand  the pre activation term  wxk uhk   still depends on the values of both wxk and uhk     due to multiplication.              a                normalized fequency     .     lin rnn  t    lin rnn  t    lin rnn  t       lin mi rnn  t    lin mi rnn  t    lin mi rnn  t                   number of epochs     c      .    .      .       .    .    .    .    .    .   activation values of h t     .      .               .      .  .     validation bpc              vanilla rnn  mi rnn simple  mi rnn general     .     normalized fequency    log  dc   dh t                   b      .                 number of epochs           d      .     .     .     .     .     .   .      .           .    .    .   activation values of h t     .     figure     a  curves of log l  norm of gradients for lin rnn  blue  and lin mi rnn  orange . time gradually  changes from           .  b  validation bpc curves for vanilla rnn  mi rnn simple using eq.    and mirnn general using eq.  .  c  histogram of vanilla rnn s hidden activations over the validation set  most  activations are saturated.  d  histogram of mi rnn s hidden activations over the validation set  most activations  are not saturated.    we next tried different initialization of w and u to test their sensitivities to the scaling. for each  model  we fix the initialization of u to uniform   .     .    and initialize w to uniform  rw   rw    where rw varies in   .     .    .    .  . table    top left panel  shows results. as we increase  the scale of w  performance of the vanilla rnn improves  suggesting that the model is able to  better utilize the input information. on the other hand  mi rnn is much more robust to different  initializations  where the scaling has almost no effect on the final performance.   . .  on different choices of the formulation  in our third experiment  we evaluated the performance of different computational building blocks   which are eq.    vanilla rnn   eq.    mi rnn simple  and eq.    mi rnn general   . from the  validation curves in figure    b   we see that both mi rnn  simple and mi rnn general yield much  better performance compared to vanilla rnn  and mi rnn general has a faster convergence speed  compared to mi rnn simple. we also compared our results to the previously published models  in table    bottom left panel  where mi rnn general achieves a test bpc of  .    which is to our  knowledge the best result for rnns on this task without complex gating cell mechanisms.   .     character level language modeling    in addition to the penn treebank dataset  we also perform character level language modeling on two  larger datasets  text   and hutter challenge wikipedia  . both of them contain    m characters from  wikipedia while text  has an alphabet size of    and hutter challenge wikipedia has an alphabet  size of    . for both datasets  we follow the training protocols in      and     respectively. we use  adam for optimization with the starting learning rate grid searched in   .      .      .     . if the  validation bpc  bits per character  does not decrease for   epochs  we half the learning rate.  we implemented multiplicative integration on both vanilla rnn and lstm  referred to as mirnn and mi lstm. the results for the text  dataset are shown in table    bottom middle panel.  all five models  including some of the previously published models  have the same number of       we perform hyper parameter search for the initialization of               b  in mi rnn general.  http   mattmahoney.net dc textdata     http   prize.hutter .net              rw       .    .   .   .     std    rnn  .    .    .    .    .    mi rnn  .    .    .    .    .       wsj corpus    cer wer    drnn ctcbeamsearch       encoder decoder       lstm ctcbeamsearch       eesen       lstm ctc wfst  ours   mi lstm ctc wfst  ours       .    .    .   .    .   .    .    .   .    .   .     penn treebank    bpc    text     bpc    rnn       hf mrnn       rnn stabalization       mi rnn  ours   linear mi rnn  ours      .     .     .     .     .      rnn smoothrelu       hf mrnn       mi rnn  ours   lstm  ours   mi lstm ours      .     .     .     .     .      hutterwikipedia    bpc    stacked lstm       gf lstm      grid lstm      mi lstm  ours      .     .     .     .      table    top  test bpcs and the standard deviation of models with different scales of weight initializations. top  right  test cers and wers on wsj corpus. bottom left  test bpcs on character level penn treebank dataset.  bottom middle  test bpcs on character level text  dataset. bottom right  test bpcs on character level hutter  prize wikipedia dataset.    parameters    m . for rnns without complex gating cell mechanisms  the first three results   our  mi rnn  with               b  initialized as      .    .       performs the best  our mi lstm  with                b  initialized as      .    .       outperforms all other models by a large margin  .  on hutter challenge wikipedia dataset  we compare our mi lstm  single layer with      unit      m  with               b  initialized as               to the previous stacked lstm    layers      m        gf lstm    layers     m       and grid lstm    layers     m     . table    bottom  right panel  shows results. despite the simple structure compared to the sophisticated connection  designs in gf lstm and grid lstm  our mi lstm outperforms all other models and achieves the  new state of the art on this task.   .     speech recognition    we next evaluate our models on wall street journal  wsj  corpus  available as ldc corpus  ldc  s b and ldc  s  b   where we use the full    hour set  si     for training  set  dev    for  validation and set  eval    for test. we follow the same data preparation process and model setting  as in       and we use    characters as the targets for the acoustic modelling. decoding is done with  the ctc      based weighted finite state transducers  wfsts       as proposed by     .  our model  referred to as mi lstm ctc wfst  consists of   bidirectional mi lstm layers  each with     units for each direction. ctc is performed on top to resolve the alignment  issue in speech transcription. for comparison  we also train a baseline model  referred to as  lstm ctc wfst  with the same size but using vanilla lstm. adam with learning rate  .      is used for optimization and gaussian weight noise with zero mean and  .   standard deviation  is injected for regularization. we evaluate our models on the character error rate  cer  without  language model and the word error rate  wer  with extended trigram language model.  table    top right panel  shows that mi lstm ctc wfst achieves quite good results on both cer  and wer compared to recent works  and it has a clear improvement over the baseline model. note  that we did not conduct a careful hyper parameter search on this task  hence one could potentially  obtain better results with better decoding schemes and regularization techniques.   .     learning skip thought vectors    next  we evaluate our multiplicative integration on the skip thought model of     . skip thought is  an encoder decoder model that attempts to learn generic  distributed sentence representations. the  model produces sentence representation that are robust and perform well in practice  as it achieves  excellent results across many different nlp tasks. the model was trained on the bookcorpus dataset  that consists of        books with            sentences. not surprisingly  a single pass through           reports better results but they use much larger models     m  which is not directly comparable.          semantic relatedness    r         mse    paraphrase detection acc f     uni skip       bi skip       combine skip          .      .      .       .      .      .       .      .      .        uni skip       bi skip       combine skip           .    .     .    .     .    .     uni skip  ours   mi uni skip  ours      .      .      .       .      .      .        uni skip  ours   mi uni skip  ours       .    .     .    .     classification    mr cr subj mpqa    uni skip         .    .    .   bi skip         .    .    .   combine skip        .    .    .       .     .     .     uni skip  ours     .    .    .   mi uni skip  ours    .    .    .       .     .     attentive reader    val. err.    lstm      bn lstm      bn everywhere      lstm  ours   mi lstm  ours   mi lstm bn  ours   mi lstm bn everywhere  ours      .       .       .       .       .       .       .        table    top left  skip thought mi on semantic relatedness task. top right  skip thought mi on paraphrase  detection task. bottom left  skip thought mi on four different classification tasks. bottom right  multiplicative  integration  with batch normalization  on teaching machines to read and comprehend task.    the training data can take up to a week on a high end gpu  as reported in      . such training  speed largely limits one to perform careful hyper parameter search. however  with multiplicative  integration  not only the training time is shortened by a factor of two  but the final performance is  also significantly improved.  we exactly follow the authors  theano implementation of the skip thought model    encoder and  decoder are single layer grus with hidden layer size of       all recurrent matrices adopt orthogonal  initialization while non recurrent weights are initialized from uniform distribution. adam is used  for optimization. we implemented multiplicative integration only for the encoder gru  embedding  mi into decoder did not provide any substantial gains . we refer our model as mi uni skip  with                b  initialized as             . we also train a baseline model with the same size  referred  to as uni skip ours   which essentially reproduces the original model of     .  during the course of training  we evaluated the skip thought vectors on the semantic relatedness  task  using sick dataset  every      updates for both mi uni skip and the baseline model  each  iteration processes a mini batch of size    . the results are shown in figure  a. note that mi uni skip  significantly outperforms the baseline  not only in terms of speed of convergence  but also in terms  of final performance. at around    k updates  mi uni skip already exceeds the best performance  achieved by the baseline  which takes about twice the number of updates.  we also evaluated both models after one week of training  with the best results being reported on six  out of eight tasks reported in       semantic relatedness task on sick dataset  paraphrase detection  task on microsoft research paraphrase corpus  and four classification benchmarks  movie review  sentiment  mr   customer product reviews  cr   subjectivity objectivity classification  subj   and  opinion polarity  mpqa . we also compared our results with the results reported on three models in  the original skip thought paper  uni skip  bi skip  combine skip. uni skip is the same model as our  baseline  bi skip is a bidirectional model of the same size  and combine skip takes the concatenation  of the vectors from uni skip and bi skip to form a      dimension vector for task evaluation. table    shows that mi uni skip dominates across all the tasks. not only it achieves higher performance  than the baseline model  but in many cases  it also outperforms the combine skip model  which has  twice the number of dimensions. clearly  multiplicative integration provides a faster and better way  to train a large scale skip thought model.   .     teaching machines to read and comprehend    in our last experiment  we show that the use of multiplicative integration can be combined with  other techniques for training rnns  and the advantages of using mi still persist. recently       introduced recurrent batch normalization. they evaluated their proposed technique on a uni     https   github.com ryankiros skip thoughts          mse     .       a      .      uni skip  ours   mi uni skip  ours      .     .     .      validation error     .       .     .       b   lstm      bn lstm      mi lstm  ours   mi lstm bn  ours      .     .       .        .                                                 number of iterations   . k   number of iterations   k   figure     a  mse curves of uni skip  ours  and mi uni skip  ours  on semantic relatedness task on sick  dataset. mi uni skip significantly outperforms baseline uni skip.  b  validation error curves on attentive reader  models. there is a clear margin between models with and without mi.    directional attentive reader model      for the question answering task using the cnn corpus   . to  test our approach  we evaluated the following four models   . a vanilla lstm attentive reader model  with a single hidden layer size      same as      as our baseline  referred to as lstm  ours    . a  multiplicative integration lstm with a single hidden size      referred to as mi lstm   . milstm with batch norm  referred to as mi lstm bn   . mi lstm with batch norm everywhere   as detailed in       referred to as mi lstm bn everywhere. we compared our models to results  reported in      referred to as lstm  bn lstm and bn lstm everywhere     .  for all mi models                b  were initialized to             . we follow the experimental  protocol of       and use exactly the same settings as theirs  except we remove the gradient clipping  for mi lstms. figure.  b shows validation curves of the baseline  lstm   mi lstm  bn lstm   and mi lstm bn  and the final validation errors of all models are reported in table    bottom right  panel. clearly  using multiplicative integration results in improved model performance regardless  of whether batch norm is used. however  the combination of mi and batch norm provides the  best performance and the fastest speed of convergence. this shows the general applicability of  multiplication integration when combining it with other optimization techniques.         relationship to previous models     .     relationship to hidden markov models    one can show that under certain constraints  mi rnn is effectively implementing the forward  algorithm of the hidden markov model hmm . a direct mapping can be constructed as follows  see       for a similar derivation . let u   rm m be the state transition probability matrix with uij    pr ht     i ht   j   w   rm n be the observation probability matrix with wij   pr xt    i ht   j . when xt is a one hot vector  e.g.  in many of the language modelling tasks   multiplying  it by w is effectively choosing a column of the observation matrix. namely  if the j th entry of xt  is one  then wxt   pr xt  ht   j . let h  be the initial state distribution with h    pr h    and   ht  t   be the alpha values in the forward algorithm of hmm  i.e.  ht   pr x    ...  xt   ht  . then  uht   pr x    ...  xt   ht    . thus ht     wxt   uht   pr xt    ht       pr x    ...  xt   ht        pr x    ...  xt     ht    . to exactly implement the forward algorithm using multiplicative integration   the matrices w and u have to be probability matrices  and xt needs to be a one hot vector. the  function   needs to be linear  and we drop all the bias terms. therefore  rnn with multiplicative  integration can be seen as a nonlinear extension of hmms. the extra freedom in parameter values  and nonlinearity makes the model more flexible compared to hmms.   .     relations to second order rnns and multiplicative rnns    mi rnn is related to the second order rnn     and the multiplicative rnn  mrnn     . we first  describe the similarities with these two models   the second order rnn involves a second order term st in a vanilla rnn  where the ith element  st i is computed by the bilinear form  st i   xtt t  i  ht     where t  i    rn m      i   m  is        note that     used a truncated version of the original dataset in order to save computation.  learning curves and the final result number are obtained by emails correspondence with authors of    .      https   github.com cooijmanstim recurrent batch normalization.git.              the ith slice of a tensor t   rm n m . multiplicative integration also involve a second order term  st     wxt uht     but in our case st i    i  wi   xt   ui   ht       xtt   wi   ui  ht      where wi and ui are ith row in w and u  and  i is the ith element of  . note that the outer product   i wi   ui is a rank   matrix. the multiplicative rnn is also a second order rnn  but which  p  i   i   xt t    pdiag vxt  q. for mi rnn  we can  approximates t by a tensor decomposition  also think of the second order term as a tensor decomposition    wxt uht     u xt  ht       diag   diag wxt  u ht   .  there are however several differences that make mi a favourable model      simpler parametrization   mi uses a rank   approximation compared to the second order rnns  and a diagonal approximation  compared to multiplicative rnn. moreover  mi rnn shares parameters across the first and second  order terms  whereas the other two models do not. as a result  the number of parameters are largely  reduced  which makes our model more practical for large scale problems  while avoiding overfitting.      easier optimization  in tensor decomposition methods  the products of three different  low rank   matrices generally makes it hard to optimize    . however  the optimization problem becomes  easier in mi  as discussed in section   and  .     general structural design vs. vanilla rnn design   multiplicative integration can be easily embedded in many other rnn structures  e.g. lstms and  grus  whereas the second order rnn and mrnn present a very specific design for modifying  vanilla rnns.  moreover  we also compared mi rnn s performance to the previous hf mrnn s results  multiplicative rnn trained by hessian free method  in table    bottom left and bottom middle panels  on  penn treebank and text  datasets. one can see that mi rnn outperforms hf mrnn on both tasks.   .     general multiplicative integration    multiplicative integration can be viewed as a general way of combining information flows from  two different sources. in particular       proposed the ladder network that achieves promising  results on semi supervised learning. in their model  they combine the lateral connections and the  backward connections via the  combinator  function by a hadamard product. the performance would  severely degrade without this product as empirically shown by     .      explored neural embedding  approaches in knowledge bases by formulating relations as bilinear and or linear mapping functions   and compared a variety of embedding models on the link prediction task. surprisingly  the best  results among all bilinear functions is the simple weighted hadamard product. they further carefully  compare the multiplicative and additive interactions and show that the multiplicative interaction  dominates the additive one.         conclusion    in this paper we proposed to use multiplicative integration  mi   a simple hadamard product to  combine information flow in recurrent neural networks. mi can be easily integrated into many popular  rnn models  including lstms and grus  while introducing almost no extra parameters. indeed   the implementation of mi requires almost no extra work beyond implementing rnn models. we also  show that mi achieves state of the art performance on four different tasks or    datasets of varying  sizes and scales. we believe that the multiplicative integration can become a default building block  for training various types of rnn models.    acknowledgments  the authors acknowledge the following agencies for funding and support  nserc  canada research  chairs  cifar  calcul quebec  compute canada  disney research and onr grant n              . the authors thank the developers of theano      and keras       and also thank jimmy ba for  many thought provoking discussions.    