introduction  the need for effective regularization methods for rnns  has seen extensive focus in recent years. while application  of dropout  srivastava et al.        to the input and output  of an rnn has been shown to be effective  zaremba et al.          dropout is destructive when naively applied to the  recurrent connections of an rnn. when naive dropout is  applied to the recurrent connections  it is almost impossible to retain information over long periods of time.  given this fundamental issue  substantial work has gone  into understanding and improving dropout when applied to  recurrent connections. of these techniques  which we shall  broadly refer to as recurrent dropout  some specific variations have gained popular usage.     salesforce research  palo alto  usa. correspondence to   stephen merity  smerity salesforce.com .    part of    th international conference on machine learning s  workshop on learning to generate natural language  sydney   australia      . copyright      by the author s .    variational rnns  gal   ghahramani        drop the  same network units at each timestep  as opposed to dropping different network units at each timestep. by performing dropout on the same units at each timestep  destructive  loss of the rnn hidden state is avoided and the same information is masked at each timestep.  rather than dropping units  another tactic is to drop updates to given network units. semeniuta et al.         perform dropout on the input gate of the lstm   hochreiter   schmidhuber        but allow the forget  gate to discard portions of the existing hidden state. zoneout  krueger et al.        prevents hidden state updates  from occurring by setting a randomly selected subset of  network unit activations in ht   to be equal to the previous  activations from ht . both of these act to prevent updates to  the hidden state while preserving existing content.  on an extreme end  work has also been done to restrict the recurrent matrices in an rnn in order to limit  their computational capacity. some rnn architectures  only allow element wise interactions  balduzzi   ghifary         bradbury et al.        seo et al.         removing  the recurrent matrix entirely  while others act to restrict the capacity by parameterizing the recurrent matrix  arjovsky et al.        wisdom et al.        jing et al.        .  other forms of regularization explicitly act upon activations such as such as batch normalization  ioffe   szegedy          recurrent batch normalization  cooijmans et al.          and layer normalization  ba et al.       . these  all introduce additional training parameters and can complicate the training process while increasing the sensitivity  of the model. norm stabilization  krueger   memisevic         penalizes the model when the norm of an rnn s hidden state changes substantially between timesteps  achieving strong results in character language modeling on and  phoneme recognition.  in this work  we revisit l  regularization in the form of activation regularization  ar  and temporal activation regularization  tar . when applied to modern baselines that do  not contain recurrent dropout or normalization techniques   ar and tar achieve comparable or superior results.  compared to other invasive regularization techniques     revisiting activation regularization for language rnns    which may require modifications to the rnn cell itself or  complex model changes  both ar and tar require no substantial modifications to the rnn or model. this enables  ar and tar to be applied to optimized rnn implementations such as the cudnn lstm which can be many times  faster than na  ve but flexible lstm implementations.     . activation regularization   . . l  activation regularization  ar     adding a prior that minimizes differences between states  has been explored in the past. this broad concept  falls under the broad concept of slowness regularization  hinton        fo ldia k        luciw   schmidhuber         jonschkowski   brock        wen et al.         which attempts to minimize l f  xt    f  xt      where l is  a loss function describing the distance between f  xt   and  f  xt     and f is an arbitrary mapping function.  temporal activation regularization  tar  is a direct descendant of this slowness regularization  minimizing    l   ht   ht      where l        k k   l  norm   ht is the output of the rnn  at timestep t  and   is a scaling coefficient.  tar penalizes any large changes in hidden state between  timesteps  encouraging the model to keep the output as consistent as possible. for the lstm  the hidden state which is  regularized is only ht   not the long term memory ct   though  this could optionally be regularized in a similar manner.                                      m    m    m    m    m    m      .     .     .     .     .     .                         where m is the dropout mask used by later parts of the  model  l        k k   l  norm   ht is the output of the  rnn at timestep t  and   is a scaling coefficient.     . . temporal activation regularization  tar     validation    model      l   m   ht      the l  penalty on the rnn activations can be applied to  ht or to m   ht  the dropped output used in the rest of the  model . in our experiments  we found that applying ar to  m   ht was more effective than applying it to neurons not  updated during the current optimization step.    parameters    table  . results over the penn treebank for testing   coefficients  for ar with base model h               dp    .   dph    . .    while l  regularization is traditionally used on the weights  of machine learning models  l  weight decay   it could  also be used on the activations. we define ar as    when applied to the output of a dense layer  ar penalizes  activations that are substantially away from    encouraging the activations to remain small. while acting implicitly rather than explicitly  this has similarities to the various  batch or layer normalization techniques.    model                              parameters    validation      m    m    m    m    m    m      .     .     .     .     .     .     table  . results over the penn treebank for testing   coefficients  for tar with base model h               dp    .   dph    . .     . experiments   . . language modeling  we benchmark activation regularization  ar  and temporal activation regularization  tar  applied to a strong nonvariational lstm baseline  . the experiment uses a preprocessed version of the penn treebank  ptb   mikolov et al.         and wikitext    wt    merity et al.       . all hyperparameters  including   for ar and   for tar  are optimized over the validation dataset. the best found hyperparameters as determined by the validation results are then  run on the test set.  ptb  as the penn treebank is a small dataset  preventing  overfitting is of considerable importance and a major focus of research. almost all competitive models rely upon  a form of recurrent dropout to ensure the rnn does not  overfit through drastic changes in the hidden state. other  aggressive dropout techniques  such as performing dropout  on the embedding layer such that entire words are dropped  from a sequence  are also frequently used.  wt   wikitext   is a dataset approximately twice as large  as ptb but with a vocabulary three times larger. the text  is also tokenized and processed in a manner similar to  datasets used for machine translation using the moses tokenizer  koehn et al.       .       pytorch word level language modeling example   https   github.com pytorch examples tree   master word language model     revisiting activation regularization for language rnns    model    parameters    validation    test    ptb  lstm  tied  h        dp    .   dph    .   ptb  lstm  tied  h        dp    .   dph    .   ptb  lstm  tied  h         dp    .    dph    .       m    m    m      .     .     .       .     .     .     ptb  lstm  tied  h                      dp    .   dph    .   ptb  lstm  tied  h                      dp    .   dph    .   ptb  lstm  tied  h                       dp    .    dph    .       m    m    m      .     .     .       .     .     .     table  . single model perplexity results over the penn treebank. models noting tied use weight tying on the embedding and softmax  weights. the top section contain models without ar or tar with the bottom section containing equivalent models using them.    model    parameters    validation    test    inan et al.          variational lstm  tied   h         inan et al.          variational lstm  tied   h          augmented loss      m    m      .     .       .     .     wt   lstm  tied  h        dp    .   dph    .       m      .       .     wt   lstm  tied  h                      dp    .   dph    .       m      .       .     table  . results over wikitext  . the increases in parameters compared to the models on ptb are due to the larger vocabulary. models  noting tied use weight tying on the embedding and softmax weights.    experiment details  all experiments use a model containing a two layer rnn. the ar and tar loss are only applied to the output of the final rnn layer  not to all layers. for the majority of experiments  we follow the medium  model size of zaremba et al.         a two layer rnn with      hidden units in each layer.  for training the model  stochastic gradient descent  sgd   without momentum was used for up to    epochs. the  learning rate began at    and was divided by four each time  validation perplexity failed to improve. l  weight regularization of      was used over all weights in the model and  gradients with norm over    were rescaled. batches consist  of    examples with each example containing    timesteps.  the loss was averaged over all examples and timesteps. all  embedding weights were uniformly initialized in the interval    .    .   and all other weights were initialized between      h     h    where h is the hidden size.  for dropout  we have two different parameters  dp and dph .  dp is the dropout rate used on the word vectors and the final  rnn output. dph is the dropout rate used on the connection between rnn layers. all models use weight tying between the embedding and softmax layer  inan et al.         press   wolf       .  evaluating ar and tar independently on ptb  to understand the potential of ar and tar  we investigate their  impact on language model perplexity when used independently in table    ar  and table    tar . while both result in a substantial reduction in perplexity  ar results in  the strongest improvement of  .   while tar only achieves     . . the drops achieved by this are equivalent to using an  lstm model with twice as many parameters   a substantial  improvement given the simplicity of ar and tar.  evaluating ar and tar jointly on ptb  when both ar  and tar are used together  we found the best result was  achieved by decreasing   and    likely as the model was  over regularized otherwise. in table   we present ptb  results for three different model sizes comparing models  without ar tar to those which use both. the model sizes  h                    were chosen to be comparable in size  to other published results. with both ar and tar  the  smallest model has an improvement of  .  over the baseline model. the improvements continue for the two larger  size models  h       and h         though the gains fall  off as the model size is increased.  comparing to state of the art ptb  in table   we summarize the current state of the art models in language modeling over the penn treebank.  the largest lstm we train  h         achieves comparable results to the recurrent highway network  rhn    zilly et al.         a human developed custom rnn architecture  but with approximately double the number of parameters. although the lstm uses twice as many parameters  the rhn runs a cell    times per timestep  referred  to as recurrence depth   resulting in far more computation.  this would likely result in the rhn being slower than the  larger lstm model during both training and prediction   especially when factoring in optimized lstm implementations such as nvidia s cudnn lstm.     revisiting activation regularization for language rnns    model    parameters    validation    test    zaremba et al.          lstm  medium   zaremba et al.          lstm  large   gal   ghahramani          variational lstm  medium   gal   ghahramani          variational lstm  medium  mc   gal   ghahramani          variational lstm  large   gal   ghahramani          variational lstm  large  mc   kim et al.          charcnn  merity et al.          pointer sentinel lstm  inan et al.          variational lstm  tied    augmented loss  inan et al.          variational lstm  tied    augmented loss  zilly et al.          variational rhn  tied   zoph   le          nas cell  tied   zoph   le          nas cell  tied       m    m    m    m    m    m    m    m    m    m    m    m    m      .     .     .     .        .     .           .     .     .     .             .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     ptb  lstm  tied  h                      dp    .   dph    .   ptb  lstm  tied  h                      dp    .   dph    .   ptb  lstm  tied  h                       dp    .    dph    .       m    m    m      .     .     .       .     .     .     table  . single model perplexity on validation and test sets for the penn treebank language modeling task. models noting tied use  weight tying on the embedding and softmax weights.    we also compare to the neural architecture search  nas   cell  zoph   le       . while zoph   le        do not  report any of the hyperparameters or what type of dropout  they used for their penn treebank result  they do note that  they performed an extensive hyperparameter search over  learning rate  weight initialization  dropout rates  and decay epoch in order to produce their best performing model.  it is possible that a large contributor to their improved result was in these tuned hyperparameters as they did not  compare their nas cell results to a standard or variational  lstm cell that was subjected to the same extensive hyperparameter search. our largest lstm results are   perplexity higher in comparison but have not undergone extensive  hyperparameter search  do not use additional regularization  techniques such as recurrent or embedding dropout  and do  not use a custom rnn cell.  wikitext   results  we compare our wikitext   results  to inan et al.        who introduced weight tying between  the embedding and softmax weights. while we did not perform any hyperparamter search over the coefficient values  of   and   for ar and tar  instead using the best results  from ptb  we find them to still be quite effective. the  baseline lstm already achieves a  .  perplexity improvement over the variational lstm models from inan et al.          including one which uses an augmented loss that  modifies standard cross entropy with temperature and a kl  divergence based loss. when the ar and tar parameters  optimized over ptb are used  perplexity falls an additional   .  perplexity. this is not as strong an improvement as seen  on the ptb dataset and may be due to the increased complexity of the dataset  larger vocabulary meaning a longer    tail of usage  different genre  and so on  or may just be due  to the lack of hyperparamter tuning.  ar and tar for gru and tanh rnn  while neither  the gru  cho et al.        or tanh rnn are traditionally  used in language modeling  we wanted to see the generality of ar and tar to other types of rnn cells. we applied the best values of   and   for an lstm cell to the  gru and tanh rnn on ptb without any further search in  table  . these values are likely quite suboptimal but are  sufficient for illustrative purposes. for the gru  perplexity improved by  .  from the baseline. this is a positive  sign given the impact of these regularization techniques on  a gru are quite different to that of an lstm. the lstm  only has ht subjected to ar and tar  leaving the long  term memory ct unregularized  but the gru uses ht both  as output at that timestep and as the hidden state input for  the next timestep. for the tanh rnn  the model did not  train to acceptable levels at all without the application of  ar and tar. for the tanh rnn  tar likely forced the  recurrent matrix to learn an identity function in order to  ensure ht could produce ht   . this would be important  given the weights in this model were randomly initialized  and suggests tar acts as an implicit identity initialization  constraint  le et al.       .     . conclusion  in this work  we revisit l  regularization in the form of  activation regularization  ar  and temporal activation regularization  tar . while simple to implement  activity regularization and temporal activity regularization are com      revisiting activation regularization for language rnns    model    parameters    validation    test    ptb  rnn  tied  h        dp    .   dph    .   ptb  rnn  tied  h                      dp    .   dph    .       m    m       .      .        .      .     ptb  gru  tied  h        dp    .   dph    .   ptb  gru  tied  h                      dp    .   dph    .       m    m      .     .       .     .     table  . single model perplexity results over the penn treebank for tanh rnn and gru. neither cell are traditionally used for language  modeling but this demonstrates the generality for ar     and tar    . values for      taken from best lstm model with no search.  models noting tied use weight tying on the embedding and softmax weights.    petitive with other far more complex regularization techniques and offer equivalent or better results. the improvements that these techniques provide can likely be combined  with other regularization techniques  such as the variational  lstm  and may lead to further improvements in performance as well  especially if subjected to an extensive hyperparameter search.    forces in the nation . the area was moved to sarajevo    and the troops were despatched to the national register of  historic places in the summer of      for the establishment  of full political and social parties . the polish language was  protected by the soviet union   which was the first polish  continental conflict of the newly formed union in north  america   and the polish front with the last of the polish  communist party .    sample generated text  for generating text samples  words were sampled using the  standard generation script contained in the pytorch word  level language modeling example. wikitext   was used  given the larger vocabulary and more realistic looking text.  neither the heosi token nor the hunki were allowed to be  selected. each paragraph is a separate sample of text with  the tokens following moses  koehn et al.         joining  words with     and dot decimal split to a  .  token.      something borrowed   is the second episode of the fourth  season of the american comedy television series the x    files . the episode was written by david mccarthy  and directed by mark sacks . it aired in the united states  on november             as a two     episode episode   watched by    .    million viewers and was the highest  rated show on the fox network .  the work of olivier  s   a large     s table with the center  of a vinyl beam   was used for bony motifs from the upper      production model via the club van x . the modified  works were released in the museum   which gave its namesake to the visual designers in hong kong .    