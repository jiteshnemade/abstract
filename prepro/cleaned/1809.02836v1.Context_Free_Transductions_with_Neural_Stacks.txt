introduction    recent work on recurrent neural network  rnn   architectures has introduced a number of models  that enhance traditional networks with differentiable implementations of common data structures.  appealing to their turing completeness  siegelmann and sontag         graves et al.        view  rnns as computational devices that learn transduction algorithms  and develop a trainable model  of random access memory that can simulate turing machine computations. in the domain of natural language processing  the prevalence of contextfree models of natural language syntax has motivated stack based architectures such as those of  grefenstette et al.        and joulin and mikolov        . by analogy to graves et al. s neural turing machines  these stack based models are designed to simulate pushdown transducer computations.  from a practical standpoint  stack based models may be seen as a way to optimize networks  for discovering dependencies of a hierarchical       equal contribution.    nature. additionally  stack based models could  potentially facilitate interpretability by imposing  structure upon the recurrent state of an rnn.  classical architectures such as simple rnns  elman         long short term memory networks   lstm  hochreiter and schmidhuber         and  gated recurrent unit networks  gru  cho et al.         represent state as black box vectors. in certain cases  these models can learn to implement  classical data structures using state vectors  kirov  and frank       . however  because state vectors are fixed in size  the inferred data structures  must be represented in a fractal encoding requiring arbitrary position. on the other hand  differentiable stacks typically increase in size throughout the course of the computation  so their performance may better scale to larger inputs. since  the ability of a differentiable stack to function correctly intrinsically requires that the information it  contains be represented in the proper format  examining the contents of a network s stack throughout the course of its computation could reveal hierarchical patterns that the network has discovered  in its training data.  this paper systematically explores the behavior of stack augmented rnns on simple computational tasks. while yogatama et al.        provide  an analysis of stack rnns based on their multipop adaptive computation stack model  our analysis is based on the existing neural stack model  of grefenstette et al.         as well as a novel enhancement thereof. we consider tasks with optimal strategies requiring either finite state memory  or a stack  or possibly a combination of the two.  we show that neural stack networks have the ability to learn to use the stack in an intuitive manner. however  we find that neural stacks are more  difficult to train than classical architectures. in  particular  our models prefer not to employ stackbased strategies when other forms of memory are     hyt   ht i    controller    hrt   vt   st i    hvt   ut   dt i    hxt   ht     rt   i    stack    hvt     st   i    figure    the neural stack architecture.    available  such as in networks with both lstm  memory and a stack.  a description of our models  including a review of grefenstette et al. s neural stacks  appears in section  . section   discusses the relationship between stack augmented rnn models  and pushdown transducers  motivating our intuition that neural stacks are a suitable architecture  for learning context free structure. the tasks we  consider are defined in section    and our experimental paradigm is described in section  . section    presents quantitative evaluation of our models   performance as well as qualitative description of  their behavior. section   concludes.         models    the neural network models considered in this paper are based on the neural stacks of grefenstette  et al.         a family of stack augmented rnn  architectures.  a neural stack model consists of  two modular components  a controller executing  the computation of the network and a stack implementing the data model of the network. at each  time step t  the controller receives an input vector  xt and a read vector rt   representing the material at the top of the stack at the end of the previous time step. we assume that the controller may  adhere to any feedforward or recurrent structure   if the controller is recurrent  then it may also receive a recurrent state vector ht   . based on xt    rt     and possibly ht     the controller computes  an output yt   a new recurrent state vector ht if  applicable  and a tuple hvt   ut   dt i containing instructions for manipulating the stack. the stack  takes these instructions and produces rt   the vector corresponding to the material at the top of the  stack after popping and pushing operations have  been performed on the basis of hvt   ut   dt i. the       code for our pytorch  paszke et al.        implementation is available at https   github.com   viking sudo rm stacknn.    contents of the stack are represented by a recurrent state matrix vt and a strength vector st . this  schema is shown in figure  .  having established the basic architecture  the  remainder of this section introduces our models  in full detail. subsection  .  describes how the  stack computes rt and updates vt and st based  on hvt   ut   dt i. subsection  .  presents the various  kinds of controllers we consider in this paper. subsection  .  presents an enhancement of grefenstette et al. s schema that allows the network to  perform computations of varying duration.   .     differentiable stacks    a stack at time t consists of sequence of vectors  hvt      vt      . . .   vt  t i  organized into a matrix  vt whose ith row is vt  i . by convention  vt  t   is the  top  element of the stack  while vt     is  the  bottom  element. each element vt  i  of the  stack is associated with a strength st  i          .  the strength of a vector vt  i  represents the  degree  to which the vector is on the stack  a strength  of   means that the vector is  fully  on the stack   while a strength of   means that the vector has  been popped from the stack. the strengths are organized into a vector st   hst      st      . . .   st  t i.  at each time step  the stack pops a number of  items from the top  pushes a new item to the top   and reads a number of items from the top  in that  order. the behavior of the popping and pushing operations is determined by the instructions  hvt   ut   dt i. the value obtained from the reading  operation is passed back to the controller as the recurrent vector rt . let us now describe each of the  three operations.  popping reduces the strength st    t      of the  top element from the previous time step by ut .  if st    t        ut   then the strength of the   t   st element after popping is simply st  t       st    t        ut . if st    t        ut   then we consider the popping operation to have  consumed   st    t       and the strength st    t      of the  next element is reduced by the  left over  strength  ut   st    t     . this process is repeated until  all strengths in st   have been reduced. for each  i   t  we compute the left over popping strength  ut  i  for the ith item as follows.  ut  i        ut    i t    relu ut  i        st    i        i   t         the strengths are then updated accordingly.    hot     ot   i    st  i    relu  st    i    ut  i    the pushing operation simply places the vector  vt at the top of the stack with strength dt . thus   vt and st  t  are updated as follows.     vt    i t  st  t    dt  vt  i     vt    i   i   t  note that st      st      . . .   st  t      have already  been updated during the popping step.  the reading operation  reads  the elements on  the top of the stack whose total strength is  . if  st  t       then only the top element is read. otherwise  the next element is read using the  leftover  strength     st  t . as in the case of popping   we may define a series of left over strengths  t        t      . . .    t  t  corresponding to each item in the  stack.         i t   t  i     relu   t  i        st  i         i   t  the result rt of the reading operation is obtained  by computing a sum of the items in the stack  weighted by their strengths  including only items  with sufficient left over strength.  rt      t  x    min  st  i    t  i     vt  i     i       .     controllers    we consider two types of controllers  linear and  lstm. the linear controller is a feedforward network consisting of a single linear layer. the network output is directly extracted from the linear layer  while the stack instructions are passed  through the sigmoid function  denoted  .               ut     wu   xt rt      bu                 bd  dt     wd   xt rt                 vt     wv   xt rt      bv         yt   wy   xt rt      by  the lstm controller maintains two state vectors   the hidden state ht and the cell state ct . the output and stack instructions are produced by passing ht through a linear layer. as in the linear  controller  the stack instructions are additionally  passed through the sigmoid function.    output  hyt   ot i    hht     rt   i    controller    hot   ot i  hrt   vt   st i  hit   ht i  hvt   ut   dt i    stack    xt  hit     it   i    input    hit   it i    hvt     st   i    it      figure    our enhanced architecture with buffers.     .     buffered networks    one limitation of many rnn architectures  including neural stacks  is that they can only compute same length transductions  at each time step   the network must accept exactly one input vector and produce exactly one output vector. this  limitation prevents neural stacks from producing  output sequences that may be longer or shorter  than the input sequence. it also prohibits neural stack networks from performing computation  steps without reading an input or producing an  output  i.e.    transitions on input or output   even  though such computation steps are a common feature of stack transduction algorithms.  a well known approach to overcoming this limitation appears in sequence to sequence models  such as sutskever et al.        and cho et al.        . there  the production of the output sequence is delayed until the input sequence has  been fully read by the network. output vectors  produced while reading the input are discarded   and the input sequence is padded with blank symbols to indicate that the network should be producing an output.  the delayed output approach solves the problem of fixed length outputs  and we adopt it for  the string reversal task described in section  .  however  delaying the output does not allow our  networks to perform streaming computations that  may interrupt the process of reading inputs or  emitting outputs. an alternative approach is to allow our networks to perform   transitions. while  graves        achieves this by dynamically repeating inputs and marking them with flags  we  augment the neural stack architecture with two  differentiable buffers  a read only input buffer and     a write only output buffer. at each time step t   the input vector xt is obtained by popping from  the input buffer with strength it   . in addition to  the output vector and stack instructions  the controller must produce an input buffer pop strength it  and an output buffer push strength ot . the output  vector is then enqueued to the output buffer with  strength ot . this enhanced architecture is shown  in figure  .  the implementation of the input and output  buffers is based on grefenstette et al.        s  neural queues  a first in first out variant of the  neural stack. like the stack  the input buffer at  time t consists of a matrix of vectors it and a vector of strengths it . similarly  the output buffer  consists of a matrix of vectors ot and a vector of  strengths ot . the input buffer is initialized so that  i  is a matrix representation of the full input sequence  with an initial strength of   for each item.  at time t  items are dequeued from the  front   of the buffer with strength it   .     it      j     t  j     relu  t  j        it    j    j      it  j    relu  it    j     t  j    next  the input vector xt is produced by reading  from the front of the buffer with strength  .         j     t  j     relu   t  j        it  j     j      xt      n  x    min  it  j    t  j     it  j     j      since the input buffer is read only  there is no push  operation. this means that unlike vt and ot    the number of rows of it is fixed to a constant  n. when the controller s computation is complete   the output vector yt is enqueued to the  back  of  the output buffer with strength ot .     yt    j t  ot  j     ot    j   j   t     ot    j t  ot  j     ot    j   j   t  after the last time step  the final output sequence is  obtained by repeatedly dequeuing the front of the  output buffer with strength   and reading the front  of the output with strength  . these dequeuing and  reading operations are identical to those defined  for the input buffer.    start    q     x          x      y  y        figure    a pdt for the string reversal task.         pushdown transducers    our decision to use a stack for nlp tasks rather  than some other differentiable data structure is  motivated by the success of context free grammars   cfgs  in describing the hierarchical phrase structure of natural language syntax. a classic theoretical result due to chomsky        shows that cfgs  generate exactly those sets of strings that are accepted by nondetermininstic pushdown automata   pdas   a model of computation that augments a  finite state machine with a stack. when enhanced  with input and output buffers  we consider neural  stacks to be an implementation of deterministic  pushdown transducers  pdts   a variant of pdas  that includes an output tape.  formally  a pdt is described by a transition  function of the form   q  x  s    hq     y  s  i  interpreted as follows  if the machine receives an  x from the input buffer and pops an s from the top  of the stack while in state q  then it sends a y to  the output buffer  pushes an s  to the stack  and  transitions to state q   . we assume that   is only  defined for finitely many configurations hq  x  si.  these configurations  combined with their corresponding values of    represent all the possible actions of a pushdown transducer.  to illustrate  let us construct    a pdt that computes the function f w  w      w  wr   where  wr is the reverse of w and   w  is a sequence  of  s of the same length as w. we can begin to  compute f using a single state q  by pushing each  symbol of w onto the stack while emitting  s as  output. when the machine has finished reading w   the stack contains the symbols of w in reverse order. in the remainder of the computation  the machine pops symbols from the stack one at a time  and sends them to the output buffer. a pictoral  representation of this pdt is shown in figure  .  each circle represents a state of the pdt  and each  action   q  x  s    hq     y  s  i is represented by an  arrow from q to q   with the label  x   y  s   s  .   observe that the two labels of the arrow from q   to itself encode a transition function implementing  the algorithm described above.  given a finite state transition function  there ex      ists an lstm that implements it. in fact  weiss  et al.        show that a deterministic k counter  automaton can be simulated by an lstm. thus   any deterministic pdt can be simulated by the  buffered stack architecture with an lstm controller.         tasks    the goal of this paper is to ascertain whether  or not stack augmented rnn architectures can  learn to perform pdt computations. to that  end  we consider six tasks designed to highlight  various features of pdt algorithms. four of  these tasks string reversal  parenthesis prediction  and the two xor evaluation tasks have  simple pdt implementations. the pdts for each  of these tasks differ in their memory requirements   they require either finite state memory or stackstructured memory  or a combination of the two.  the remaining two tasks boolean formula evaluation and subject auxiliary agreement are designed to determine whether or not neural stacks  can be applied to complex use cases that are  thought to be compatible with stack based techniques.   .     string reversal    in the string reversal task  the network must compute the function f from the previous section. as  discussed there  the string reversal task can be  performed straightforwardly by pushing all input  symbols to the stack and then popping all symbols  from the stack. the purpose of this task is to serve  as a baseline test for whether or not a controller  can learn to use a stack in principle. since in  the general case  correctly producing wr requires  recording w in the stack  we evaluate the network  solely based on the portion of its output where wr  should appear  immediately after reading the last  symbol of w.   .     xor evaluation    we consider two tasks that require the network to  implement the xor function. in the cumulative  xor evaluation task  the network reads an input  string of  s and  s. at each time step  the network  must output the xor of all the input symbols it  has seen so far. the delayed xor evaluation task  is similar  except that the most recent input symbol  is excluded from the xor computation.    as shown in the left of figure    the xor evaluation tasks can be computed by a pdt without  using the stack. thus  we use xor evaluation  to test the versatility of the stack by assessing  whether a feedforward controller can learn to use  it as unstructured memory.  the cumulative xor evaluation task presents  the linear controller with a theoretical challenge  because single layer linear networks cannot compute the xor function  minsky and papert       .  however  in the delayed xor evaluation task   the delay between reading an input symbol and  incorporating it into the xor gives the network  two linear layers to compute xor when unravelled through time. therefore  we expect that the  linear model should be able to perform the delayed xor evaluation task  but not the cumulative xor evaluation task.  the discrepancy between the cumulative and  the delayed xor evaluation tasks for the linear controller highlights the importance of timing in stack algorithms. since the our enhanced  architecture from subsection  .  can perform  transitions  we expect it to perform the cumulative xor evaluation task with a linear controller by learning to introduce the necessary delay. thus  the xor tasks allow us to test whether  our buffered model can learn to optimize the timing of its computation.   .     parenthesis prediction    the parenthesis prediction task is a simplified language modelling task. at each time step t  the  network reads the tth symbol of some string and  must attempt to output the  t     st symbol. the  strings are sequences of well nested parentheses  generated by the following cfg.  s st ts t  t  t      t  t      we evaluate the network only when the correct  prediction is   or  . this restriction allows for a  deterministic pdt solution  shown in the right of  figure  .  unlike string reversal and xor evaluation   the parenthesis prediction task relies on both the  stack and the finite state control. thus  the parenthesis prediction task tests whether or not neural stack models can learn to combine different                                                      start         start         q                                   q                                                        figure    pdts for cumulative xor evaluation  left  and parenthesis prediction  right  tasks. the symbol    represents the bottom of the stack.    types of memory. furthermore  since contextfree languages can be canonically represented as  homomorphic images of well nested parentheses   chomsky and schu tzenberger         the parenthesis prediction task may be used to gauge the  suitability of neural stacks for context free language modelling.   .     verb over the incorrect ones. in sentences with embedded clauses  the network must be able to identify the subject of the verb among several possible  candidates in order to conjugate the verb.  here  we consider sentences generated by a  small  unambiguous cfg that models a fragment  of english.    boolean formula evaluation    s   npsing has   npplur have    in the boolean formula evaluation task  the network reads a boolean formula in reverse polish notation generated by the following cfg.    np   npsing   npplur  npsing   the lobster  pp   relsing   npplur   the lobsters  pp   relplur     s ss  ss     pp   in np    s t f  at each time step  the network must output the  truth value of the longest sub formula ending at  the input symbol.  the boolean formula evaluation task tests the  ability of neural stacks to infer complex computations over the stack. in this case  the network must  store previously computed values on the stack and  evaluate boolean operations over these stored values. this technique is reminiscent of shift reduce  parsing  making the boolean formula evaluation  task a testing ground for the possibility of applying  neural stacks to natural language parsing.   .     subject auxiliary agreement    the subject auxiliary agreement task is inspired  by linzen et al.         who investigate whether  or not lstms can learn structure sensitive longdistance dependencies in natural language syntax.  there  the authors train lstm models that perform language modelling on prefixes of sentences  drawn from corpora. the last word of each prefix  is a verb  and the models are evaluated solely on  whether or not they prefer the correct form of the    relsing   that has vp   relobj  relplur   that have vp   relobj  relobj   that npsing has devoured  relobj   that npplur have devoured  vp   slept   devoured np    as in the parenthesis prediction task  the network  performs language modelling  but is only evaluated when the correct prediction is an auxiliary  verb  i.e.  has or have .         experiments    we conducted four experiments designed to assess  various aspects of the behavior of neural stacks.  in each experiment  models are trained on a generated dataset consisting of     input output string  pairings encoded in one hot representation. training occurs in mini batches containing    string  pairings each. at the end of each epoch  the model  is evaluated on a generated development set of      examples. training terminates when five consecutive epochs fail to exceed the highest development     accuracy attained. the sizes of the lstm controllers  recurrent state vectors are fixed to     and   with the exception of experiment   described below  the sizes of the vectors placed on the stack are  fixed to  . after training is complete  each trained  model is evaluated on a testing set of      generated strings  each of which is at least roughly twice  as long as the strings used for training.    trials are  performed for each set of experimental conditions.  experiment   tests the propensity of trained  neural stack models to use the stack. we train  both the standard neural stack model and our enhanced buffered model from subsection  .  to perform the string reversal task using the linear controller. to compare the stack with unstructured  memory  we also train the standard neural stack  model using the lstm controller as well as an  lstm model without a stack. training and development data are obtained from sequences of  s  and  s randomly generated with an average length  of   . the testing data have an average length of    .  experiment   considers the xor evaluation  tasks. we train standard models with a linear controller on the delayed xor task and an lstm  controller on the cumulative xor task to test the  network s ability to use the stack as unstructured  state. we also train both a standard and a buffered  model on the cumulative xor evaluation task using the linear controller to test the network s ability to use our buffering mechanism to infer optimal  timing for computation steps. training and development data are obtained from randomly generated sequences of  s and  s fixed to a length of   .  the testing data are fixed to a length of   . the  vectors placed on the stack are fixed to a size of  .  in experiment    we attempt to perform the  parenthesis prediction task using standard models with various types of memory  a linear controller with no stack  which has no memory  a  linear controller with a stack  which has stackstructured memory  an lstm controller with no  stack  which has unstructured memory  and an  lstm controller with a stack  which has both  stack structured and unstructured memory.  sequences of well nested parentheses are generated by the cfg from the previous section. the  training and development data are obtained by randomly sampling from the set of strings of derivation depth at most    which contains strings of  length up to   . the testing data are of depth       and length up to    .  experiment   compares the standard models  with linear and lstm controllers against a baseline consisting of an lstm controller with no  stack. whereas experiments     presented the  network with tasks designed to showcase various features of the neural stack architecture  the  goal of this experiment is to gauge the extent  to which stack structured memory may improve  the network s performance on more sophisticated  tasks. we train the three types of models on the  boolean formula evaluation task and the subject   auxiliary agreement task. data for both tasks  are generated by the cfgs given in section  .  the boolean formulae for training and development are randomly sampled from the set of strings  of derivation depth at most    having a maximum  length of     while the testing data are sampled  from derivations of depth at most    with a maximum length of   . the sentence prefixes are of  depth    and maximum length    during the training phase  and depth    and maximum length     during the final evaluation round.         results    our results are shown in table  . the networks  we trained were able to achieve a median accuracy of at least   .   during the training phase  in    of the    experimental conditions involving  a stack augmented architecture. however  many  of these conditions include trials in which the  model performed considerably worse during training than the median. this suggests that while  stack augmented networks are able to perform our  tasks in principle  they may be more difficult to  train than traditional rnn architectures. note that  there is substantially less variation in the performance of the lstm networks without a stack.  in experiment    the standard network with  the linear controller performs perfectly both during the training phase and in the final testing  phase. the buffered network performed nearly  as well during the training phase  but its performance failed to generalize to longer strings. the  lstm network achieved roughly the same performance both with and without a stack  substantially worse than the linear controller. the leftmost graphic in figure   shows that the linear controller pushes a copy of its input to the stack and  then pops the copy to produce the output. as suggested by an anonymous reviewer  we also consid      task  reversal  reversal  reversal  reversal  xor  xor  xor  delayed xor  parenthesis  parenthesis  parenthesis  parenthesis  formula  formula  formula  agreement  agreement  agreement    buffered  no  yes  no  no  no  no  yes  no  no  no  no  no  no  no  no  no  no  no    controller  linear  linear  lstm  lstm  linear  lstm  linear  linear  linear  linear  lstm  lstm  linear  lstm  lstm  linear  lstm  lstm    stack  yes  yes  yes  no  yes  yes  yes  yes  yes  no  yes  no  yes  yes  no  yes  yes  no    min    .     .     .     .     .      .     .      .     .     .      .      .     .     .     .     .     .     .     med     .     .     .     .     .      .     .      .     .     .      .      .     .     .     .     .     .     .     max     .     .     .     .     .      .      .      .     .     .      .      .     .     .     .     .     .      .     min    .     .     .     .     .     .     .      .     .     .     .     .     .     .     .     .     .     .     med     .     .     .     .     .      .     .      .     .     .     .     .     .     .     .     .     .     .     max     .     .     .     .     .      .     .      .     .     .     .     .     .     .     .     .     .     .     table    the minimum  median  and maximum accuracy     attained by the    models for each experimental  condition during the last epoch of the training phase  left  and the final testing phase  right .    ered a variant of this task in which certain alphabet symbols are excluded from the reversed output. the center graphic in figure   shows that  for this task  the linear controller learns a strategy in which only symbols included in the reversed output are pushed to the stack. the rightmost graphic shows that lstm controller behaves  differently from the linear controller  exhibiting  uniform pushing and popping behavior throughout the computation. this suggests that under  our experimental conditions  the lstm controller  prefers to rely on its recurrent state for memory  rather than the stack  even though such a strategy  does not scale to the final testing round.  the models in experiment   perform as we expected. the unbuffered model with the linear controller performed at chance  in line with the inability of the linear controller to compute xor.  the rest of the models were able to achieve accuracy above   .   both in the training phase and in  the final testing phase. the buffered network was  successfully able to delay its computation in the  cumulative xor evaluation task. the leftmost  graphic in figure   illustrates the network s behavior in the delayed xor evaluation task  and  shows that the linear controller uses the stack as  unstructured memory an unsurprising observation given the nature of the task. note that the    vectors pushed onto the stack in the presence of  input symbol   vary between two possible values  that represent the current parity.  in experiment    the linear model without a  stack performs fairly well during training  achieving a median accuracy of   .  . this is because    .   of  s and  s in the training data are immediately followed by  s and  s  respectively  so it is  possible to attain   .   accuracy by predicting    and   when reading   and   and by always predicting   when reading   or  . linear models with the  stack perform better  but as shown by the rightmost graphic in figure    they do not make use of  a stack based strategy  since they never pop   but  instead appear to use the top of the stack as unstructured memory. the lstm models perform  slightly better  achieving      accuracy during  the training phase. however  the lstm controller  still suffers significantly in the final testing phase  with or without a stack  suggesting that the lstm  models are not employing a stack based strategy.  in experiment    the boolean formula evaluation task is performed easily  with a median accuracy exceeding   .   for all models both on  the development set and the testing set. this is  most likely because  on average  three quarters of  the nodes in a boolean formula either require no  context for evaluation  because they are atomic        symbols  linear controller  input          . . .    output  . . .             symbols  linear controller  input                . . .    output  . . .       . . .      symbols  lstm controller  input          . . .    output  . . .           figure    diagrams of network computation on the reversal task with linear and lstm controllers. in each  diagram  the input may consist of   or   distinct alphabet symbols  but only the symbols   and   are included in  the output. columns indicate the pop strengths  push strengths  and pushed vectors throughout the course of the  computation  along with the input and predicted output in one hot notation. lighter colors indicate higher values.    delayed xor  linear controller  input                output                  parenthesis  linear controller  input                      output                        figure    diagrams of network computation for the delayed xor and parenthesis tasks with a linear controller.    or make use of limited context  because they are  boolean formulas of depth one . the linear controller performed worse on average than the lstm  models on the agreement task  though the highestperforming linear models achieved a comparable  accuracy to their lstm counterparts. again  the  performance of the lstm networks is unaffected  by the presence of the stack  suggesting that our  trained models prefer to use their recurrent state  over the stack.         conclusion    we have shown in experiments   and   that it is  possible in principle to train an rnn to operate  a stack and input output buffers in the intended  way. there  the tasks involved have only one optimal solution  string reversal cannot be performed  without recording the string  and the linear controller cannot solve cumulative xor evaluation  without introducing a delay. in the other experi     ments  our models were able to find approximate  solutions that rely on unstructured memory  and  the stack augmented lstms always favored such  solutions over using the stack.  as we saw in experiments   and    training  examples that require full usage of the stack are  rare in practice  making the long term benefits of  stack based strategies unattractive to greedy optimization. however  the usage of a stack is necessary for a general solution to all of the problems  we have explored  with the exception of the xor  evaluation tasks. while gradual improvements in  performance may be obtained by optimizing the  usage of unstructured memory  the discrete nature  of most stack based solutions means that finding  such solutions often requires a substantial level of  serendipity. our results then raise the question of  how to incentivize controllers toward stack based  strategies during training. we leave this question  to future work.     