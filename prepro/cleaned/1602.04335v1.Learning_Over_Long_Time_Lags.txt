introduction  with the explosion of large data sets  known as  big data   the conventional machine learning methods have  hardship to process data for further processing and decision making tasks. the development of computational  machines such as cluster computers and graphical processing units  gpus  have facilitated advances in novel  machine learning methods  such as deep neural networks  anns .  the multi layer perceptron artificial neural networks  mlp anns  are promising methods for non linear  temporal applications     . deep learning in neural networks is a representation technique with the ability of  receiving input data and finding its representation for further processing and decision making. such machines  are made from non linear but simple units  which can provide different levels of data representation through  their multi layer architecture. the higher layers provide a more abstract representation of data and suppresses  irrelevant variations     . many naturally occurring phenomena are complex and non local sequences such as  music  speech  or human motion are inherently sequential    .  feed forward sequential memory networks  fsmn  model long term dependencies for time series data using  feed forward neural networks  ffnns . the memory blocks in fsmn use a short term memory mechanism.  these blocks in hidden layers use a tapped delay line structure to encode the long context information into  a fixed size representation     . the modified version of ffnns by adding recurrent connections is called  recurrent neural networks  rnns   which are capable of modelling sequential data for sequence recognition   sequence production  and time series prediction    .  the rnns are made of high dimensional hidden states with non linear dynamic     . the structure of  hidden states work as the memory of network. the state of the hidden layer at a time is dependent on its state at  previous time step     . this enables the rnns to store  remember  and process past complex inputs for long  time periods. the rnn can map an input sequence to the output sequence at the current time step and predict  the sequence in the next time step. the speech processing and predicting the next term in a language modelling  task are examples of this approach     .  development of gradient descent based optimization algorithms has provided a good opportunity for training rnn. the gradient descent is simple to implement and has accelerated practical achievements in developing  rnns     . however  gradient descent comes with some challenges and difficulties such as vanishing and exploding gradient problems  which are discussed in detail in the next sections.  the ability to learn long term dependencies through time is the key factor that distinguishes rnns from  other deep learning models. this paper provides an overview in this emerging field with focus on the recent  advances in memory models in rnns. we believe this article can facilitate the research path for new comers as  well as professionals in the filed.  the rest of the paper overviews the conventional rnns in section   and a basic model of long short term  memory  lstm  model in section  . recent advances in lstm are discussed in section   and the structurally  preprint submitted to arxive.org as a draft. the author welcomes reader s comments.    february              constrained rnns are reviewed in section  . the gated recurrent unit and memory networks are discussed in  section   and    respectively. the paper is concluded in section  .   . conventional recurrent neural network  y     output layer    y     y     yp  who    hidden layer    h     h     whh    h     h     hm    x     hm    x     xn    yp  who    h     h     whh  hm    y     y     x     x     t    h     h     hm  wih    xn    x     t       a  fodled srnn.    yp  who    wih    wih  input layer    y     whh    x   t      xn  time     b  unfolded srnn through time.    figure    a simple recurrent neural network  srnn  and its unfolded structure through time. to keep the figure  simple  biases are not shown.  a rnn is a ffnn with recurrent cycles over time. a simple rnn  srnn   also known as elman network   refers to a one step rnn            . as it is illustrated in figure  a  a folded srnn with one hidden layer  has a recurrent loop over its hidden layer. as the learning procedure proceeds through time  the weight of this  connection whh updates at every step  figure  b. the rnns are classified as supervised machine learning  algorithms    . to train such learning machines  we need a training dataset such as x and a disjoint test dataset  such as z. the training and test sets are made of input target pairs  where we train the network with the training  set x and evaluate it with the test set z. the objective of training procedure is to minimize the error between  the input target pairs by optimizing the weights of connections in the learning machine.   . . model architecture  a srnn is made of three type of layers which are input  hidden  and output layers. each layer is consisted  of some units  as shown in figure  a. the input layer is consisted of n input units  defined as a sequence of  vectors through time t. the vectors at each time step  ...  xt     xt   xt   ...  are consisted of n elements such as  xt    x    x    ...  xn  . the input units are fully connected to hidden units in the hidden layer in a srnn     .  the connections from input layer to hidden layer are defined with a weight matrix wih . the hidden layer is  consisted of m hidden units ht    h    h    ...  hm  . as figure  b shows  the hidden units are connected to each  other through time with recurrent connections. these units are initiated before training the machine  such that  can address the network state before seeing the input sequences     . in practice  non zero elements improve  overall performance and stability of the learning machine     . the hidden layer structure  i.e. the memory of  state space or  memory  of the system   is defined as   ht   fh  ot             ot   wih xt   whh ht     bh           where  and fh     is the hidden layer activation function and bh is the bias vector of the hidden units  . the hidden  units are connected to the output layer with weighted connections who . the output layer has p units yt     y    y    ...  yp   which are computed as   yt   fo  who ht   bo             where fo     is the activations functions and bo is the bias vector in the output layer. the input target pairs are  sequences through time and the above steps are repeated consequently over time t       ...  t  .  the rnns are dynamic systems consisted of certain non linear state equations  as in eqs.     and    . this  dynamic system iterates through time and in each time step  it receives the input vector and updates the current    the hidden state model in eq.   is sometimes mentioned as h   w  t  ih xt   whh fh  ht       bh   where both equations are  equivalent           .      .      .            .            .              net                         a  linear.      .              net                               b  piecewise linear.        .      .     out         out                 net                         c  tanh net .      .     out              out         out         out               .    .       .       .    .                        net     d  threshold.                                 net                        e  sin net  until saturation.             net     f  logistic      exp  net  .    figure    most common activation functions.  hidden states. the updated values are then used to provide a prediction at the output layer. the hidden state of  rnn is a set of values that summarizes all the unique necessary features about the past states of the machine  over a number of time steps. this integrated features help the machine to define future behaviour of the output  parameters at the output layer      . the non linearity structure in each unit of the rnn is simple. this structure  is capable of modelling rich dynamics  if it is well designed and trained with proper hyper parameters setting.   . . activation function  the non linear functions are more powerful than linear functions due to their ability to find non linear  boundaries. such non linearity in hidden layers of rnns empowers them to learn input target relationships.  this is while multiple linear hidden layers act as a single linear hidden layer in linear models                .  many activation functions are proposed in the literature. the hyperbolic tangent and logistic sigmoid function  are the most popular ones which are defined as   tanh x       e x      e x               and              e x  respectively. these two activation functions are equal as they are related such as     x         x       tanh x         .                     the rectified linear unit  relu  is another type of activation function    . the relu computes the output  as   y x    max x               which leads to sparser gradients and faster training     . a s shaped version of relu is recently proposed in       to learn both convex and non convex function.  selection of activation function is mostly dependent on the application. for example for networks where  the output is in the range         the logistic sigmoid is suitable. some of the most popular activation functions  are sigmoid  tanh  and relu functions. the layout of some activation function is presented in figure  . the          sigmoid function is a common choice  which takes a real value as input and squashes it to the range of       .  the softmax function is normally used in the output layer for classification applications  where a cross entropy  approach is used for training. a detailed analyze of different activation functions is provided in     .   . . loss function  this error in rnns is defined by a loss function  that is generally a function of estimated output yt and the  target value zt at time t   t  x  l y  z     lt  yt   zt         t      which is an overall summation of losses in each time step     . some popular loss function are euclidean  distance  hamming distance  and maximum likelihood  where the proper loss function is mostly selected based  on the application and nature of data sequence.   . . back propagation through time  bptt   back propagation through time  bptt  is a generalization of back propagation for ffnns      . this  method is originated from optimal control theories such as automatic differentiation in the reverse accumulation  mode and pontryagin s minimum principle for optimal control of dynamical systems from one state to another  under constraints     . the standard bptt method for learning rnns  unfolds  the network in time and  propagates error signals backwards through time  figure  b.  by considering the network parameters as the set      whh   wih   who   bh   bi   bo   and ht as the  hidden state of network at time t  we can write the gradients as    l x  lt     .          t    t           the expansion of loss function gradients at time t is   x  lt  ht  h    lt  .  . k              ht  hk     t            k       h     where   k is the partial derivative   immediate  partial derivative  which describes how the parameters in the  set   affect the loss function at the previous time steps  i.e. k   t . in order to transport the error through time  from time step t back to time step k we have   t  y   ht   hi      hk   hi              i k      so that the above term can be seen as a jacobian matrix for the hidden state parameters in eq.    such as   t  y  i k      t  y      hi     wthh diag fh  hi        hi              i k           where the f     is the element wise derivative of function f     and diag    is the diagonal matrix.   . . vanishing gradient problem  the most popular method for optimizing connection weights with respect to the loss function is gradient  descent. the gradients of rnn are computationally cheap  particularly with bptt     . however  training  rnns using bptt to compute error derivatives has some challenges     . it backs to the unstable relationship  between the dynamics and parameters of rnn that makes gradient descent ineffective.  training rnns with gradient descent has some difficulties in learning long range temporal dependencies           . one reason is exponential decay of gradient while back propagating through time  called vanishing  gradient problem. the vanishing gradients problem refers to the exponential shrinking of gradients magnitude  as they are propagated back through time     . this phenomena causes the memory to ignore long term dependencies and hardly learn the correlation between temporally distant events. there are two reasons for that            standard non linear functions such as sigmoid function have a gradient which is almost everywhere close  to zero     the magnitude of gradient is multiplied repeatedly by the recurrent matrix as it is back propagated  through time     . in this case  when the eigenvalues of the recurrent matrix become less than one  the gradient  converges to zero rapidly. this happens normally after      steps of back propagation     .  when the rnn is under training on long sequences  e.g.     time steps   the gradients shrink when the  weights are small. product of a set of real numbers can shrink explode to zero infinity  respectively. in algebra  we have the same rule for the matrices instead of real numbers along some direction. by considering   as the  spectral radius of the recurrent weight matrix whh   in the case of long term components  it is necessary for           to explode as t          . singular values can generalize it for to the non linear function fh     in eq.      by bounding it with     r such as        diag fh  hk         .        by considering the bound in eq.       eq.       and the jacobian matrix         hk     hk        hk           wthh   .  diag fh  hk         .   hk    for  k we have            h               such as     r for each step k. by continuing it  from the other side  we can consider     hk    k  over different time steps and adding the loss function component we can have    lt y  hi     lt             t k          ht   hi   ht  t                    i k    this equation shows that as t   k gets larger  the long term dependencies move toward zero and the vanishing  problem happens. finally  we can see that the sufficient condition for the gradient vanishing problem to appear  is that the largest singular value of the recurrent weights matrix whh  i.e.      satisfy             . pascanu  and et. al. have analyzed the problem in more detail     .   . . exploding gradient problem  another major problem in training rnn using bptt is the exploding gradient problem          . when  the rnn is under training on long sequences  e.g.     time steps   the gradients explode when the weights are  big. the exploding gradient problem refers to the explosion of long term components due to the large increase  in the norm of the gradient during training sequences with long term dependencies. as it is stated in       the  necessary condition for this problem to happen is         .  in order to overcome the exploding vanishing problem  many methods have been proposed recently. in        mikolov proposed a gradient norm clipping method to avoid the exploding gradient problem           .  this approach made it possible to train rnn models with simple tools such as bptt and stochastic gradient  descent on large datasets. in a similar approach  pascanu has proposed an almost similar method to mikolov   by introducing a hyper parameter as threshold for norm clipping the gradients whenever required     . this  parameter can be set by heuristics  however  the training procedure is not very sensitive to that and behaves  well for rather small thresholds     . the performance results on penn treebank dataset shows that as both the  training and test error improve in general  the clipping gradients solves an optimization issue and does not act as  a regularizer. comparing to other models  this method can manage very abrupt changes in norm. the approach  in      has presents a better theoretical foundation  however  both approaches behave similarly and perform as  well as the hessian free trained model           .   . long short term memory  the long short term memory  lstm  model was proposed in      to deal with the vanishing gradient  problem     . the lstm model changes the structure of hidden units from logistic or tanh to memory cells.  gates control flow of information to hidden neurons by controlling inputs and outputs of memory cells. the  gates are logistic units with learned weights on connections coming from the input and also the memory cells at  the previous time step     .  rnns equipped with lstm architecture struggle when receive very long data sequences. process of such  sequences is time consuming for lstm model and places high demand of memory on the network. many  attempts with focus on lstm have been made to increase its performance and speed. in a later version of          y     y     yp    lstm memory  block      x     lstm memory  block      x     xn    figure    a lstm network with n inputs  p outputs  and one hidden layer. the hidden layers is consisted of  two lstm memory blocks. for simplicity  only connections for the lstm memory block   is shown.  lstm  a forget gate is added to the original structure. this gates learn weights to control the decay rate of  analogue value stored in the memory cell           . if the forget gate does not apply decay and the input and  output gates are off during different time steps  the memory cell can hold its value. therefore  the gradient of  the error with respect to the memory value stays constant     .  despite of major success of lstm  this method suffers from high complexity in the hidden layer. for  identical size of hidden layers  the lstm has about four times more parameter than srnn model     . this is  understandable  since the objective at the time of proposing this method was to introduce any scheme that could  learn long range dependencies rather than to find the minimal or optimal scheme     . the other challenge that  faces lstm is learning long data sequences. this problem is partly met by developing multi dimensional and  grid lstm networks  which are discussed in the next section.  the lstm method has achieved impressive performance in learning long range dependencies for hand  writing recognition       handwriting generation       sequence to sequence mapping       speech recognition              and phoneme classification     .   . . standard lstm architecture  a lstm network with two memory cells is presented in figure        . this network has n inputs  p  outputs  and one hidden layer  i.e. consisted of the memory cells . each memory cell is consisted of four inputs  but one output  figure  . a typical lstm cell is made of input  forget  and output gates and a cell activation  component     . this units receive the activation signals from different sources and control the activation of the  cell by the designed multipliers. the lstm gates can prevent the rest of the network from changing the value  of the memory cells for multiple time steps. therefore  lstm model can preserve signals and propagate errors  for much longer than srnns. such properties allow lstm networks to process complex data with separated  interdependencies. the forget gate multiplies the previous state of the cell. this is while the input and output  of the cell are multiplied by the input and output gates     . the cell does not use any activation function. the  gate activation function is usually the logistic sigmoid function. therefore  the gate activations behave between  zero and one which represent gate close and gate open  respectively     . the cell input and output activation  functions are usually tanh or logistic sigmoid. in some cases the cell activation gate s function is the identity  function.  the input gate in lstm is defined as        c  gti     wigi xt   whgi ht     wgc gi gt      bg i              where wigi is the weight matrix from the input layer to the input gate  whgi is the weight matrix from hidden  state to the input gate  wgc gi is the weight matrix from cell activation to the input gate  and bgi is the bias of  the input gate. the forget gate is defined as   c  gtf     wigf xt   whgf ht     wgc gf gt      bg f              where wigf is the weight matrix from the input layer to the forget gate  whgf is the weight matrix from        ff       output gate  fh       forget gate    cell    ff       ff       input gate  fg       figure    the lstm memory block with one cell.  hidden state to the forget gate  wgc gf is the weight matrix from cell activation to the forget gate  and bgf is the  bias of the forget gate. the cell gate is defined as   c  gtc   gti tanh wigc xt   whgc ht     bgc     gtf gt              where wigc is the weight matrix from the input layer to the cell gate  whgc is the weight matrix from hidden  state to the cell gate  and bgc is the bias of the cell gate. the output gate is defined as   gto     wigo xt   whgo ht     wgc go gtc   bgo              where wigo is the weight matrix from the input layer to the output gate  whgo is the weight matrix from  hidden state to the output gate  wgc go is the weight matrix from cell activation to the output gate  and bgo is the  bias of the output gate. the hidden state is computed as   ht   gto tanh gtc  .             . advances in long short term memory  in this section  we discuss recent advances in developing lstm based models and other mechanisms for  learning long term dependencies of data.   . . s lstm  the s lstm model is designed to overcome the gradient vanishing problem and learn longer term dependencies from input  compared to the lstm network. a s lstm network is made of s lstm memory blocks  and works based on a hierarchical structure. a typical memory block is made of input and output gates. in  this tree structure  the memory of multiple descendant cells over time periods are reflected on a memory cell  recursively. refer to      for more details about s lstm memory cell.  the s lstm learns long term dependencies over the input by considering information from long distances  on the tree  i.e. branches  to the principal  i.e. root . in practice  a gating signal is working in the range of         enforced with a logistic sigmoid function.  the s lstm method can achieve competitive results comparing with the recursive and lstm model. however  its performance is not not compared with other state of the art lstm models. the s lstm model has the  potential of extension to other lstm models.   . . stacked lstm  stack of multiple layers in ffnns results in a deep ffnn. the same idea is applicable to lstms by  stacking different hidden layers of hidden layers with lstm cells in space     . this deep structure of lstm  increases the network capacity           . in stacking  the same hidden layer in eq.     is used but for l layers  such as   hlt   fh  wi l   h l htl     whl hl hlt     blh                y t       y t     y t       h t       h t     h t       h t       h t     h t       x t       x t     x t       figure    unfolded through time bi directional recurrent neural network  brnn .  where the hidden vector sequence hl is computed over time t      ...  t for l      ...  l. the initial hidden  vector sequence is defined using the input sequences h     x    ...  xt       . the output of network is then  computed as   yt   fo  wh l o hl  t   b               combination of stack of lstm layers with different rnn structures for different applications needs investigation. one example is combination of stack of lstm layers with frequency domain convolutional neural  networks          .  in stacked lstm a stack pointer can determine which cell in the lstm provides state and memory cell  of previous time step     . in such control structure for sequence to sequence neural networks  not only the  controller can push to and pop from the top of the stack in constant time but also an lstm maintain a continuous  space embedding of the stack contents          .   . . bidirectional lstm  the conventional rnns are only considering previous context for training. this is while in many applications such as speech recognition it is useful to explore the future context as well     . bidirectional rnns   brnns  utilize two separate hidden layers at each time step. the network processes input sequence once in  the forward direction and once in the backward direction as presented in figure  .            in figure    the forward and backward hidden sequences are denoted by h t and h t at time t  respectively.  the forward hidden sequence is computed as             ht   fh  w         ih    xt   w   h t     b     hh    h            where it is iterated over t      ...  t . the backward layer is             ht   fh  w         ih    xt   w   h t     b     hh    h            which is iterated over time t   t  ...     i.e. backward over time . the output sequence yt at time t is computed  as         yt   w  ht   w  ht   bo .        ho    ho    bptt is one option to train brnns  however  the forward and backward pass operation is slightly more  complicated because the update of state and output neurons can no longer be done one at a time     . the other  shortcoming of such networks is their design for input sequences with known starts and ends  such as spoken  sentences to be labelled by their phonemes           .  it is possible to increase capacity of brnns by developing its hidden layers in space using stack hidden  layers with lstm cells  called deep bidirectional lstm  blstm      . bidirectional lstm networks are  more powerful than unidirectional ones     .  blstm rnn theoretically associates all information of input sequence during computation. the distributed  representation feature of blstm is crucial for different applications such as language understanding     .          a model is proposed in      to detect steps and estimate the correct step length for location estimation and  navigation applications.  the sliding window approach in      allows online recognition as well as frame wise randomization for  speech transcription applications. it results in faster and more stable convergence     .   . . multidimensional lstm  the classical lstm model has a single self connection which is controlled by a single forget gate. its activation is considered as one dimensional lstm. the multi dimensional lstm  mdlstm  uses interconnection  from previous state of cell to extend the memory of lstm along every n dimensions           .  the mnlstm receives inputs in a n dimensional arrangement  e.g. an image . hidden state vectors  h    ...  hn and memory vectors m    ...  mn are fed to each input of the array from the previous state for each  dimension. the memory vector at each step t is computed as   m     n  x    gjf   mj   gju   gjc            j      where the gates are computed using eq.     to eq.    .  spatial lstm  slstm  is a particular case of mdlstm     . slstm is a two dimensional grid for image  modelling. this model generates hidden state vector for a particular pixel by sequentially reading the pixels in  its small neighbourhood     . the state of the pixel is generated by feeding the state hidden vector into a  factorized mixture of conditional gaussian scale mixtures  mcgsms      .   . . grid lstm  the mdlstm model becomes unstable  as the gird size and lstm depth in space grows. the grid lstm  model provides a solution by altering the computation of output memory vectors. this method targets deep  sequential computation of multi dimensional data. the model connects lstm cells along the spatio temporal  dimensions of input data and between the layers.  unlike the mdlstm model  the block computes n transforms and outputs n hidden state vectors and n  memory vectors. the hidden sate vector for dimension j is        hj   lst m  h  mj   wju   wjf     wjo   wjc              where lst m is the standard lstm procedure and h is concatenation of input hidden state vectors   h    h    ...  hn  t .            a two dimension grid lstm network adds lstm cells along the spatial dimension to a stacked lstm. a  three or more dimensional lstm  is similar to mslstm  but has added lstm cells along the spatial depth and  performs n  way interaction. more details on grid lstm are provided in     . a key advantage of grid lstm  is that its depth  size of short term memory  and number of parameters are not confounded and independently  tunable. this is while number of parameters in lstm grows quadratically with the size of its short term memory      .   . . differential recurrent neural networks  the gates in conventional lstm indecisively consider the dynamic structure of input sequences. this results  in not capturing importance of spatio temporal dynamic patters in noticeable motion patterns     . differential  rnn  drnn  refers to detecting and capturing of important spatio temporal sequences to learn dynamics of  actions in input      . such lstm gate monitors alternations in information gain of important motions between successive frames. this change of information is detectable by computing the derivative of hidden states   dos  at time step t such as  dht   dt. a large dos reveals sudden change of actions state. this means the  spatio temporal structure contains informative dynamics. in this situation  the gates in figure   allow flow of  information to update the memory cell. small dos keeps protects the memory cell from any affect by the input.  to be more specific  the unit controls the input gate unit as   r   r   x  ht     r       wgi o yt     wgi i xt   bgi     gti       wg i d   t r   r                    z t     x t     z t      t     s  dns t     dtn    i t     dns t   dtn    o t     f t     figure    architecture of the drnn model at time t. the input gate i and the forget gate f are controlled by the  dos at times t     and t respectively.  the forget gate unit as   r   r   x  ht     r     gtf       wg f d    wgf o yt     wgf i xt   bgf      r    t  r              and the output gate unit as  r   r   x  ht   r     gto       wgo d  r    wgo o yt     wgo i xt   bgo     t  r              where the dos has an upper order limit of r. training of drnns is performed using bptt. this model is  examined for   order and   order drnns  where better performance is reported compared with the conventional  lstm on the msr action d dataset. however  more experiments for different application is necessary. the  dos seems to be a valuable intelligence about structure of input sequence.  an other version of differential rnns  drnns  is proposed in      to learn the dynamics described by the  differential equations. however  this method is not based on lstm cells.   . . highway networks  the idea behind highway networks is to facilitate training of very deep neural networks using an adaptive  gating method. this kind of lstm inspired network helps unlimited information flow across many layers on  information paths  highways       . a key contribution is training of very deep highway networks with sgd.  plain networks  i.e. networks that apply non liner transform on its input  are hard to optimize in large depths      . despite plain networks  specific non linear transformations and derivation of a suitable initialization  scheme is not essential in highway networks     .  the high networks could successfully train ffn with up to     layers of depth     . experimental results  show well generalization of highway networks to unseen data.  the highway networks along with convolutional neural networks  cnns  are examined for neural language  modelling in     . the results show that using more than two highway layers does not improve the performance.  one of the challenges is using appropriate number of layers with respect to the input data size.   . . other lstm models  the local global lstm  lg lstm  architecture is initially proposed for semantic object parsing     . the  objective is to improve exploitation of complex local  i.e. neighbourhood of a pixel  and global  i.e. whole  image  contextual information on each position of an image. the current version of lg lstm has appended  a stack of lstm layers to intermediate convolutional layers. this technique directly enhances visual features  and allows an end to end learning of network parameters     . performance comparison of lg lstm with a  variety of cnn models on three public datasets show high test scores     . it is expected that by this model can  achieve more success by replacing all convolutional layers with lg lstm layers.  the matching lstm  mlstm  is initially proposed for natural language inference. the matching mechanism stores  remembers  the critical results for the final prediction and forgets the less important matchings         y t   h t     r    s t          x t   figure    recurrent neural network with context features  longer memory .      . the last hidden state of the mlstm is useful to predict the relationship between the premise and the  hypothesis. the difference with other methods is that instead of a whole sentence embeddings of the premise  and the hypothesis  the slstm performs a word by word matching of the hypothesis with the premise     .  the proposed model in      considers the recurrence in both time and frequency  named f t lstm. this  model generates a summary of the spectral information by scanning the frequency bands using a frequency  lstm. then  it feeds the output layers activations as inputs to a lstm. the formulation of frequency lstm  is similar to the time lstm     .  a convolutional lstm  convlstm  model with convolutional structures in both the input to state and  state to state transitions for precipitation now casting is proposed in     . this model uses a stack of multiple  convlstm layers to construct an end to end trainable model     .   . structurally constrained recurrent neural networks  the fact that the state of the hidden units changes fast at every time step is used to propose a novel approach  to overcome vanishing gradient descent problem by learning longer memory and learn contextual features using stochastic gradient descent  called structurally constrained recurrent network  scrn        figure  . in  this approach  the srn structure is extended by adding a specific recurrent matrix equal to identity to detect  longer term dependencies. the fully connected recurrent matrix  called hidden layer  produces a set of quickly  changing hidden units while the diagonal matrix  called context layer  support slow change of the state of the  context units     . in this way  state of the hidden layer stays static and changes are fed from external inputs.  even though this model can prevent gradient of the recurrent matrix vanishing  but is not efficient in training      . in this model  for a dictionary of size d  st is the state of the context units which is defined as   st          bxt    st              where   is the context layer weight  normally set to  .    bd s is the context embedding matrix  and xt is the  input. the hidden layer is defined as   ht     p st   axt   rht                where ad m is the token embedding matrix  pp m is the connection matrix between hidden and context layers   rm m is the hidden layer  ht     weights matrix  and   .  is the sigmoid functions defined as      .      exp x             yt   f  u ht   v st                x     finally  the output yt is defined as     where f is the soft max function  u and v are the output weight matrices of hidden and context layers  respectively. it is interesting that there is no non linearity applied to the state of the context units in this model.           c    h    h  c    input  output    figure    gated recurrent unit  gru .  in adaptive context features the weights of the context layer are learned for each unit to capture context from  different time delays. the analysis shows as long as the standard hidden layer is utilized in the model  learning  of the self recurrent weights does not seem to be important. this is while fixing the weights of the context  layer to be constant  forces the hidden units to capture information on the same time scale. the scrn model  is evaluated on penn treebank dataset. the presented results in      show that the scrn method has bigger  gains over stronger baseline comparing to the proposed model in    . also  the learning longer memory model  claims that it has similar performance  but with less complexity  comparing to the lstm model     .  although the scrn model can prevent gradient of the recurrent matrix vanishing  but is not efficient in  training. the analyze of using adaptive context features  where the weights of the context layer are learned for  each unit to capture context from different time delays  shows that learning of the self recurrent weights does  not seem to be important  as long as one uses also the standard hidden layer in the model     . this is while  fixing the weights of the context layer to be constant  forces the hidden units to capture information on the same  time scale.   . gated recurrent unit  a gated recurrent unit  gru  is proposed in     to make each recurrent unit to adaptively capture dependencies of different time scales. both the lstm unit and the gru utilize gating units. these units without  using a separate memory cell modulate the flow of information inside the unit     . block diagram of a gru is  presented in figure  . the activation in a gru is linearly modelled as   ht        zt  ht     zt h t            zt     wz xt   uz gt    .            where the update gate zt is defined as     the update gate controls update value of the activation. the candidate activation is computed as  h t   tanh wxt   u  rt   ht                 where rt is a set of rest gates computed as   rt     wr xt   ur ht                where it allows the unit to forget the previous state by reading the first symbol of an input sequence.  similar to lstm  the gru computes a linear sum between the existing state and the newly computed state   however  the gru exposes the whole state at each time step    .  the graph neural networks  gnns  are proposed for feature learning of graph structured inputs           .  the gru is used in      to modify the graph neural networks  called gated graph sequence neural networks   ggs nns . this modified version of gnn unrolls the recurrence for a fixed number of steps and uses bptt  in order to compute gradients     .            . memory networks  the conventional rnns have small memory size to store and remember facts from past inputs           .  memory networks  memnns  utilizes successful learning methods for inference with a readable and writable  memory component. a memnn is consisted of input  response  generalization  and output feature map components           . this networks is not easy to train using backpropagation and requires supervision at each layer      . a less supervision oriented version of memnns is end to end memnns  which can be trained end to end  from input output pairs     . it generates an output after a number of time steps and the intermediary steps use  memory input output operations to update the internal state     . the memnn is a promising research pathway  and needs for establishment.  recurrent memory network  rmn  takes advantage of the lstm as well as memory network     . the  memory block takes the hidden state of the lstm and compares it to the most recent inputs using an attention  mechanism. the rmn algorithm analyses the attention weights of trained model and extracts knowledge from  the retained information in the lstm over time     . this model is developed for language modelling and is  tested on three large datasets. the results show performance of the algorithm versus lstm model  however  it  needs more development.  the episodic memory is inspired from semantic and episodic memories  which are necessary for complex  reasoning in brain     . the episodic memory is named as the memory of the dynamic memory network  framework developed for natural language processing     . the memory refers to the generated representation  from some facts. the facts are retrieved from the inputs conditioned on the question. this results in a final  representation by reasoning on the facts. the module performs several passes over the facts  while focusing on  different facts. the output of each pass is called an episode  which is summarized into the memory     .  a relevant work to memnns is the dynamic memory networks  dmns . the memnns in      focus on  adding a memory component for natural language question answering     . the generalization and output  feature map parts of the memnns have some similar functionalities with the episodic memory in dmss. the  memnns process sentences independently     . this is while the dmss process sentences via a sequence  model     . the performance results on the facebook babi dataset show the dmn passes    task with accuracy  of more than     while the memnn passes    tasks     .   . conclusion  one of the main advances in rnns is the long short term memory  lstm  model. this model could  enhance learning long term dependencies in rnns drastically. the lstm model is the core of many developed  models in rnns  such as in bidirectional rnns and grid rnns. in addition to the introduced potential further  research pathways in the paper  other research models are under development using memory networks and gates  recurrent units. these models are recent and need further development in core structure as well as for specific  applications. the primary results on different tasks show promising performance of these models.  