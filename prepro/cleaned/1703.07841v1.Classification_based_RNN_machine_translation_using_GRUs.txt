introduction    machine translation is an increasingly important field of study  as the need for natural language translation is rapidly  rising in a world that is growing increasingly multilingual and interconnected. at the same time  machine translation is  an incredibly difficult task whose problems span a range of linguistic fields and is difficult even for human translators.  previous efforts have been centered on statistical machine translation  but the continuous and exponential increase in  computing power and advances in neural network training have enabled researchers to use recurrent neural networks  to approach this problem.  currently  neural networks such as lstms and bidirectional rnns are trained for this purpose. however  methods  described in many of these research papers are relatively complex and frequently non intuitive. the approach taken  by feed forward neural network researchers  however  involves using the neural network as a classifier to predict  probabilities  and is a simpler and more attractive framework to build upon. in this work  we attempt to remove the  common constraint in feed forward networks that require input sequences to have a fixed length. instead  we propose  to build a relatively simple gru neural network model  which both allows variable length input and is able to act as a  classifier applied to the task of machine translation.         background and related work    the approach for translation using recurrent neural networks was mainly based on the model given by sutskever et.  al.      of recursively inputting output into input to generate a translation. in their paper  they trained pairs of long  short term memory models  lstm s   one lstm served as a feature extractor to convert a variable length sentence  into a fixed length vector such that sentences with similar meanings are mapped close together  and the other lstm  takes the word vectors as hidden states and proceeds to output words one at a time  incrementally building up the  translation. it was not entirely clear how this pair of lstm training was done in the context of back propagation   and we decided to focus on their alternative model of having a single rnn to perform both feature extraction and  translation. the model we used was also very closely related to that of cho et al.      which introduced the gru and  applied it as an rnn encoder decoder machine translation model as an alternative to lstm.  however  the classification approach taken here is closer related to previous work done by schwenk       with feedforward neural networks that take in a fixed input sentence and output the probability of a sequence of words occurring  next  as well as the work of devlin et al.  who only predicted the next word    . the difference in our approach is that        the gru used here can take in a variable length sentence  and is much more flexible than the models by schwenk and  devlin et al.  which were restricted to only accepting input of a pre specified fixed length.         basic framework of the learning models    the key ingredient for training a machine translation model is a large corpus of sentences in the source language that  has been correctly translated into sentences in the target language. for a powerful translator  this training corpus must  be very large and the translations must be perfectly accurate.  there are other specifications and requirements that need to be met as well. the model requires both source and target  languages to draw from fixed vocabulary sets  in this work  we restrict both the input and output vocabulary sets to  each consist of        distinct tokens  representing the        most frequently appearing words in the training set for  each language  along with   additional tokens added to represent the not found token and the end of sentence token.  to decrease the number of unique words  all words were converted into lowercase  but were neither stemmed nor  lemmatized. this is because grammatical inflections in words are very important to overall translation quality  as they  carry heavy semantic value that needs to be preserved during translation.  prior to use in the model  words were converted into word vectors using a standard continuous word to vector embedding    . the translation model would eventually be trained to recognize that semantically similar words should be  weighted to carry similar representations  and using word embeddings that already indicate semantic similarity from  the outset should reduce a lot of training effort on the part of the translation model. furthermore  we expect higher  quality word embedding from specialized representation models  such as glove      than any word embedding built  into the translation neural network such as the one used by cho et al.      because they are trained on a much larger  dataset and their linguistic capabilities have been proven to be very strong.  the training was based on stochastic gradient descent with mini batches of     sentence pairs each. to take full  advantage of gpu parallelization  batches were prepared so that within each batch bi   all of the source language input  sequences consisted of the same number of tokens fi   and all of the target language output sequences consisted of the  same number of tokens ei   although it was not necessary that fi be the same as ei for any given batch. this condition  was enforced to prevent part of the batch dropping out of the back propagation gradient calculations midway  in the  event that the some sequences reached completion before other sequences in the batch. if there were not enough  sentence pairs in the training set corresponding to a given pair of lengths  fi   ei   to fill a minibatch of size      then  the entire batch bi was dropped from the dataset altogether  this measure filtered out a large number of small batches   which had been observed to learn much less than full size batches  despite taking almost the same amount of time  to train. additionally  both fi and ei were hard capped with a maximum of     because the neural network would  have trouble handling translations between lengthy sentences of longer than    tokens in either the source or target  language.         model    in this section  we describe the procedure that the deep learning model is used to perform machine translation  once  the preprocessing and setup from section   has been implemented.   .     automatic machine translation    after the input text in the source language has been represented as sequences of word embedding vectors of fixed  length  our translation model works as follows   for a given sequence of input word vectors  x    . . .   xf   encoding a sentence in the source language  as well as the  sequence of output word vectors  y    . . .   yk   encoding the tokens in the translated sentence in the target language so  far  the neural network estimates the conditional probability of the word vector representation of the next word in the  translated sequence yk   . thus  part of the input sequence to the neural network would be a complete sequence of  source language word vectors and the remainder of the input would be a sequence in progress consisting of targetlanguage word vectors.  the output vector of this neural network is a vector of length         with components corresponding to each of the  words in the target language vocabulary  along with the two special tokens representing not found and end of sentence.        the entry at position i in this vector  for     i            is a real valued number between   and   representing the  probability that the ith word in the target language vocabulary set  vi   is the next word in the translation sequence.  if the neural network is treated as a function f   then its operation is defined as   f   x    . . .   xf     y    . . .   yk       p  yk     vi   x    . . .   xf     y    . . .   yk    i   ...         as the translation is built up word by word  the neural network can be treated as a classifier that takes in a sequence  of input vectors and predicts the  class   or vocabulary word  that should appear next in the sequence.  finally  to calculate the conditional probability of obtaining the output translation sentence y    . . .   ye given the input  sentence word sequence x    . . .   xf for the purpose of translation  the probability may be decomposed into a product  of conditional probabilities that the neural network is able to compute     p   y    . . .   ye    x    . . .   xf        e    y    p  yk     x    . . .   xf     y    . . .   yk       k      the predicted translation is then the sentence with the highest overall conditional probability  where the probabilities  may be obtained by calculating each term in the above product using the neural network. at each stage  the input is  the source sentence sequence  x    . . .   xf   and the incrementally built up target sequence  y    . . .   yk  .   .     the neural network model  grus    the main contribution of deep learning to the task of automatic machine translation  in the approach taken by this work   is the computation of the conditional probabilities for the next translated word given the progress of the translation so  far. this approach has been applied using various specializations of neural networks for the task  including long shortterm memory networks     and bidirectional recurrent neural networks    . in this work  we explore the possibility of  using gated recurrent units in the model.  a gated recurrent unit  gru  was initially proposed by cho et. al.     for their translation task. the idea behind this  approach is to keep the current content at each step in the neural network  and add the new contents to that  while  forgetting any past information that no longer adds additional information to the present state. unlike the lstm  approach that define a separate memory cell  the gru resets a gate and stores only the relevant new content to the  unit. the two approaches of gru and lstm in handling variable length inputs are presented for contrast in figure  .  given an input sequence x    x    x    ...  xt    the standard rnn computes the activation by  ht   a w xt   u ht      where a is a smooth activation function such as hyperbolic tangent function. the gru computes an update gate as  ztj     j  wz xt   uz ht      this update gate is the sum of current state at t and the recent state which controls the amount of previous memory to  be kept. in the lstm  the update gate has an additional component that is the memory cell at time t. similar to the  update gate  a reset gate is computed based on the current input vector and hidden state  but with different weights   rtj     j  wv xt   uv ht      now the activation is computed as  j  h t   tanhj  w xt   u  rt    ht         where represents pointwise multiplication. if the reset gate is zero  then the previous state will be ignored and only  the current input is used for updating. the final output at time t is computed as a weighted average of h t and ht  ht   zt    ht          zt         h t     figure    long short term memory  left  and gated recurrent units  right  gating. c and c  stand for the memory cell  and the new memory cell content in lstm. r and z are the reset and update gates  and h and h  are the activation and  the candidate activation in gru. figure copied from chung  j  et al.  empirical evaluation of gated recurrent neural  networks on sequence modeling    .         training methods and implementation    for this work  we made use of the publicly available europarl  corpus      with french as the source language and  english as the target language. this dataset contains about two million instances of english and french sentence pairs  across a large variety of sentence lengths. translation was done from french to english because this report is intended  for an english speaking audience  and a model with english language output allows us to assess the quality of the  translation directly and independently. furthermore  this approach distinguishes our work from that of many other  papers in the field  where english is frequently taken to be the source language.  for the english word vector embedding  we used the pre trained model made publicly available by the researchers for  glove on their website at http   nlp.stanford.edu projects glove . for french  however  a pre trained embedding was  not conveniently available  so a model was built using glove as part of this work  trained using the entire corpus of  french language wikipedia. although this did provide a workable word embedding for use in the translation model   the size of the training set for these french word embeddings was limited to only the half a billion tokens that were  available  in contrast with the    billion tokens that were used to train the provided english word embeddings. in both  cases  the representations that were returned belong to a     dimensional vector space.  training was performed using batched stochastic gradient descent  where each batch contained about     sentences.  the model was set to have   layers  in light of the success observed by sutskever et. al       however  the number of  nodes in the present model is set at     per layer  in contrast with the      used by the earlier work  in order to speed  up training and reduce potential overfitting.  in this work  the focus was on training the neural network to work as an effective classifier  which is the key component  of the translation framework laid out in section  . . there were two ways to approach this in the context of automatic  translation  as outlined in the following sections.   .     method      in this training method  the focus is entirely upon building a strong classifier under the belief that everything has gone  perfectly up to the current position in the translation. given a sequence of french input word vectors  x    . . .   xf   and  the corresponding sequence of correct english translation word vectors  y    . . .   ye    this method generates predicted       the european parliament dataset  restricted largely to political topics and certain styles of speaking  is not representative of  speech as a whole  which means the model might not be able to generalize well under other circumstances. however  it is not easy  to obtain high quality translation data  and we have to make do with what is available.          translation words y k one at a time  but uses the ground truth translations instead of its own predictions every time it  looks for the next word in the sequence    . input    x    . . .   xf         output  y     expect  y    . input    x    . . .   xf     y      output  y     expect  y   ...   . input    x    . . .   xf     y    . . .   ye       output  y e   expect  ye  this model is built upon high quality input and should therefore act a powerful classifier. however  since it is only  trained upon flawless input data  it might encounter difficulties in practice if any word happens to be mistranslated partway through a sequence  as the model does not have experience correcting itself from a partially incorrect translation  sequence.   .     method      the focus of the second training method is to build a classifier that is able to handle robustly correct itself from  intermediate translation errors. as in the previous approach  this method generates predicted translation words y k  one at a time  but now uses the model s own predicted translation  rather than the ground truth  to predict the next  translated word in the output sequence    . input    x    . . .   xf         output  y     expect  y    . input    x    . . .   xf     y       output  y     expect  y   ...   . input    x    . . .   xf     y     . . .   ye          output  y e   expect  ye  this model is much more practical and robust than the previous one  as it is trained on its own output  much like in the  situations where the model is applied in practice  as a result  there should be a much smaller difference between test  error and training error  and the model will perform better in real applications.  the major disadvantage with this approach is that training is a much more difficult and time consuming task. for  much of the early part of training  the output predicted words will be close to random and the neural network will have  to learn how to use this random noise to predict the next correct word. not only would each step in the prediction  process be slower during the training period  but the required number of training iterations would rise by orders of  magnitude  compared with the optimistic approach of method  .   .     technical details    some of the specific decisions made regarding the implementation of model training for this work are described below.   . intial weights were set using the glorot initialization     with weights sampled from the uniform distribution.  this is designed to set weights that are neither too small nor too large  so as to avoid vanishing or exploding  signals.   . we did not expect many issues with vanishing gradients  since the gru based model is generally able to  safely handle this problem. to avoid the unlikely risk of exploding gradients  a hard clip for gradient values  was kept at    .   . the entire collection of batches is shuffled at the start of every training epoch. training then proceeds stochastically through the sequence of batches. there were about      batches in total and a single epoch took      hours using a single nvidia     ti processor.   . the training error for each word prediction in the sequence was computed by taking the cross entropy between  the predicted vector of conditional probabilities vector produced by the neural network  and the ground truth  vector which has a value of   at the index of the correct next word and   for all other indices.   . nesterov momentum      was used during the gradient descent process in an attempt to speed up the training  procedure.             results    in general  the trained neural network for this work did not perform as well as other state of the art models  but was  able to achieve a modest success considering the constrained time and resources that were available.  of the two  methods described in section    the second method is believed to provide a stronger and more robust translation  model. however  in practice  the training procedure was prohibitively slow as the model could not identify useful  patterns in translation quickly from the original randomly generated translations. as a result  only the first method  was successfully implemented and tested.  the results of the translation model can be qualitatively evaluated by looking at some of the automatically translated  sentences  and comparing them with the correct english translations. a sample of some translations can be seen in  table  . for comparison  the translation offered by google translate is also provided.  through the present model  translation quality quickly deteriorates for longer sentences  even those within the training  set. the results on some of the shorter sentences  on the other hand  are reasonable and sometimes even creative.  however  it is clear that this model is not powerful enough to fully translate by itself  achieving a validation test score  of only   .     that is  given an input sequence of words in french and a partial sequence of words in the english  translation so far  the probability of identifying the next word in the translation correctly is a little more than   in  .  a possible reason for the struggling performance of this model overall is that the neural network was trained to estimate  the conditional probability given a correct sequence of inputs  but the translation process builds upon predicted output  words that may not be correct. once the predicted word sequence goes  off track   it no longer has a good context to  translate from  and having never learned how to go back to the correct sequence  the model will understandably face  difficulty. it should be noted that if the model is asked to predict the next word when given an input sequence of words  in french and a partial sequence of words in the correct english translation  the probability of identifying the next  word in the translation correctly is nearly doubled  at about   .  .          recommendations and conclusions    the gru based model presented in this work showed early signs of potential for leading to a powerful automatic  translator. although the resulting translations were not always of the highest quality  there is almost always a visible  link to the intended translation  and there is frequently some coherency within the overall prediction. as a starting  attempt at a new approach with tight constraints on time and resources  the present success may even be considered  quite promising.  in future work  it would be interesting to try using a larger training set  more nodes per hidden layer in the neural  network  and a larger dataset for training the french language word vectors. these adjustments would certainly  amplify the time required for training  but it is possible that they may lead to a more capable translator. based on the  solitary model examined as part of this work  it not clear what the effects that many of the parameters  such as number  of nodes and the size of the vocabulary  would have on the ability of the resulting translator. this would certainly  present an interesting avenue of future research.         acknowledgments    the authors would like to extend grateful acknowledgments to dr. ghodsi for teaching the course and creating an  environment of enthusiasm for learning  both for the students and our algorithms. thank you as well for taking the  time to mark our final projects during the winter holiday.         it is unfortunate that despite repeated and earnest consultation with sharcnet support services  technical issues prevented the  research team from obtaining the full benefit of sharcnet resources in time to complete this work.     perhaps     is not particularly good either  but machine translation is a difficult problem  and it is comforting to note that even  google translate has issues with translating some sentences as well.          english translated sentence  am much more space .    few things are being used for  the journey  am delighted to see our next  meeting  president on a point of order    english correct sentence  google translate  set  i ve got plenty of room  i have a lot of space.  test  a light raincoat is ideal for the a light raincoat is ideal for test  trip  travel.  i look forward to our next meet  i look forward to our next meet  test  ing  ing.  madam president on a point of madam chair c is a procedural training  order  motion.  you send a doctor to a doctor  will you send someone go get a are you send someone to fetch a test  doctor  doctor.  please please call for this si  please rise then for this minute i invite you to stand for this training  lence  silence  minute of silence.  he was born it was very difficult since he was dressed in black he as he was dressed in black he test  to speak . nf  looked like a priest  looked like a priest.  will have a painful job but he she told him a joke but he didnt she told him a joke but he did test  did not have his surprise  think it was funny  not find it funny.  have had a debate on this sub  you have requested a debate on you wanted a debate on this in training  ject in the next part session in this subject in the course of the the coming days during this sesthe debate . nf . nf .  next few days during this part sion.  session  declare resumed the session of i declare resumed the session resumed session i declare re  training  the european parliament which of the european parliament ad  sumed the session of the eurowas held on thursday    octo  journed on friday    decem  pean parliament adjourned on  ber which i have had to say and ber      and i would like once friday    december and i rei am sure you will confirm that again to wish you a happy new new my vows in the hope that  my meeting was good news .  year in the hope that you en  you had a good holiday.  joyed a pleasant festive period  table    translated phrases obtained using our model  in contrast with the correct translation and the result obtained from google  translate.  nf  implies that the model predicted a word outside of the vocabulary  or a potential sentence break.    