introduction  there have been many variations on this simple  design. however  it is still widely used and we are not aware of  a gated rnn architecture that outperforms lstm in a broad  sense while still being as simple and efficient. in this paper we  propose a modified lstm like architecture. our architecture is  still simple and achieves better performance on the tasks that we  tested on. we also introduce a new rnn performance benchmark  that uses the handwritten digits and stresses several important  network capabilities.    i. introduction  sequential data can take many forms. written text  video data   language  and many other forms of information are naturally  sequential. designing models for predicting sequential data   or otherwise extracting information from a sequence is an  important problem in machine learning. often  recurrent neural  networks are used for this task. unlike a non recurrent network   a recurrent network s input at a given time step consists of  any new information along with the output of the network at  the previous time step. since the network receives both new  input as well as its previous output  it can be said to have a   memory   since its previous activations will affect the current  output. training a rnn model with gradient descent  or similar  gradient based optimization algorithms is subject to the usual  problem of vanishing gradients    . at each time step  the  gradient diminishes and eventually disappears. consequently   if the rnn needs information from more than a few time steps  ago to compute the correct output at the current time step   it will be incapable of making an accurate prediction. the  model  long short term memory        greatly mitigated this  problem. lstm incorporates  gates   which are neurons that  use a sigmoid activation  and are multiplied with the output of  other neurons. using gates  the lstm can adaptively ignore  certain inputs. lstm also maintains a set of values that are  protected by the gates and that do not get passed through an  activation function.  in this work we develop a modification to lstm that aims  to make better use of the existing lstm structure while using  a small number of extra parameters. we claim that there are  three issues with lstm with forget gates. first  forget gates  impose an exponential decay on the memory  which may  not be appropriate in some cases. second  the memory cells  cannot communicate or exchange information without opening  the input and output gates  which also control the flow of  information outside the memory cells. third  the hyperbolic    tangent function is not ideal since lstm memory values can  grow large  but the the hyperbolic tangent has a very small  gradient when its input value is large.  in our modified version  the forget gate is replaced with  a functional layer that resides between the input and output  gates. we call this modification lstm with working memory   henceforth abbreviated lstwm. lstwm incorporates an  extra layer that operates directly on the stored data. rather  than multiplying previous cell values by a forget gate  it uses  a convex combination of the current memory cell value and  the output of this extra layer whose input consists of previous  memory cell values. in addition  we find that using a logarithmbased activation function improves performance with both the  lstm architecture and our modified variant. we evaluate this  method on the hutter challenge dataset      as well as a task  designed using the mnist      handwritten digit dataset.  ii. a rchitecture and t raining  we begin with a review of lstm. since nearly all modern  implementations use forget gates  we will call lstm with  forget gates standard lstm or just lstm. the term  memory  cell  will be used to refer to the inner recurrence with a weight  of one and  lstm cell  to refer to an entire  individual lstm  unit.  a. background on lstm  we introduce notation to be used throughout the rest of the  paper  xt is the input vector at time t  yt is the network layer   .   output at time t       .  e   x and f is an activation function.  the standard lstm formulation includes an input gate  output  gate  and usually a forget gate  introduced in     . the output  of an lstm unit is computed as follows   a   f  w    xt   yt       b       wgi    xt   yt       bgi              o         wgo    xt   yt       bgo              f          wgs    xt   yt       bgs             g  g  g            i     ct   g     i      f      a g    yt   g     o     f  ct      ct                  it should be noted that other variants of lstm exist  such  as lstm with peepholes     and more recently associative  lstm    . we chose lstm with forget gates as it is a simple  yet commonly used lstm configuration.     fig.  . lstm with forget gates. the orange circles are multiplicative gate  units  the white circle is the addition operator. double arrows represent the  output of a neuron with multiple inputs  while single arrows represent the path  of a single value.    fig.  . lstwm. the orange square represents a convex combination of the  two inputs on the right.    large magnitude value for a longer period of time. for this  reason  we feel that the lstm is a stronger base upon which  to build.    the input and output gate values  g  i  and g  o    serve to  regulate incoming and outgoing data  protecting the memory  cell value. the forget gate can be used to clear the memory b. lstwm  cell when its data is no longer needed. if the input output  for a regular lstm to operate on the values stored in its  gates are closed  and the forget gate is open  then there is no memory cell and store them again  it must open both the  transformation on the memory cell values. this is an extreme output gate and the input gate. this releases the value to  example  since the gate cells will likely take different values the downstream network  and new input from the upstream  at any given timestep and can never be fully open or closed. network must also  to some degree  be allowed to enter the  however  it illustrates the main point of lstm  which is that cell. we propose that a network may make better use of its  it can store data for an arbitrary amount of time  in principle. input and output gates if they weren t serving the two  not  lstm with forget gates does not share the same mathemat  necessarily related  purposes of regulating recurrent data as  ical properties of the original lstm design. the memory cell well as regulating inputs and outputs to and from other parts  value decays exponentially due to the forget gate. this is of the network. additionally  the activation function is applied  important  since the network has a finite memory capacity to the memory cell values before they are multiplied by the  and the most recent information is often more relevant than output gate. typically  the hyperbolic tangent function is used   the older information. it may  however  be useful to decay and if the memory cell value already has a high magnitude   that information in a more intelligent manner. another point the errors entering the memory cell will be very small. we  is that the information in the memory cells cannot be used argue that a non saturating activation function will confer a  without releasing it into the outer recurrence. this exposes significant performance increase. the output of a lstwm cell  them to the downstream network. a memory cell must also  to is computed in the following manner   some degree  accept information from the upstream network  to perform computation on the recurrent information.  a   f  w    xt   yt       b        another related architecture in this domain is the gated   i   g     wgi    xt   yt       bgi         recurrent unit  or gru      which uses a convex combination   o   instead of an input gate. it does not use an output gate. this  g     wgo    xt   yt       bgo         makes the architecture easier to implement  since there is   s   g     wgs    xt   yt       bgs          only one set of recurrent values per layer. this also does not   rl    c      ct                share the theoretical properties of the original lstm  since   rr    the memory cell values are replaced by their input rather than  c      ct               summed with their input.   rl     rr    it   f  wv  ct     wv  c    w v  c    bv      empirical studies          comparing the performance of        lstm and gru are somewhat inconclusive. despite this  we   s    s   rt   g  ct       .    g   it        hypothesize that a potential weak point of the gru is that its   i   ct   g  a   rt        cells cannot accumulate large values as evidence of the strong   o   presence of a certain feature. suppose a gru sees the same  yt   g  f  ct          feature three times in a row  and its input gate is open. the  where f is an activation function  either  cell will  roughly speaking  be replaced three times  but there  is comparatively little evidence that the gru saw this feature      ln  x      if x     more than once. on the other hand  the lstm memory cell  f  x     has been increased three times  and it can arguably retain this  ln x       otherwise     fig.  . wikipedia text prediction task. two layer networks.    or    fig.  . wikipedia text prediction task. single layer networks.    biases for the inner layer  the network can also achieve a  traditional forget gate. since this layer does not use many extra  this logarithm based activation function does not saturate parameters  we can compare equal width networks. wv    wv     and can better handle larger inputs than tanh. fig.   illustrates wv  and bv  are initialized with zeros  so the network starts  both functions. the recurrent cell values may grow quite large  out as a standard lstm. in summary  an lstwm network  causing the hyperbolic tangent function to quickly saturate and can modify its memory in a more complex manner without  gradients to disappear. to obtain good performance from our necessarily accepting new values or exposing its current values.  design  we wanted to develop a non saturating function that since the inner layer only uses the previous memory cell values   can still squash its input. earlier works have used logarithm  it can be computed in parallel with any downstream network  based activations  and the function we used appears in      and does not present a computation bottleneck if implemented  in a parallel manner.  and originally in    .  this architecture was adapted from a design in a previous  while the rectified linear unit      relu  is a common  version       of this work  which used normal forget gates  choice for non recurrent architectures  they are usually not  and  included  an extra layer and extra gate after the memory  suitable for lstm cells since they only have positive outputs  cell  update.  we  found that the previous four gate architecture   although they have been used with some success in lstm  did  not  perform  as well on tasks that required a precise  and rnns  see             and exploding feedback loops are  memory.  likely   having  three different memory operations at  more likely when the activation function does not apply some  each  timestep  resulted  in  excessive changes in to the memory.   squashing effect . however  tanh is less suited for large inputs  setting  appropriate  initial  bias values helped the situation in  due to the small values of its derivative outside the small  some  cases   however   we  found better designs that did not  interval around zero.   s   require  as  much  hand tuning.  our first attempt at resolving the  the forget gate output has been renamed g   and now  issue  was  removing  the  forget  gate. removing the forget gate  controls the convex combination of the previous memory cell  from  our  earlier  design  did  not  yield good result by itself.  value ct   with the output of the inner layer it . the weight  figures     and     illustrate  lstm  and lstwm  respectively.  vectors wv  and wv  only connect a neuron to its left and right  the  i  subscript  indicates  that  this  is  the  ith unit in a layer. the  neighbors. in software  we did not implement this as a matrix  multiplication  but as an element wise roll of the input which white double arrows indicate an input that determines a gate  gets multiplied by weight vectors and then has bias vectors value  as opposed to a value that gets multiplied by a gate .     training  regularization and other details  given that  added. the function   x  y  is an element wise roll where x is  the input vector and y is the number of times to roll left or right. the memory cells are not necessarily decaying in an exponential  for example                             and                    manner at each timestep  it is important to control their           . so the inter neuron connections within the memory magnitude. rather than just using a regularization term on  cells are sparse and local. the reason for choosing a sparse the network weights themselves  we also regularize the overall  layer is that dense layers grow quadratically with respect to magnitude of the memory cells at each timestep.  layer width. we wanted to compare equal width networks   when training an ann of any type  a regularization term  since a lstm s memory capacity is determined by the number is usually included in the cost function. often  the l  norms  of individual memory cells. by setting near zero weights and of the weight matrices are added together and multiplied by  f  x    tanh x      architecture  lstm     tanh  lstwm     tanh  lstm     log  lstwm     log  lstm         tanh  lstwm         tanh  lstm         log  lstwm         log    bpc on test set after training   .      .      .      .      .      .      .      .       fig.  . performance on text prediction task.    iii. e xperiments    fig.  . comparison of tanh and log based activation function    we have two experimental tasks to test our network on  text  prediction and a combination digit recognition and addition task.  the networks were trained using adam       an optimization  algorithm based on gradient descent  with the following settings      .         .       .   .    iv. t ext p rediction  a very small constant. this keeps the weight magnitudes in  like many other works e.g.      we use the hutter challenge  check. for our architecture to train quickly  it is important      dataset as a performance test. this is a dataset of text  to use an additional regularization term. given a batch of and xml from wikipedia. the objective is to predict the  training data  we take the squared mean of the absolute value next character in the sequence.  note that we only use the  plus the mean absolute value of the memory cell magnitudes dataset itself  and this benchmark is unrelated to the hutter  at each timestep for every element in the batch. in other compression challenge  the first     of the data was used as  words       mean  cells      mean  cells    for some small a training set and the last    for testing. error is measured  constant  . we found that using          or          in bits per character  bpc  which is identical to cross entropy  worked well. similar regularization terms are discussed in       error  except that the base   logarithm is used instead of the  although they differ in that they are applied to the change in natural log. we used a batch size of    and no gradient clipping.  memory cell values from one timestep to the next  rather than to reduce the amount of training time needed  we used lengththeir magnitude. using direct connections between the inner     sequences for the first epoch  and then used length       cells can amplify gradients and produce quickly exploding sequences for the remaining five epochs.  values. by adding this regularization term  we penalize weight  figures   and   show a running average of the training  configurations that encourage uncontrollable memory cell error and   shows the bpc on the test set after training. the  values. since the cells values get squashed by the activation results are close for this particular task  with lstwm taking a  function before being multiplied by the output gate  regularizing slight advantage. notice that for this test  the logarithm based  these values shapes the optimization landscape in a way that activation does carry some benefit  and the best performing  encourages faster learning. this is also true to some extent for network was indeed lstwm with the logarithmic activation.  regular lstm  which benefits from this type of regularization  as well. we also applied this regularization function to the a. training information  weights themselves. lstwm can learn effectively without  given the popularity of this task as a benchmark  a quick  extra regularization  but it will learn slowly in some cases.  training phase is desirable. input vectors are traditionally given  note that we use the square of abs mean not the mean  in a one hot format. however  the network layers can be quite  squared. using the square of abs mean allows some values to wide  and each cell has many connections. we found that using  grow large  and only encourages most of the values to be small. a slightly larger nonzero value in the vectors resulted in quicker  this is important  especially for the memory cells  because the training. instead of using a value of  .  for the single non zero  ability of a neural network to generalize depends on its ability element in the input vectors  we used log n     .  where n  to abstract away details and ignore small changes in input. is the number of input symbols. in this case  n        since  indeed  this is the entire purpose of using sigmoid shaped there are     distinct characters in this dataset. on tasks where  squashing functions. if the goal were to keep every single there are many symbols  the mean magnitude of the elements  memory cell within tanh s  gradient zone   one could use the of the input vector is not as small  which accelerated training  hyperbolic cosine as a regularization function since it grows in our experiments.  very large outside this range.  another method we used to speed up training was using a  we used python and theano      to implement our network pre train epoch with shorter sequences. the shorter sequences  and experiments.  mean that there are more iterations in this epoch and the     fig.  . example input sequences for combination digit task    fig.  . inputs colored and separated into columns to show individual input  columns    iterations are performed more quickly. this rapid pre training  phase moves the parameters to a better starting point more  quickly. this method could also be used in any task where the  input is an arbitrary length sequence. longer sequences result  in better quality updates  but they take proportionally longer  to compute. we also added a very small amount of gaussian  noise to the inputs during the pre train epoch  but not during  the main training phase. fast and noisy updates work well  early in the training process and reduce the amount of time  necessary to train networks.    fig.  . digit combo task training error.    architecture  lstm         tanh  lstwm         tanh  lstm         log  lstwm         log      of correct sum outputs   out of                                   fig.   . performance on digit combo task.    the forget gate biases are often      which indicates low forgetgate activity  i.e. it tends to forget things quickly. this task  was developed as a way to test memory functionality  since  v. d igit recognition and addition combined task  text recognition does not stress this capability.  this task is a combination of digit recognition and addition.  another interesting property of this task is that it demonother works  e.g.             use the mnist dataset as a strates the ability of an architecture to form a generalized  performance test for artificial neural networks. addition is also algorithm. increasing the length by concatenating more digit  a common task for evaluating the viability of new rnn designs  images will remove non algorithmic minimums from the search  and usually the numbers to be added are explicitly encoded by space. in other words  the network will need to learn an  a single input vector at each timestep. the difference with addition algorithm to correctly process a longer sequence.  this task is that we horizontally concatenate a number of simply  memorizing  the correct outputs will not work when  images from mnist and train on the sum of those digits. the network does not have enough parameters  to work on  this large concatenated image is presented to the network in longer sequences  it must learn an addition algorithm. this  column vectors. after the network has seen the entire image  can also be applied without the strict memory requirements  it should output the sum of the digits in the input. no error of back propagating to the first digit. in this case  one would  is propagated until the end of the image. this makes the task simply train to output the running sum at frequent intervals  a useful benchmark for memory  generalization ability  and rather than just at the end of the sequence. in the boundary  vision. the training targets are placeholders until the end  and case where the number of digit images is one  it becomes a  then two  more if needed  size    vectors corresponding to simple digit recognition task.  the two digits of the sum. a similar but non sequential task  we trained in a similar manner to the text prediction  appears in    .  experiment. each training input consists of randomly selected  for this task  the error coming from the end of the sequence digits along with their sum as a target. the same adam  must be back propagated all the way to the first digit image. if parameters were used as in the text prediction task. we trained  this does not occur  the model will not correctly learn. unlike on sequences with four concatenated digit images. therefore   text prediction  this requires a robust memory and the ability each input sequence was     input vectors with three more  to back propagate the error an arbitrary number of time steps. placeholders at the end. as a preprocessing step  the digit  we suspect that text prediction is a task where memory values images were added with gaussian  scaled down          are often replaced. for text prediction  at the end of training  noise  blurred   x  kernel  gaussian   mean subtracted and     number of extra parameters used is linear  not quadratic  with  respect to layer width  so we consider it a good improvement  given the small size increase. we claim that using the inner  layer  along with using a logarithm based activation will offer  a modest but significant performance benefit. another notable  feature of lstwm is that the training error decreases more  quickly compared to standard lstm.  vi. c onclusion  we discussed several interesting properties of lstm and  introduced a modified lstm architecture that outperforms  lstm in several cases using few additional parameters. we  observe that a logarithm based activation function works well  in lstm and our modified variant. finally  we presented a  benchmark for recurrent networks based on digit recognition.  