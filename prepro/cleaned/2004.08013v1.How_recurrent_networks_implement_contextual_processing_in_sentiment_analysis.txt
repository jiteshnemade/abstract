introduction  neural networks do a remarkable job at learning structure  in natural data. these architectures exploit complex relationships between inputs to perform tasks at state of the art  levels  lecun et al.       . despite this amazing performance  we still only have a rudimentary understanding of  exactly how these networks work  castelvecchi       .  rigorously understanding how networks solve important  tasks is a central challenge in deep learning. this under   equal contribution   google research  brain team  mountain view  california  usa. correspondence to  niru maheswaranathan  nirum google.com   david sussillo  sussillo google.com .    standing is lacking because we use complex optimization  procedures to set the parameters of ever more complex  network architectures. our inability to fundamentally understand how a trained system works makes it difficult to  identify biases in the network or training data  control for adversarial input  bracket or bound network behavior  suggest  ways of improving efficiency or accuracy  and elucidate the   potentially simple  core mechanisms underlying the task.  in this work  we focus on building general tools and analyses to understand how recurrent neural networks  rnns   process contextual effects. by contextual effects  we mean  effects where the interpretation of an input depends on surrounding inputs. for example  in natural language  the  interpretation of words is modified by preceding words for  negation   not bad  vs  bad   or emphasis   extremely awesome  vs  awesome    quirk et al.        horn   kato        . we analyze networks trained to perform sentiment  classification  a commonly studied natural language processing  nlp  task  wiegand et al.        mohammad         zhang et al.       .  when interpreting or understanding how recurrent networks  work  one line of research uses attribution methods to analyze sensitivity to particular inputs  simonyan et al.         ribeiro et al.        li et al.        arras et al.        murdoch et al.       . these methods utilize gradients of the  network loss or unit activations with respect to inputs. a second line of research uses tools from dynamical systems analysis to decompose rnn trajectories  zipser        tsung    cottrell        casey        rodriguez et al.        sussillo    barak        mante et al.        cueva   wei         jordan et al.        maheswaranathan et al.      b . here   we merge these lines of research to understand contextual  processing of large numbers of inputs in recurrent systems.  in particular  our key insight is that rich contextual processing can be understood as a composition of precisely organized input driven state deflections combined with highly  organized transient dynamics. critically  the former can  be as understood using jacobians with respect to the input   analogous to attribution methods  while the latter can be  understood through jacobians with respect to the recurrent  state  analogous to work on rnn dynamics.  our main contributions in this work are     a data driven method for identifying contextual inputs      how recurrent networks implement contextual processing   b            model prediction  logit     principal component        a                        principal component       probe sentences                                  baseline   this movie is awesome. i  like it.     negation   this movie is not awesome.  i don t like it.     intensifier   this movie is extremely awesome.  i definitely like it.                           time  t     figure  . rnns and contextual processing.  a  approximate line attractor dynamics in a gru trained to perform sentiment classification.  pca visualization of trajectories of the system during an example positive  green  and negative  red  review. the approximate line  attractor is visualized as the light red to green line  colored according to the readout  logit .  b  evolution of rnn predictions  logits   when processing example reviews over time  tokens in the input sentence . gray  baseline    this movie is awesome. i like it   with pad  tokens inserted to align it with the other examples   blue  intensifier    this movie is extremely awesome. i definitely like it.  orange   negation    this movie is not awesome. i don t like it.  the rnn correctly handles the intensifier and negation context.                  a breakdown of the types of contextual effects learned   analysis of the strength and timescale of these effects   a mathematical model that describes this behavior   and a demonstration that simple and interpretable baseline models  augmented to incorporate this new understanding  recover almost all of the accuracy of the  nonlinear rnns.    movie is awesome. i like it.    one with intensifiers   this  movie is extremely awesome. i definitely like it.    and  one with negation   this movie is not awesome. i don t  like it.  . the rnn is capable of correctly assessing the  sentiment in these reviews. below  we break down exactly  how the rnn is able to accomplish this.     . preliminaries   . background     . . linearization and expansion points    previous work  maheswaranathan et al.      b  analyzed recurrent networks trained to perform sentiment classification.  they found that rnns learned to store the current prediction  as the location along a  d manifold of approximate fixed  points of the dynamics  called a line attractor  seung         mante et al.       . additionally  they found that words  with positive and negative valence drive the hidden state  along this line attractor  which is aligned with the readout.  a recap of this analysis is presented in figure  a  for a gated  recurrent unit  gru   cho et al.        trained using the  yelp      dataset  zhang et al.       .    we denote the hidden state of a recurrent network at time t  as a vector  ht . similarly  the input to the network at time t  is given by a vector xt . we use f to denote a function that  applies the recurrent network update  i.e. ht   f  ht     xt  .  the rnn defines an input driven discrete time dynamical  system that sequentially processes inputs  in this case words  in a document encoded as a sequence of one hot input vectors. the final prediction  logit  is an affine projection  or  readout  of the hidden state. in this work  we focus on binary  sentiment classification  thus the logit is a scalar  wt ht  b    with readout weights w and bias b.    critically  the mechanisms discussed in maheswaranathan  et al.      b  involved only integration of valence tokens  along the line attractor. we tested the rnn on the same task  after randomly shuffling the input words. shuffling breaks  apart important contextual phrases  such as  really awesome   and  not bad . on the shuffled test examples  the accuracy  drops by       fig.   a   close to the performance of a bagof words  bow  baseline model. thus  there are additional  accuracy gains  present in the best performing rnns  that  are not explainable using prior known mechanisms.    we can write the first order approximation to the rnn  dynamics  khalil        maheswaranathan et al.      b   around an expansion point  he   xe   as     the capability of rnns to understand context can also be  observed by probing the rnn with examples that contain  contextual effects. figure  b shows the predictions of the  rnn in response to three probe reviews  a baseline   this    ht   f  he   xe   jrec   he  xe    ht    jinp   he  xe    xt        where  ht     ht     he    xt   xt   xe   and   jrec   jinp   are jacobian matrices computed at the expansion  point. in particular   the recurrent jacobian         f  h x i  rec  jij   he  xe      hj  defines the recurrent local dynam      inp   h x i  ics and the input jacobian jij    he  xe      f x  dej  fines the system s sensitivity to inputs.     how recurrent networks implement contextual processing     . . linearization to understand dynamics     ht   jrec   h   x     ht     jinp   h   x     xt .           to study the effects of modifier tokens  we also need to  analyze the system away from approximate fixed points. in  particular  we analyze the system at hmod   defined as the  state after processing a particular modifier word. to do this   we linearize with respect to just the inputs for a single time  step  expanding only in x  around xe        ht   ht   f  hmod        jinp   hmod     xt             where equation     does not expand in h  to focus on input  sensitivity. we make extensive use of the input jacobian in  equation     in our analysis in   .                            . . linearization to understand modifier words          pc    we use equation     to study integration dynamics around  approximate fixed points  as in maheswaranathan et al.       b . these approximate fixed points are found numerically  see supp. mat.  b for details .          pc       fixed points are points in state space that remain the same  when applying the rnn  h    f  h  x   . if the expansion  point  he   xe   is an approximate fixed point of the dynamics   equation     simplifies to a linear dynamical system     pc       figure  . state space plot of a gru trained on a toy language  to isolate effects of modifier tokens. this example shows a line  attractor  line from red to green  that performs integration of valence  value from    to     bold black numbers . modifiers such  as  extremely   blue  and  not   orange  achieve their effects by  deflecting the state away from the line attractor to hextremely and  hnot   respectively. the effect of these deflections is to modify  the valence of the input when projected onto the readout. six  trajectories are shown  all starting from hpre . gray shows the single step trajectory for  good   n      and  bad   h     . in blue  are the single step trajectories for  extremely good   n      and   extremely bad   h       and the multi step trajectories for  not  good   n      and  not bad   h      are in orange. we defined the  duration of the effect of the  not  modifier to be   tokens.     . toy language to isolate modifier dynamics  in order to illuminate how negation or emphasis is implemented in rnns we developed a small toy language  to isolate modifier effects. the language consisted of a  small number of valence tokens  each with integer valence                    analogous to words such as  awful    bad    the    good   and  awesome   respectively.  in addition  we added two modifier words  an intensifier  that doubled the valence of the next input  and a negator  that flipped the sign of the valence of the next four inputs   analogous to words such as  extremely  and  not  .  we generated reviews using this language by randomly ordering the tokens and trained rnns to track the corresponding sentiment  defined as the cumulative sum of potentially  modified valence inputs. for example  after training  the  rnn correctly integrated  good  as      extremely good   as    and  not good  as   . we analyzed the networks  using the methods developed by maheswaranathan et al.       b . an example state space plot from a trained network is shown in figure  . note that this rnn also exhibits  line attractor dynamics.  we draw two key insights from this exercise  see supp.  mat.  a for a full analysis . first  modifiers achieve their  effects by deflecting the state away from the line attractor  as opposed to along it  the latter of which is what valence    words do. we found that the effect of this deflection away  from the line attractor on the valence of the subsequent word  was very well approximated by wt jinp   hmod     xval  supp.  fig.      where hmod is the state after the modifier input   xval is the valence word following the modifier  and w and  bias b are the readout weights and bias. as shown in the  supp. mat.  a  removing the projection of jinp   hmod     x  into the modification subspace removes the effect of the  modifier  supp. fig.    .  second  the deflection of the state away from the line attractor caused by xmod remains for the duration of the modifier  effect. for example  in the toy language we confined the  temporal extent of the modification effects of  extremely   and  not  to one word and four words  respectively  and the  corresponding deflections off the line attractor remain for  one and four time steps  respectively. finally  the transient  dynamics associated with valence modification can be isolated in the local linear dynamics around fixed points on the  line attractor  supp. fig.         .     . reverse engineering contextual processing  we turn our attention now to natural language  studying our  best performing rnn  a gru  trained to perform sentiment  classification on the yelp      dataset  zhang et al.       .      a      . . identifying modifier words    count                 modifier  tokens                                change in input jacobian     jinp  f     for rnns trained on real world datasets  we did not know  what words might act as modifiers  therefore  we wanted  a data driven approach to identify them. inspired by the  toy model  we looked for particular words that deflected  the hidden state to dimensions where the input jacobian  changed substantially  as that indicated differential processing of inputs. we defined the change in input jacobian after  a particular input  xmod   as   df  dx        hmod        df  dx                                   extremely    not                     principal component       figure  . identifying modifier words. histogram of frobenius  norm of change in input jacobian. note the log scaling on the  x axis  implying this distribution is heavy tailed. we defined a  modifier token as anything with a norm greater than  . . example  words above this threshold that are intuitively modifiers include   not    never    overall   and  definitely . additional words found  by this measure that are not as intuitive include   zero    two     poisoning   and  worst .     jinp  hmod      modifier component       how recurrent networks implement contextual processing            h         where hmod   f  h    xmod   is the hidden state after processing xmod   starting from a point on the line attractor  h   .  thus   jinp  hmod measures how the system s processing of  words changes as a function of a a preceding word  xmod .  we studied the size of the changes in input processing by  computing the frobenius norm  k jinp  hmod kf   for each of  the top        most common words in the dataset. the  resulting distribution is shown in figure   and is evidently  heavy tailed. examining words with large values reveals  common negators  e.g.  not    never   and intensifiers  e.g.   very    definitely  . we selected a set of modifier words for  further analysis by keeping words whose change in input  jacobian was greater than an arbitrary threshold of  .   .   . . analyzing example modifier words   . . . dynamics of example modifier transients  we next looked at the dynamics of the hidden state in response to example modifier words   extremely  or  not       we suppress the h  in the notation as it was held constant in  our analyses.     none of the presented results are particularly sensitive to this  choice.     b      not        .  tokens   extremely        .  tokens    distance from  line attractor                        time  t           figure  . impulse response to a modifier word.  a  two trajectories of the network hidden state in response to modifier words   extremely  and  not   projected onto the top pca component   x axis  as well as the top modifier component  y axis   see text for  details.  b  distance from the line attractor after a single modifier  word. circles show response of the rnn  line is an exponential fit.    followed by a series of pad tokens. figure  a shows the  impulse response to  extremely  and  not   projected into  a two dimensional subspace for visualization  . each modifier word deflects the hidden state off of the line attractor   which then relaxes back over the course of      time steps   similarly to the toy example. figure  b shows the same  impulse response as a function of time  quantified using the  euclidean distance from the line attractor. these transients   induced by modifiers  explore parts of the rnn state space  that critically are not contained in the top two pca dimensions  fig  a . it was only through focusing on the change  in input jacobian that we were able to identify them.   . . . m odifier barcodes  next  we asked how the system leverages these transient  dynamics to enable contextual processing. to answer this   we analyzed how the input jacobian changes along these  transients in comparison to their values at the line attractor.  however  a complication arises because a modifier may have  differing effects on each word and there are many words in  the vocabulary. therefore we developed a visualization  a  modifier barcode  to study the effect of changing jacobians.       the two dimensional subspace in fig   consists of the top  pca component  x axis  and the top modifier component  defined  later in   . .  .      .    .     .      the    not    extremely       .   positive words     .    very         extremely           .      but     .             .    zero              negative words     .    five    never  not     .     .                      modifier component       change in input jacobian     jinp  f           .     modifier component       change in prediction  logit     how recurrent networks implement contextual processing         figure  . barcodes for visualizing the effects of modifiers. the  barcode is a quantitative signature of a particular modifier word.  shown are the barcodes for  the   gray    extremely   blue   and   not   orange . each point in a barcode is the change in the model s  prediction in response to a particular valence word when a modifier  word precedes it  e.g.  not great    not bad    not amazing    etc.  orange    for the top     positive words  e.g.  amazing     awesome   left  and the top     most negative words  e.g.  hate     awful   right . for example the  not  barcode shows a negative or  reduced change for positive valence words and a strong positive  output change for negative words.    figure  . low dimensional modifier subspace. projection of modifier points  teal circles  onto the top two modifier components   obtained by performing principal components analysis  pca  on  the set of states after processing modifier words. highlighted are a  number of example modifiers  including  not  and  extremely    from the previous figure. emphasizers  e.g.  extremely    very    are on one side of the subspace  whereas negators  e.g.  not     never  are on the other side.    barcodes are intended to quantify how a given modifier  word affects processing of future inputs. for a given probe  word x  we define a barcode for a modifier word as     these effects over the entire set of identified modifier words.  in particular  we are interested in understanding how many  different types of modifiers are there  and whether we can  succinctly describe their effects and dynamics.    barcodemod  x    wt  jinp  hmod x            mod    where h  is the rnn state after a modifier word has been  processed. as there were        words in the vocabulary  we instead selected     positive and     negative words   e.g.  awesome  and  awful   to comprise the barcode  .  figure   shows three barcodes  for the words  the    extremely   and  not . for words that are not modifiers  such  as  the    the effects of inputs before and after the word  are the same  thus the barcode values are close to zero. for  intensifiers  such as  extremely   we see that positive words   left half of figure    and negative words  right half  are  accentuated . for negators  such as  not   positive and  negative words are made relatively more negative or more  positive  respectively. in summary  the modifier barcode  allows us to easily assess what the effects of a particular  modifier are  through the changing input jacobian.   . . summary across all modifier words   . . . m odifier subspace  so far we have analyzed the modifier dynamics and barcodes  for a couple of example modifiers. next  we will summarize     words were selected by taking the     largest  most positive   and     smallest  most negative  logistic regression weights  using  a separately fit logistic regression classifier  bag of words model .  results do not depend on the particular set of probe words chosen.    first  we collected all of the deflections in the hidden state  induced by modifier words. this is a set of points in the  rnn hidden state where there are substantial changes to  the input jacobian. we ran principal components analysis  on this set of points to identify a low dimensional modifier  subspace  also ensuring that the subspace is orthogonal to  the line attractor  . in particular  two components explained    .   of the variance  suggesting that the dominant modifier  effects can be understood as occurring in a  d plane.  we projected all of the modifier inputs into this  d subspace  for visualization to highlight the distribution of modifiers in  this subspace  as well as some example modifiers  fig.   .  negators  words like  not  and  never   live in one part of  the subspace  whereas intensifiers  words like  extremely   and  definitely   live in another part. interestingly  these  two types of modifiers share the same subspace.   . . . fast and slow modifier dynamics  we then studied the dynamics in the modifier subspace. in  particular  the dynamics of how the hidden state evolves in  this subspace determines the length of the effects of each  modifier  called the  scope  in linguistics  quirk et al.         horn   kato        . figure   shows the dynamics of mod     we do this additional orthogonalization step to ensure that  the identified modifier subspace does not have any valence effects   which occur along the line attractor.     how recurrent networks implement contextual processing   a      b          we computed barcodes for the top two modifier axes   fig.    . we saw that qualitatively  the two dimensions have  similar effects  so the predominant difference between them  is presumably the difference in timescales discussed above.          timescale                                           modifier component       mod. comp.                  mod. comp.       modifier component            figure  . timescales of modifier effects.  a  hidden state dynamics of the impulse response to modifier inputs. note the line  attractor projects out of the page.  b  distribution of the estimated  timescale of the decay along each modifier component across  modifier words.    change in prediction  logit     ifiers in the  d subspace  along with a quantification of  the timescale of the decay in each modifier dimension or  component  fig.  b . we observed that the first modifier  component is faster  mean      .  tokens   whereas the  second one is slower  mean      .  tokens   though there  is spread in the distributions in both dimensions. this argues that the scope of modifier words in rnns trained to  perform sentiment classification lasts for tens of tokens and  agrees with estimates of the scope of negation from human  annotators  taboada et al.        strohm   klinger       .   .     however  there are still rich patterns of effects in terms  of the barcode weights for individual valence words. for  example  the barcode weight corresponding to the valence  word  stars  has a strong projection on the first modifier  component  but not the second. this suggests that modifier  words that induce important contextual changes for  stars    e.g.  zero stars  and  five stars   should preferentially have  larger projections onto the first modifier component  and  not the second. indeed  we see that  zero  and  five  are  included in the set of modifier words  and have projections  only on the first  faster modifier component  fig.   . by  tuning the input jacobian for different words along these two  modifier components  the network is able to correspondingly  tune the timescale of their effects.   . . . p erturbation experiment  finally  to test whether the modifier subspace is necessary  for contextual computation  we performed a perturbation  experiment. to do this  we evaluated the network as usual  except at every timestep we projected the hidden state out  of the modifier subspace. doing this  we reasoned  should  interrupt contextual processing  but leave integration of valence intact. indeed  we find that this perturbation has these  expected effects both on single examples and across all test  examples  supp. fig.      f .     . other contextual effects     .     in addition to modifiers  we found contextual effects that  are active during network processing of the beginning and  end of documents.     .     .      . . beginning of documents      .   positive words    negative words    modifier component     fast       .  words   modifier component     slow       .  words     figure  . barcodes for the principal components  eigenvectors  of  the modification subspace. these barcodes show how the two  dimensions of the modification subspace support rich modification  of valence words with two time constants.     . . . s ummary of modifier barcodes  beyond dynamics  we were also interested in how the input  jacobian changes in this two dimensional subspace. in  particular  we wanted to understand whether or not there  are different types of modification effects. to look at this     after training  we found that the rnn s initial state  h   a  vector of trainable parameters  is far from the line attractor  fig.  a   which surprised us. the reason for this was  revealed when we computed the modifier barcode corresponding to the learned initial state  fig.  a . this barcode  has a signature very much like that of an intensifier word  such as  extremely   in that it accentuates both positive  and negative valence. moreover  we can also see that h     when projected into the modifier subspace  has a significant  projection on both modifier components  fig.  b . the implication of this is that words at the beginning of a document  will be emphasized relative to words later in the document.       although we focus on the top two components in the main  text  the barcodes for the top    modifier components are presented  in supp. fig.   .     how recurrent networks implement contextual processing     .    .     .      c           d   .           prediction  logit      b    .     modifier component       change in prediction  logit      a     end of document  eod  effects       t    t    t    t                    awesome  good  bad         awful                               .   positive words negative words                           modifier component           time  t     transient suppression    beginning of document  bod  effects     .      .      .         positive  words         negative  words    figure  . contextual effects that occur at the beginning  a b  and end  c d  of documents.  a  barcode measured at the initial state  h     reveals that the initial state emphasizes sentiment.  b  projection of the  trained  initial state  gray circle  t      onto the modifier subspace  and corresponding dynamics in response to    pad tokens  gray line  t      . . .      . the initial state initially has a large projection onto  both modifier components  which means that words in the beginning of each document are emphasized  weighted more strongly  relative  to words in the middle of the document.  c  when a valence word enters the network it introduces a transient deflection on the readout  that decays in      steps. this means that words at the end of a review are modified  as the transient has not had time to decay. in the four  examples shown  the effect is to emphasize these words.  d  summary of the amount of transient suppression across many words.    presumably  the rnn learned to do this because it improves  training performance. we tested this hypothesis directly  using a perturbation experiment. we projected h  out of  the  d modifier subspace  and then re tested the gru using  this new initial state. we found that this perturbation caused  a drop in accuracy of  .    on the test set and  .    on  the train set  corresponding to an decrease in    and       correct reviews  respectively. importantly  projecting out a  random  d subspace did not affect the accuracy  indicating  that this accuracy effect is unique to the modifier subspace.   . . end of documents  we also found contextual effects that emphasized words at  the end of documents. here  we identified a different mechanism responsible  short term transient dynamics. when we  looked at the impulse response of the system  fig.  c   we  found that the projection onto the readout in response these  tokens is initially large  but then decays over the course of  around    tokens  until it settles to the steady state valence  of each word. we quantified this effect by computing the  ratio of the steady state valence to its initial valence. we did  this for a large set of positive and negative words  fig.  d    and find consistent effects. the implication is that words at  the end of a review are also emphasized  as their transient  effects do not have time to decay away. we further identified  two linear modes of the recurrent dynamics responsible for  these transient effects  analyzed in supp. mat.  e .  we also tested that end of document emphasis matters to  performance with another perturbation experiment. we  added    pad tokens to the end of each review  thus allowing  the transient dynamics to decay and thereby removing their  effects along the readout. we evaluated the rnn on this  padded dataset and found that the test accuracy dropped by     .    and the train accuracy by  .    corresponding to  incorrectly categorizing    and      reviews  respectively.     . additional rnn architectures  so far  we have analyzed a particular rnn architecture   a gru. we additionally trained and analyzed other rnn  architectures  including an lstm  hochreiter   schmidhuber         update gate rnn  collins et al.         and a  vanilla rnn. we found that all of the gated architectures  were capable of processing modifiers. in particular  shuffling inputs reduced accuracy for the gated architectures  by a similar amount  figure   a   and they have similar  responses to test phrases with modifier words  figure   b .  finally  we repeated the analyses in figures     for the  lstm  which are shown in supp. mat.  c. this hints at  universal mechanisms underlying contextual processing in  gated rnns  maheswaranathan et al.      a .     . bilinear model captures modifier effects  taken together  our analyses reveals the mechanisms by  which rnns perform contextual processing. in particular   the key mechanism involves changing the input jacobian   jinp   in the modifier subspace  and modifier words explicitly project the state into that subspace.  we can synthesize how the input jacobian changes by expressing it as the sum of the input jacobian at the line attractor  jinp   h        with p additional bilinear terms   jinp   hmod       jinp   h           p  x        hmod  h   t mp ap      p              how recurrent networks implement contextual processing   b          logistic regression  vanilla rnn  ugrnn  lstm  gru    test accuracy                          prediction     a     gru    shuffled    vanilla rnn     .      .      .      .      .      .      .      .      .      .      .      .      .      .      .          original    ugrnn     .      .            lstm     .      .       .    extremely awesome  awesome  not awesome .    extremely awful    extremely awesome   .  awesome  not awesome  extremely awful  awful  not awful     .     figure   . summary across different architectures.  a  we performed a control experiment by randomly shuffling the tokens of each  example sequence. we then classified these shuffled reviews using the rnns trained on the original  non shuffled  data. these results  show that word order matters for gated rnns but not for the vanilla rnn or bow model  thus the gated rnns take word order into  account.  b  example predictions for rnns for the phrases  extremely awesome    awesome    not awesome    extremely awful     awful   and  not awful . the gated rnn architectures show similar  modulated responses to these tokens while the vanilla rnn shows  little modulation of the valences of either  awesome  or  awful   regardless of the preceding modifier word.    where ap is a fixed matrix  with the same dimensions as  the input jacobian  and mp is a modifier component. thus  we capture the variation in jinp as a function of hmod by  starting with the input jacobian at h    a point along the line  attractor  and adding a small number of bilinear terms to it.  the additional p terms consist of two components. the first    hmod  h   t mp   a scalar  captures the strength or amount  of modification as a function of the projection of the hidden  state onto the modifier component mp . as the hidden state  decays back to the line attractor  the modification effects naturally decay along with the dot product  ht  h   t mp   with  h    hmod . the second term  ap   models the corresponding change in the input jacobian. these ap terms capture  variation in the barcodes in figure  . we fit the parameters   mp and ap   by regressing eq.     against the actual input  jacobian for the set of modifier words  . this approximation  captures           and     of the input jacobian variation  across all modification tokens for p              respectively.     . augmented baseline models with insights  from rnns recover performance  the result that the bilinear model defined in equation      achieves     of the variance in jinp   hmod     with only p      terms motivates baseline models of the form  wt jinp   h           p  x        hmod  h   t mp wt ap   b         p      where w and b are the readout weights and bias  respectively. first  we note that the first term wt jinp   h        rw  is analogous to the weights     rw in a bag of words     this regression can be solved in closed form via a singular  value decomposition  svd .    model  bow . the additional p terms can be interpreted as  additional   mod weights that activate after a modifier word  occurs and whose effect decays as hmod returns to the line  attractor. thus  we propose augmented models of the form     t  p  x  x  p    t      f m    m    t   mod  t    b        t      p      where we have transitioned to indexing   by time t  which  selects the weight for the word occurring at index t in the  p  review. modifier weights  mod   t  are defined separately from  the baseline valence weights   t .  we model the decaying effect of modifiers via convolution of an indicator that signals the location of each modifier word   m  t   with a causal exponential filter f m  s      m exp  s   m    with scaling  m and timescale   m .  all of the augmented baseline models have the form given  by equation      with different choices for what modifiers to  include. in particular  we trained the following models  full  descriptions are in supp. mat.  g    bag of words  bow   baseline model with no modifiers.  convolution of modifier words  comw   bow plus  convolutional modifiers with the same regression  weights  modifiers do not have unique weights .  convolution of bod eod  bow plus two additional  modifier tokens for the beginning and end of the document  bod eod   with the same regression weights.  comw    mod   bow plus conv. modifiers with learned  regression weights  modifiers have unique weights .  convolution of bod eod    bod    eod   bow plus  bod eod modifiers  with learned regression weights.  comw    mod    bod    eod   bow plus convolutional  and bod eod modifiers  with learned regression  weights  the most powerful of the above models .     how recurrent networks implement contextual processing    for all of our baseline experiments we set the number of  modifiers  m        and the number of modifier   mod  vectors to p      as p     explains     of the variance of  jinp   hmod     in the bilinear model in    . the models were  trained using the adam optimizer  kingma   ba       .  we selected hyperparameters  learning rate  learning rate  decay  momentum  an    regularization penalty  and dropout  rate  via a validation set using random search. we found  dropout directly on the input words to be a useful regularizer  for the more powerful models.  results for these augmented baselines are in table  . the  classic bow model achieved   .     on the yelp dataset   and represents a baseline that does not implement contextual  processing. the additional five baseline models range in  increasing modeling power  with the most powerful baseline  model achieving   .     a test accuracy that is very close  to the best performing rnn      of the difference .    the art performance on sentiment classification  dieng et al.         johnson   zhang        zhang et al.         without  explicitly building in contextual effects  often fine tuning  larger models trained using unsupervised methods  howard    ruder        sun et al.        yang et al.       .  methods for interpreting and understanding the computations performed by recurrent networks include  inspecting  individual units  karpathy et al.         using input and  structural probes  socher et al.        hewitt   manning          visualizing salient inputs  li et al.        ribeiro  et al.        arras et al.        murdoch et al.         analyzing and clustering rnn dynamics  elman               strobelt et al.        ming et al.        maheswaranathan  et al.      b   and studying changes due to perturbed or  dropped inputs  ka da r et al.       . for a review of these  methods  and others  see belinkov   glass       .      . discussion  table  . test accuracies across baseline models and rnns for the  yelp and imdb datsets.  y elp         imdb    baseline models  bag of words  c onvolution of bod   eod tokens  c onv.   bod   eod tokens     bod     eod  c onvolution of m odifier w ords  c o mw   c o mw     mod  c o mw     mod     bod     eod      .       .       .       .       .       .         .       .       .       .       .       .       rnn models  gru  c ho et al .         lstm  h ochreiter   s chmidhuber          u pdate g ate rnn  c ollins et al .         vanilla rnn      .       .       .       .         .       .       .       .         . related work  understanding context has long been an important challenge  in natural language processing  morante   sporleder         mohammad       . in particular  including pairs of words   bigrams  as features in simple bow models significantly  improves classification performance  wang   manning        . polanyi   zaenen        introduced the idea of   contextual valence shifters   which model contextual effects by shifting valence for a hand crafted set of modifier  words. further work refined these ideas  demonstrating  their usefulness in improving sentiment classification accuracy  kennedy   inkpen        taboada et al.          identifying the amount and scope of the shifts combining  human annotators  ruppenhofer et al.        schulder et al.         kiritchenko   mohammad        and automated  methods  choi   cardie        ikeda et al.        liu    seneff        li et al.        boubel et al.         and finally  using these lexicons to regularize rnns  teng et al.         qian et al.       . more recently  due to larger and more  readily available datasets  neural networks achieve state of     in summary  we provide tools and analyses that elucidate  how neural networks implement contextual processing.  here  we focused on unidirectional recurrent networks. extending these tools to other tasks and architectures  such  as those that use attention  bahdanau et al.        vaswani  et al.         is a promising future research direction. for  example  we predict that the reverse direction of a bidirectional rnn  schulder et al.        would reveal backwards  modifiers  e.g. the suffix  less which has a suppressive effect  on the preceding word stem  as in soulless .  our analysis reveals rich modifier effects and timescales  learned by the rnn. these properties are remarkably consistent with properties of modifiers from the linguistics literature  including  the length or scope of contextual effects  lasting a few words  chapman et al.        taboada et al.         reitan et al.        strohm   klinger         the  asymmetry in the strength and number of negators vs intensifiers  horn        kennedy   inkpen        taboada  et al.        schulder et al.         the relative weighting of  different intensifiers  ruppenhofer et al.         and the fact  that negation is better modeled as an additive effect rather  than a multiplicative effect  zhu et al.       .  this speaks to the general scientific program of analyzing  optimized universal dynamical systems to provide insights  into the underlying structure of natural data.    acknowledgements  the authors wish to thank alex h. williams  larry abbott  jascha sohl dickstein  and surya ganguli for helpful  discussions.     how recurrent networks implement contextual processing    