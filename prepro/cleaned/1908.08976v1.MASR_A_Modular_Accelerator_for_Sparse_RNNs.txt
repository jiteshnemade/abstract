introduction  automatic speech recognition  asr  is at the foundation  of many popular services  streamlining the human machine  interface         . recent advances in asr have come from  replacing traditional methods based on gaussian mixture  models and hidden markov models with deep learning  namely  recurrent neural networks  rnns . rnns learn relationships  in time series data by establishing a temporal context called  the hidden state partial predictions between time adjacent  neurons that improve the interpretation of sequential data   e.g.  spoken utterances . today  rnns are the state of theart solution for highly accurate asr                   .  the hidden state of rnns introduces a unique memory  consumption problem that is addressed in this paper. figure    compares the fraction of memory used by activations and  weights across four deep learning models. well known  cnnbased image classification models devote most of their memory  resources to storing weights. in contrast  nearly     of  the memory needed for deep speech    ds    a stateof the art  rnn based asr model is for activations  both  inputs and hidden states   which consumes significant onchip storage. this does not preclude the issue of weights  also consuming significant memory    mb for deep speech    . these memory requirements are a result of asr rnns  often using bidirectional layers   inputs to each layer are  to appear in   th international conference on parallel architecture and  compilation techniques  pact          activation memory   input   hidden states    mb    mb   mb    mb    fig.    the memory footprint of activations is higher in  deep speech    ds    an asr rnn  than in standard cnns.  thus  to reduce storage costs of asr rnns  memory system  optimizations are needed for both activations and weights.  processed twice  once forwards in time and once backwards   and work over hundreds to thousands of time steps  i.e.    to     seconds     . the hidden state size scales with the number  of time steps  and separate weights are maintained for forward  and backward passes.  aggressive optimizations are needed to reduce the memory  costs of storing both activations and weights  as well as the  heavy processing load. one promising solution is leveraging  sparsity for storage and computational efficiency. however   while many techniques for weight pruning and compression  have been proposed  relatively little has been done to compress  activations. to improve computational efficiency  inferences  can be computed directly on the sparse encoding. sparse  processing allows the hardware to elide all null operations  at the expense of introducing irregularity. irregularity leads  to hardware inefficiency from low utilization  and the optimal  sparse encoding is application dependent. in addition to not  considering activation sparsity  existing  cnn centric solutions                       are either not applicable to rnns or perform  poorly. enabling ubiquitous asr requires accelerating rnns  with algorithm architecture co design for sparse storage and  efficient execution.  this paper presents masr  a modular accelerator to efficiently process sparse rnns. through algorithm architecture  co design  masr achieves high hardware utilization while  never wasting area nor energy on superfluous computation.  to demonstrate the efficacy of the proposed technique  we  start by aggressively optimizing our baseline rnn with  knowledge distillation  language modeling  weight pruning   and quantization. the key research contributions of masr  fall into three categories  a hardware accelerator that exploits  sparsity in both weights and activations to skip null values in  execution and storage  a co designed sparse encoding technique  for both activations and weights that enables highly parallel  architectures  and a mechanism for dynamic load balancing to     table i  comparing masr to related work in terms of  support for sparse execution and storage running rnns.  sparse weight  exec. storage  e pur       minerva       sparsenn       camb x       ese eie      masr    sparse act.  exec. storage    dyn. load  balancing    x  x  x  x  x    x  x  x    x  x         .   .     .     .                     .                                         .          .          .               bitmask  masr   pushes complexity to logic    x    compact values  .   .     weight matrix       x                                                                                                                            .          .   .      .  .   .             compressed sparse row  eie   col pointers         complexity in  memory  row pointers                                                         maximize hardware utilization.  row  sparse asr rnn accelerator algorithmic optimizations  pointers   i.e.  knowledge distillation  and masr s co designed microactivations  activation mask  architecture exploit sparsity in weights and activations leading  weight mask  col pointers  to improved performance  area  and energy by          and  activations      compared to a dense asr rnn baseline.  weights  in order to reduce storage and computational burdens of asr  rnns  activations must be sparse. however  unlike cnns  rnn  eie  masr  activations  inputs and hidden states  are not typically sparse. to  achieve hidden state sparsity we use knowledge distillation       to train rnns with relu  at no loss in accuracy compared fig.    compared to compressed sparse row encoding  e.g.   to gru baselines with tanh. furthermore  we maintain input eie       masr s bit mask encoding pushes the complexity  sparsity across layers by refactoring the batch normalization in sparse encoding away from storing pointers to logic. while  operation. these modifications expose sufficient sparsity in storage for costly row pointers in eie scales with the number  rnns to co design our sparse activation weight encoding.  of pes  masr s sparse encoding storage overhead is constant  sparse encoding masr s low cost and scalable sparse   providing scalability.  encoding technique  provides a    area     energy  and  .    performance benefit relative to a start of the art sparse dnn deployed across a wide set of applications and platforms        accelerator    .                  . given their ability to achieve state of the art  masr s sparse encoding format is co designed with the accuracy in a broad range of applications  dnns have gained  underlying architecture to address both compute and storage a lot of attention from the architecture community. however   bottlenecks. existing sparse encodings  in addition to not much of the effort has been devoted to optimizing dnns with  compressing activations  exhibit high meta data costs stemming only fc and cnn layers                                          from encoding overheads. masr proposes a binary mask                                                                    sparse encoding scheme for both weights and activations. in           . rnns  used widely in asr and natural language  storing bits rather than pointers  masr replaces expensive processing  pose unique challenges. for instance  activations  memory addressing with cheap bit wise operations.   inputs and hidden states  generated at run time comprise a  dynamic load balancing masr dynamically balances load higher fraction of the memory consumption in rnns than in  from the irregular distribution of non zero activations to fc cnns  figure   .  improve performance by up to     and achieve high macbeyond accelerators for dnns and cnns  other work has  utilization across a wide range of parallel design points.  investigated rnns  and search algorithms for asr and machine  irregularity introduced by sparsity can lead to poor hardware  translation                                         . shown in  utilization                 . masr is designed to maximize  table i  e pur      provides a hardware accelerator that  utilization by      considering both intra  and inter neuron  maximizes weight locality in dense rnns. similarly  the  parallelism  and     employing a decoupled pipeline to separate  authors in                  leverage the temporal locality of  the irregularity from sparsity from the computation of partials.  dense rnns to accelerate them on fpgas. in contrast  masr  once work is issued to the backend  the pipeline does not  exploits sparsity in both weights and activations to further  stall  regardless of the sparsity pattern. the remaining source  improve performance  area  and energy efficiency.  of low utilization arises from load imbalance   pipelines with  to accelerate asr  the authors of      design a memorymore sparsity complete before others. to improve hardware  efficient viterbi search accelerator. this targets language  utilization we propose a dynamic load balancing technique to  models that are run after processing all timesteps and layers  re distribute activations at run time with negligible area and  in the rnn. however  with even large language models  stateenergy overheads.  of the art asr models           spend over     of their  execution time on the rnns  section iii c   making rnns  ii. r elated w ork  the performance bottleneck and the focus of this paper.  accelerating asr rnns deep neural networks entail a  exploiting sparsity for hardware efficiency table i comgeneral class of machine learning models that have been pares masr to previous hardware accelerators based on their        gt      batch norm    batch norm    batch norm    yt      yt    yt                     relu            gt    vh    relu    vx wx    ht      wh    vh    relu      relu  vx wx    ht    gt   relu      wh    table ii  ds       model before optimizations    .  wer   convolution         relu    ht      bidirectional gru    fully connected    layers                   parameters       k      m      k    a. recurrent neural networks    vx wx    recurrent layers build context in time series data using  hidden states to learn feature patterns in long term data. there  are three popular recurrent layers  vanilla rnn  hereafter  fig.    bidirectional rnn layer. xt   ht   and gt are the input  referred to as rnn   gru       and lstm     . they differ  and hidden states at time step t. wx   wh   vx   and vh are the  in how new inputs are used to update hidden state. rnns use  forward and backward weights.  two sets of weights  one for inputs and one for hidden states.  grus and lstms expand upon rnns with additional gated  support for sparsity in weights and activations  and dynamic skip connections. these can improve accuracy by increasing  load balancing. typically  previous work either exploits sparsity expressiveness  however  the additional connections increase  in weights or activations  but not both                         model size  where grus and lstms have    and    more                   leaving key performance  area  and energy weights than rnns  respectively.  savings on the table. dnn accelerators that do exploit sparsity  recurrent layers can either be unidirectional or bidirectional.  in both weights and activations use dataflows and sparse unidirectional layers update hidden states based entirely on  encodings specific to cnns          . thus  to highlight the information from previous time steps. bidirectional layers  key contributions made in this paper  we provide in depth maintain two separate hidden states  one based on inputs  comparisons to eie    .  from the past and one based on inputs from the future. while  while eie     exploits sparsity in both weights and acti  bidirectional layers can achieve higher accuracy  they require  vations  it does not store activations in a compressed format. twice the number of parameters and operations.  furthermore  eie uses compressed sparse row  csr  encoding.  figure   illustrates a bidirectional rnn layer with relu  as shown in figure    csr maintains separate row and column activation and batch normalization. from the bottom  first a  pointers to track non zero weights. row pointer storage scales time series input xt is transformed by matrices wx and vx to  with the number of hardware pes  levying high memory costs produce input intermediates  red . hidden states ht   and gt    in more parallel architectures. in contrast  masr uses a simpler are transformed by matrices wh and vh   respectively  to produce  sparse encoding that pushes the complexity of computing forward and backward hidden intermediates  blue . new hidden  addresses for sparse parameters away from memory and into states ht and gt are then computed by passing the sum of the  low cost logic. this facilitates scaling the architecture to highly input and hidden intermediates through relu. the sum of  parallel designs  see section iv b for details .  these hidden states is output as yt .  load balancing for sparse neural networks exploiting  sparsity in weights and activations comes at the expense of  introducing irregularity into an otherwise regular workload.  irregularity leads to low hardware utilization from load im  b. target model  deep speech    balance. prior work considers pruning to statically balance  deep speech    ds       is an industry and academic  weight sparsity     . however  we find that the main source of  standard speech to text benchmark     . it directly maps input  imbalance in rnns is the distribution of non zero activations.  speech spectrograms to characters. table ii describes the  thus  masr exploits a novel dynamic load balancing technique  architecture using an implementation based on grus. first   that balances non zero activations at run time  section viii .  a pair of cnn layers extract relevant features from the input  spectrogram and reduce the length. next  bidirectional recurrent  layers  which can either be gru or rnn layers      learn timeiii. automatic s peech r ecognition  series context. finally  a fc layer makes output predictions   a probability distribution over characters at each time step.  automatic speech recognition  asr  transcribes an input x this distribution can be combined with a language model to  into text. the input speech is represented as a discrete time produce better transcribed text.  series of continuous feature vectors x    . . .   xt derived from a  our models for asr are trained using the open source  spectrogram of power normalized audio clips. current state  ds  implementation in pytorch                  on the openof the art models for asr rely heavily on deep learning for source librispeech corpus    . the gru network  described  acoustic modeling           . recently  rnns have become in table ii  has a word error rate  wer  of   .   comparable  the standard end to end deep learning approach for asr      to ds  networks with a greedy decoder     and the target for     . this section first provides an overview of rnns and how standardized speech to text benchmarks     . the gru layers  they are used in asr. we then simplify the neural networks make up over     of the model s parameters. thus  this paper  to establish an efficient baseline rnn for asr.  focuses on optimizing the recurrent layers of ds .  xt      xt    xt            gru   teacher   distill  distill gru  rnn    parameters  based on absolute value  and then retraining the  network                 . the number of non zero parameters  can be reduced to     in the already distilled rnns  isoaccuracy .  sparse linear quantization reduces the storage overheads  of parameters by transforming them from    bit floating  point type to reduced precision. by applying simple linear  quantization                       the pruned and distilled network  can be represented in fixed point format with    bits. however   after pruning  the remaining non zero parameters follow a  skewed distribution with either high negative or high positive  magnitude. thus  before applying quantization  we separately  scale the magnitude of the positive and negative weights to  fit within the range       . this enables further reducing the  precision down to    bits without sacrificing accuracy.    acoustic  model  masr lm  beam      beam          beam           x    fig.    left  after distillation  a   layer bidirectional rnn  reaches the accuracy of a   layer bidirectional gru reducing  the number of parameters by   . right  language modeling  reduces the wer from    down to   .  with a beam width of     . language modeling accounts for only     of cpu time.  c. an efficient rnn baseline  masr builds on a very efficient baseline design that includes  several previously proposed optimization techniques to improve  performance  on chip area  and energy costs of dnns. these  techniques knowledge distillation  language modeling  weight  pruning  and quantization were adapted for asr rnns.  knowledge distillation is a technique used to train a  smaller  less complex student network to mimic the predictions  of a large  pre trained teacher network by penalizing it for  diverging from the teacher s scores     . the teacher network  is the   layer bidirectional gru  shown in table ii  while the  student models are a   layer gru and a   layer rnn. using  distillation alone is insufficient to recover the baseline accuracy  of the teacher network. instead we start with distillation  and then fine tune for asr with ctc      for   epochs.  this combination yields student networks with the same  accuracy as the teacher  figure   . compared to the   layer  teacher gru and a   layer rnn  iso accuracy using traditional  supervised learning   the distilled   layer rnn has    and   .   fewer parameters  respectively.  all recurrent layers have      hidden units  wherein weight matrices are    x   .   knowledge distillation reducing a dense dnns to smaller  ones  improves performance and energy significantly and  provides immediate benefits on cpus  gpus  and specialized  hardware.  language modeling is a post processing step that reduces  the wer by modifying the output of the rnns  after all layers  and timesteps  based on language semantics and structure     .  this can be done greedily or using beam search  which  maintains many likely speech to text transcriptions  determined  by the beam width . figure   right  shows the decrease in  wer as we increase the beam width from    greedy  to      in increments of power of two. while the execution time for  previous generations of asr models has been dominated by  the language model       this is not the case for newer ones like  ds . with a beam width of      only     of the cpu time  is spent on performing beam search  the rest is spent running  the rnn. furthermore  previous work has proposed specialize  hardware to accelerate beam search by at least        . thus   for the purposes of this study  we focus on accelerating the  core rnn layers  the main performance bottleneck.  weight pruning eliminates less important weights and  transforms dense matrix vector multiplications to sparse ones.  pruning is performed by iteratively zeroing and masking out    table iii  efficient rnn baseline after model optimizations.  layer type    activation    layers    params    bitwidth    nz      bi dir rnn    relu           m                together  the above mentioned optimizations improve performance  area  and energy by  .        and     respectively.  the parameters for the efficient baseline are shown in table iii.    d. supporting recurrent networks more generally  while the remainder of this paper focuses on accelerating  the asr rnn baseline  the key contributions of masr apply  to recurrent neural networks with weight and activation sparsity  more generally. first  rnns have crafted various speech  recognition networks  notably transducer  e.g.  ds    seq seq   and attention based architectures. previous work has trained  these architectures with relu activated rnns  enabling the  activation sparsity that masr exploits     . next  the ds  style  rnn studied in this paper forms the encoder in multi stage  asr networks           . finally  the core micro architectural  contributions also apply to gru based networks with pruned  weights and relu non linearity for sparse activations. such  relu activated grus have also been used in transducer   seq seq  and attention based speech recognition networks                 .  in order to optimize this vast design space of recurrent neural  networks  masr can be configured with a combination of  dynamic and design time parameters. dynamic  run time parameters include number of hidden units  number of timesteps   and whether the rnn is uni directional or bi directional  see  section v for details . design time parameters include whether  the network is an rnn or gru  and the maximum recurrent  network size supported.  iv. o ptimizing dynamic activations  as shown in figure    activations are the primary memory  bottleneck of rnns. this section presents the methods used to  enforce sparsity in activations and the proposed sparse encoding  algorithm.        csr  eie   identify weights    read activation    get weight address    start row pointer    uncompressed  activation    weight column  indices    end row pointer    w    w    w    w       read weight  weights    w    w    w    w       masr              compressed  .   .      .   .                       and                             lnzd        .   .   .   mask                   compressed  .   .   .     x     weight mask                        popcount    weight            act. mask                        popcount    act       activations    activations    pe     pe     pe     horizontal pes    x   x     pe     pe     pe     activations    activations    activations         output activations      y      fig.    overall topology of how weight matrices  input  activations  and output activations are split across the masr  architecture. masr is organized as a  d array of horizontal   output neuron dimension  and vertical  input neuron dimension  pes lanes.    sparse  address         activations         x    y     work mask         w    w     w    w        vertical pes    mask              x     weights       x     lane       a  block diagram of compressed sparse row encoding  eie  and  masr s bitmask encoding. all memory blocks are in purple.  compared to csr  memory centric sparse encoding technique  masr  proposes using a logic centric sparse encoding technique.    x     lane      activations    lane      weight mask    x     weights    population  count    input activations    leading  nz detect    x     lane      activation mask         lanes    read  weights and activation    compute  address    find next work    read sparsity masks    weight matrix         activations    transform employs non zero shifts  i.e.         that map sparse  inputs to dense ones. however  during inference  the linear  transformation can be statically refactored into the next layer s  weights at zero cost          x   k  xsp   k  k       k                            b  concrete example of masr s sparse encoding. step   determines  the pairs of non zero weights and activations  to produce the work  mask  using a logical and. step   computes the next non zero weight  and activation to fetch from using with a leading non zero detect.  finally  step   evaluates the address of sparse weights and activations  stored compactly in memory.    fig.    masr s sparse encoding compute sparse addresses for  weights and activations in logic using low cost hardware.    to refactor this computation  we multiply the next layer s  weights and biases by the k  and k  constants  respectively.  note that this refactoring is applicable to a broader set of neural  networks that use batch normalization through depth           .  after refactoring  inputs are on average     zeros.    a. activation sparsification    sequential processing  the core computation kernels of  rnns are the matrix vector multiplications for the input and  hidden states  ht   relu wx xt  wh ht    . these kernels can be b. compact activation storage  computed either in parallel or sequentially in time. e pur       operating over compressed weights and activations introproposes maximizing weight locality by first computing the  duces  two challenges      aligning pairs of non zero weights  input connections  wx xt   for all time steps in parallel  followed  and activations  and     generating addresses for weights and  by the recurrent connections. even with aggressive    bit  activations stored compactly in memory. masr addresses these  quantization  this approach requires significant on chip storage  challenges by co designing a sparse encoding technique for    .  mb   outweighing the benefits of reducing on chip storage  both activations and weights. as shown in figure  a  the sparse  through weight reuse.  encoding technique uses a combination of bitmasks  a leading  masr computes each time step sequentially. sequential non zero detects  lnzd   and population counts. we start by  processing halves the amount of intermediate values to store. reading the weight and activation bitmasks. the bitmasks track  more importantly  as we use a relu activation function  by the sparsity pattern as bit vectors  where non zero entries are  sequentially processing each time step intermediates are sparse represented as ones. next  a bitwise and between the weight  and amenable to compression.  and activation masks  determines pairs of non zero weights  hidden state sparsity to further reduce on chip storage and activations and produces the work mask. the ones in the  requirements for activations  masr makes use of the sparsity work mask denote the absolute minimum work to compute. a  in inputs and hidden states. recall that our efficient baseline lnzd over the work mask determines the index of the next  model is a   layer rnn with relu  i.e.  max    x . training non zero weight and activation to fetch from memory. finally   with relu causes     of the hidden state values to be zero. population counts of the weight and activation masks  up to  input sparsity input sparsity is lost due to batch normal  the index specified by the lnzd  evaluates addresses of sparse  ization  a regularization technique that makes training larger weight and activations stored compactly in memory.  models easier  between layers. the operation adjusts and  example masr encoding figure  b provides a concrete  scales activations to have a zero mean and unit variance  example of masr s sparse encoding. the logical and between  x     x    sp           . where xsp represents the sparse inputs the weight        and activation        masks produces the        and             and   represent learned parameters. the linear work mask       . the lnzd over the work mask points        sparse weights    activation mask  register    o  accelerator memory    pe  inputs  sram  hidden  state  sram    inputs  mask  sram  hidden  mask  sram    pe  horiz    compact activation  register file    pe  vert  pe    relu    pe  pe    queue    weight mask    weight mask  activation mask  work  mask    pipeline stage    compute sparse  address    activation mask  work  mask    lnzd  amask    popcount    pipeline stage    read weights  and activations  act  sparse  addr    leading  nz detect    activation  register    activation    lane  n    partial sum  accumulator  biases  sram    pipeline stage    find next work    activation mask    activation  mask  register    horizontal  lanes  per pe  lane       frontend  matching nnz work    ctr    weight mask  sram    lnzd  wmask    popcount    weight  sparse  addr    pipeline stage    mac    activation  pos  acc  x         queue       neg  acc  weights  sram    weight    vvadd unit    masr    pe    lane    fig.    masr accelerator design highlighting the overall system architecture  left   a pe  center   and a lane  right . blocks  outlined in red represent tunable micro architectural parameters swept in the design space exploration. also in color  activation  registers  blue   and srams for binary masks  purple  and for compressed sparse activations and weights  green .  v. t he masr a rchitecture  as shown in figure    masr is composed of a  d array of  processing elements  pes  lanes that evenly split each weight  matrix in the horizontal  i.e.  output neurons  and vertical   i.e.  input neurons  dimensions. each pe is a collection of  lanes that share a local activation register file. each lane has  its own local weight and weight mask srams that store an  equal portion of the matrix. compact weight matrices  only  non zero elements  are loaded from off accelerator memories  directly into local srams. output neurons are computed  by accumulating the partial products across lanes in vertical  pes  i.e.  lane   in pe  and p  determine y  in figure   .  decoupling the execution across the    to      lanes is crucial  for extracting parallelism of the irregular sparse computations  at scale. figure   shows the detailed architecture for masr   focusing on rnn computations outlined in figure  . the  modular design is centered around the  d array of pes and  decoupled  pipelined lanes. in this section  we first explain how  bidirectional rnn computations are mapped to masr. then  we show how the underlying lane micro architecture handles  sparsity in weights and activations.    to index  . population counts up to index   for the weight         and activation        masks  compute the weight      and activation     addresses  respectively.  comparing masr to run length and csr the optimal  encoding is application specific and depends on sparsity and  matrix size. previous sparse dnn accelerators typically use  run length encoding or csr                     .  run length encoding maintains a step index that stores the  distance between non zero weights     . however  it does  not design for sparsity in activations  leaving key storage   performance  and energy savings on the table.  csr considers sparsity in both weights and activations. as  shown in figure  a  csr first reads the non zero activation  address  encoded using a run length style step index. the  non zero activation address then indexes separate row pointer  memories to identify the first and last non zero weights  corresponding to the given input activation. finally  the row  pointers are used to read column indices  also encoded using  a run length style step index  which generate the address of  weights stored compactly. while this approach works well for  model with high sparsity  it suffers from two main drawbacks.  first  null activations are skipped in execution not storage.  second  while column pointers scale with the number of nonzero weights  each mac pe maintains its own set of row  pointers in csr. as a result  row pointer memory scales  with the number parallel macs pes. figure   shows that as  the hardware scales from    to     parallel macs pes  row  pointers dominate the memory footprint.    a. mapping rnn computations to masr  to process speech samples  the accelerator runs each layer  of the bidirectional rnn in order. within each layer  the  accelerator first executes all time steps in the forward direction  and then in the backward direction. recall that each time step  of the rnn comprises two matrix vector multiplications  a  vector vector addition and relu  ht   relu wx xt  wh ht    .  to begin processing a layer  all weights for the forward pass   wx and wh   are loaded from off accelerator memory into the  compact weight srams within the lanes. weight srams have  a word width of    bits    weight each . likewise  all inputs   xt   are loaded into compact activation srams  which have a  word width of    bits  six activations each .  while masr processes all time steps in the forward pass   weights for the backward pass are concurrently loaded into  separate srams. this double buffering of the forward and  backward weights reduces performance penalties from not having the entire layer s weights stored locally on chip. similarly   activations beyond     timesteps  the average length of speech  samples in librispeech  are also double buffered. section vii c  discusses the design decisions of double buffering.    low overhead and scalable sparse encoding in contrast   the memory footprint for masr s sparse encoding technique  does not scale with the number of parallel macs pes. this  is a result of eliminating the row pointers and identifying  the necessary sparse weights and activations by computing  the alignment in logic. for instance  masr computes the  address of non zero weight and activation pairs in logic  as  shown in figure  a. the memory overheads for encoding  sparsity in masr are limited to binary masks  which are  determined by the size neural network model and not the  number of macs pes. thus  masr has a significantly lower  memory footprint compared to previous sparse neural network  accelerators  i.e.  eie  ese             see section ix for a  detailed quantitative comparison.        energy    area    fig.    left  energy performance and area performance pareto frontiers of accelerator designs  sweeping microarchitectural  parameters shown in figure  . center  speedup of varying masr designs normalized to a cpu running a dense baseline  rnn. right  breakdown of performance benefits for each proposed optimization on masr designs.  area efficiency  read power  leakage power                                            lanes                        table iv  masr design parameters  lanes  weights per lane  kb   weight masks per lane  kb   total weight  kb   total activations  kb   weight width  bits   activation width  bits   technology node  frequency  mhz     power  mw     mib   mm                   fig.    impact of increasing parallelism  lanes  on sram area  efficiency  dynamic read power  and leakage power.                                   .     .                        nm          file. given that lanes are decoupled  increasing the number  of lanes per pe requires additional ports to the physical  register file  which increases the register file s size and cost  per access. note that masr s decoupled pe lane architecture  does not depend on complex crossbar architectures that can  limit efficiency of highly parallel sparse dnn accelerators.  section vii explores the design space encompassed by these  parameters. table iv illustrates the parameters for lanesx     lanesx     and lanesx    .  outside of the pes  the masr architecture has two additional  parameters  depth of the back end queues and number of  banks for activation srams. the partial sum accumulators  accumulate the output of each column once all lanes in the  given vertical slice finish generating their partial product. lanes  that finish early are stalled  reducing the performance of the  overall design. increasing the depth of the back end queues  reduces this back pressure. however  this comes at an area  and energy cost  given each lane pushes partial products to  a separate back end queue. in addition  the vector vector add  unit can be parallelized. this involves not only duplicating the  number of adders but also partitioning the activation srams  into multiple banks  see section viii for details .    hidden state computation  for each time step t  the  accelerator first processes the matrix vector multiplication  for hidden state  wh ht   . this computation is initiated by  loading the previous time step s hidden states from the compact  activation sram to compact activation register files within  each pe. the entirety of the matrix vector multiplication is  parallelized across the  d array of pes with multiple decoupled  lanes. horizontal lanes evenly split columns  output dimension   of the matrix  while vertical lanes evenly split rows  input  dimension . this enables balancing parallelism across the input  and output dimensions. each lane is responsible for computing  partial products for a subset of rows and columns in the matrixvector product. as lanes finish processing each column  the  partial sum accumulator sums the partial products for each  output. an     element register file stores the outputs.  finishing one time step  the above sequence repeats  to process the matrix vector multiplication for inputs  wx xt .  once both matrix vector multiplications have been processed   the vector vector add unit accumulates the biases  input  intermediates  and hidden intermediates. the resulting output  values are thresholded with relu and compactly written to  the hidden state sram for the subsequent time step. this  b. the masr lane  completes one time step of the rnn layer.  the masr lane is the main computational workhorse for  hardware implications of parallelism  the number of  parallel lanes determines the degree of parallelism and how sparse vector matrix multiplication. each pipelined lane is  weights are partitioned across srams. with      lanes  each organized in two phases  front end and back end. intuitively      lane s weight and weight mask sram stores       of the the front end decodes work from weight and activation masks   parameters. similarly  number of vertical lanes determines the whereas the back end performs macs after accessing compact  size of the compact activation register files. for example  with weights and activations. this eliminates wasted work in the        vertical lanes  each register file only tracks     of the values. back end  regardless of the distribution of sparse weights   horizontal lanes in a row process the same portion of the activations  and outputs.  activation vectors. to reduce the cost of duplicated activations   the front end first reads the binary masks  for both weights  horizontal lanes within a pe share a physical activation register and activations. the binary masks are then anded together        lanesx      lanesx      lanesx       lanesx       lanesx       lanesx        fig.     the plots on the left summarizes area  top row   energy  middle row   and power  bottom row  tradeoffs for the  fully optimized designs for various masr design points. on the right we breakdown each optimization and each resource   weights  activations  sparse encoding masks  registers  and logic . the breakdowns are for the optimized baseline  distilled   section iii c   optimized  weight pruning   and act  sparse activations  fully optimized .  table v  topology of masr pareto front points.  accel  lanesx    lanesx    lanesx     lanesx     lanesx     lanesx        horiz lanes                            vert lanes                        vii. p erf.  a rea   e nergy  and p ower b enefits  optimal configuration of masr s modular architecture  depends on the intended use case. this section presents results  of an extensive design space exploration of masr s free  parameters that exposes energy performance tradeoffs. we then  analyze the performance  area  and energy power breakdowns  for points along the pareto frontier of the design space in  order to quantify the benefits of each optimization and identify  where resources are being consumed. for all experiments in this  section  we fix the depth of the back end accumulator queue  to a single element and assume one bank activation srams.  we report the performance  area  and energy consumed for an  accelerator provisioned to run a full seven seconds of speech   the average sample length in the librispeech corpus  across  multi layer bidirectional rnns. finally  we discuss how the  design scales when running shorter and longer speech samples.    horiz pes                      and the resulting work mask represents the absolute minimum  non zero weights and activations to accumulate.  the back end has four pipeline stages. stage   receives the  work mask and uses a single cycle lnzd to find the next  pair of non zero weights and activations. stage   computes  the relative addresses using the lnzd output and population  counts of the weight and activation masks. stage   reads the  weights sram and activation register file. stage   evaluates  the mac. separate accumulators are maintained for the positive  and negative weights as they were quantized separately  see  in section   . when the computation for the output neuron  finishes  the partial sum is pushed onto the queue.    a. design space exploration  masr s modularity enables both high performance and low  power solutions. the tunable microarchitectural parameters  considered in the design space  outlined in figure    include  the number of horizontal lanes  number of vertical lanes  and  number of horizontal pes. all possible configurations are swept  so that the total number of lanes ranges from   to      at  powers of    with a maximum of    lanes in either dimension.  sweeping the total number of lanes produces the energy areaperformance pareto frontiers illustrated in figure    left . as  parallelism increases  execution time and energy consumption  decrease while area increases. this is a result of partitioning  srams into smaller arrays in order to support the bandwidth  needed for more parallel datapaths. figure   shows that  partitioning srams decreases the power per read and perbit area efficiency. even in highly parallel architectures such as  lanesx      sram leakage is a small fraction of the overall  energy due to the highly optimized   nm finfet libraries.  in addition to lane count the organization of lanes pes  has an impact on accelerator performance. table v shows  pareto optimal designs tend to have more horizontal lanes  than vertical ones. increasing the number of vertical lanes    vi. e valuation m ethodology  the accelerator design space we explore is vast and each  point is evaluated running the entire forward and backward  passes of the bidirectional rnn. we validate a custom cyclelevel c   simulator of the accelerator with a synthesized  rtl implementation. we annotate the simulator based on ppa  characterizations from synthesized rtl using a commercial    nm finfet standard cell library at  ghz. to model the  sram area  energy  and power consumption  we use a  commercial memory compiler in the same process.  we also evaluate the benefits of sparsity on cpus and gpus  by profiling gemm and spmv kernels on real machines.  for cpu baselines we run the eigen library on a desktop  intel core i      k with simd support  using the  o  and   ffast math compiler flags. the gpu baselines run gemm  and spmv kernels provided by deep bench       using  cublas cusparse libraries on a nvidia gtx      gpu.        reduces the number of rows each lane processes  and thus also  reduces the activation register file size. for example  with    vertical lanes  the activation register files contain    words   with    vertical lanes  they contain   . processing a small  number of activations leads to load imbalance across lanes   degrading performance. a solution to this problem is discussed  in section viii. the following sections detail the performance   area  energy  and power characteristics of the optimal designs.    weights  read  from  dram    activations fetched from dram  logic  energy    sram energy    dram energy    b. performance  figure    center  shows the speedup by running a dense    layer bidirectional rnn and the efficient baseline on masr  and a gpu  normalized to running the dense network on a cpu.  performance is measured as the execution time to process the  full   seconds of speech. while the more programmable gpu  benefits from knowledge distillation   .    and weight pruning        it is unable to exploit activation sparsity. the masr  designs benefit from knowledge distillation  weight pruning   and sparse activation execution  improving the performance of  the accelerator beyond that of the more programmable systems.  figure    right  breaks down the performance benefit that  each optimization offers. we find that the overall benefits of  sparse optimizations diminish as parallelism increases. while  each design observes speedup from knowledge distillation  and sparse weight execution  speedup from sparse activation  execution does not scale as gracefully. for example  we  find sparse activations provide up to  .    speedup on the  lanesx   design  while their benefit on lanesx     is  reduced to  .   due to higher load imbalance in more parallel  accelerator topologies. section viii proposes low cost solutions  to balance dynamic activations at run time   improving  speedup from sparse execution by  .   and allowing even  the most parallel designs to achieve near linear speedup.  output sparsity in addition to exploiting input neuron  sparsity  xt and ht   prior work exploits sparsity in output  neurons           . this requires predicting output neurons  that will be masked by relu. while the focus of paper is  on exploiting weight and activation sparsity  masr can also  support output sparsity. in particular  input intermediates  wx xt    and hidden state intermediates  wh ht   follow distinct distributions. batch normalizaton only operates on inputs  xt  zero  mean  causing hidden state intermediates to be more negative  than input intermediates. highly negative hidden intermediates   computed first  will likely be zeroed out by the relu function  even after accumulating with input intermediates. thus  input  intermediate calculations are skipped if the corresponding  hidden intermediate is sufficiently negative  akin to the output  sparsity predictor in     . figure   right  shows that output  predication  op  improves performance by up to    .    fig.     energy breakdown of lanesx    running speech  of varying length overlaid with the distribution of samples  found in librispeech. dram energy cost  for double buffering  weights and activations  is small.    with varying degrees of parallelism  three sets of resource  breakdown bars correspond to design variants provisioned to  run a dense   layer bidirectional rnn  base   the efficient  baseline that applies knowledge distillation and weight pruning   opt   and with sparse activations  as .  area  figure     top row  right  shows the area footprints of  each accelerator design. partitioning weight and weight mask  srams diminishes the benefits of compactly storing weights  in more parallel designs. for instance  for the lanesx    architecture  starting from  base   weight sparsity  opt  reduces  area from  . mm  down to  . mm    . mm  benefit . on the  other hand  weight sparsity reduces lanesx     area from   . mm  to  . mm    . mm  benefit .  after compressing the weights  quantized activations consume up to     of the accelerator area  especially in the  smaller lanesx       designs. compact activation storage  reduces the memory area consumed by activations by   .  this corresponds to reducing the area devoted to activations  alone from  . mm  to  . mm  . given masr s modular design   compact activation storage provides the same area benefit to all  design variants  input and hidden state memories are maintained  outside of the pes lanes. this modularity also facilitates scaling  the architecture to domains that may require processing much  larger speech samples    . for instance  provisioning masr to  process up to    seconds of speech  compact activation storage  would save  . mm    reducing overall area by  .  .  after weights and activations  the remaining area is consumed by registers and logic. the increase in register area  across more parallel designs is dominated tracking more  weights and activation bitmasks per lane. these bitmasks  account for over     of the register area. the secondary  consumers      of the register area are the backend queues.  energy and power  the energy breakdown across each  accelerator design is shown figure    middle row . the left  column shows that lanesx    is the energy optimal design  c. area  energy  and power  point even though lanesx     uses smaller srams that  figure    shows the area  energy  and power breakdowns for dissipate less read power. the reason is two fold  per read  each design point along the energy performance pareto frontier. energy cost plateaus in the most parallel accelerator designs   the left column illustrates the overall trends as the accelerator and the proportion of power consumed by registers increases  scales to more parallel design points. the remaining columns for larger accelerators. as previously discussed  the first  on the right side provide detailed resource breakdowns. to effect occurs because the smaller memories used in the more  understand the benefits of each optimization in accelerators parallel designs  lanesx    and lanesx      do not reduce        resource constrained     . x speedup  trade o  stall for idle cycles    parallelize vv add    lanesx      lanesx       lanesx        fig.     left  as we scale the number of activation sram banks in the lanesx     architecture  the cycles spent on vvadd  decreases. center  scaling the depth for back end accumulator queue trades off stalls for idle cycles. right  the impact of  increasing the number of activation sram banks  vv  combined with either horizontal load balancing  vv hlb  or vertical  load balancing  vv vlb  on the performances of lanesx    lanesx     and lanesx    ..  dynamic read power proportionally to capacity reduction. the  second effect comes from more parallel designs requiring the  maintenance of more active states.  generally  energy savings come from doing less work and  making fewer sram accesses. for example  figure     middle   shows that compared to the dense rnn  base   the efficient  baseline reduces the energy consumed by  .    i.e.   .    from knowledge distillation     from weight pruning  across  all accelerator designs. similarly  sparse activation execution  further reduces energy by around  .  .  because of masr s sparse encoding mechanism  sparsity  optimizations impact energy more than power. as long as  work remains  a mac is issued to the lane on every cycle   keeping power relatively constant. the  .   power reduction  between dense rnn  base  and optimized baseline  opt  comes  from decreasing the size of weight srams by storing fewer  non zero parameters  figure     middle and bottom rows .  d. supporting speech of arbitrary length  asr models comprise millions of parameters which cannot  be realistically stored on chip. the storage requirements are  further exacerbated when considering activation memory for  longer speech samples. to minimize on chip sram  masr  double buffers both weights and activations.  performance and area using lpddr  as off accelerator  memory  the performance and energy penalty of double  buffering is relatively low. masr double buffers forwards  and backwards weights in separate srams. lpddr  supports  a bandwidth of   . gb s while dissipating    mw of power      . at this rate it takes  .   ms to read in a layer s weights.  this corresponds  roughly  to the time it takes process      timesteps of speech. thus  to avoid memory contention  masr  stores activations for the first     timesteps  the average length  in librispeech  on chip. activations for later timesteps are  double buffered within an     element register  which incurs  negligible area overheads. similarly  there is no performance  penalty since the time to read activations from lpddr  is  strictly less than the time to process a single timestep.  energy figure    illustrates the dram energy cost relative  to on chip sram and logic for samples from    timesteps  to      timesteps. for samples less than     timesteps  the  dram energy consists solely of reading weights. this energy  overhead is amortized with longer speech samples. for instance         at     timesteps  dram consumes about     of the energy.  speech samples longer than     timesteps  incur an additional  energy penalty for reading activations from dram  however   dram energy remains a small fraction compared to sram and  logic. thus  masr supports arbitrary length speech samples  at negligible performance  area  and energy cost.  viii. s calability for end   to   end rnn  as the number of parallel lanes increases  two main  performance bottlenecks emerge  vector vector add  vvadd   operations and load imbalance. we address these bottlenecks  by      parallelizing the vvadd operations with multi banked  activation srams  and     dynamic load balancing for sparse  activations. these optimizations improve performance by up  to  .    allowing highly parallel designs to achieve high mac  utilization while executing with sparse weights and activations.  parallelizing vvadd each time step in rnns includes  two matrix vector multiplications and a vvadd  i.e.  wx xt    wh ht    . although the matrix vector multiplications are the  core kernels  figure     left  shows that with a single activation  sram bank  the lanesx     design s mac utilization is  only      while the largest fraction of cycles       devoted to  vvadd. this is due to bandwidth limitations of the activation  srams. recall that the word width of the activation sram is     bits  limiting vvadd operations to   per cycle. partitioning  the compact activation memory into smaller banks enables  parallelizing the computation. partitioning the memory into    banks decreases the fraction of cycles spent on vvadd  operations from     to     and increases the mac utilization  from     to     at a negligible area penalty.  a. dynamic activation load balancing  after parallelizing the vvadd operation  the next bottleneck  to scaling performance is load imbalance  a result of irregular  sparsity. previous work uses load balance aware pruning  where  the network is pruned during training such that each mac gets  the same number of non zero weights     . this static method  does not work for dynamic activation sparsity. moreover   the main source of load imbalance in rnns is the uneven  distribution of non zero activations across pes. to address  this dynamic load imbalance  we first trade off stall cycles for  idle cycles by increasing the depth of back end queues. we  then propose a low cost solution to dynamically redistribute     fig.     masr lanesx   placed and routed layout  b. scaling rnn size and sparsity  fig.     energy savings  top   energy consumption  middle    and performance impact of scaling to larger rnns  bottom   as density in weights and activations varies on lanesx   .  work to idle lanes  which improves the mac utilization and  performance of the lanesx     architecture by  .  .  figure     center  illustrates the trade off between stall and  idle cycles as the back end accumulator queue depth increases.  stalls are caused by back pressure from the accumulator. idle  cycles are caused by some lanes computing their partial outputs  faster than others. with a queue depth of one  lanes spend      of their time stalled  and     of their time in the idle state.  by increasing the depth of the queues to    the fraction of stall  cycles falls to    whereas the fraction of idle cycles climbs  to    .  . mmm  . this comes at a negligible area penalty of   . mm  for the largest lanesx     design.  balancing non zero activations the high percentage of idle  cycles suggests redistributing work to lanes that finish early   by balancing load across both horizontal lanes and vertical  lanes. with horizontal load balancing  work is distributed to  lanes within the same pe. since all lanes within a pe process  the same activations  horizontal load balancing requires storing  additional copies of compact weights for neighboring lanes.  in practice  we only duplicate about     of the weights.  vertical load balancing distributes work to lanes across vertical  pes. this involves duplicating not only weights but also  activations. given activations are stored in local registers  the  cost of duplicating them is negligible. duplicating weights and  activations to enable load balancing also eliminates the need  for complex crossbar inter connects that often limit efficiency  for sparse dnn accelerators at scale.  hardware utilization figure     right  illustrates the impact  of each optimization on the performance of lanesx     lanesx     and lanesx    . to highlight how well each  design variant parallelizes sparse rnns  we normalize the  performance of each to its theoretical speedup over serial  execution. vertical load balancing outperforms horizontal load  balancing  because redistributing work across pes balances  the number of non zero activations  the main source of  load imbalance. moreover  lanesx    lanesx     and  lanesx     designs achieve           and     utilization.        recent advances in the machine learning community allow  training sparser networks without sacrificing accuracy                  . this suggests further performance and energy  improvements may be possible with even higher sparsity. to  study the robustness and scalability of masr  we artificially  scale weight and activation non zero ratios  using synthetic  rnn benchmarks  for the energy optimal lanesx    design.  figure    top  middle  plots the energy savings and consumption as the non zero ratio in weights and activations scales  from     to    . as the energy saved from sparse execution  depends on the non zero ratio  not model size   we consider  rnns with      hidden states. masr s energy efficiency  improves with greater sparsity  a result of fewer memory  accesses. for example  the energy savings at non zero ratios  of     and     are     and     respectively. at lower  non zero ratios  sparse encoding overheads limit savings as  weight masks dominate energy consumption.  figure    bottom  plots the impact of sparsity on performance across a range of network sizes. as expected  speedup  from sparse execution improves with greater sparsity. for  rnns with      hidden units  sparse execution yields a   .    and     speedup with     and     non zeros  respectively.  finally  we find that performance improvements of sparse  execution scale better for larger models. for instance  with      non zeros  the speedup is     for the rnns with       hidden unit. this due to better load balancing in larger models.  thus  we expect masr s architecture to scale well with larger  asr rnns and advanced pruning techniques are applied.  c. hardware implementation  results shown thus far are based on cycle level c    simulations with power models derived from synthesized rtl.  a pe of a lanesx   rtl was placed and routed  as shown  in figure     using a commercial   nm finfet standard cell  library and memory compiler. we validate our simulation results  within     power and     area and find negligible difference  in performance. a fabricated soc based on lanesx   design  has been received from fabrication.  ix. d iscussion  this section provides a quantitative comparison between the  masr accelerator and two other accelerators  shown in table i      from    to     parallel macs lanes. in addition to the area  benefits  masr consumes    and    less energy than ese  and eie  respectively  as it scales beyond     macs lanes.  these energy savings come from masr s lower overhead  sparse encoding mechanism. for each row in the matrix  a pe  in ese and eie reads two row pointers to determine the first  and last non zero weights. row pointer accesses scale with  the number of parallel  and  like the area overheads  energy  consumption is dominated by these row pointers for more  parallel designs. masr eliminates the cost of reading row  pointers by computing the sparse indexing in logic.  x. c onclusion  fig.     area normalized to lanesx    top  and energy  normalized to lanesx     bottom  consumed by masr   eie and ese as parallel macs lanes scale. all accelerators  are implemented in same   nm process technology.    we present masr  a novel bidirectional rnn accelerator  for on chip asr that exploits sparsity in both dynamic  activations and static weights  compacts storage of non zero  parameters  and wastes no energy at all on null computations.  compared to a state of the art sparse dnn accelerator       masr improves performance  area  and energy by  .         and     respectively. masr s modular architecture provides  scalable designs ranging from resource constrained low power  iot applications to highly parallel datacenter deployments.    for sparse neural networks with different design objectives  ese        weight sparsity  and eie      both weight and activation  sparsity . based on each accelerator s memory access patterns   we accumulate the cost of the weights memory  activations  memory  and sparse indexing. for fair comparison  each design  is implemented as a specialized asic with the same   nm  acknowledgements  finfet process and the same optimized rnn  see table iii.  this work was supported by the applications driving  performance assuming the same weight sparsity  activation  sparsity  and number of parallel macs pes  hardware utilization architectures  ada  research center  a jump center codetermines performance differences between the accelerators. sponsored by src and darpa  the nsf under ccf           the lanesx    design for masr demonstrates an     and intel corporation.  utilization  compared to     in eie    . this is a result  of ensuring no wasted work with the binary mask sparse  encoding and re distributing sparse activations for dynamic  load balancing in masr. for instance  csr adds superfluous  non zero values  up to     wasted work  and does not account  for imbalance in non zero activations. by exploiting activation  sparsity  masr has    higher performance than ese.  area figure     top  compares the area footprints of masr   ese  and eie  all normalized to the area of the smallest masr  design  lanesx     as the designs scale from    to      parallel macs lanes. the area for ese and eie are equivalent   as both store activations densely and weights compactly  using  csr. for smaller architectures  such as those with    or     parallel macs lanes  masr s area savings come from  compactly storing sparse activations.  area savings are more pronounced as the accelerators scale  to higher parallelism due to masr s lower overhead sparse  encoding mechanism. storage for row pointers  used in csr in  ese and eie  scales with the number of macs and dominates  for accelerators with more than     parallel macs . instead of  explicitly storing the row pointers in memory  masr computes  the addresses for sparse weights and activation in logic  see  section iv b for details . this consumes a fixed amount of  memory regardless of the number of parallel lanes. as a  result  masr has at least    smaller on chip area footprint  as accelerators scale beyond     macs lanes.  energy figure     bottom  compares the energy consumed  by masr  ese  and eie  all normalized to the energy of  the energy optimal design  lanesx     as the designs scale         