introduction    recurrent neural networks  rnns  have emerged as one of the most powerful tools for modeling sequences         . they are extensively used in a wide variety of applications including language modeling  speech  recognition  machine translation and computer vision                  . rnns are similar to the popular  feed forward networks  ffns   but unlike ffns  allow for cyclical connectivity in the nodes. this enables  them to have exceptional expressive ability  permitting them to model highly complex sequences. this  expressiveness  however  comes at the cost of training difficulty  especially in the presence of long term  dependencies        . this difficulty  commonly termed as the  vanishing exploding  gradient problem   arises due to the recursive nature of the network. depending on the eigenvalues of the hidden to hidden  node connection matrix during the back propagation through time  bptt  algorithm  the errors either  get recursively amplified or diminished making the training problem highly ill conditioned. consequently   this issue precludes the use of methods which are unaware of the curvature of the problem  such as stochastic  gradient descent  sgd   for rnn training tasks.  many attempts have been made to address the problem of training rnns. some propose the use of  alternate architectures  for e.g. gated recurrent units  grus      and long short term memory  lstm        models. these network architectures do not suffer as severely from gradient related problems  and hence   it is possible to use simple and well studied methods like sgd for training  thus obviating the need for more  sophisticated methods. other efforts for alleviating the problem of training rnns have been centered around  designing training algorithms which incorporate curvature information in some form  see for e.g. hessian free  newton          and nesterov accelerated gradient     .  first order methods such as adagrad     and adam       employ diagonal scaling of the gradients and  consequently achieve invariance to diagonal re scaling of the gradients. these methods have low per iteration  cost and have demonstrated good performance on a large number of deep learning tasks. second order  methods like hessian free newton      and k fac       allow for non diagonal scaling of the gradients using  highly expressive hessian information  but tend to either have higher per iteration costs or require non trivial    department    department    of industrial engineering and management sciences  northwestern university  evanston  il  usa.  of engineering sciences and applied mathematics  northwestern university evanston  il  usa.          information about the structure of the graph. we defer the discussion of these algorithms to the following  section.  in this paper  we present adaqn  a novel  stochastic  quasi newton algorithm for training rnns. the  algorithm attempts to reap the merits of both first  and second order methods by judiciously incorporating  curvature information while retaining a low per iteration cost. our algorithmic framework is inspired by  that of stochastic quasi newton  sqn       which is designed for stochastic convex problems. the proposed  algorithm is designed to ensure practical viability for solving rnn training problems.  the paper is organized as follows. we end the introduction by establishing notation that will be used  throughout the paper. in section    we discuss popular algorithms for training rnns and also discuss  stochastic quasi newton methods. in section    we describe our proposed algorithm in detail and emphasize  its distinguishing features. we present numerical results on language modeling tasks in section  . finally   we discuss possible extensions of this work and present concluding remarks in sections   and   respectively.     .     notation    the problem of training rnns can be stated as the following optimization problem   m    minn f  w       w r      x  fi  w .  m i        .      here  fi is the rnn training error corresponding to a data point denoted by index i. we assume there are  m data points. during each iteration  the algorithm samples data points bk                  m . the iterate at  the k th iteration is denoted by wk and the  stochastic  gradient computed on this mini batch is denoted by    b f  wk  . in particular  the notation      bj f  wk   can be verbally stated as the gradient computed at wk using     k  the mini batch used for gradient computation during iteration j. for ease of notation  we may eliminate the  subscript bk whenever the batch and the point of gradient evaluation correspond to the iterate index  in     wk   to mean      b f  wk  . unless otherwise specified  hk denotes any positive definite  other words  we use  f  k  matrix and the step length is denoted by  k . the positive definiteness of a matrix h is expressed using the  notation h    . lastly  we denote the ith component of a vector v   rn by  v i and use v   to represent  element wise square.         related work    in this section  we discuss several methods that have been proposed for training rnns. in its most general  form  the update equation for these methods can be expressed as     wk     vk pk    wk     wk    k hk   f      .         wk   is a stochastic gradient computed using batch bk   hk is a positive definite matrix representing  where  f  an approximation to the inverse hessian matrix  pk is a search direction  usually wk   wk     associated with  a momentum term  and vk     is the relative scaling of the direction pk .     .     stochastic first order methods    inarguably  the simplest stochastic first order method is sgd whose updates can be represented in the form  of   .   by setting hk   i and vk   pk    . momentum based variants of sgd  such as nesterov accelerated  gradient       use pk    wk   wk     with a tuned value of vk . while sgd has demonstrated superior  performance on a multitude of neural network training problems      in the specific case of rnn training   sgd has failed to stay competitive owing to the  vanishing exploding  gradients problem        .  there are diagonally scaled first order algorithms that perform well on the rnn training task. these  algorithms can be interpreted as attempts to devise second order methods via inexpensive diagonal hessian  approximations. adagrad     allows for the independent scaling of each variable  thus partly addressing  the issues arising from ill conditioning. adagrad can be written in the general updating form by setting          vk   pk     and by updating hk  which is a diagonal matrix  as        hk  ii   qp  k          j     f  wj   i             where       is used to prevent numerical instability arising from dividing by small quantities.  another first order stochastic method that is known to perform well empirically in rnn training is adam      . the update  which is a combination of rmsprop      and momentum  can be represented as follows  in the form of   .     vk           pk      k    x     k j                wj      f     wk               f    j      rk      k  x  j       k j              wj                 f        hk  ii   p  .   rk  i        the diagonal scaling of the gradient elements in adagrad and adam allows for infrequently occurring  features  with low gradient components  to have larger step sizes in order to be effectively learned  at a rate  comparable to that of frequently occurring features. this causes the iterate updates to be more stable by  controlling the effect of large  in magnitude  gradient components  to some extent reducing the problem of   vanishing exploding  gradients. however  these methods are not completely immune to curvature problems.  this is especially true when the eigenvectors of    f  wk   do not align with the co ordinate axes. in this  case  the zig zagging  or bouncing  behavior commonly observed for sgd may occur even for methods like  adagrad and adam.     .     stochastic second order methods    let us first consider the hessian free newton methods  hf  proposed in         . these methods can  be represented in the form of   .   by setting hk to be an approximation to the inverse of the hessian  matrix     f  wk     as described below  with the circumstantial use of momentum to improve convergence.  hf is a second order optimization method that has two major ingredients   i  it implicitly creates and  solves quadratic models using matrix vector products with the gauss newton matrix obtained using the   pearlmutter trick  and  ii  it uses the conjugate gradient method  cg  for solving the sub problems  inexactly. recently       proposed k fac  a method that computes a second order step by constructing an  invertible approximation of a neural networks  fisher information matrix in an online fashion. the authors  claim that the increased quality of the step offsets the increase in the per iteration cost of the algorithm.  our algorithm adaqn belongs to the class of stochastic quasi newton methods which use a non diagonal  scaling of the gradient  while retaining low per iteration cost. we begin by briefly surveying past work in  this class of methods.     .     stochastic quasi newton methods    recently  several stochastic quasi newton algorithms have been developed for large scale machine learning  problems  olbfgs           res       sdbfgs       sfo      and sqn    . these methods can be represented in the form of   .   by setting vk   pk     and using a quasi newton approximation for the matrix hk .  the methods enumerated above differ in three major aspects   i  the update rule for the curvature pairs used  in the computation of the quasi newton matrix   ii  the frequency of updating  and  iii  the applicability to  non convex problems. with the exception of sdbfgs  all aforementioned methods have been designed to  solve convex optimization problems. in all these methods  careful attention must be taken to monitor the  quality of the curvature information that is used.  the res and sdbfgs algorithms control the quality of the steps by modifying the bfgs update rule              . specifically  the update equations take on the following form   sk   wk     wk      b f  wk            b f  wk      sk    yk      k  k      hk      bk     bk      ykt yk  ykt sk         bk sk stk bk  stk bk sk      .      .         i.      .      this ensures that the hessian approximations are uniformly bounded away from singularity  thus preventing  the steps from becoming arbitrarily large. further  in these methods  the line search is replaced by a decaying  step size rule. note that at the k th iteration  the gradients used during updates   .   are both evaluated on  bk . olbfgs  is similar to the above methods except no   modification is used. in the equations above  bk  and hk denote approximations to the hessian and inverse hessian matrices respectively.  finally  in      the authors propose a novel quasi newton framework  sqn  in which they recommend the  decoupling of the stochastic gradient calculation from the curvature estimate. the bfgs matrix is updated  once every l iterations as opposed to every iteration  which is in contrast to other methods described above.  the authors prescribe the following curvature pair updates   st   w t   w t        where w t         h f  w t  st    yt      t       l    tl  x    wi        .      i  t   l      .      where t is the curvature pair update counter  l is the update frequency  also called the aggregation length   and ht is a mini batch used for computing the sub sampled hessian matrix. the iterate difference  s  is based  on the average of the iterates over the last  l iterations  intuitively allowing for more stable approximations.  on the other hand  the gradient differences  y  are not computed using gradients at all  rather they are  computed using a hessian vector product representing the approximate curvature along the direction s.  the structure of the curvature pair updates proposed in sqn has several appealing features. firstly   updating curvature information  and thus the hessian approximation  every l iterations  where l is typically  between   and     considerably reduces the computational cost. additionally  more computational effort  can be expended for the curvature computation since this cost is amortized over l iterations. further  as  explained in      the use of the hessian vector product in lieu of gradient differences allows for a more robust  estimation of the curvature  especially in cases when ksk is small and the gradients are noisy.  the sqn algorithm was designed specifically for convex optimization problems arising in machine learning  and its extension to rnn training is not trivial. in the following section  we describe adaqn  our  proposed algorithm  which uses the algorithmic framework of sqn as a foundation. more specifically  it  retains the ability to decouple the iterate and update cycles along with the associated benefit of investing  more effort in gaining curvature information.         adaqn    in this section  we describe the proposed algorithm in detail. specifically  we address key ingredients of the  algorithm  including  i  the initial l bfgs scaling   ii  step quality control   iii  choice of hessian matrix  for curvature pair computation  and  iv  the suggested choice of hyper parameters. the pseudo code for  adaqn is given in algorithm  .          algorithm   adaqn  inputs  w    l     sequence of batches bk with  bk     b for all k  ml       mf                        .                                                                                                                                         set t     and w o   ws      initialize accumulated fisher information matrix fifo container f  of maximum size mf and l bfgs  curvature pair containers s  y of maximum size ml .  randomly choose a mini batch as monitoring set m  for k            ... do     wk      compute adaqn updates using two loop recursion  wk     wk    hk  f        store  f  wk   f  wk  t in f   ws   ws   wk      running sum of iterates for average computation  if mod  k  l      then  w n   wls    compute average iterate  ws        clear accumulated sum of iterates  if t     then  if fm  w n      fm  w o   then    check for step rejection  clear l bfgs memory and the accumulated fisher information container f  .  wk   w o    return to previous aggregated point  continue  end if  s   w n   w o    compute curvature pair  p f       compute curvature pair  y    f       i   f i   s     if st y       st s then    check for sufficient curvature  store curvature pairs st and yt in containers s and y respectively  w o   w n  end if  else  w o   w n  end if  t t    end if  end for       wk   f     wk  t   in step   is for ease of notation  in practice it is  we emphasize that the storage of   f     sufficient to store  f  wk   and compute y in step    without explicitly constructing the matrix. also  the    k is computed via the two loop recursion using the available curvature pairs  search direction pk    hk  f   s  y    and thus the matrix hk  the approximation to the inverse hessian matrix  is never constructed  refer  to section a. . further  in algorithm    we specify a fixed monitoring set m  a feature of the algorithm  that was set for ease of exposition. in practice  this set can be changed to allow for lower bias in the step  acceptance criterion.     .            choice of hk    for l bfgs    firstly  we discuss the most important ingredient of the proposed algorithm  the initial scaling of the l bfgs       matrix. for l bfgs  in both the deterministic and stochastic settings  a matrix hk must be provided   which is an estimate of the scale of the problem. this choice is crucial since the relative scale of the step  in  each direction  is directly related to it. in deterministic optimization          hk         stk yk  i  ykt yk      .      is found to work well on a wide variety of applications  and is often prescribed     . stochastic variants  of l bfgs  including obfgs  res and sqn  prescribe the use of this initialization    .   . however   this is dissatisfying in the context of rnn training for two reasons. firstly  as mentioned in the previous        sections  the issue of  vanishing exploding  gradients makes the problems highly ill conditioned  using a  scalar initialization of the l bfgs matrix does not address this issue. secondly  since s and y are noisy  estimates of the true iterate and gradient differences  the scaling suggested in   .   could introduce adversarial  scale to the problem  causing performance deterioration.  to counter these problems  we suggest an initialization of the inverse hessian matrix based on accumulated gradient information. specifically  we set        hk  ii   qp  k               j     f  wj   i           i      ...  n.      .      we direct the reader to section a.  for details on how the above initialization is used as part of the l bfgs  two loop recursion. we emphasize that this initialization is   i  a diagonal matrix with non constant diagonal  entries   ii  has a cost comparable to   .    and  iii  is identical to the scaling matrix used by adagrad at  each iteration. this choice is motivated by our observation of adagrad s stable performance on many rnn  learning tasks. by initializing l bfgs with an adagrad like scaling matrix  we impart a better scale in  the l bfgs matrix  and also allow for implicit safeguarding of the proposed method. indeed  in iterations  where no curvature pairs are stored  the adaqn and adagrad steps are identical in form.     .     step acceptance and control    while curvature information can be used to improve convergence rates  noisy or stale curvature information  may in fact deteriorate performance    . sqn attempts to prevent this problem by using large batch hessianvector products in   .  . other methods attempt to control the quality of the steps by modifying the l bfgs  update rule to ensure that hk     for all k. however  we have found that these do not work well in practice.  instead  we control the quality of the steps by judiciously choosing the curvature pairs used by l bfgs.  we attempt to store curvature pairs during each cycle but skip the updating if the calculated curvature is  small  see      for details regarding skipping in quasi newton methods. further  we flush the memory when  the step quality deteriorates  allowing for more reliable steps till the memory builds up again.  the proposed criterion  line    of algorithm    is an inexpensive heuristic wherein the functions are  evaluated on a monitoring set  and   approximates the effect of noise on the function evaluations. a step  is rejected if the function value of the new aggregated point is significantly worse  measured by    than  the previous. in this case  we reset the memory of l bfgs which allows the algorithm to preclude the  deteriorating effect of any stored curvature pairs. the algorithm resumes to take adagrad steps and  build up the curvature estimate again. we report that  as an alternative to the proposed criterion  a more  sophisticated criterion such as relative improvement   fm  w n     fm  w o                   fm  w o    delivered similar performance on our test problems.  in the case when the sufficient curvature condition  line    of algorithm    is not satisfied  the storage  of the curvature pair is skipped. in deterministic optimization  this problem is avoided by conducting a  wolfe line search. if the curvature information for a given step is inadequate  the line search attempts to  look for points further along the search path. we extend this idea to the rnn setting by not updating w o  when this happens. this allows us to move further  and possibly glean curvature information in subsequent  update attempts. we have experimentally found this safeguarding to be crucial for the robust performance  of adaqn. that being said  such rejection happens infrequently and the average l bfgs memory per epoch  remains high for all of our reported experiments  see section   .     .     choice of curvature information matrix    as in sqn  the iterate difference s in our algorithm is computed using aggregated iterates and the gradient  difference y is computed through a matrix vector product  refer to equations   .   and   .  . the choice  of curvature matrix for the computation of y must address the trade off between obtaining informative  curvature information and the computational expense of its acquisition. recent work suggests that the        fisher information matrix  fim  yields a better estimate of the curvature of the problem as compared to  the true hessian matrix  which is a natural choice   see for e.g.     .  given a function f parametrized by a random variable x   the  true  fim at a point w is given by  f  w    ex   fx  w  fx  w t  .  since the distribution for x is almost never known  the empirical fisher information matrix  efim  is often  used in practice. the efim can be expressed as follows  f   w         x   i f  w  i f  w t     h       .       i h    where h                  m .  notice from equation   .    that the efim is guaranteed to be positive semi definite  a property that  does not hold for the true hessian matrix. the use of the fim  or efim  in second order methods allows  for attractive theoretical and practical properties. we exclude these results for brevity and refer the reader  to      for a detailed survey regarding this topic.  given these observations and results  the use of the efim may seem like a reasonable choice for the  hessian matrix approximation used in the computation of yt  see equation   .   . however  the use of this  matrix  even infrequently  increases the amortized per iteration cost as compared to state of the art firstorder stochastic methods. further  unlike second order methods which rely on relatively accurate curvature  information to generate good steps  quasi newton methods are able to generate high quality steps even with  crude curvature information     . in this direction  we propose the use of a modified version of the empirical  fisher information matrix that uses historical values of stochastic gradients  which were already computed  as part of the step  thus reducing the computational cost considerably. this reduction  comes at the expense  of storage and potentially noisy estimates due to stale gradient approximations. we call this approximation  of the efim the accumulated fisher information matrix  afim  and denote it by f  . given a memory budget  of mf   the afim at the k th iteration is given by  f   wk     pk    k  x         j k mf     bj   j k mf        bj f  wj   bj f  wj  t .      .       for the purpose of our implementation  we maintain a finite length fifo container f  for storing the  stochastic gradients as they are computed. whenever the algorithm enters lines        we reject the step   and the contents of f  along with the l bfgs memory are cleared. by clearing f    we also allow for additional  safeguarding of future iterates against noisy gradients in the f  container that may have contributed in the  generation of the poor step.     .     choice of hyper parameters    adaqn has a set of hyper parameters that require tuning for competitive performance. other than the  step size and batch size  which needs to be tuned for all aforementioned methods  the only hyper parameter  exposed to the user is l. we prescribe l to be chosen from               . we experimentally observed that  the performance was not highly sensitive to the choice of   and l. often  l     and the same step length  as used for adagrad gave desirable performance. the other hyper parameters have intuitive default values  which we have found to work well for a variety of applications. additional details about the offline tuning  costs of adaqn as compared to adagrad and adam can be found in section  .     .     cost    given the nature of the proposed algorithm  a reasonable question is about the per iteration cost. let us  begin by first considering the per iteration cost of other popular methods. for simplicity  we assume that the  cost of the gradient computation is o n   which is a reasonable assumption in the context of deep learning.  sgd has one of the cheapest per iteration costs  with the only significant expense being the computation  of the mini batch stochastic gradient. thus  sgd has a per iteration complexity of o n . adagrad and        adam also have the same per iteration complexity since the auxiliary operations only involve dot products  and elementary vector operations. further  these algorithms have o    space complexity. on the other  hand  second order methods have higher per iteration complexity since each iteration requires an inexact  solution of a linear system  and possibly  storage of the pre conditioning matrices.  the per iteration time complexity of our algorithm consists of three components   i  the cost of gradient  computation   ii  the cost of the l bfgs two loop recursion  and  iii  the amortized cost of computing the  curvature pair. thus  the overall cost can be written as  o n      z      gradient computation          m n     zl           two loop recursion    m nl      f  z      .      .       cost of computing curvature pair    given the prescription of l      mf       and ml       the cost per iteration remains at o n . the  memory requirement of our algorithm is also o n  since we require the storage of up to mf    ml vectors  of size n.  this result is similar to the one presented in    . the difference in the complexity arises in the third term  of   .    due to our choice of the accumulated fisher information matrix as opposed to using a sub sampled  hessian approximation. it is not imperative for our algorithm to use afim for the computation of yt   .  .  we can instead use the efim   .     which would allow for a lower memory requirement  from  mf  ml  n to  ml n  at the expense of added computation during curvature pair estimation. however  the time complexity  would remain linear in n for either choice. as we mention in section  .   by using the accumulated fisher  information matrix  we avoid the need for additional computation at the expense of memory  a choice we  have found to work well in practice.         numerical results    in this section  we present numerical evidence demonstrating the viability of the proposed algorithm for  training rnns. we also present meta data regarding the experiments which suggests that the performance  difference between adaqn and its competitors  and adagrad in particular  can be attributed primarily  to the incorporation of curvature.     .     language modeling    for benchmarking  we compared the performance of adaqn against adagrad and adam on two language  modeling  lm  tasks  character level lm and word level lm. for the character level lm task       we report  results on two data sets  the tale of two cities  dickens  and the complete works of friedrich nietzsche   nietzsche  . the former has    k characters while the latter has    k. we used the penn tree data set for  the word level lm task     . this data set consists of    k training words with   k words in its vocabulary.  for all tasks  we used an rnn with   recurrent layers. the input and output layer sizes were determined  by the vocabulary of the data set. the character level and word level lms were constructed with     and      nodes per layer respectively. the weights were randomly initialized from n      .   . unless otherwise  specified  the activation function used was tanh. the sequence length was chosen to be    for both cases. for  readability  we exclude other popular methods that did not consistently perform competitively. in particular   sgd  with or without momentum  was not found to be competitive despite significant tuning. for adaqn   adagrad and adam  all hyper parameters were set using a grid search. in particular  step sizes were tuned  for all three methods. adam needed coarse tuning for            in the vicinity of the suggested values. for  adaqn  the value of l was chosen from               . the rest of the hyper parameters  mf   ml         were  set at their recommended values for all experiments  refer to algorithm   . it can thus be seen that the  offline tuning costs of adaqn are comparable to those of adagrad and adam. we ran all experiments  for     epochs and present the results  testing error  in figure  .          character level lm   dickens    character level lm   nietzsche   .      .           adam  adagrad  adaqn    log    error      .           log           error            .            .            .                    epochs  word level lm   tanh                           epochs            .     log    error      .                  error         epochs  word level lm   relu           .           log          .            .                  epochs           character level lm   dickens                              epochs  word level lm   tanh                                       epochs           average l bfgs memory epoch average l bfgs memory epoch    average l bfgs memory epoch average l bfgs memory epoch    figure    numerical results on lm tasks    character level lm   nietzsche                              epochs  word level lm   relu                    epochs                                figure    average l bfgs memory per epoch  it is clear from figure   that adaqn presents a non trivial improvement over both adagrad and  adam on all tasks with tanh activation function. specifically  we emphasize the performance gain over  adagrad  the method which adaqn is safeguarded by. on the character level task with relu activation         adaqn performed better than adam but worse than adagrad. we point out that experiments with other   including larger  data sets yielded results of similar nature.     .     average l bfgs memory per epoch    given the safeguarded nature of our algorithm  a natural question regarding the numerical results presented  pertains to the effect of the safeguarding on the performance of the algorithm. to answer this question  we  report the average l bfgs memory per epoch in figure  . this is computed by a running sum initialized at    at the start of each new epoch. a value greater than   indicates that at least one curvature pair was present  in the memory  in expectation  during a given epoch. higher average values of l bfgs memory suggest that  more directions of curvature were successfully explored  thus  the safeguarding was less necessary. lower  values  on the other hand  suggest that the curvature information was either not informative  leading to  skipping  or led to deterioration of performance  leading to step rejection .  the word level lm task with the relu activation function has interesting outcomes. it can be seen from  figure   that the performance of adagrad is similar to that of adaqn for the first    epochs but then  adagrad continues to make progress while the performance of adaqn stagnates. during the same time   the average l bfgs memory drops significantly suggesting that safeguarding was necessary and that  the  curvature information was not informative enough and even caused deterioration in performance  evidenced  by occasional increase in the function value .     .     mnist classification from pixel sequence    a challenging toy problem for rnns is that of image classification given pixel sequences     . for this  problem  the image pixels are presented sequentially to the network one at a time and the network must  predict the corresponding category. this long range dependency makes the rnn difficult to train. we  report results for the popular mnist data set. for this experiment  we used a setup similar to that of       with two modifications  we used tanh activation function instead of relu and initialized all weights from  n      .    instead of using their initialization trick. the results are reported in figure  .  log           error     adam  adagrad  adaqn                                                    accuracy                                                                  average l bfgs memory epoch                                                                                                   epochs                figure    numerical results on mnist with sequence of pixels           as can be seen from the figure    adam and adagrad struggle to make progress and stagnate at an  error value close to that of the initial point. on the other hand  adaqn is able to significantly improve  the error values  and also achieves superior classification accuracy rates. experiments on other toy problems  with long range dependencies  such as the addition problem       yielded similar results.     .     lstms    in order to ascertain the viability of adaqn on other architectures  we conducted additional experiments  using the lstm models. the experimental setup is similar to the one discussed in section  .  with the  modification that   recurrent  lstm  layers were used instead of  . the results are reported in figure  .    dickens   av. l bfgs memory epoch        log    error     character level lm   dickens   .           adam  adagrad  adaqn          .                          epochs  character level lm   nietzsche    log    error                      epochs  nietzsche   av. l bfgs memory epoch         .                                             .                   log    error                       epochs  word level penn tree                      epochs  penn tree   av. l bfgs memory epoch         .               .                            epochs                    epochs                figure    numerical results on lstms  the results in figure   suggest mixed results. for the character level lm tasks  the performance of  adagrad and adaqn was comparable while the performance of adam was better. for the word level lm  task  the performance of adaqn was superior to that of both adagrad and adam.         discussion    the results presented in the previous section suggest that adaqn is competitive with popular algorithms for  training rnns. however  adaqn is not restricted to this class of problems. indeed  preliminary results on  other architectures  such as feed forward networks  delivered promising performance. it may be possible  to further improve the performance of the algorithm by modifying the update rule and frequency. in this  direction  we discuss the practicality of using momentum in such an algorithm and possible heuristics to  allow the algorithm to adapt the cycle length l as opposed to tuning it to a constant value.  recent work by      suggests superior performance of momentum based methods on a wide variety of  learning tasks. these methods  with the right initialization  have been shown to outperform sophisticated  methods such as the hessian free newton method. however  recent efforts suggest the use of second order  methods in conjunction with momentum         . in this case  one interpretation of momentum is that  of providing a pre conditioner to the cg sub solver. significant performance gains through the inclusion           of momentum have been reported when the gradients are reliable     . we hypothesize that performance  gains can be obtained through careful inclusion of momentum for methods like adaqn as well. however   the design of such an algorithm  and efficacy of using momentum like ideas is an open question for future  research.  lastly  we discuss the role of the aggregation cycle length l on the performance of the algorithm. if l is  chosen to be too large  the aggregation points will be too far apart possibly leading to incorrect curvature  estimation. if l is too small  then the iterates change insufficiently before an update attempt is made leading  to skipping of update pairs. besides the issue of curvature quality  the choice of l also has ramifications on the  cost of the algorithm as discussed in section  . . thus  a natural extension of adaqn is an algorithm where  l can be allowed to adapt during the course of the algorithm. l could be increased or decreased depending  on the quality of the estimated curvature  while being bounded to ensure that the cost of updating is kept  at a reasonable level. the removal of this hyper parameter will not only obviate the need for tuning  but  will also allow for a more robust performance.         conclusions    in this paper  we present a novel quasi newton method  adaqn  for training rnns. the algorithm judiciously incorporates curvature information while retaining a low per iteration cost. the algorithm builds  upon the framework proposed in      which was designed for convex optimization problems. we discuss  the key ingredients of our algorithm  such as  the scaling of the l bfgs matrices using historical gradients  curvature pair updating and step acceptance criterion  and  suggest the use of an accumulated fisher  information matrix during the computation of a curvature pair. we examine the per iteration time and  space complexity of adaqn and show that it is of the same order of magnitude as popular first order methods. finally  we present numerical results for two language modeling tasks and demonstrate competitive  performance of adaqn as compared to popular algorithms used for training rnns.           