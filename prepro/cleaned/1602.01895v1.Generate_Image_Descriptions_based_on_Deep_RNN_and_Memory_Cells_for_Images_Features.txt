introduction    generating natural language descriptions for images has became an attractive research topic in recent years. the task is to generate sentences or  phrases to summarize and describe the contents  shown in images. with this technique  the machines are enabled to imitate the behaviour of human beings who are able to capture the semantic meaning encoded in images. some previous  work from gupta and mannem         kulkarni et al.        and desmond elliott and frank  keller        designed templates for the sentence  descriptions. the task is to fill in the templates  based on the images. however  these approaches  strongly limited the capability of models to generate sentence descriptions to only fixed patterns.  other approaches transfer this task into a multimodal embedding problem. these work from  farhadi et al.         jia et al.         socher et  al.         ordonez et al.        overlap with the  scope of information retrieval. the goal is to map  the images with sentences appearing in the train     song han  stanford university  songhan stanford.edu    ing dataset together in a multimodal space. however  these models are only capable of returning  sentence descriptions that existed in the training  dataset.  most of the state of the art approaches are  based on neural networks. these work combined  convolutional neural network  cnn  with recurrent neural network  rnn  to generate image descriptions. karpathy        develop a multimodal  rnn for this task. in this neural network  the image features extracted from the vggnet  a pretrained cnn proposed in simonyan and zisserman         are fed into a rnn. conditioned on  the image features and previous words  the rnn  will generate a sequence of words recurrently to  describe the images. similar to kaparthy s work   vinyals vinyals et al.        used the googlenet  cnn to extract image features and train a lstm   in hochreiter and schmidhuber         as sequence generator. mao et al.        report a deep  complex multimodal rnn for sentence generation.  in our approach  vggnet is employed to extract image features and a deep multilayer rnn is  chosen as a sequence generator  on top of which  we informatively added memory gate that controls  image feeding. in each time step of rnn     we  feed word in current time step as well as the image  features into the hidden layer of rnn. inspired by  the ideas in rolls and deco        that the visual  perception depends on short term memory and has  the recurrent natural  a memory gate is designed  to control the input of image features to the hidden layer. the output of memory gate depends  on the output of hidden layer at the previous time  step. before feeding into the hidden layer  the image features are multiplied by the output of gate  element wisely. therefore  the memory gates act  as memory cells for image features. our model     in the rnn language model  the time step is defined as  the position of word in sentence.     is trained on the flickr k and flickr  k datasets  from hodosh et al.       . we evaluate the bleu  score  proposed in papineni et al.         of our  model on the test datasets of both flickr k and  flickr  k. the preliminary results show that the  performance of our model outperforms the stateof the art work.        .     the architecture of model  image features representation    cnn has been proved as a powerful tool to extract  image features  and has been widely used in image  classification  krizhevsky et al.          object  detection  girshick et al.         and other tasks.  in this paper  we select the deep and powerful vggnet to extract image features. specifically  each  raw image is fed into the vggnet as input. after  the forward propagation  the last fully connected  layer will output a      dimensions vector as the  image features for each image.   .     w s has the dimension of h by d where h is the  dimension of hidden layer and d is the dimension  of word vector. w h has the dimension of h by h.  w d has the dimension of v by h with v as the  vocabulary size. bd and bh are the bias terms. vector y t  represents the probability of each word in  the vocabulary to be the next word conditioned on  the input words from time step   to t.  in our model  to improve the model capacity   we increase the depth of rnn by adding multiple hidden layers which is the same as deep transition rnn  dt rnn  model reported by pascanu  et al.       . pascanu et al.        shows that  the dt rnn is able to increase the size of family  of functions it can represent in language modeling.  unlike the standard rnn in equation   with only  a single hidden layer at each time step  n hidden  layers are stacked together at each time step in dtrnn. the forward propagation of this deep rnn  model is   h   t    f  w s x t    w h hn  t        bh      sentence representation    h   t    f  w h h   t    bh      the sentence can be represented as a sequence of  single word. the time step t is defined as the index of t th word in the sentence to represent the  position of each word. suppose the sentence contains t words  the time step of first word is t       the second word is t      and for the last word  is t   t . for each sentence  we add a special  start token at the first time step to indicate the  start of the sentence  as well as the end token at  the last time step as the end of each sentence.  the single word is represented as a vector.  some pretrained word vector models have been  developed such as word vec by mikolov et al.         and glove by pennington et al.       .  however  in this model  we trained the word vectors from scratch instead of directly adopted the  pretrained model  since generally the retrained  word vector will achieve higher performance for  specific task.  the standard rnn model can be expressed as   h t    f  w s x t    w h h t        bh             y t    sof tmax w d h t    bd      ...         h    h    hn  t    f  w hn     t    b    y t    sof tmax w d hn  t    bd    where h   t   h   t   ...  hn  t  represent the output  of n hidden layers at t. the word vector x t   as well as the output of last hidden layer at previous time step hn  t      are fed into the first  hidden layer h   t  at t. then  output of current  hidden layer feeds into the next hidden layer consecutively. the output y t  depends on the output  of last hidden layer at current time step hn  t . in  our model  the deep rnn in equation   is chosen  as the sequence learner of sentences.   .     memory cells for image features    we consider how to control feeding the image features into the deep rnn. instead of feeding the  image features directly  we add a gate to control  the magnitude of image feature feeds. the value  of the gate depends on the state of hidden layers at  previous time step.  h   t    f  w s x t    w h hn  t          where h t  is the output of the hidden layer at time  step t  x t  is the word vector for the word at t   h t      is the output of hidden layer for the previous time step t      f    is the activation function.    g t     w i cn n  i    bi     bh    g    g    g t      w hn  t        b              where i represents the raw image  and cn n  i   is the image features extracted by cnn. w i has  the dimension of h by      which maps the image features to the same space of hidden layers of rnn. g t  is the output of gate  and   is  the element wise multiplication. w g transfers the  value of the last hidden layer in the previous time  step  hn  t      in the equation  to the gate g t .  bi and bg are the bias terms. here we use the   activation function and the value of g t  ranges from    to  .  based on equation    the image features are fed  into the first hidden layer at each time step  multiplied by the output of gate. since the value of gate  depends on the last hidden layer of previous time  step  the gate controls how much information from  image is still needed for the current time step. in  the case of g t       the image features are not fed  into rnn  while for g t       we feed full image  features at each time step.  combining equations   and   together  this  model can be represented as   g t      w g hn  t        bg    h   t    f  w s x t    w h hn  t        g t     w i cn n  i    bi     bh    h   t    f  w h h   t    bh    ...         h    h    hn  t    f  w hn     t    b    y t    sof tmax w d hn  t    bd    figure   shows the architecture of this model.    figure    the architecture of this model.  recall the work of karpathy         image features are only fed at the first time step of rnn.  due to the vanishing gradient problem  image features will not be learned well with long sentence    and deep network. however  our model feed image features into rnn at each time step. therefore  our model is still able to learn information  from the image even for larger time steps. the  magnitude of image features is conditioned on the  hidden state of previous time step. in another  word  the image features are encoded based on the  status of how well our model has learned.  compared with other work of vinyals et al.         based on lstm and work of mao et  al.        based on multimodal embeddings  our  model has the advantage of lower model complexity and easier to train.        .     experiments  dataset    we experimented on the flickr k and flickr  k  datasets introduced in hodosh et al.       . each  image in these datasets is described by   independent sentences. therefore  for each image  we  can create   samples with each one as an imagesentence pair. we have      and       images for  flickr k and flickr  k respectively. each dataset  has been splited into development data with       images  test data with      images and the rest images as training data. the data preprocessing procedure is the same as the work of karpathy       .   .     training    during training  cross entropy loss was chosen  as the loss function. stochastic gradient descent   sgd  with minibatch size of     image sentence  pairs was used during training. to make the model  converge faster  rmsprop annealing policy hinton et al.        was adopted  where the step  size of each parameter is scaled by the windowaveraged norm of its gradient.  to overcome the vanishing gradient problem   relu is chosen as the activation function. also   we adopted the element wise clip gradient tricks   where we clipped the gradient to  . to regularize  the model  we add l  norm of weights to the loss  function  and as zaremba et al.        suggested   we used dropout ratio of  .  to all the layers except  for the hidden layers.  as equation   indicates  a model with large n  has deeper hidden layers  which leads to a large  capacity. considering the size of the dataset is not  large and in order to prevent overfitting  we adopt  a small n     with   hidden layers in the experiments in equation  .     we find    epochs are enough to train this  model for both datasets  and the hidden size was  tuned to     to achieve the best performance.   .     generate image description    the sentence description for each image in test  dataset is generated by feeding the image features  into the trained model with a start token. at  each time step  we can directly choose the word  corresponds to the one with highest probability  in vocabulary as the output word  which is also  the input word of next time step. following this  method  we can generate a sentence recurrently  until we reach the end token.  to evaluate the performance  we use the bleu  score as evaluation metrics which has been widely  adopted in the papers focus on this topic  karpathy          vinyals et al.         mao et al.        .  the bleu score will evaluate the similarity of  the generated sentences with the ground truth sentences. table   and table   show the bleu score  for several models.  model  our model  karpathy         mao et al.         vinyals et al.         vinyals  net on vggnet    b      .     .     .     .     .     b      .     .     .     .     .     b      .     .     .     .     .     b      .     .              table    the bleu score on flickr k for different models. b n is the bleu score up to n gram.    model  our model  karpathy         mao et al.         vinyals et al.         vinyals  net on vggnet    b      .     .     .     .        b      .     .     .     .        b      .     .     .     .        b      .     .        .        table    the bleu score on flickr  k for different models. b n is the bleu score up to n gram.  as shown on table   and table    our model  outperforms the results from karpathy        and  mao et al.       . while the performance of  our model is lower than the original work from  vinyals et al.       . however  this is because  in the original work of vinyals et al.          the authors used the googlenet  in szegedy et al.          to extract the image features  while we    used vggnet. therefore  it is unfair to directly  compare the bleu score of our model with results reported by vinyals et al.       .  to make a fair comparison with the network in vinyals et al.          we have  downloaded the reproduced version of vinyals   model from http   cs.stanford.edu   people karpathy neuraltalk . in this  reproduced model trained on flickr k  the image  features feed into vinyals  model are extracted by  vggnet  which is the same as the case in our  model. from the last row of table    we can find  that the performance of our model is better than  the model in vinyals et al.        if both models use the vggnet image features. note that  even though the reproduced model of vinyals et  al.        based on flickr  k dataset is unavailable now  our model still outperforms other stateof the art works.  we also tried to feed image features only at first  time step  i.e.  set g t      except for the first time  step  as well as feed full image features at each  time step  i e.  set g t      for all time steps .  but the results show that the performance all of  these two schemes are lower than feeding image  features at each time step with memory cells.         conclusion    in this paper  we developed a new model for generating image descriptions. the image features  extracted from vggnet are fed into each time  step of a multilayer deep rnn  where the image features vector is element wisely multiplied  by a memory vector determined by the state of  the hidden layer at previous time step. experiments on flickr k and flickr  k datasets show  that this model achieves higher performance on  bleu score. our model also benefit from its low  complexity and ease of training.  as the extension of this work  we will train our  model on a larger dataset such as mscoco  and  will increase the number of hidden layers at each  time step to further improve the performance of  our model. we will also try to adopt other cnns  such as googlenet to extract image features. also   in this work  we do not fine tune the cnns on  the new datasets  in future  we will try to train the  model and tune the cnns together.     