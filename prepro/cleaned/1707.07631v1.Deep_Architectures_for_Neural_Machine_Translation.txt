introduction    neural machine translation  nmt  is a wellestablished approach that yields the best results  on most language pairs  bojar et al.        cettolo et al.       . most systems are based on the  sequence to sequence model with attention  bahdanau et al.        which employs single layer recurrent neural networks both in the encoder and in  the decoder.    unlike feed forward networks where depth is  straightforwardly defined as the number of noninput layers  recurrent neural network architectures with multiple layers allow different connection schemes  pascanu et al.        that give rise to  different  orthogonal  definitions of depth  zhang  et al.        which can affect the model performance depending on a given task. this is further complicated in sequence to sequence models  as they contain multiple sub networks  recurrent  or feed forward  each of which can be deep in different ways  giving rise to a large number of possible configurations.  in this work we focus on stacked and deep transition recurrent architectures as defined by pascanu et al.       . different types of stacked architectures have been successfully used for nmt   zhou et al.        wu et al.       . however   there is a lack of empirical comparisons of different deep architectures. deep transition architectures have been successfully used for language  modeling  zilly et al.         but not for nmt  so far. we evaluate these architectures  both  alone and in combination  varying the connection scheme between the different components and  their depth over the different dimensions  measuring the performance of the different configurations  on the wmt news translation task.   related work includes that of britz et al.          who have performed an exploration of nmt architectures in parallel to our work. their experiments  which are largely orthogonal to ours   focus on embedding size  rnn cell type  gru  vs. lstm   network depth  defined according  to the architecture of wu et al.          attention mechanism and beam size. gehring et al.         recently proposed a nmt architecture  based on convolutions over fixed sized windows     http   www.statmt.org wmt     translation task.html     rather than rnns  and they reported results for  different model depths and attention mechanism  configurations. a similar feedforward architecture which uses multiple pervasive attention mechanisms rather than convolutions was proposed by  vaswani et al.         who also report results for  different model depths.    ...  ...  ...         nmt architectures    all the architectures that we consider in this work  are gru  cho et al.      a  sequence to sequence  transducers  sutskever et al.        cho et al.       b  with attention  bahdanau et al.       . in  this section we describe the baseline system and  the variants that we evaluated.   .     baseline architecture    as our baseline  we use the nmt architecture implemented in nematus  which is described in more  depth by sennrich et al.      b . we augment it  with layer normalization  ba et al.         which  we have found to both improve translation quality  and make training considerably faster.  for our discussion  it is relevant that the baseline architecture already exhibits two types of  depth     recurrence transition depth in the decoder  rnn which consists of two gru transitions  per output word with an attention mechanism  in between  as described in firat and cho        .    feed forward depth in the attention network  that computes the alignment scores and in the  output network that predicts the target words.  both these networks are multi layer perceptrons with one tanh hidden layer.   .     deep transition architectures    in a deep transition rnn  dt rnn   at each time  step the next state is computed by the sequential application of multiple transition layers  effectively using a feed forward network embedded inside the recurrent cell. in our experiments  these  layers are gru transition blocks with independently trainable parameters  connected such that  the  state  output of one of them is used as the   state  input of the next one. note that each of  these gru transition is not individually recurrent   recurrence only occurs at the level of the whole  multi layer cell  as the  state  output of the last    figure    deep transition decoder  gru transition for the current time step is carried  over as the  state  input of the first gru transition  for the next time step.  applying this architecture to nmt is a novel  contribution.   . .  deep transition encoder  as in a baseline shallow nematus system  the encoder is a bidirectional recurrent neural network.  let ls be the encoder recurrence depth  then for  the i th source word in the forward direction the              forward source word state h i   h i ls is computed as                    h i     gru  xi   h i   ls                   h i k   gruk    h i k   for     k   ls  where the input to the first gru transition is the  word embedding xi   while the other gru transitions have no external inputs. recurrence occurs        as the previous word state h i   ls enters the computation in the first gru transition for the current  word.  the reverse source word states are computed similarly and concatenated to the forward ones to  form  the bidirectional  source word states c    nh           io  h i ls h i ls .   . .  deep transition decoder  the deep transition decoder is obtained by extending the baseline decoder in a similar way. recall  that the baseline decoder of nematus already has  a transition depth of two  with the first gru transition receiving as input the embedding of the previous target word and the second gru transition  receiving as input a context vector computed by  the attention mechanism. we extend this decoder     ...    architecture to an arbitrary transition depth lt as  follows   ...    ...    ...    ...    ...    ...    sj     gru   yj     sj   lt    sj     gru   att c  sj      sj      sj k   gruk     sj k     for     k   lt  where yj   is the embedding of the previous target  word and att c  si     is the context vector computed by the attention mechanism. gru transitions other than the first two do not have external  inputs. the target word state vector sj   sj lt is  then used by the feed forward output network to  predict the current target word. a diagram of this  architecture is shown in figure  .  the output network can be also made deeper by  adding more feed forward hidden layers.   .     stacked architectures    a stacked rnn is obtained by having multiple  rnns  grus in our experiments  run for the same  number of time steps  connected such that at each  step the bottom rnn takes  external  inputs from  the outside  while each of the higher rnn takes  as its  external  input the  state  output of the one  below it. residual connections between states at  different depth  he et al.        are also used to  improve information flow. note that unlike deep  transition grus  here each gru transition block  constitutes a cell that is individually recurrent  as it  has its own state that is carried over between time  steps.   . .     stacked encoder    in this work we consider two types of bidirectional  stacked encoders  an architecture similar to zhou  et al.        which we denote here as alternating  encoder  figure     and one similar to wu et al.         which we denote as biunidirectional encoder  figure   .  our contribution is the empirical comparison of  these architectures  both in isolation and in combination with the deep transition architecture.  we do not consider stacked unidirectional encoders  sutskever et al.        as bidirectional encoders have been shown to outperform them  e.g.  britz et al.        .  alternating stacked encoder the forward part  of the encoder consists of a stack of gru recurrent  neural networks  the first one processing words in    figure    alternating stacked encoder  zhou et al.        .  the forward direction  the second one in the backward direction  and so on  in alternating directions. for an encoder stack depth ds   and a source  sentence length n   the forward source word state           wi      w i ds is computed as                          w i     h i     gru  xi   h i                           h i  k   gru k    w i  k     h i    k  for      k   ds                       w i  k   h i    k    h i  k     gru k            w i j    for      k       ds             h i j      w i j    for     j   ds                where we assume that h   k and h n    k are zero  vectors. note the residual connections  at each  level above the first one  the word state of the pre   vious level    w i j   is added to the recurrent state        of the gru cell h i j to compute the the word state     for the current level    w i j .  the backward part of the encoder has the same  structure  except that the first level of the stack  processes the words in the backward direction and  the subsequent levels alternate directions.  the forward and backward word states are then  concatenated to form bidirectional word states        c        w i ds    w  i ds   . a diagram of this architecture is shown in figure  .  biunidirectional stacked encoder in this encoder the forward and backward parts are shallow  as in the baseline architecture. their word  states are concatenated to form shallow bidirec       that are then  tional word states wi       w i      w  i    used as inputs for subsequent stacked grus which  operate only in the forward sentence direction   hence the name  biunidirectional . since residual connections are also present  the higher depth     ...    tions between the levels.  sj       gru     yj     sj            ...    cj     att c  sj        ...    sj       gru     cj     sj        rj     sj        ...    ...    figure    biunidirectional stacked encoder  wu  et al.       .  ...    sj k     gruk  rj k     sj   k      rj k   sj k     rj k    for     k   dt    note that the higher levels have transition depth  one  unlike the base level which has two.    ...  ...    figure    stacked rnn decoder    stacked rgru the higher rnns are grus  whose  external  input is the concatenation of the  state below and the context vector from the base  rnn. formally  the states sj k   of the higher  rnns are computed as   sj k     gruk   rj k     cj       sj   k        grus have a state size twice that of the base  ones. this architecture has shorter maximum information propagation paths than the alternating  encoder  suggesting that it may be less expressive   but it has the advantage of enabling implementations with higher model parallelism. a diagram of  this architecture is shown in figure  .  in principle  alternating and biunidirectional  stacked encoders can be combined by having dsa  alternating layers followed by dsb unidirectional  layers.    for     k   dt    this is similar to the deep decoder by wu et al.        .  stacked cgru the higher rnns are conditional grus  each with an independent attention  mechanism. each level has two gru transitions  per step j  with a new context vector cj k computed  in between   sj k     gruk    rj k     sj   k         . .     stacked decoder    cj k   att c  sj k      sj k     gruk    cj k   sj          a stacked decoder can be obtained by stacking  rnns which operate in the forward sentence direction. a diagram of this architecture is shown in  figure  .  note that the base rnn is always a conditional  gru  cgru  firat and cho        which has transition depth at least two due to the way that the  context vectors generated by the attention mechanism are used in nematus. this opens up the possibility of several architectural variants which we  evaluated in this work     for     k   dt    note that unlike the stacked gru and rgru  the  higher levels have transition depth two.  stacked crgru the higher rnns are conditional grus but they reuse the context vectors  from the base rnn. like the cgru there are two  gru transition per step  but they reuse the context  vector cj   computed at the first level of the stack   sj k     gruk    rj k     sj   k        stacked gru the higher rnns are simple  grus which receive as input the state from the  previous level of the stack  with residual connec     sj k     gruk    cj     sj        for     k   dt      .     bideep architectures    we introduce the bideep rnn  a novel architecture obtained by combining deep transitions with  stacking.  a bideep encoder is obtained by replacing the  ds individually recurrent gru cells of a stacked  encoder with multi layer deep transition cells each  composed by ls gru transition blocks.  for instance  the bideep alternating encoder is  defined as follows                          w i     h i     dtgru  xi   h i                           h i  k   dtgru k    w i  k     h i    k  for      k   ds                       h i  k     dtgru k      w i  k   h i    k          w i j    for      k       ds             h i j      w i j    for     j   ds    where each multi layer cell dtgruk is defined  as     and the number of parameters. for translation quality  we report case sensitive  detokenized  b leu  measured with mteval v  a.pl  on newstest      newstest      and newstest    .  we release the code under an open source license  including it in the official nematus repository.  the configuration files needed to replicate  our experiments are available in a separate repository.    .     our first experiment is concerned with layer normalization. we are interested to see how essential layer normalization is for our deep architectures  and compare the effect of layer normalization on a baseline system  and a system with an  alternating encoder with stacked depth  . results  are shown in table  . we find that layer normalization is similarly effective for both the shallow  baseline model and the deep encoder  yielding an  average improvement of  .    b leu  and reducing training time substantially. therefore we use  it for all the subsequent experiments.   .     vk     gruk    ink   statek    vk t   gruk t     vkt      for     k   ls  dtgruk  ink   statek     vk ls  it is also possible to have different transition  depths at each stacking level.  bideep decoders are similarly defined  replacing the recurrent cells  gru  rgru  cgru or crgru  with deep transition multi layer cells.         experiments    all experiments were performed with nematus   sennrich et al.      b   following sennrich et al.       a  in their choice of preprocessing and hyperparameters. for experiments with deep models  we increase the depth by a factor of   compared to the baseline for most experiments  in preliminary experiments  we observed diminishing  returns for deeper models.  we trained on the parallel english german  training data of wmt      news translation task   using newstest     as validation set. we used  early stopping on the validation cross entropy and  selected the best model based on validation b leu.  we report cross entropy  ce  on newstest       training speed  on a single titan x  pascal  gpu      layer normalization    deep encoders    in table   we report experimental results for different architectures of deep encoders  while the  decoder is kept shallow.  we find that all the deep encoders perform substantially better than baseline    .    .  b leu    with no consistent quality differences between  each other. in terms of number of parameters and  training speed  the deep transition encoder performs best  followed by the alternating stacked  encoder and finally the biunidirectional encoder   note that we trained on a single gpu  the biunidirectional encoder may be comparatively faster  on multiple gpus due to its higher model parallelism .   .     deep decoders    table   shows results for different decoder architectures  while the encoder is shallow. we find that  the deep decoders all improve the cross entropy   but the b leu results are more varied  deep output   decreases b leu scores  but note that the baseline       https   github.com edinburghnlp   nematus     https   github.com avmb   deep nmt architectures     deep feed forward output with shallow rnns in both the  encoder and decoder     encoder    ce    baseline   layer normalization  alternating  depth      layer normalization      .      .      .      .              .     .     .     .     b leu          .     .     .     .     parameters  m           .     .     .     .       .     .      .      .     training speed   words s                             early stop       minibatches                     table    layer normalization results. english german wmt   data.  encoder  shallow  alternating  biunidirectional  deep transition    s. bidir.                depth  s. forw.          ce  trans.                  .      .      .      .              .     .     .     .     b leu          .     .     .     .     parameters  m           .     .     .     .       .      .      .      .     training speed   words s                             table    deep encoder results. english german wmt   data. parameters and speed are highlighted  for the deep recurrent models.  has already some depth   stacked gru performs  similarly to the baseline    .    .  b leu  and  stacked rgru possibly slightly better    .    .   b leu .  other deep rnn decoders achieve higher gains.  the best results    .  b leu on average  are  achieved by the stacked conditional gru with independent multi step attention  cgru . this decoder  however  is the slowest one and has the most  parameters.  the deep transition decoder performs well    .   b leu on average  in terms of quality and is the  fastest and smallest of all the deep decoders that  have shown quality improvements.  the stacked conditional gru with reused attention  crgru  achieves smaller improvements     .  b leu on average  and has speed and  size intermediate between the deep transition and  stacked cgru decoders.   .     deep encoders and decoders    table   shows results for models where both the  encoder and the decoder are deep  in addition to  the results of the best deep encoder  the deep transition encoder    shallow decoder reported here  for ease of comparison.  compared to deep transition encoder alone  we  generally see improvements in cross entropy  but  not in b leu. we evaluate architectures similar to  zhou et al.         alternating encoder   stacked  gru decoder  and  wu et al.         biunidirectional encoder   stacked rgru decoder   though  they are not straight replications since we used  gru cells rather than lstms and the implementation details are different. we find that the former architecture performs better in terms of b leu    scores  model size and training speed.  the other variants of alternating encoder    stacked or deep transition decoder perform similarly to alternating encoder   stacked rgru decoder  but do not improve b leu scores over the  best deep encoder with shallow decoder. applying the bideep architecture while keeping the  total depth the same yields small improvements  over the best deep encoder    .  b leu on average   while the improvement in cross entropy is  stronger. we conjecture that deep decoders may be  better at handling subtle target side linguistic phenomena that are not well captured by the   gram  precision based b leu evaluation.  finally  we evaluate a subset of architectures  with a combined depth that is   times that of the  baseline. among the large models  the bideep  model yields substantial improvements  average    .  b leu over the best deep encoder    .   b leu over the shallow baseline   in addition to  cross entropy improvements. the stacked only  model  on the other hand  performs similarly to the  smaller models  despite having even more parameters than the bideep model. this shows that it is  useful to combine deep transitions with stacking   as they provide two orthogonal kinds of depth that  are both beneficial for neural machine translation.   .     error analysis    one theoretical difference between a stacked rnn  and a deep transition rnn is that the distance in  the computation graph between timesteps is increased for deep transition rnns. while this allows for arguably more expressive computations  to be represented  in principle it could reduce the  ability to remember information over long dis      decoder  shallow  stacked  stacked  stacked  stacked  deep transition  deep output    high rnn  stacked  gru  rgru  cgru  crgru       decoder rnn depth  trans.  type                                              output  depth                         ce    .      .      .      .      .      .      .              .     .     .     .     .     .     .     b leu          .     .     .     .     .     .     .             .     .     .     .     .     .     .     params.   m     .      .      .      .      .      .     .     training speed   words s                                               table    deep decoder results. english german wmt   data. parameters and speed are highlighted  for the deep recurrent models.  encoder    decoder    decoder high  rnn type  shallow  shallow  deep tran.  shallow   zhou et al.         ours   alternating  stacked  gru   wu et al.         ours   biunidir.  stacked  rgru  alternating  stacked  rgru  alternating  stacked  cgru  deep tran.  deep tran.  bideep altern.  bideep  rgru  bideep altern.  bideep  rgru  alternating  stacked  rgru    encoder depth  bidir. forw. trans.                decoder depth  stacked  trans.                ce    .      .              .     .     b leu          .     .             .     .     params.   m     .      .     training speed   words s                                            .        .       .       .        .                                                                                                                       .      .      .      .      .      .      .        .     .     .     .     .     .     .       .     .     .     .     .     .     .       .     .     .     .     .     .     .        .      .      .      .      .      .      .                                              table    deep encoder decoder results. english german wmt   data. transition depth     means    in the base rnn of the stack and   in the higher rnns. the last two models are large and their results  are highlighted separately.       some decisions may not require the information to be  passed on the target side because the decisions may be possi           .    accuracy    tances  since each layer may lose information during forward computation or backpropagation. this  may not be a significant issue in the encoder   as the attention mechanism provides short paths  from any source word state to the decoder  but  the decoder contains no such shortcuts between its  states  therefore it might be possible that this negatively affects its ability to model long distance relationships in the target text  such as subject verb  agreement.  here  we seek to answer this question by testing our models on lingeval    sennrich          a test set which provides contrastive translation  pairs for different types of errors. for the example of subject verb agreement  contrastive translations are created from a reference translation by  changing the grammatical number of the verb  and  we can measure how often the nmt model prefers  the correct reference over the contrastive variant.  in figure    we show accuracy as a function of  the distance between subject and verb. we find  that information is successfully passed over long  distances by the deep recurrent transition network.  even for decisions that require information to be  carried over    or more words  or at least     gru  transitions    the deep recurrent transition network     .   shallow gru  stacked gru  deep transition gru  bideep gru     .       .                  distance                   figure    subject verb agreement accuracy as a  function of distance between subject and verb.  achieves an accuracy of over   .    n          higher than the shallow decoder    .     and similar to the stacked gru    .   . the highest accuracy    .    is achieved by the bideep network.         conclusions    in this work we presented and evaluated multiple  architectures to increase the model depth of neural  machine translation systems.  we showed that alternating stacked encoders   zhou et al.        outperform biunidirectional  ble based on source side information.     stacked encoders  wu et al.         both in accuracy and  single gpu  speed. we showed that  deep transition architectures  which we first applied to nmt  perform comparably to the stacked  ones in terms of accuracy  b leu  cross entropy  and long distance syntactic agreement   and better  in terms of speed and number of parameters.  we found that depth improves b leu scores especially in the encoder. decoder depth  however   still improves cross entropy if not strongly b leu  scores.  the best results are obtained by our bideep  architecture which combines both stacked depth  and transition depth in both the  alternating  encoder and the decoder  yielding better accuracy for  the same number of parameters than systems with  only one kind of depth.  we recommend to use combined architectures  when maximum accuracy is the goal  or use deep  transition architectures when speed or model size  are a concern  as deep transition performs very  positively in the quality speed and quality size  trade off.  while this paper only reports results for one  translation direction  the effectiveness of the presented architectures across different data conditions and language pairs was confirmed in followup work. for the shared news translation task  of this year s conference on machine translation   wmt     we built deep models for    translation directions  using a deep transition architecture  or a stacked architecture  alternating encoder and  rgru decoder   and observe improvements for the  majority of translation directions  sennrich et al.       a .    dzmitry bahdanau  kyunghyun cho  and yoshua bengio.     . neural machine translation by jointly  learning to align and translate. in proceedings of  the international conference on learning representations  iclr .  ondr ej bojar  rajen chatterjee  christian federmann   yvette graham  barry haddow  matthias huck   antonio jimeno yepes  philipp koehn  varvara  logacheva  christof monz  matteo negri  aurelie neveol  mariana neves  martin popel  matt  post  raphael rubino  carolina scarton  lucia specia  marco turchi  karin verspoor  and marcos  zampieri.     . findings of the      conference  on machine translation  wmt   . in proceedings  of the first conference on machine translation  volume    shared task papers. association for computational linguistics  berlin  germany  pages          .  denny britz  anna goldie  minh thang luong  and  quoc v. le.     . massive exploration of neural machine translation architectures.  corr  abs     .     .  mauro cettolo  jan niehues  sebastian st ker  luisa  bentivogli  and marcello federico.     . report on  the   th iwslt evaluation campaign. in iwslt      . seattle  usa.  kyunghyun cho  b van merrienboer  dzmitry bahdanau  and yoshua bengio.     a. on the properties of neural machine translation  encoder decoder  approaches. in eighth workshop on syntax  semantics and structure in statistical translation  ssst        .    acknowledgments    kyunghyun cho  bart van merrienboer  caglar gulcehre  dzmitry bahdanau  fethi bougares  holger schwenk  and yoshua bengio.     b. learning phrase representations using rnn encoder   decoder for statistical machine translation. in proceedings of the      conference on empirical methods in natural language processing  emnlp .  doha  qatar  pages          .    the research presented in this publication was  conducted in cooperation with samsung electronics polska sp. z o.o.   samsung r d institute  poland.    orhan firat and kyunghyun cho.     .  conditional gated recurrent unit with attention  mechanism.  https   github.com nyu dl dl mttutorial blob master docs cgru.pdf.  published  online  version adbaeea.    this project received funding from the  european union s horizon      research  and innovation programme under grant agreements         qt             himl  and          summa .    jonas gehring  michael auli  david grangier  denis yarats  and yann n. dauphin.     . convolutional sequence to sequence learning. corr  abs     .     .    