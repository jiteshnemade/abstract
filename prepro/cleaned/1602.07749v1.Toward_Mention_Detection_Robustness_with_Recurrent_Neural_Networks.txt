introduction  one of the crucial steps toward understanding natural languages is mention detection  md   whose  goal is to identify entity mentions  whether named   nominal  the president  or pronominal  he  she    and classify them into some predefined types of interest in text such as person  organization  or location. this is an extension of the named  entity recognition  ner  task which only aims to  extract entity names. md is necessary for many  higher level applications such as relation extraction  knowledge population  information retrieval   question answering and so on.       work carried out during an internship at ibm    traditionally  both md and ner are formalized  as sequential labeling problems  thereby being  solved by some linear graphical models such as  hidden markov models  hmms   maximum entropy markov models  memms  or conditional  random fields  crfs   lafferty et al.       . although these graphical models have been adopted  well to achieve the top performance for md  there  are still at least three problems we want to focus in  this work    i  the first problem is the performance loss  of the mention detectors when they are trained  on some domain  the source domain  and applied to other domains  the target domains . the  problem might originate from various mismatches  between the source and the target domains  domain shifts  such as the vocabulary difference  the  distribution mismatches etc  blitzer et al.         daume        plank and moschitti       .   ii  second  in mention detection  we might  need to capture a long context  possibly covering  the whole sentence  to correctly predict the type  for a word. for instance  consider the following  sentence with the pronominal  they    now  the reason that france  russia and  germany are against war is because they have suffered much from the past war.  in this sentence  the correct type gpe  for   they  can only be inferred from its gpe references   france    russia  and  germany  which  are far way from the pronominal  they  of interest. the challenge is come up with the models  that can encode and utilize these long range dependency context effectively.   iii  the third challenge is to be able to quickly  adapt the current techniques for md so that they  can perform well on new languages.  in this paper  we propose to address these  problems for md via recurrent neural networks   rnns  which offer a decent recurrent mechanism to embed the sentence context into a dis     geographical political entity     tributed representation and employ it to decode  the sentences. besides  as rnns replace the symbolic forms of words in the sentences with their  word embeddings  the distributed representation  that captures the general syntactic and semantic  properties of words  collobert and weston         mnih and hinton        turian et al.         they  can alleviate the lexical sparsity  induce more general feature representation  thus generalizing well  across domains  nguyen and grishman      b .  this also helps rnns to quickly and effectively  adapt to new languages which just require word  embeddings as the only new knowledge we need  to obtain. finally  we can achieve the task specific  word embeddings for md to improve the overall performance by updating the initial pre trained  word embeddings during the course of training in  rnns.  the recent emerging interest in deep learning has produced many successful applications of  rnns for nlp problems such as machine translation  cho et al.      a  bahdanau et al.          semantic role labeling  zhou and xu        etc.  however  to the best of our knowledge  there has  been no previous work employing rnns for md  on the cross domain and language settings so far.  to summarize  the main contributions of this paper are as follow    . we perform a systematic investigation on  various rnn architectures and word embedding  techniques that are motivated from linguistic observations for md.   . we achieve the state of the art performance  for md both in the general setting and in the crossdomain setting with the bidirectional modeling applied to rnns.   . we demonstrate the portability of the rnn  models for md to new languages by their significant improvement with large margins over the best  reported system for named entity recognition in  dutch.      related work  both  named  entity  recognition   bikel et al.         borthwick et al.         tjong kim sang and de meulder         florian et al.         miller et al.         ando and zhang         suzuki and isozaki         ratinov and roth         lin and wu         turian et al.         ritter et al.         passos et al.         cherry and guo         and mention detection  florian et al.           have been extensively studied with various evaluation in the last decades  muc    muc   conll     conll    and ace. the  previous work on md has examined the cascade models  florian et al.         transferred  knowledge from rich resource languages to  low resource ones via machine translation   zitouni and florian        or improved the  systems on noisy input  florian et al.       .  besides  some recent work also tries to solve md  jointly with other tasks such as relation or event  extraction to benefit from their inter dependencies   roth and yih         kate and mooney         li and ji      a  li et al.      b . however  none  of these work investigates rnns for md on the  cross domain and language settings as we do in  this paper.  regarding neural networks  a large volume  of work has devoted to the application of  deep learning to nlp in the last few years   centering around several network architecture such as convolutional neural networks   cnns   yih et al.         shen et al.         kalchbrenner et al.         kim         zeng et al.         dos santos et al.      a   dos santos and guimar es      b    recurrent recursive  neural  networks   socher et al.         cho et al.      a   bahdanau et al.         zhou and xu         tai et al.         to name a few. for ner   collobert et al.        propose a cnn based  framework while mesnil et al.        and yao et  al.              investigate the rnns for the slot  filling problem in spoken language understanding.  although our work also examines the rnns  we  consider the mention detection problem with an  emphasis on the robustness of the models in the  domain shifts and language changes which has  never been explored in the literature before.  finally  for the robustness in the domain  adaptation setting  the early work has focused on the sequential labeling tasks such  as part of speech tagging or name tagging   blitzer et al.         huang and yates         daume         xiao and guo         schnabel and sch tze       .  recent work  has drawn attention to relation extraction   plank and moschitti        nguyen et al.      a   gormley et al.       . in the field of neural  networks  to the best of our knowledge  there  is only one work from nguyen and grishman       b  that evaluates cnns for event detection  in the cross domain setting.       models  we formalize the mention detection problem as a  sequential labeling task. given a sentence x    w  w  . . . wn   where wi is the i th word and n is  the length of the sentence  we want to predict the  label sequence y   y  y  . . . yn for x  where  yi is the label for wi . the labels yi follow the  bio  encoding to capture the entity mentions in  x  ratinov and roth       .  in order to prepare the sentence for rnns  we  first transform each word wi into a real valued  vector using the concatenation of two vectors ei  and fi   wi    ei   fi      where     ei is the word embedding vector of wi   obtained by training a language model on a  large corpus  discussed later .    fi is a binary vector encompassing different  features for wi . in this work  we are utilizing four types of features  capitalization   gazetteers  triggers  whether wi is present in  a list of trigger words  or not  and cache  the  label that is assigned to wi sometime before  in the document .  we then enrich this vector representation by  including the word vectors in a context window of vc for each word in the sentence to capture the short term dependencies for prediction   mesnil et al.       . this effectively converts wi  into the context window version of the concatenated vectors  xi    wi vc   . . .   wi   . . .   wi vc  .  given the new input representation  we describe  the rnns to be investigated in this work below.   .     the basic models    in standard recurrent neural networks  at each time  step  word position in sentence  i  we have three  main vectors  the input vector xi   ri   the hidden  vector hi   rh and the output vector oi   ro  i   h and o are the dimensions of the input vectors   the dimension of the hidden vectors and the number of possible labels for each word respectively .  the output vector oi is the probabilistic distribution over the possible labels for the word xi and  obtained from hi via the softmax function     oi     w hi       ezm    zm     p z  k  ke       for simplicity  we are using the word wi and its realvalued vector representation interchangeably.     trigger words are the words that are often followed by  entity names in sentences such as  president    mr.  etc.    regarding the hidden vectors or units hi   there  are two major methods to obtain them from the  current input and the last hidden and output vectors  leading to two different rnn variants     in the elman model  elman         called  elman  the hidden vector from the previous step hi     along with the input in the current step xi   constitute the inputs to compute  the current hidden state hi    hi     u xi   v hi                 in the jordan model  jordan         called  jordan  the output vector from the previous step oi   is fed into the current hidden  layer rather than the hidden vector from the  previous steps hi   . the rationale in this  topology is to introduce the label from the  preceding step as a feature for current prediction   hi     u xi   v oi               in the formula above    is the sigmoid activation function    z      e   z and w   u   v are  the same weight matrices for all time steps  to be  learned during training. the unfolded dependency  graphs for the two models are given in figure  .   .  gated recurrent units  the hidden units in the two basic models above  are essentially the standard feed forward neural  networks that take the vectors hi     oi   and xi  as inputs and do a linear transformation followed  by a nonlinearity to generate the hidden vector hi .  the elman and jordan models are then basically a stack of these hidden units. unfortunately   this staking mechanism causes the so called   vanishing gradient  and  exploding gradient   problems  bengio et al.         making it challenging to train the networks properly in practice   pascanu et al.       . these problems are addressed by the long short term memory units   lstm    hochreiter and schmidhuber         graves et al.        that propose the idea of memory cells with four gates to allow the information  storage and access over a long period of time.  in this work  we apply another version of memory units with only two gates  reset and update    called gated recurrent units  grus  from cho  et al.      a       . gru is shown to be  much simpler than lstm in terms of computation and implementation but still achieves the     h     o     o     o     on      on    hn    h     h     h     hn      hn    xn    x     x     x     xn      xn    o     o     o     on      on    h     h     h     hn      x     x     x     xn      o     jordan    elman    figure    the elman and jordan models  comparable performance in machine translation   bahdanau et al.       .  the introduction of grus into the models elman and jordan amounts to two new models   named elman gru and jordan gru respectively  with two new methods to compute the  hidden vectors hi . the formula for elman gru  is adopted directly from cho et al.      b  and  given below   hi   zi   h i        zi     hi    h i     wh xi   uh  ri   hi       zi     wz xi   uz hi               ri     wr xi   ur hi      where wh   wz   wr   rh i   uh   uz   ur    and   is the element wise multiplication  operation.  we cannot directly apply the formula above to  the jordan gru model since the dimensions  of the output vectors oi and the hidden vector hi  are different in general. for jordan gru  we  first need to transform the output vector oi into the  hidden vector space  leading to the following formula   rh h     .  the extended networks  one of the limitations of the four basic models presented above is their incapacity to incorporate the  future context information that might be crucial to  the prediction in the current step. for instance   consider the first word  liverpool  in the following sentence   liverpool suffered an upset first home league  defeat of the season  beaten     by a guy whittingham goal for sheffield wednesday.  in this case  the correct label organization  can only be detected if we first go over the whole  sentence and then utilize the context words after   liverpool  to decide its label.  the limitation of the four models is originated  from their mechanism to perform a single pass  over the sentences from left to right and make the  prediction for a word once they first encounter it.  in the following  we investigate two different networks to overcome this limitation.    hi   zi   o i        zi     ti       . .  the contextual networks  the contextual networks are motivated by the  rnn encoder decoder models that have become  very popular in neural machine translation recently  cho et al.      a  bahdanau et al.       .  in these networks  we first run a rnn re over  the whole sentence x   x  x  . . . xn to collect the  hidden vector sequence c    c    . . .   cn   where ci is  the hidden vector for the i th step in the sentence.  for convenience  this process is denoted by     ti     t oi      re  x  x  . . . xn     c    c    . . .   cn    o i     wo xi   uo  ri   ti         the final hidden vector cn is then considered  as a distributed representation of x  encoding the  global context or topic information for x  the encoding phrase  and thus possibly being helpful for  the label prediction of each word in x. consequently  we perform the second rnn rd over x  to decode the sentence in which cn is used as an  additional input in computing the hidden units for    zi     wz xi   uz ti      ri     wr xi   ur ti      where t   rh o .            o     o     h         l     o     o     h     h     hn     n                  on    o     o     o     on      on    hn    h     h     h     hn      hn                       n       n    r     r     r     rn      rn    l     l     l     ln      ln    x     x     x     xn      xn    on       n    r     r     r     rn      rn    l     l     l     ln      ln    x     x     x     xn      xn    rn    l     rn      figure    the bidirectional models. the model on the right is from mesnil et al.        with the forward  and backward context size of  . l    rn   are the zero vectors.  every time step  the decoding phrase .  note that re  the encoding model  should be an  elman model  while rd  the decoding model  can  be any elman or jordan model. as an example   the formula for rd   elman is   hi     u xi   v hi     scn     . .  the bidirectional networks  the bidirectional networks involve three passes  over the sentence  in which the first two passes  are designated to encode the sentence while the  third pass is responsible for decoding. the procedure for the sentence x   x  x  . . . xn is below    i  run the first rnn ref from left to right  over x  x  . . . xn to obtain the first hidden vector  or output vector sequence  depending on whether  ref is an elman or jordan network respectively    ref  x  x  . . . xn     l    l    . . .   ln  forward encoding .   ii  run the second rnn reb from right  to left over x  x  . . . xn to obtain the second hidden vector or output vector sequence   reb  xn xn   . . . x      rn   rn     . . .   r   backward encoding .   iii  obtain the concatenated sequence                . . .    n where  i    li   ri  .   iv  decode the sentence with the third rnn  rd  the decoding model  using   as the input  vector  i.e  replacing xi by  i in the formula                and    .  conceptually  the encoding rnns ref and reb  can be different but in this work  for simplicity and  consistency  we assume that we only have a single  encoding model  i.e  ref   reb   re . once       from now on  for convenience  the term  elman models   refers to the elman and elman gru models. the same  implication applies for the jordan models.    again  re and rd can be any model in  elman   jordan  elman gru  jordan gru .  the observation is  at the time step i  the forward hidden vector li represents the encoding for  the past word context  from position   to i  while  the backward hidden vector ri is the summary  for the future word context  from position n to  i . consequently  the concatenated vector  i     li   ri   constitutes a distributed representation that  is specific to the word at position i but still encapsulates the context information over the whole  sentence at the same time. this effectively provides the networks a much richer representation  to decode the sentence. the bidirectional network  for re   elman and rd   jordan is given on  the left of figure  .  we notice that mesnil et al.        also investigate the bidirectional models for the task of slot  filling in spoken language understanding. however  compared to the work presented here  mesnil et al.        does not use any special transition  memory cells  like the grus we are employing  in this paper  to avoid numerical stability issues   pascanu et al.       . besides  they form the inputs   for the decoding phase from a larger context  of the forward and backward encoding outputs   while performing word wise  independent classification  in contrast  we use only the current output  vectors in the forward and backward encodings for     but perform recursive computations to decode  the sentence via the rnn model rd  demonstrated  on the right of figure   .   .  training and inference  we train the networks locally. in particular  each  training example consists of a word xi and its corresponding label yi in a sentence x   x  x  . . . xn   denoted by e    xi   yi   x  . in the encoding  phase  we first compute the necessary inputs ac      input    projection    output    input    projection    wt      wt      input    output    projection    output    wt    concatenate    wt      wt      wt      sum    wt    wt    wt    wt      wt      wt      wt      wt      wt      skip gram    cbow    c concat    figure    methods to train word embeddings  cording to the specific model of interest. this  can be the original input vectors x    x    . . .   xn in  the four basic models or the concatenated vectors            . . .    n in the bidirectional models. for  the contextual models  we additionally have the  context vector cn . eventually  in the decoding  phase  an sequence of vd input vectors preceding  the current position i is fed into the decoding network rd to obtain the output vector sequence. the  last vector in this output sequence corresponds to  the probabilistic label distribution for the current  position i  to be used to compute the objective  function. for example  in the bidirectional models  the input sequence for the decoding phase is   i vd  i vd    . . .  i while the output sequence is   re   i vd  i vd    . . .  i     oi vd oi vd    . . . oi .  in this work  we employ the stochastic gradient  descent algorithm  to update the parameters via  minimizing the negative log likelihood objective  function  nll e      log oi  yi   .  finally  besides the weight matrices in the networks  the word embeddings are also optimized  during training to obtain the task specific word  embeddings for md. the gradients are computed  using the back propagation through time algorithm  mozer        and inference is performed  by running the networks over the whole sentences  and taking argmax over the output sequence  yi    argmax oi  .    els to train word embeddings have been proposed  recently in mikolov et al.      a      b  that  introduce two log linear models  i.e the continuous bag of words model  cbow  and the continuous skip gram model  skip gram . the cbow  model attempts to predict the current word based  on the average of the context word vectors while  the skip gram model aims to predict the surrounding words in a sentence given the current word. in  this work  besides the cbow and skip gram models  we examine a concatenation based variant of  cbow  c concat  to train word embeddings  and compare the three models to gain insights into  which kind of model is effective to obtain word  representations for the md task. the objective  of c concat is to predict the target word using the concatenation of the vectors of the words  surrounding it  motivated from our strategy to decide the label for a word based on the concatenated context vectors. intuitively  the c concat  model would perform better than cbow due to  the close relatedness between the decoding strategies of c concat and the md methods. cbow   skip gram and c concat are illustrated in figure  .      word representation    our main focus in this work is to evaluate the robustness of the md systems across  domains and languages.  in order to investigate the robustness across domains  following the prior work  plank and moschitti         nguyen et al.      a   we utilize the ace       dataset which contains   domains  broadcast news   bn   newswire  nw   broadcast conversation  bc      following collobert et al.         we pre train  word embeddings from a large corpus and employ them to initialize the word representations  in the models. one of the state of the art mod   we try the adadelta algorithm  zeiler        and the  dropout regularization but do not see much difference.      experiments   .  dataset     telephone conversation  cts   weblogs  wl   usenet   un  and   entity types  person  organization  gpe   location  facility  weapon  vehicle. the union of  bn and nw is considered as a single domain  called  news. we take half of bc as the only development  data and use the remaining data and domains for  evaluation. some statistics about the domains are  given in table  . as shown in plank and moschitti          the vocabulary of the domains is quite different.  for completeness   we also test the  rnns  system  on  the  named  entity recognition for english using the  conll      dataset   florian et al.         tjong kim sang and de meulder         and  compare the performance with the state ofthe art neural network system on this dataset   collobert et al.       . regarding the robustness  across languages  we further evaluate the rnn  models on the conll      dataset for dutch  named entity recognition   carreras et al.         tjong kim sang       . both conll datasets  come along with the training data  validation data  and test data  annotated for   types of entities   person  organization  location and miscellaneous.  domain  news  bc  cts  wl  un  total     docs                                sents                                          mentions                                          for word representation  we train the word  embeddings for english from the gigaword corpus augmented with the newsgroups data from  bolt  broad operational language technologies     billion tokens  while the entire dutch  wikipedia pages      million tokens  are extracted to train the dutch word embeddings. we  utilize the word vec toolkit   modified to add the  c concat model  to learn the word representations. following baroni et al.         we use the  context window of    subsampling set to  e    and  negative sampling with the number of instances set  to   . the dimension of the vectors is set to      to make it comparable with the word vec toolkit.   .  model architecture experiments   . .  model architecture evaluation  in this section  we evaluate different rnn models by training the models on the news domain  and report the performance on the development  set. as presented in the previous sections  we have    basic models m    elman  jordan  elman gru  jordan gru     contextual models  two choices for the encoding model re in   elman  elman gru  and   choices for the  decoding model rd   m    and    bidirectional  models    choices for the encoding and decoding  models re   rd in m . the performance for the basic models  the contextual models and the bidirectional models are shown in table    table   and  table   respectively  .    table    ace      dataset    model rd    elman  jordan  elman gru  jordan gru    finally  we use the standard iob  tagging  schema for the ace      dataset and the dutch  conll dataset while the iobes tagging schema  is applied for the english conll dataset to ensure  the compatibility with collobert et al.       .   .     resources and parameters    in all the experiments for rnns below  we employ  the context window vc      the decoding window  vd    . we find that the optimal number of hidden  units  or the dimension of the hidden vectors  and  the learning rate vary according to the dataset. for  the ace      dataset  we utilize     hidden units  with learning rate    .   while these numbers are      and  .   respectively for the conll datasets.  note that the number of hidden units is kept the  same in both the encoding phase and the decoding  phase.          http   www.cnts.ua.ac.be conll     ner   http   www.cnts.ua.ac.be conll     ner     f     .      .      .      .      table    the basic models  performance  re elman  rd  elman    .    jordan    .    elman gru    .    jordan gru    .      elman gru    .      .      .      .      table    the contextual models  performance  there are several important observations from  the three tables    elman vs jordan  in the encoding phase  the  elman models consistently outperform the jordan  models when the same decoding model is applied          https   code.google.com p word vec     the experiments in this section use c concat to pretrain word embeddings.     re elman  rd  elman    .    jordan    .    elman gru    .    jordan gru    .    re jordan  rd  elman    .    jordan    .    elman gru    .    jordan gru    .      elman gru    .      .      .      .    jordan gru    .      .      .      .      table    the bidirectional models  performance  in the bidirectional architecture. in the decoding  phase  however  it turns out that the jordan models  are better most of the time over different model  architectures  basic  contextual or bidirectional .   with vs without grus  the trends are quite  mixed in the comparison between the cases with  and without grus. in particular  for the encoding  part  given the same decoding model  grus are  very helpful in the bidirectional architecture while  this is not the case for the contextual architecture.  for the decoding part  we can only see the clear  benefit of grus in the basic models and the bidirectional architecture when re is a jordan model.   regarding different model architectures  in  general  the bidirectional models are more effective than the contextual models and the basic models  confirming the effectiveness of bidirectional  modeling to achieve a richer representation for  md.  the best basic model  f      .      the best  contextual model  f      .     and the best bidirectional model with  f      .     are called basic  context and bidirect respectively. in  the following  we only focus on these best models  in the experiments.   . .     comparison to other bidirectional  rnn work  mesnil et al.        also present a rnn system with bidirectional modeling for the slot filling task. as described in section  . .   the major difference between the bidirectional models in  this work and mesnil el al.        s is the recurrence in our decoding phase. table   compares the performance of the bidirectional model  from mesnil et al.         called mesnil  and  the bidirect model. in order to verify the effectiveness of recurrence in decoding  the performance of mesnil incorporated with the jordan gru model in the decoding phase  mesnil jordan gru  is also reported.  in general  we see that the bidirectional model  in this work is much better than the model in mesnil et al.        for md. this is significant with    model  mesnil         mesnil   jordan gru  bidirect    p    .      .      .      r    .      .      .      f     .      .      .      table    comparison to mesnil et al.       .  p    .   and a large margin  an absolute improvement of  .    . more interestingly  mesnil is further improved when it is augmented with  the jordan gru decoding  verifying the importance of recurrence in decoding for md.   .  word embedding evaluation  the section investigates the effectiveness of different techniques to learn word embeddings to initialize the rnns for md. table   presents the performance of the basic  context and bidirect  models on the development set  trained on news   when the cbow  skip gram and c concat  techniques are utilized to obtain word embeddings  from the same english corpus. we also report the  performance of the models when they are initialized with the word vec word embeddings from  mikolov et al.      a      b   trained with the  skip gram model on     billion words of google  news   word vec . all of these word embeddings are updated during the training of the rnns  to induce the task specific word embeddings . finally  for comparison purpose  the performance  for the two following scenarios is also included    i  the word vectors are initialized randomly  not  using any pre trained word embeddings   random   and  ii  the word vectors are loaded from  the c concat pre trained word embeddings but  fixed during the rnn training  fixed .  word  embeddings  random  fixed  word vec  cbow  skip gram  c concat    basic    .      .      .      .      .      .      model  context    .      .      .      .      .      .      bidirect    .      .      .      .      .      .      table    word embedding comparison  the first observation is that we need to borrow some pre trained word embeddings and update them during the training process to improve  the md performance  comparing c concat   random and fixed . second  c concat is  much better than cbow  confirming our hypothesis about the similarity between the decodings  of c bow and md in section  . third  we  do not see much difference in terms of md performance when we enlarge the corpus to learn     word embeddings  comparing skip gram and  word vec that is trained with the skip gram  model on a much larger corpus . finally  we  achieve the best performance when we apply the  c concat technique in the bidirect model.  from now on  for consistency  we use the cconcat word embeddings in all the experiments  below.   .     comparison to the state of the art     . .     the ace      dataset for mention  detection  the state of the art systems for mention detection  on the ace      dataset have been the joint extraction system for entity mentions and relations  from li and ji      a  and the information networks to unify the outputs of three information  extraction tasks  entity mentions  relations and  events using structured perceptron from li et al.       b . they extensively hand design a large  set of features  parsers  gazetteers  word clusters   coreference etc  to capture the inter dependencies  between different tasks. in this section  besides comparing the rnn systems above with  these state of the art systems  we also implement  a maximum entropy markov model  memm   system     following the description and features  in florian et al.               and include it in  the comparison for completeness   . for this comparison  following li and ji      a   we remove  the documents from the two informal domains cts  and un  and then randomly split the remaining      documents into   parts      for training     for  development  and the rest    for blind test. the  performance of the systems on the blind test set is  presented in table  .    ploying in this system. consequently  from now  on  we would utilize this memm system as the  baseline in the following experiments. second   all the three rnn systems  basic  context  and bidirect substantially outperform the stateof the art system with large margins. in fact  we  achieve the best performance on this dataset with  the bidirect model  once again testifying to the  benefit of bidirectional modeling for md.   . .  the conll      dataset for english  named entity recognition  this section further assess the rnn systems on  the similar task of named entity recognition  for english to compare them with other neural  network approaches for completeness. on the  conll      dataset for english ner  the best  neural network system so far is collobert et al.        . this system  called cnn sentence  employs convolutional neural networks to encode the  sentences and then decodes it at the sentence level.  table   shows the performance of cnn sentence  and our rnn systems on this dataset.  system  cnn sentence  basic  context  bidirect    f     .      .      .      .      table    performance on english conll     .  as we can see from the table  the rnn systems  are on par with the cnn sentence system from  collobert et al.        except the context system that is worse in this case. we actually accomplish the best performance with the bidirect  model  thus further demonstrating its virtue.   .  cross domain experiments    system  joint system  li and ji      a   joint system  li et al.      b   memm  basic  context  bidirect    p    .     .     .     .     .     .     r    .     .     .     .     .     .     f     .     .     .     .     .     .     table    performance for md on ace.  there are two main conclusions from the table.  first  our memm system is already better than  the state of the art system on this dataset  possibly  due to the superiority of the features we are em      we tried the crf model  but it is worse than memm in  our case.      we notice that the four features we are using in the rnn  models  section    are also included in the feature set of the  implemented memm system.    one of the main problems we want to address  in this work is the robustness across domains of  the md systems. this section tests the memm   the baseline  and the rnn systems on the  cross domain settings to gain an insights into  their operation when the domain changes. in  particular  for the first experiment  following  the previous work of domain adaptation on the  ace      dataset  plank and moschitti         nguyen and grishman         nguyen et al.      a   we treat news as the  source domain and the other domains  bc  cts  wl  and un as the target domains. we then examine  the systems on two scenarios   i  the systems are  trained and tested on the source domain via   fold  cross validation  in domain performance   and     system  memm  basic  context  bidirect    in domain    .      .      .      .       without features  bc  cts  wl    .      .      .      .      .      .      .      .      .      .      .      .       un    .      .      .      .       in domain    .      .      .      .      with features  bc  cts    .      .      .      .      .      .      .      .       wl    .      .      .      .      un    .      .      .      .      table    system s performance on the cross domain setting. cells marked with  designate the bidirect models that significantly outperform  p    .    the memm model on the specified domains.    bc  cts  wl  un    bc    .      .      .      .      memm  cts  wl    .     .      .     .      .     .      .     .      un    .      .      .      .      bc    .      .      .      .      bidirect  cts  wl    .     .      .     .      .     .      .     .      un    .      .      .      .      bc   .     .       .     .       bidirect memm  cts  wl  un   .     .     .      .      .     .      .      .     .     .     .     .       table     comparison between memm and bidirect. cells marked with  designate the statistical  significance  p    .   . the columns and rows correspond to the source and target domains respectively.   ii  the systems are trained on the source domain  but evaluated on the target domains. besides  in  order to understand the effect of the features on  the systems  we report the systems  performance  in both the inclusion and exclusion of the features  described in section  . table   presents the  results.  to summarize  we find that the rnn systems significantly outperform the memm system  across all the target domains when the features are  not applied. the bidirect system still yields  the best performance among systems being investigated  except in domain bc . this is also the case  in the inclusion of features and demonstrates the  robustness of the bidirect model in the domain  shifts. we further support this result in table     where we report the performance of the memm  and bidirect systems  with features  on different domain assignments for the source and the target domains. finally  we also see that the features  are very useful for both the memm and the rnns.   .     named entity recognition for dutch    the previous sections have dealt with mention detection for english. in this section  we want to explores the capacity of the systems to quickly and  effectively adapt to a new language. in particular  we evaluate the systems on the named entity  recognition task  the simplified version of the md  task  for dutch using the conll      dataset.  the state of the art performance for this dataset is  due to carreras et al.        in the conll       evaluation and nothman et al.       . very recently  while we were preparing this paper  gillick  el al.        introduce a multilingual language  processing system and also report the performance    on this dataset. table    compares the systems.  system  state of the art in conll  nothman et al.         gillick el al.         gillick el al.          memm  basic  bidirect    p    .      .      .      .      r    .      .      .      .      f     .      .      .      .      .      .      .      table     performance on dutch conll     .  note that the systems in gillick el al.        are  also based on rnns and the row labeled with    for gillick el al.        corresponds to the system  trained on multiple datasets instead of the single  conll dataset for dutch  so not being comparable to ours.  the most important conclusion from the table  is that the rnn models in this work significantly  outperform memm as well as the other comparable system by large margins  up to     reduction in relative error . this proves that the proposed rnn systems are less subject to the language changes than memm and the other systems. finally  bidirect is also significantly better than basic  testifying to its robustness across  languages.      conclusion  we systematically investigate various rnns to  solve the md problem which suggests that bidirectional modeling is a very helpful mechanism  for this task. the comparison between the rnn  models and the state of the art systems in the literature reveals the strong promise of the rnn models. in particular  the bidirectional model achieves  the best performance in the general setting  up to     reduction in relative error  and outperforms     a very strong baseline of the feature based exponential models in the cross domain setting  thus  demonstrating its robustness across domains. we  also show that the rnn models are more portable  to new languages as they are significantly better  than the best reported systems for ner in dutch   up to     reduction in relative error . in the future  we plan to apply the bidirectional modeling  technique to other tasks as well as study the combination of different network architectures and resources to further improve the performance of the  systems.    acknowledgment  we would like to thank ralph grishman for valuable suggestions.    