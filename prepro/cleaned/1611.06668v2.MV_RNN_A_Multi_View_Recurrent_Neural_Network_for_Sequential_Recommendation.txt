introduction    r    ecently   with the development of internet  applications  with sequential information have become numerous and multilateral  such as web page recommendation and click prediction.  based on sequential recommendation methods  these applications  could predict a user s following behaviors to improve user experience. taking online shopping as an example  after a user buys  an item  the application would predict a list of items that the user  might buy in the near future. further  we can consider the purchase  behaviors as a sequence in the time order. due to sparse user  feedbacks  sequential recommendation usually encounters the item  cold start problem. thus  our task here concentrates on the sequential recommendation based on user historical implicit feedback  and alleviating the item cold start problem. as shown in figure     we observe that a user will look at corresponding images and  text descriptions before he or she buys items. intuitively  we can  alleviate the item cold start problem by modeling additional multimodal information like images and text descriptions. besides   we try to find a more effective way of incorporating additional  information into sequence modeling.  as for the recommendation  collaborative filtering methods are  widely used. matrix factorization  mf  methods       become the  first choice  and learn latent representations of users and items.  in order to alleviate the cold start problem  multiple additional  information can be adopted  such as attribute information                         accepted as a regular paper of tkde.  qiang cui  shu wu  qiang liu and liang wang are with the center for  research on intelligent perception and computing  cripac   national  laboratory of pattern recognition  nlpr   institute of automation  chinese academy of sciences  casia  and university of chinese academy of  sciences  ucas   beijing          china.  e mail  cuiqiang     ia.ac.cn    shu.wu  qiang.liu  wangliang  nlpr.ia.ac.cn.  wen zhong is with the university of southern california.  e mail  wenzhong usc.edu    text      images         and so on. although these methods can  utilize different types of features  they usually capture the user s  static interest and have much difficulty in capturing sequential  information. long term interest should be weakened while shortterm interest should become prominent relatively    .  on the other hand  markov chain  mc  methods          are widely studied for sequential recommendation by learning  the transition matrix. they predict the next behavior based on  recent behaviors as the transition matrix gives the probability  among different states. however  mc methods could not well  build the user s long term interest due to the markov assumption.  they usually consider recent behaviors and ignore the longterm interest. besides  after constructing the real world dataset  of sequential scenarios like shopping and clicking  the transition  probability among different states is established. the additional  information no longer has any effect on this probability.  recently  recurrent neural network  rnn  methods have  shown great achievements in machine translation       sequential  click prediction       location prediction       next basket recommendation       multi behavioral sequential prediction       and  so on. besides  long short term memory      and gated recurrent  unit      are developed because of the gradient vanishing and  explosion problem. they can hold the long term dependency and  have been applied to many tasks        . these rnn methods          are more promising than factorizing personalized markov  chains      and other conventional mc methods.  the existing sequential recommendation methods have difficulty in alleviating the problem of item cold start. a good  choice is to apply rnn and incorporate additional multi modal  features  like images and text descriptions. recently  the parallel  rnns model  p rnns       incorporates additional information  for session based recommendation. the p rnns model deals with  multi source data by separate subnets which are trained one by  one. it builds multiple user s interests based on different views     journal of latex class files  vol.     no.    august              image                                                    user    sequential  recommendation    text  description    one teaspoon  women s donnies  shorts    american apparel    steve madden    women s ponte    women s ecentric    sleeveless skater  dress    slip on fashion  sneake    rebecca minkoff  mini mac  convertible  cross body handbag    fig.  . diagram of a user s purchase sequence. a user buys different items at different time. we make use of the image and text description  associated with each item to build the sequential recommendation model. the goal is to recommend items a user would buy in the near future  and  alleviate item cold start by incorporating multiple additional information into sequence modeling.    and combines the results at the end of each subset together. this  way may not well leverage the advantage of multi view data. we  need to consider how to more effectively incorporate additional  information to model sequential behaviors.  in view of the above analysis  we propose a model called  multi view recurrent neural network  mv rnn  for sequential  recommendation and alleviating the item cold start problem. first   we gain visual and textual features from images and text descriptions respectively. these multi modal features are complementary  to understand the item and user s interest. a latent vector is defined  for each item to represent the indirectly observable representation.  these multi view features are used as the input of mv rnn   and three different combinations are explored. feature concatenation and fusion naturally come to mind. more importantly   we introduce a multi modal fusion model  called multi modal  marginalized denoising autoencoder   mdae . this model can  help to learn more robust features and handle items with missing  modalities. next  we design a separate structure and a united  structure for mv rnn to explore an effective way to handle  multi view features. one applies multiple rnn units separately  at every input time  and multiple hidden states of these units are  concatenated together at the same time. the other employs a single  rnn unit to deal with the multi view features at once to learn  a united hidden state. the mv rnn model adopts the recurrent  structure to capture dynamic changes in user s interest. finally  we  employ the bayesian personalized ranking framework     and the  backpropagation through time algorithm      to learn parameters.  the main contributions are listed as follows                  we design a representation of item with multi view features. these features comprise of indirectly observable   latent  feature and directly observable  e.g.  visual and  textual  feature. three combinations of multi view features are developed  especially our  mdae.  to explore a more effective way to handle multi view  inputs  mv rnn applies a separate structure and a united  structure. compared to dealing with each view separately   handling multi view features by a united structure can  better leverage the advantage of different views.  experiments on two large real world datasets reveal that    mv rnn is effective and outperforms the state of the art  methods.  the rest of the paper is organized as follows. section    reviews previous work on sequential recommendation  cold start   and multi modal representation learning. mv rnn is detailly  introduced in section   from the perspective of input  hidden state   and output. in section    we conduct extensive experiments. at  last  we conclude the paper in section  .         r elated w ork    in this section  we review several related works including collaborative filtering  markov chain based methods  recurrent neural  networks  and multi modal representation learning.   .     collaborative filtering    there are two main methods of collaborative filtering  cf    neighborhood models and latent factor models     . neighborhood  models have practical benefits  but they usually focus on a small  subset of items or users. latent factor models have the global  perspective  and thus they tend to be more accurate. recently   matrix factorization  mf  models belonging to latent factor models become fundamental because of its scalability and accuracy.  mf absorbs rich additional information to alleviate the cold start  problem  like item s attribute or user s demographics           .  text such as reviews is used along with the development of online  searching     . zhao et al. extend mf by combining visual data  like posters and still frames of a movie to understand the movie  and user s interest    . however  none of these methods could  reflect the changes in user s interest over time.  in recent years  pairwise methods become the state of the art  for implicit feedback    . these methods can directly optimize the  ranking of feedbacks and assume positive items are preferable than  negative items. rendle et al.     propose a bayesian personalized  ranking  bpr  framework to maximize the difference of user s  preferences between positive and negative items. recently  bpr is  extended to combine more information like users  social relations      . other information like visual signals is accommodated by  vbpr      which applies visual features of product images to     journal of latex class files  vol.     no.    august         discover user s visual interest and better understand items. similar  to mf methods  they only learn general tastes of users.   .     markov chain based methods    in addition to conventional cf methods  sequential methods are  popular for the recommendation and they mostly rely on markov  chains  mc . rendle et al.      make a combination of mf and  mc to learn both general taste and current effect for the nextbasket recommendation. chen et al.     build a markov model  integrated with the forgetting mechanism to weaken long term  interest and highlight short term interest for item recommendation.  however  the markov assumption hinders learning the long term  dependency because it assumes the next state only related to the  last state. the high variable order mc models can make the next  state related to multiple previous states  which results in a high  computational cost. this problem can be solved by only considering the state to state probability with balancing parameters  which  ignores the set to state probability        . it is difficult for mc  methods to model the long term dependency.  on the other hand  there are few markov models involving  multiple features. chen et al.          propose a two view latent  subspace markov network to do image retrieval  annotation and  so on. their model is more like multi view data fusion and is  not suitable for sequential recommendation. mc is based on the  probability among different states. in the sequential scenario  this  probability is independent of the additional content information.   .     recurrent neural networks    recently  recurrent neural networks become more and more powerful. owing to its recurrent structure  rnn can better extract  the temporal dependencies. rnn based sequential click prediction       gains the state of the art performance. yu et al.      take the  representation of a basket acquired by pooling operation as the  input of rnn  which is most effective for next basket recommendation. liu et al.      incorporate time specific and distancespecific transition matrices into rnn to predict next location. liu  et al.      combine rnn and the log bilinear model      to make  multi behavioral prediction. compared with traditional sequential  methods  rnn is more promising.  due to the gradient vanishing and explosion problem            standard rnn fails to hold the long term dependency. lots of  work have been done to alleviate this problem  and the gated activation function achieves a success  like long short term memory   lstm       and gated recurrent unit  gru      . sutskever et  al.      apply a multilayered lstm to encode the input sequence  and another lstm to decode the target sequence in translation  task. their work also demonstrates lstm can easily handle long  sentences. chung et al.      propose gated feedback rnns to  investigate the character level language modeling. bengio s work  finds that gru lstm are both certainly better than the basic rnn  and gru is comparable to lstm on sequence modeling     .  recently  rnn is developed to model multi view features.  hidasi et al. introduce the basic rnn model to do the sessionbased recommendation task       then develop the p rnns model  to incorporate rich features     . the p rnns model builds  subnets for each view separately. this is similar to the latent  interest and visual interest in vbpr    . two rnns are used  to make video recommendation by using the image and make  product recommendation by using text description. compared  with the basic rnn model with only id feature  the performance         improvement of p rnns is not significant. cao et al. model multiview features collected by the mobile phone to predict the mood  score     . obviously  there are large differences between features  in their work  and they apply the late fusion to explore interactions.   .     multi modal representation learning    there are several main multi modal representation learning methods  probabilistic graphical models  kernel based methods and  neural networks     . it is often intractable and complicated  to obtain exact inference for probabilistic models. because of  the eigenvalue problem  kernel based methods occupy a lot of  memory and time. on the contrary  neural networks are tractable  to handle the high dimensional data. recently  due to the success  of deep neural networks  dnns   traditional methods tend to  combine deep structures.  for methods based on dnns  two main training strategies  are widely used  canonical correlation analysis  cca  and  autoencoder  ae      . cca based methods can make the two  modalities maximally correlated. recently  deep cca is proposed       but it needs a large minibatch to optimize     . based on  cca and ae  a deep canonically correlated autoencoder model  is proposed      for feature learning. the constraint conditions  would be too complicated if cca based methods are used in our  work. accordingly  ae based methods would be promising.  ae based methods are very powerful to learn compact representations. ae could reproduce the input signal as far as possible  and find the principal component. vincent et al. design the  denoising ae  dae  by setting some input data to zero in a  probabilistic manner     . after that  vincent et al. design the  stacked denoising ae  sdae  and find that a single matrix is  enough to do the encoding and decoding steps     . ngiam et  al. introduce the bimodal deep denoising autoencoder     . in  this way  the hidden layer could learn the shared representation  from different modalities. later  chen et al.      propose the  marginalized denoising ae  mdae  model  which finishes off  the nonlinear transfer function and learns a linear transfer matrix.  furthermore  wang et al.      propose a coupled mdae model  to deal with cross domain learning problems. we introduce a   mdae model to generate multi modal fusion representation.         p roposed mv rnn m odel    in this section  we propose a multi view recurrent neural network  mv rnn  model. we first formulate the problem. next  we  explore   strategies to combine multi view features at the input  to represent the item. then we investigate   structures to model  multi view features at the hidden state to build user representation.  finally  all the variants of mv rnn can be trained with the  bayesian personalized ranking  bpr  framework and the back  propagation through time  bptt  algorithm.   .     problem formulation    in order to simplify the problem formulation of sequential recommendation  we take purchase histories of online shopping for  instance. let u    u    ...  u u     and i    i    ...i i    represent  the sets of users and items respectively. use i u    iu    ...  iu i u      to denote the items that the user u has purchased in chronological  order  and the t th item iut   i . additionally  an image and a text  description are available for each item i   i . given each user s  history i u   our goal is to recommend a list of items that a user  may purchase. the notation is listed in table   for clarity.     journal of latex class files  vol.     no.    august            t     x upq       x upq    bpr       bpr         h    h  lstm    w    t  x upq    u    w    ft    bpr  t    t             xt    h  lstm    h  lstm    w    u    gt         f        g       x    i  e    im     i     et  vt    v    f       g          t    x    i    imt      i t      xt    ft    gt    t  x    i  e  v    e  v    it    ift    igt        concatenation    u         mdae  x     ixt    imt    it    et    vt        fusion  xt    f  t    ft    g  t    gt    ixt  e    it    imt    v    fig.  . diagram of the mv rnn model. the multi view input consists of latent feature and additional visual and textual features. concatenation   fusion and  mdae are three kinds of combinations of multi view features. the hidden state captures dynamic changes in the user s interest.    sequential recommendation usually encounters the cold start problem as feedbacks are too sparse to learn fine representations of  users and items. modeling multi view features is an effective  way to alleviate this issue. these features are usually obtained  from different data sources  and have different numerical ranges  as well as different dimensions. therefore  the raw features need  be normalized to a same range to obtain x  f and g  and should  better be embedded to d dimensional vectors to obtain ix   if and  ig . none of them is sequence data and they are aligned with each  other by the item id.    table    notation.  notation    explanation    u   i  iu  pu  vu  t u  p  q  x tupq  f  g  e  v  if   ig  ix   im  d  df   dg  hx   hm  u  w   b    set of users  set of items  sequence of user u  sequences of training  validation and test of user u  positive item  negative item  difference of preference of u towards p and q at the t th time  high dimensional visual and textual features of an item  embedding matrices for f   g  low dimensional visual and textual features of an item  latent feature  multi modal fusion feature built by if and ig  dimensions of ix   f   g  latent and multi modal fusion features of a user  transition matrices and bias for recurrent neural network     .     representation of item with multi view features    representation of item is used as the input of our mv rnn model.  three different combinations of multi view features are shown in  figure    and details are as follows.   . .     multi view features    there are two basic types of multi view features of an item   indirectly observable view and directly observable view. the former view is latent feature  which is widely used in recommender  systems. the latent feature of an item is defined by a vector     ix   x     i x   rd           the latter view refers to the additional multi modal information  that is presented externally  like image  text description  category  label  video  and so on. they can provide very important information for the item. for example  image can directly show the color   text description can provide the clothing size.  the multi modal features consist of visual and textual features   f and g  in our work. they are obtained by googlenet       and glove      weighted by tf idf respectively. the two kinds  of features are      dimensional and     dimensional vectors  respectively. due to the difference of f and g  we learn two  linear embedding matrices e and v to transform the original  high dimensional features to embedded low dimensional visual  and textual features  if and ig       if   ef      i f   rd           ig   v g     i g   rd            . .  feature concatenation  the most natural method to combine multi view features is concatenation. intuitively  the item representation is i    ix   if   ig  .  the i is a  d dimensional vector  and its dimension will increase  with the number of features. the capacity and complexity of this  method will also increase subsequently.   . .  feature fusion  fusion can be directly established by the addition operation  without nonlinear transformation     im   if   ig      i m   rd           please note that features with similar contents are suitable for  fusion. therefore  if and ig are fused as the multi modal fusion  feature im   and this process can make the model more concise.  benefiting from linear embedding and linear transformation  im  can hold all the information from f and g. then we obtain item  representation i    ix   im   by concatenation.  although concatenation and fusion are easy to utilize  they  still have three issues. first  both concatenation and fusion do not  have an explicit objective which is able to explore correlations  across modalities     . second  they are unhandy to use in such  a situation where items in the test set have missing modalities      . third  no matter the combination of if   ig is concatenation  or fusion  useful information is entered into the model as well as  noise. therefore  more robust structures and parameters  e  v    need to be learned.   . .  multi modal marginalized denoising autoencoder  we introduce a new fusion method to combine the multi modal  information to learn fusion feature. this method can go further to  leverage the advantage of different modalities  learn more robust  features and tackle the missing modalities problem.     journal of latex class files  vol.     no.    august              cxt           cxt cmt                      cmt    tanh         fxt         zxt         gxt  oxt  tanh                          fmt         zmt         gmt  omt  tanh                hxt hmt       hmt  imt multi modal fusion feature    latent feature     a  separate structure    ct    tanh    tanh    hxt    ixt    c t      ft              zt    gt  ot  tanh           ht      ht       ixt   imt       united feature     b  unified structure    fig.  . diagram of hidden state structures of the mv rnn model. we devise a separate structure and a united structure. the two structures handle  the multi view input features at the input by multiple rnn units and by one rnn unit each time respectively.    this method is based on the mdae model     . it learns  a linear mapping m and minimizes the reconstruction loss  l t  m t    where t  is the corrupted version of original feature t.  however  mdae has no hidden layer. later  the coupled mdae       modifies the original mdae with two mappings in a linear  way l t  m t m t  . m t  and m t m t  represent the encoding  and decoding processes respectively. based on these works  we  introduce a multi modal mdae model  called  mdae  to learn  fusion feature. details are as follows.  encoder decoder. the encoding process is represented by eqs.    and    and the corresponding hidden layer is built by eq.  .  in the decoding process  we need to reconstruct the multi modal  input features. the mapping matrix in decoding process is just the  transpose of the mapping matrix in encoding process     .    f    e t im           g    v t im    in our introduced  mdae model  we omit bias term and apply  original features f and g instead of corrupted version as input.  the denoising operation is discussed in section  . . the final  representation of an item is also i    ix   im  .  objective function. the mdae model minimizes the overall  quadratic reconstruction loss for one modality        m    x  ti   m t m t i       argmin      m i                       where m is the number of samples. we extend this to form the  objective function of  mdae      m       x kfi   f  i k   kgi   g  i k         argmin                m i     df     dg    the df and dg are the original dimensions of visual and textual  features respectively  where  df          and  dg         in our  work. they are used as balance factors.   .     modeling of multi view features on hidden state    user representation is expressed by the hidden state of our mvrnn model. two different ways are explored to model the multiview features built at the input. in detail  figures   a  and   b   reveal the separate and united hidden state structures respectively.  specifically  the illustration is based on ix and im .   . .     long short term memory    conventional rnn suffers from the gradient vanishing and explosion problem  so that it fails to learn long term dependencies            . gated activation function is proposed to solve this issue.  we chose the widely used lstm      and it is denoted by        f t     u   xt   w   ht     b           z t     u   xt   w   ht     b           g t   tanh u   xt   w   ht     b          ct   f t ct     z t g t        ot     u   xt   w   ht     b        ht   ot tanh ct  where means element wise product between two variables  t is  the time step  xt   rd is the input feature. transition matrices  u       rd d transfer the current input. recurrent connections  w       rd d delivers the sequential information. b      rd  are bias terms. the f t   z t   g t   ct   ot   ht are the f orget gate   input gate  update gate  cell  output gate and the hidden  state  respectively. in our work  we apply a lstm    function to  substitute the original formulas in equation           ht   lstm u xt   w ht     b    ht   rd         where u is a set of four matrices u       and so do the w   b.   . .  separate multi view rnn  a natural way to handle multi view features is to apply separate  rnn units. each unit is used for each kind of feature. in this stage   our mv rnn is a two unit model  as shown in figure   a .  we apply one rnn unit to model the latent feature and  apply another rnn unit to model the multi modal fusion feature.  formulation is defined by         htx   lstm u x itx   w x ht       b     htx   rd      a   x  x        htm   lstm u m itm   w m ht    htm   rd      b   m   bm    where htx and htm are defined as a user s latent interest and multimodal fusion interest at the t th input. u x is a set of four matrices   u x      rd d . similarly  w x   bx   u m   w m and bm are sets  of three matrices or vectors  where subscripts x and m represent  the latent modeling and multi modal modeling.  multi view user representation is the concatenation of hidden  states from the two rnn units. they are linked together at every  time step in our work.        ht   htx   htm    ht   r d          where ht is the user s general interest. but it may not be able  to leverage the connection between multi view features  as we     journal of latex class files  vol.     no.    august              model them in two rnn units separately and build discrete user s  interests. thus we tend to develop a single rnn unit to handle  multi view features simultaneously.    table    datasets. we list the numbers of users  items  feedbacks and sparsity  of each dataset respectively.   a  datasets    core  used throughout the experiment.     . .  united multi view rnn  we incorporate the multi modal fusion feature into one rnn unit  together with the latent feature. in such situation  our mv rnn  is a one unit model  as shown in figure   b . this structure can  capture the relation between multi view features and construct the  united user s interest  which promotes the model to have more  promising performance.             ht   lstm u itx   itm   w ht     b    ht   r d       where ht is the complete user s interest  not a simple combination  of a user s different interests in eq.   . we apply  u       one factor  consisting of u       r d  d because we have itx   itm   r d    and so do the w   b.  via the  mdae model and the united structure  we finally  model the item s multiple  latent  visual and textual  features and  the user s interest in the same feature space. our mv rnn model  benefits from this united viewpoint.   .     model learning    after discussing the input and hidden state of the mv rnn model   we introduce the training procedure on output. no matter what  kind of combinations of features at input or structures of hidden  state  the bpr     framework is always suitable. bpr is a powerful  pairwise method for implicit feedback  and it has been widely  used in many works                   . besides  as a  mdae  model is introduced  we need to carefully consider the multimodal reconstruction loss. a united objective function needs to  be constructed. the description is also based on ix and im .  the training set s is made by  u  p  q  triples  where u  represents the user  p and q denote the positive and negative items  respectively. item p is selected from a user s purchase history i u    while item q is randomly chosen from the rest items  i   i u  . a  negative item is regenerated for each positive item in each epoch.    s     u  p  q  u   u   p   i u   q   i   i u      users    items    feedbacks    sparsity                                                                     .        .         b  sub datesets for the controlled study in section  . . .  dataset  taobao     core   taobao     core   taobao     core     users    items    feedbacks    sparsity                                                                                           .        .        .        the preference of bpr and the reconstruction loss of our  mdae  model. the final objective function is defined as         argmin             ln   x upq         r    x        a kfp   f  p k    kfq   f  q k            df        k k                        ra         u p q  s     gp   g  p   gq   g  q    dg          where   denotes a set of parameters      x  e  v   u   w   b .  x is the set of all items  latent features. u   w and b are the sets  of the matrices or vectors represented in previous equations.         is the regularization parameter. please note that  ev is introduced  to regularize embedding matrices e and v . then  mv rnn can  be learned by the mini batch gradient descent and parameters are  updated by classical bptt     .  after the training  we obtain the fixed representations of   . then x  e and v are reused to obtain each item s final  representation. we recalculate each user s sequential hidden states   and the last hidden state denotes a user s final representation.            given the training set  we calculate the difference of user s  preferences between positive and negative items on output at every  time step. at the t th time step  it can be computed by    x tupq   x tup   x tuq      t   t      ht  ip   it    q    dataset  taobao  amazon            where it    and it    p  qh represent  i positive hand negative  i inputs ret    t   t    t    t   t    spectively  ip   ixp   imp   iq   ixq   imq .  the objective function combines bpr and our  mdae by a  minimal form. the mv rnn can simultaneously model these two  kinds of losses. bpr maximizes the following formula   x      k k              argmax  ln   x upq             u p q  s    it is transformed to the minimal form in our work. next   mdae  loss represented in eq.   is extended along with the bpr. because  we compute preference at every output using positive and negative  items  we need to minimize all the visual and textual encoderdecoder losses. last  we introduce a multiplicator ra to leverage         e xperimental r esults and a nalysis    in this section  we conduct experiments on two real world datasets.  first  experimental settings are introduced. then a hyperparameter  optimization is performed. next  we make a comparison between  mv rnn and baselines  and a denoising experiment is conducted  for our  mdae. the last subsection is cold start analysis on items.   .    . .     experimental settings  datasets    experiments are conducted on two datasets collected from  taobao  and amazon  . the basic statistics are listed in table   . both datasets have massive sequential implicit feedbacks  and  each item contains an image and a text description. we apply the  filtering strategy called k  core             . each user purchases  at least k items and each item is bought by at least k users. we  set k    and also hold users with no more than     items  because  users with very long sequences   i u          may scalp items.   . https   tianchi.shuju.aliyun.com datalab dataset.htm id      . http   jmcauley.ucsd.edu data amazon     journal of latex class files  vol.     no.    august              table    the best parameters acquired on the validation set for all methods.    dataset              parameter    bpr    vbpr    gru lstm    p rnn    based on gru lstm    based on gru    based on gru lstm    con.    fus.     mdae  u     mdae  u     mdae  u     mdae  u    taobao         ev  ra     .         .    .             .            .            .       .         .       .         .       .    .         .       .    .        .       .    .          .       .    .         amazon         ev  ra     .            .       .            .           .           .      .         .      .         .      .    .         .      .        .         .      .        .        .      .        .       taobao is a dataset for clothing matching competition on  tianchi  platform. we use user historical data and item  features  image  text  to make the sequential recommendation. its time span is from    jun      to    jun     .  amazon contains many reviews and product metadata          . we use one large category clothing  shoes  and jewelry located in the second half of the website.  we acquire the sequential implicit feedback from review  histories where the ratings range from   to    obtain the  images and text data from product metadata. the original  time span is between    sep      and    jul     . as  feedbacks in previous years are too sparse  we only keep  feedbacks within the most recent two years.     . .  multi modal features  multi modal features are obtained by using the existing methods.  they are normalized to the same range by min max normalization.  then  they are used as the input features  f and g .  the visual feature is obtained by the googlenet      implemented by bvlc caffe deep learning framework     . this  network has    layers and has been pre trained on  . m imagenet ilsvrc     images     . we apply the output of layer  pool   x  s  to obtain      dimensional visual features. they  are all positive and are normalized to range      .  .  to generate the textual features of items  a text description  of each item is collected firstly. on taobao  we directly use item  titles which have already been segmented and disordered by the  data provider. on amazon  we combine each item s category and  title as its text data. then we adopt the glove model      weighted  by tf idf      to obtain each word s feature and weight. finally   the weighted feature for each item is computed to obtain    dimensional textual features. their values are in the vicinity of  zero and are normalized to range    .    .  .   . .  evaluation metrics  performance is evaluated on test set by recall  mean average  precision  map       and normalized discounted cumulative  gain  ndcg      . the former one is an evaluation of unranked  retrieval sets  while the latter two reflect the order of items. here  we consider top k  e.g.  k          recommendations. besides   the area under the roc curve  auc         is introduced to  evaluate the overall performance.  data is divided by time. we use feedbacks in first     of  the time for training      for validation and the rest     for test.  same as p rnns  hyperparameters are optimized on the validation  set  and all models are retrained on the full training set  training  and validation sets  before obtaining final results on the test set.   . https   tianchi.shuju.aliyun.com      . .     comparisons    we compare mv rnn with several comparative baselines                             random  items are randomly ranked for all users. the  auc of this method is  .     .  pop  this baseline recommends the most popular items  in the training set for each user u.  bpr  this method refers to the bpr mf for implicit  feedback    . it optimizes the difference of user s preferences for positive and negative items. the corresponding  pairwise training procedure has been applied to many  sequential tasks            .  vbpr  introduced in      this is an extended method with  visual features based on bpr. it firstly incorporates visual  information to build the user s interest.  lstm  this sequential baseline trained with bpr is  developed for next basket recommendation     . instead  of basic rnn  lstm is used in our work. both bpr and  lstm only model the latent feature.  p rnn  the p rnns is a feature rich model for sessionbased recommendation     . it has   structures and   training strategies. according to its experiments  we choose the  best variant  parallel  res  .    we design   combinations of input and   structures for the  hidden state. there are   variants implemented as mv rnn con.   mv rnn fus.  mv rnn  mdae  u and mv rnn  mdae u. the former   variants are built by the united structure  while  the last one has the separate structure. the prefix  mv rnn    can be omitted  and the   variants can be abbreviated as con.   fus.   mdae  u and  mdae  u respectively. the con. has  the highest dimension of hidden state  h   r d    while the rest  has the same dimension  h   r d  . additionally  we need to  initialize parameters   to the same range  e.g.  uniform distribution    .    .  . the initial hidden state h  of each sequence is  always zero. the learning rate is fixed at      .  for all methods.  besides  the mini batch size for training is set as   and users with  similar lengths are grouped into one batch. this length adjustment  can greatly speed up training     . complete codes for all models  are written by using theano and are available on github  . all  experimental results are also listed on this website.   .    . .     optimization on validation set  regularization parameter    the best parameters for regularization are listed in table  . they  are chosen by the evaluations of all the metrics on validation set  under the dimension d     .   . https   github.com cuiqiang     mv rnn     journal of latex class files  vol.     no.    august              table    the performance difference of our mv rnn on validation set between using different baselines  gru  lstm .  based on gru  dataset    based on lstm               method    auc    recall    map    ndcg    taobao    gru  con.  fus.   mdae  u   mdae  u     .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .       amazon    gru  con.  fus.   mdae  u   mdae  u     .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .       pop    bpr    vbpr    lstm    p rnn     .       .                method    map    ndcg    lstm  con.  fus.   mdae  u   mdae  u     .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .       lstm  con.  fus.   mdae  u   mdae  u     .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .       con.    fus.     mdae  u     .      .      .      .      .      .      .      .     auc    recall     mdae  u           .      .     .    .           .      .      .      .      .      .                            .         dimension     .                        .         dimension                            dimension                dimension     .           .      .    .      .           .             .    .      .      .      .      .                .       .      .         dimension          .    .                             .      .            a  taobao     .       .            .                               .    .         dimension           .                     dimension                            dimension     .           b  amazon    fig.  . recall     map     ndcg    and auc performances on validation set with varied dimensions of latent feature d                   .     .      .    .     in this optimization process     is firstly selected based  on basic methods  bpr  gru and lstm   then  ev   ra are  chosen by grid search. the ranges of these three parameters are       ev     .      .       .        .   and ra     .      .      .       .      . with  the reduction of data size from  taobao to amazon  the best       ev   ra almost all get bigger.     .      .      .          . .  baseline selection  although several studies explore  the difference between gru   .                           and  lstm             few people  do comparisons  for sequential  dimension  dimension  recommendation. this part aims for completeness. shown in table     the result is the performance by using the best parameters  obtained in section  . . . please note that all values of recall   map and ndcg in tables               and figure   are  represented in percentage.  obviously  the performance of mv rnn based on lstm is  better than that based on gru in most cases  except the con.  and fus. based on lstm on taobao. although lstm has more  parameters  it also has the better model capacity. as a long as the  model size is not significantly bigger  we should always consider  the model with the best architecture. therefore  in all the following  experiments  we consider lstm as the baseline instead of gru  and our mv rnn is based on lstm.           . .     dimension analysis        the   .  dimension analysis is investigated in figure  . we illustrate  the performances of top    and auc on the validation set. the  dimensions are set as d                   .     with the increasing of dimension  performances of top     metrics have similar trends with each other on both datasets. bpr   .       and vbpr  tend  to get worse.  they             have similar  trends as  well as     absolute values. itdimension  is difficult to tell the difference between dimension  vbpr  and bpr on recall  map  and ndcg  especially on taobao. the  p rnn model is not sensitive to dimension. the lstm and mvrnn models obtain better performance with the increasing of  dimension on taobao  while they almost do not change with the  dimension on amazon. on the other hand  aucs of all models  are much stable with different dimensions. vbpr has obviously  better performance than bpr on both datasets. the   variants of  mv rnn are nearly coincident with each other. the auc is not  sensitive to the dimension.  generally  it is obvious that lstm is a very strong baseline.  apparently  our mv rnn model is the best. the optimal dimension is chosen as d      and it is applied to other experiments.           journal of latex class files  vol.     no.    august              table    evaluation of different methods on the test set with the dimension of latent vector d     . we generate top    and    items for each user.  because of the structure of concatenation  the hidden state dimension of con. is much larger than the others.  taobao  p    method    amazon                          auc    recall    map    ndcg    recall    map    ndcg               p               auc    recall    map    ndcg    recall    map    ndcg    random  pop  bpr  vbpr  lstm  p rnn          .      .      .      .      .      .        .      .      .      .      .      .        .      .      .      .      .      .        .      .      .      .      .      .        .      .      .      .      .      .        .      .      .      .      .      .        .      .      .      .      .      .             .      .      .      .      .      .        .      .      .      .      .      .        .      .      .      .      .      .        .      .      .      .      .      .        .      .      .      .      .      .        .      .      .      .      .      .        .      .      .      .      .      .       con.  fus.          .      .        .      .        .      .        .      .        .      .        .      .        .      .             .      .        .      .        .      .        .      .        .      .        .      .        .      .        mdae  u     .    .    .    .      .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .    .    .    .      .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        mdae  u     .    .    .    .      .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .    .    .    .      .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .       table    results of the controlled study in section  . . .    dataset     .                method    auc    recall    map    ndcg    taobao     core     lstm  con.     .      .        .      .        .      .        .      .       taobao     core     lstm  con.     .      .        .      .        .      .        .      .       taobao     core     lstm  con.     .      .         .       .         .       .        .      .       analysis of experimental results    table   illustrates all performances on two datasets with four  evaluation metrics. recall  map and ndcg focus on local performance  while auc reflects global performance.   . .     performance comparison    from a global perspective  additional multi modal information of  items  e.g  image and text description  is indeed beneficial. vbpr  beats bpr. mv rnn outperforms lstm model. our mv rnn  can effectively model the additional information. for example  the  con. has almost more than     and more than     improvements  over lstm on taobao and amazon respectively with respect to  recall  map and ndcg. its improvements of auc over lstm  are both around     on two datasets. as for the rest   variants  which have hidden states of the same length   mdae  u performs  best. in a perspective of statics and dynamics  although both  trained by the bpr framework to maximize the difference of user s  preferences towards positive and negative items  lstm beats bpr  by a large margin. the recurrent structure of lstm can capture  sequential information which is helpful for the recommendation.   mdae and denoising. in this part  we analyze the four variants of mv rnn and focus on the  mdae. the con. almost  always beats the fus. but not too much. the highest hidden  state dimension of con. improves its capacity. this phenomenon  also shows that feature addition has no great damage to multimodal modeling. then  we embody the advantage of  mdae and    introduce a training setting called denoising . it can help to learn  more robust features and acquire the best performance.  the denoising ae is first proposed for image classification on  the mnist database. it can make features more robust and avoid  learning the identity function by using corrupted input. identity  function means just mapping the original input to its copy  which  happens in the encoding process in ae  e.g.  f   ef  . it is easy  to obtain a denoising ae just by a stochastic corruption operation  on input. the original corruption mechanism randomly sets some  of an input feature to zero with probability     p    . while in  our experiment  we make feature itself corrupted.  this denoising is conducted for  mdae. in this setting  we  make some multi modal data corrupted in the encoding process  and still reconstruct both modalities in the decoding step. training   mdae still requires all the data in table   a . the corruption levels are set as p     .    .    .    .   and p     .    .    .    .    for taobao and amazon respectively. if p    .   the input data in  the encoding process is complete. the results are still obtained on  the original test set where all items have all features. results are  shown in eight rows at the bottom of the table  .  obviously  performance can become better than the original   p       by denoising   especially the recall  map and ndcg.  more importantly   mdae  u performs best. it is able to be better  than con.  although con. has the highest hidden state dimension.  when we randomly reset some features to zero in the encoding  process  the noise in the whole input data is reduced. however   by reconstructing both modalities in the decoding step  the fusion  feature of our  mdae can still keep the useful information in  both modalities. our  mdae can acquire more robust features.  the best corruption levels for  mdae  u  u are p    .   .   and p    .   .  on two datasets respectively.  the  mdae  u  u are a one unit model with the united  structure and a two unit model with the separate structure respectively. in table    the one unit model outperforms the two unit  model. a united inner structure can better leverage the advantage  of multi view features. the separate structure may be not able to  well model the connection between different views.  p rnn vs. mv rnn. the session based p rnn model also  incorporates additional features  but it is comparable to lstm.     journal of latex class files  vol.     no.    august               table    a setting called missing is introduced and measured on an artificial test set  where some items  multi modal features are missing  deleted . this  setting aims to study the ability of mv rnn to handle missing modalities.  missing   taobao    mv rnn               p    missing   amazon               auc    recall    map    ndcg    recall    map    ndcg               p               auc    recall    map    ndcg    recall    map    ndcg    con.  fus.          .      .        .      .        .      .        .      .        .      .        .      .        .      .             .      .        .      .        .      .        .      .        .      .        .      .        .      .        mdae  u     .    .    .    .      .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .    .    .    .      .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        mdae  u     .    .    .    .      .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .    .    .    .      .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .        .      .      .      .       if we carefully examine the results of p rnn in its original  paper       we find that most results of p rnn are also close  to the basic model   id only  in their paper . the reason is  varied as p rnn is substantially different from our mv rnn.  the first one is feature normalization. multi view features must  be normalized to the same range  but only visual features are  normalized in their work. next  different from our strategy in  eq.     p rnn uses output weight matrix to compute the user s  scores on items. this matrix improves the capacity of a model  but increases the learning difficulty  especially for the modeling  of visual and textual features. we experimented with using this  matrix on our con.  but its performance is very close to that of  lstm. then  different subnets within p rnn are trained one by  one  which can not well construct the connection among multiview features.   . .  a controlled study  in table    the metrics  recall  map and ndcg  seem to be low   especially on taobao. therefore  we conduct a controlled study to  explore the factors that influence the metrics.  reducing the number of items  search space  may be helpful.  we extract three sub datasets from taobao by increasing the  filtering strategy as              core. the statistics are shown in  table   b . in this way  the search space is greatly reduced. then   we perform experiments by using lstm and con.. accordingly   we need to re select the best parameters and the results are shown  in table  . with the increasing of k   the three metrics get bigger.  metrics of taobao     core  are obviously bigger than that of the  other datasts. this may be because the sparsity of taobao    core  is clearly small. at the same time  our method con. is always  better than lstm  which shows the effectiveness of our mv rnn.  therefore  although the absolute values on taobao are small  they  are related to the dataset itself  e.g.  sparsity .  in summary  our mv rnn model is better than the others.  mv rnn can well model multi view features and achieves the  best and stable performance in different situations. the denoising  of  mdae is a good setting to improve performance. besides   special strategies used in p rnn are not necessary for handling  multi view features. feature concatenation is natural but very  useful. a united structure with simultaneous training strategy is  easy to use and is better than the separate subnets built for each  view in p rnn. these conclusions of joint learning are also  confirmed by the previous works  like a multi view model for  cross domain user modeling     .     .     analysis of missing modalities in test set    multi modal methods usually hold an assumption that all modalities are available. however  in practice  certain modality is often  missing  like an item without the visual feature. in such case  our   mdae is theoretically better than the concatenation and fusion.  to verify this  we introduce a setting of test set called missing .  first  we artificially modify the test set. we set one third of items  without visual features  one third without textual features  and the  last one third with all the multi modal features. then  the training  procedure also applies the denoising   and the only difference  between sections  .  and  .  is that missing here is evaluated on  our artificial test set. the result is shown in table  .  experimental results indicate that our  mdae is very promising for tackling missing modalities problem. both  mdae  u  u  perform very well and  mdae  u is more successful. for example   mdae  u under p    .  increases by about    percent  with respect to con. on recall  map and ndcg on taobao.  this improvement acquired by  mdae  u under p    .  on  amazon is about   percent. besides   mdae  u  u also have  some increases on auc over con. and fus.. our  mdae is greatly  better than others in this missing setting and it can effectively  handle the items with missing modalities.   .     analysis of cold start    we investigate the performance of mv rnn on cold start items  in the test set. these items usually account for a large proportion  and cold start is an intractable problem in practical recommender  systems. previous works like vbpr     usually only consider cold  start items and neglect the rest. while in our work  we expand  this general setting because the rest items may produce a large  volume of feedbacks. two new experimental settings are designed   recall    and auc are applied to test the performance  as shown  in table  . furthermore  we compute the improvement to analyze  the effect of multi modal information on cold start items. the  improvements are shown in table   and figure  .   . .     subsets of test set    according to each item s support number in the test set  we divide  items into three subsets  cold start        active       and  whole  test set . numbers of items of each subset are listed in  table   a . the cold start items account for   .   and   .   on  taobao and amazon respectively.     journal of latex class files  vol.     no.    august               table    cold start performance on two datasets under the evaluation of recall    and auc with dimension of latent feature d     .   a  numbers of items in each subset and each bin of the test set. numbers of feedbacks are also counted.  subsets of test set    dataset    bins of test set    cold start    active                                                                                                                 taobao    items  feedbacks                                                                                                                                                                                                                                   amazon    items  feedbacks                                                                                                                                                                                     b  evaluation of cold start performance on taobao. the interval is the accumulation of several bins.  eva.    method    subsets of test set        p    intervals of test set        cold start    active    whole                                                                                                   all    recall         lstm  con.  fus.   mdae  u   mdae  u     .    .      .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .       auc    lstm  con.  fus.   mdae  u   mdae  u     .    .      .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        c  evaluation of cold start performance on amazon. the interval is the accumulation of several bins.  eva.    method    recall         auc    p    subsets of test set        intervals of test set        cold start    active    whole                                                                                                   all    lstm  con.  fus.   mdae  u   mdae  u     .    .      .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .       lstm  con.  fus.   mdae  u   mdae  u     .    .      .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .        .      .      .      .      .       table    based on the cold start performance in table    we compute improvements     on each subset. the best corruption levels p for our   mdae  u  u is the same as in table    and we omit the p in this table. the cold refers to the cold start.    method    taobao   recall     cold    con. vs. lstm  fus. vs. lstm   mdae  u vs. lstm   mdae  u vs. lstm       .       .      .       .      active      .      .      .      .      whole      .      .      .      .      taobao   auc  cold     .      .     .     .      active      .      .      .      .      amazon   recall       amazon   auc    whole    cold    active    whole    cold    active    whole      .      .      .      .             .      .      .      .        .      .      .      .        .      .      .      .       .     .     .     .        .      .      .      .                                   from the perspective of basic performance  as shown in tables    b  and   c   the best values are scattered in four variants. it is  difficult to draw a consistent conclusion.    improvements of mv rnn are over     on active. mv rnn  not only has a significant improvement on cold start items but also  has a sufficient improvement on active items.    as for the improvement shown in table    most improvements  on cold start are higher than those on whole  and are much  higher than those on active. comparatively  the basic model like  lstm has difficulty in predicting cold start items  while it is  easier to obtain good performance on active items. thus on the  contrast  it is easy to design a model to substantially enhance the  performance on cold start  while it is more difficult to acquire  obvious improvement on active. under such situation  our mvrnn still performs very well on active. for example  most    in table    there are some surprising improvements about  recall    on amazon. we specify the improvement of mv rnn  over lstm as            because the performance of lstm  on cold start is zero. this poor performance of lstm can be  explained from the perspective of probability. when we train a  sequence  we practically apply lstm to model a joint probability  p x           xt    where xi represents an item. when we predict  n items in corresponding test sequence  we actually predict a  conditional probability p xt            xt n  x           xt  . because the     journal of latex class files  vol.     no.    august       con. vs. gru          fus. vs. gru     mdae  u vs. gru            mdae  u vs. gru                                                                                                                                                                       all                                            interval                                                all                                     all    interval     a  taobao                                                                                                                                                                                       all                        interval                                              interval           b  amazon  fig.  . based on the cold start performance in table    we calculate improvements     on each interval. the best corruption levels p for our   mdae  u  u is the same as in table    and we omit the p in this figure.      .   cold start items and the corresponding   .   feedbacks on  amazon result in limited interactions among users and items  both  probabilities are very small. therefore  it is very hard to make  accurate recommendation under recall    on amazon. after  we incorporate the additional content information    variants of  mv rnn have performances of  .       .       .     and   .     respectively. the absolute values are small  but we obtain  very large but reasonable improvements. this strange and extreme  phenomenon exactly reflects the great power of additional content  information and the powerful modeling capability of mv rnn.                . .          intervals          of     test      set                                   all    according to interval  the support number of each item in the test set  we  divide items into ten bins  e.g.                         . for example   bin        has the items that appear for   or   times. numbers of  items in each bin are listed in table   a . in order to alleviate the  fluctuation of performance on each bin  performance is recorded  on cumulative bins  e.g.          which are called intervals.  when the bin number increases  performance becomes better   as seen from tables   b  and   c . that is because it is easier to  predict frequent items. on taobao  there is a strange phenomenon.  performance decreases first on a few bins in the front and then  increases. as the decrement is not significant  we can still think  the performance is growing. then we mainly focus on the analysis  of improvements. for better representation  improvements are  illustrated by curves in figure  .  these growth curves do not always have the same change on  two datasets. on taobao  curves tend to be flat. on amazon  as  the bin has a larger proportion of cold start items  seeing from  the right side of a figure to its left side   the improvement almost  becomes larger. this indicates that multi modal information is indeed beneficial to relieve cold start. in other words  when the cold    start problem gets     worse on small bins with a bigger proportion of  cold start items  multi modal information can significantly relieve  this problem. because cold start items have few interactions with  users  directly related multi modal information would effectively  represent the item s characteristics and the user s interest.  auc is much    more stable than recall   . we consider the  difference of user s preferences towards positive and negative  items in auc  and the bpr training process exactly maximizes  this difference. for recall    curves  there is a large difference  between taobao and amazon. these curves on taobao are separate from each other  but they almost come together in the last     interval all. perhaps  because     of   the small  proportion  of   feedbacks                                       on the interval            .     there would be some fluctuations  interval  in the performance of each model. these curves on amazon have  an obvious increasing law when the bin number gets smaller.  for auc curves  the situation is much better. on taobao  most  improvements are stable. for example  improvements of mv rnn  are around    . on amazon  the smaller the bin number  the  larger the improvement.  these curves  especially those on amazon  can greatly support  the following conclusion. multi modal information can significantly relieve the item cold start problem. besides  the worse the  cold start  the more powerful the multi modal information.   . .     visualization of learned features    in this part  we make the visualization of learned features  by similarity retrieval to investigate whether they are correlated or complementary. there are five different input features  ix   if   ig   im    ix   im   represented in eqs.           . given a query  item  we select top   most similar items based on the euclidean  distance for each kind of feature. the features are acquired by   mdae  u under p    .  and the results are shown in figure  .                journal of latex class files  vol.     no.    august         query    feature    top   similar items    ix    if          image and text description could indeed significantly alleviate the  item cold start problem.  in the future  we would investigate the item detection and  segmentation in images. the items in images often have a large  proportion of unrelated background  especially in the taobao  dataset. we would like to obtain the more accurate item representation. these can motivate the model to improve performance.    ig    r eferences  im      ix   im                       fig.  . visualization of similarity retrieval based on the euclidean distance. features are acquired by  mdae  u under p    .  on taobao.    obviously  the similar items under different kinds of features  vary greatly  and the multi view  latent  visual  textual  features  are complementary to each other.     for the latent feature ix    the similar items are greatly different from each other as ix are  just learned by the feedback. if the latent features of two items  are similar  probably because they were both purchased by many  people.     whether it is item itself or the background in the image   the top   items based on the visual feature if are very similar  in appearance. however  the second and the forth items in this  line obviously belong to other categories. the visual feature is  powerful but can not reflect the intrinsic characteristics of items   like material of clothes.     on the other hand  the textual feature  ig is acquired by the item description. it can truly reflect what the  product is and can ignore the effect of the background in an image   but it is not intuitive to show the color  shape  etc.     the fusion  feature im is a combination of if and ig . it mainly integrates  the external and intrinsic characteristics of the item  such as the  style and material of clothes. however  such characteristics can  not generate precise recommendation because there is no one toone match between each characteristic and each item.     the  final item feature  ix   im   fuses ix   if   ig . it can fully reflect the  characteristics of an item and help to understand the user s overall  interest. in summary  multi view features ix   if   ig used in our  work are complementary.         conclusion    in this work  we have proposed a novel multi view recurrent model   mv rnn  for sequential recommendation and alleviating the  item cold start problem. first  we construct comprehensive item  representation with latent  visual and textual features by three  different combinations. a  mdae model is introduced to build  the fusion feature based on visual and textual features. then the  user s interest is captured by the recurrent structure. we devise two  types of inner structures to handle multi view features. next  we  design a united objective function to combine the preference loss  of bpr and the reconstruction loss of our  mdae. experiments  validate the state of the art performance of mv rnn. the fusion  feature of  mdae helps to learn more robust features and tackle  the missing modalities problem. experiments confirm that a united  inner structure can better leverage the advantage of multi view  features than a separate one. the multi modal information like the    