introduction    recurrent neural networks  rnns  are a class of neural networks suitable for  time series processing. in particular  gated rnns         such as long short term  memory  lstm  and gated recurrent units  gru   are fully trained recurrent  models that implement adaptive gates able to address signals characterized by  multiple time scales dynamics. recently  within the reservoir computing  rc       framework  the deep echo state network  deepesn  model has been proposed as extremely efficient way to design and training of deep neural networks  for temporal data  with the intrinsic ability to represent hierarchical and distributed temporal features          .  in this paper  we investigate different approaches to rnn modeling  i.e.   untrained stacked layers and fully trained gated architectures   through an experimental comparison between rc and fully trained rnns on challenging realworld prediction tasks characterized by multivariate time series. in particular   we perform a comparison between deepesn  lstm and gru models on   polyphonic music tasks    . since these datasets are characterized by sequences with  high dimensionality and complex temporal sequences  these challenging tasks  are particularly suitable for rnns evaluation    . moreover  we consider esn  and simple rnn  simple recurrent network   srn  as baseline approaches for  deepesn and gated rnns  respectively. the models are evaluated in terms of  predictive accuracy and computation efficiency.  in a context in which the model design is difficult  especially for fully trained  rnns  this paper would provide a first glimpse in the experimental comparison  between different state of the art recurrent models on multivariate time series  prediction tasks which still lacks in literature.          deep echo state networks    deepesns     extend echo state network  esn      models to the deep learning  paradigm. fig.   shows an example of a deepesn architecture composed by a  hierarchy of nl reservoirs  coupled by a readout output layer.  in the following equations  u t    rnu and x l   t    rnr represent the external input and state of the l th reservoir layer at step t  respectively. omitting  bias terms for the ease of notation  and using leaking rate reservoir units  the  state transition of the first recurrent layer is described as follows   x     t         a     x     t        a    f  win u t    w                x     t                  while for each layer l     the state computation is performed as follows    l     x l   t         a l   x l   t        a l  f  w l  x l     t    w  x l   t      .          l     in eq.   and    win   rnr  nu represents the matrix of input weights  w     rnr  nr is the matrix of the recurrent weights of layer l  w l    rnr  nr is  the matrix that collects the inter layer weights from layer l     to layer l  a l  is  the leaky parameter at layer l and f is the activation function of recurrent units  implemented by a hyperbolic tangent  f   tanh . finally  the  global  state  of the deepesn is given by the concatenation of all the states encoded in the  recurrent layers of the architecture x t     x     t   . . .   x nl    t     rnl nr .    fig.    hierarchical architecture of deepesn.  l  the weights in matrices win and  w l   n  l   are randomly initialized from a  uniform distribution and re scaled such that kwin k      and kw l  k      respectively  where   is an input scaling parameter. recurrent layers are initialized  in order to satisfy the necessary condition for the echo state property of deep      l     l  esns     . accordingly  values in  w   n  initialized from unil   are randomly         l   form distribution and re scaled such that max  l nl        a l   i   a l  w         where   is the spectral radius of its matrix argument  i.e. the maximum among  its eigenvalues in modulus. the standard esn case is obtained considering deepesn with   single layer  i.e. when nl    .  the output of the network at time step t is computed by the readout as a linear combination of the activation of reservoir units  as follows  y t    wout x t       where wout   rny  nl nr is the matrix of output weights. this combination  allows to differently weight the contributions of the multiple dynamics developed  in the network s state. the training of the network is performed only on the  readout layer by means of direct numerical methods. finally  as pre training  technique we use the intrinsic plasticity  ip  adaptation for deep recurrent architectures  particularly effective for deepesn and esn architectures       .         experimental comparison    in this section we present the results of the experimental comparison performed  between randomized and fully trained rnns. the approaches are assessed on  polyphonic music tasks defined in    . in particular  we consider the following   datasets    piano midi.de  musedata   jsbchorales and nottingham. a  polyphonic music task is defined as a next step prediction on               and      dimensional sequences for piano midi.de  musedata  jsbchorales and nottingham datasets  respectively. since these datasets consist in high dimensional  time series characterized by heterogeneous sequences  sparse vector representations and complex temporal dependencies involved at different time scales  they  are considered challenging real world benchmarks for rnns    .  models  performance is measured by using the expected frame level accuracy   acc   commonly adopted as prediction accuracy in polyphonic music tasks       and computed as follows   acc   pt    t      pt    t      t p  t       pt    t      t p  t   f p  t       pt    t      f n  t                 where t is the total number of time steps  while t p  t   f p  t  and f n  t  respectively denote the numbers of true positive  false positive and false negative  notes predicted at time step t.  concerning deepesn and esn approaches  we considered reservoirs initialized with    of connectivity. moreover  we performed a model selection on  the major hyper parameters considering spectral radius   and leaky integrator a  values in   .    .    .    .    .    .    and input scaling   values in   .    .    .  .  training of the readout was performed through ridge regression        with regularization coefficient  r in                             . moreover  based on the results of the design analysis in     on polyphonic music tasks  we set up deepesn  with nl      layers composed by nr       units  and esn with nr         recurrent units. we used an ip adaptation configured as in        with a standard  deviation of  ip    . .  for what regards fully trained rnns  we used the adam learning algorithm       with a maximum of      epochs. in order to regularize the learning process   we applied dropout methods  a clipping gradient with a value of   and an early  stopping with a patience value of   . then  we performed a model selection    piano midi.de  www.piano midi.de    musedata  www.musedata.org   jsbchorales   chorales by j. s. bach   nottingham  ifdo.ca   seymour nottingham nottingham.html  .     model  deepesn  esn  srn  lstm  gru  deepesn  esn  srn  lstm  gru  deepesn  esn  srn  lstm  gru  deepesn  esn  srn  lstm  gru    total recurrent units free parameters test acc  piano midi.de                  .     .                       .     .                      .     .                      .     .                      .     .       musedata                  .     .                       .     .                      .     .                      .     .                      .     .       jsbchorales                  .     .                       .     .                      .     .                      .     .                      .     .       nottingham                  .     .                       .     .                      .     .                      .     .                      .     .         computation time                                                                                                                     table    free parameters and test acc achieved by deepesn  srn  lstm and  gru. computation time represents the seconds to complete training and test.  considering learning rate values in                             and dropout values  in   .    .    .    .    .  .  since randomized and fully trained rnns implement different learning approaches  it is difficult to set up a fair experimental comparison between them.  however  we faced these difficulties by considering a comparable number of  free parameters for all the models. the number of recurrent units and freeparameters considered in the models is shown in the second and third columns  of tab.  . each model is individually selected on the validation sets through a  grid search on hyper parameters ranges. we independently generated   guesses  for each network hyper parametrization  for random initialization   and averaged  the results over such guesses.  in accordance with the different characteristics of the considered training approaches  direct methods for rc and iterative methods for fully trained models   we preferred the most efficient method in all the considered cases. accordingly   we used a matlab implementation for deepesn and esn models  and a keras  implementation for fully trained rnns. we measured the time in seconds spent  by models in training and test procedures  performing experiments on a cpu      intel xeon e    .  ghz     cores  in the case of rc approaches  and on a gpu   tesla p    pcie   gb  in the case of fully trained rnns  with the same aim  to give the best resource to each of them.  tab.   shows the number of recurrent units  the number of free parameters   the predictive accuracy and the computation time  in seconds  achieved by deepesn  esn  srn  lstm and gru models. for what regards the comparison  between rc approaches in terms of predictive performance  results indicate that  deepesn outperformed esn with an accuracy improvement of  .      .       .    and  .    on piano midi.de  musedata  jsbchorales and nottingha  tasks  respectively. concerning the comparison between fully trained rnns   gru obtained a similar accuracy to srn and lstm models on jsbchorales  task and it outperformed them on piano midi.de  musedata and nottingham  tasks.  the efficiency assessments show that deepesn requires about less that one  order of magnitude of computation time with respect to fully trained rnns   boosting the already striking efficiency of standard esn models. moreover  while  esn benefits in terms of efficiency only by exploiting the sparsity of reservoirs   with    of connectivity   in the case of deepesn the benefit is intrinsically due  to the architectural constraints involved by layering      and are obtained also  with fully connected layers .  overall  the deepesn model outperformed all the other approaches on   out  of   tasks  resulting extremely more efficient with respect to fully trained rnns.         conclusions    in this paper  we performed an experimental comparison between radomized  and fully trained rnns on challenging real world tasks characterized by multivariate time series. this kind of comparisons in complex temporal tasks  that  is practically absent in literature especially for what regards efficiency aspects   offered the opportunity to assess efficient alternative models  esn and deepesn in particular  to typical rnn approaches  lstm and gru . moreover  we  assessed also the effectiveness of layering in deep recurrent architectures with a  large number of layers  i.e.     .  concerning fully trained rnns  gru outperformed the other gated rnns  on   out of   tasks and it was more efficient than lstm in most cases. the  effectiveness of gru approaches found in our experiments is in line with the  literature that deals with the design of adaptive gates in recurrent architectures.  for what regards randomized rnns  the results show that deepesn is able  to outperform esn in terms of prediction accuracy and efficiency on all tasks.  interestingly  this highlights that the layering aspect allows us to improve the  effectiveness of rc approaches on multiple time scales processing. overall  the  deepesn model outperformed other approaches in terms of prediction accuracy  on   out of   tasks. finally  deepesn required much less time in computation  time with respect to the others models resulting in an extremely efficient model  able to compete with the state of the art on challenging time series tasks.     more in general  it is interesting to highlight the gain in the prediction accuracy showed by the multiple time scales processing capability obtained by  layering in deep rc models and by using adaptive gates in fully trained rnns  in comparison to the respective baselines  esn and srn  respectively . also   it is particularly interesting to note the comparison between models with the  capability to learn multiple time scales dynamics  lstm and gru  and models showing an intrinsic capability to develop such kind of hierarchical temporal  representations  deepesn   which was completely lacking in literature.  in addition to provide insights on such general issues  this paper would contribute to show a practical way to efficiently approach the design of learning  models in the scenario of deep rnn  extending the set of tools available to the  users for complex time series tasks. indeed  the first empirical results provided  in this paper seem to indicate that some classes of models are sometimes uncritically adopted  i.e. despite their cost  guided by the natural popularity due to  their software availability  gru  lstm . the same diffusion of software tools  deserve more effort on the side of the other models  deepesn class   although  the first instances are already available .    