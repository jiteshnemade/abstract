introduction    exciting progress  haber and ruthotto        lu et al.        ruthotto et al.         chen et al.        has been made to unveil the common nature behind various transformations used in machine learning models  such as neural networks haykin        and  normalizing flows  rezende and mohamed         realized by a sequence of transformations between hidden states  these iterative updates can be viewed as integration of  either discrete or continuous differential equations. such startling correspondence not  only deepens our understanding of the inner workings of neural network based machine          learning algorithms  but also offer advanced numerical integration methods obtained  over the past century to the design of better learning architectures.  haber et. al. are the first to map the residual neural network s  resnet s  composition rules between hidden variables to the euler discretization of continuous differential  equations  and the stability of resnet training to the stability of the equivalent numerical integration methods. leveraging such mapping  they significantly improve resnet s  stability by choosing the appropriate weight matrices whose spectrum properties guarantee its stable propagation. however  their analysis is limited to one kind of numerical  integration method applied to resnet. more recently  chen et. al. replace the conventional neural network with its continuous limit  ordinary differential equations  odes .  these neural odes enjoy many advantages over conventional neural networks  backpropagation is replaced by integration of conjugate variables representing the gradients  of the hidden variables  stability is improved with the use of adaptive numerical integration methods for odes  it can learn efficiently from time sequential data that are  generated at unevenly separated physical times  and other improvements in the parameter and memory efficiencies. yet such fully continuous extension of neural network  also faces its own challenges  it is inconvenient to use mini batches with neural odes   specific error in the backward integration of conjugate variable for state trajectory reconstruction can be amplified  and neural ode s large scale implementation cannot  directly benefit from the emerging hardware developed for tensor multiplication.  since temporal discretization is nonetheless unavoidable in the machine level integration of odes  why not keep the neural network paradigm but include a larger family  of ode integration methods in addition to euler discretization  a more generalized          correspondence will open the door to complex neural network architectures inspired by  physical dynamics represented by odes. in particular  we are interested in recurrent  neural networks  rnns  for their generality  conventional deep neural networks can  be regarded as the trivial type of rnn with trivial recurrence  and their capability to  learn complex dynamics that require temporal memories.  as indispensable tools for machine translation  robotic control  speech recognition  and various time sequential tasks  rnns are nonetheless limited in their application  due to their susceptibility to training instability that can be amplified by the recurrent connectivity. various architectural redesigns are introduced to mitigate this problem  hochreiter and schmidhuber        cho et al.        wermter et al.        jaeger  et al.        bengio et al.        cho et al.        koutnik et al.        mhaskar and  poggio        arjovsky et al.      b  jing et al.       . these improved stability guarantees also come with an expense of additional architectural complexity  jozefowicz  et al.        karpathy et al.        alpay et al.        greff et al.       . they point  to an underlying trade off between the stability  temporal dynamics and architectural  complexity of rnn that is yet to be found. establishing specific connections between  more general ode methods with composition rules of rnn architectures can be the  first step towards understanding this stability complexity trade off.  in fact  runge kutta methods  runge        kutta        are generalizations to  euler s method  which numerically solve odes with higher orders of functional nonlinearity through higher stages of recursion in their discrete integration rules. since a  higher order time derivative can be transformed to coupled first order odes  an n stage  runge kutta with t coupled variables thus represents a tth order ode with nth order          temporal non linearity. different orders of runge kutta methods not only facilitate different orders of convergence guarantees to the numerical integration  they also provide a  simple but accurate understanding of the underlying dynamics embodied by the odes.  in this work  we establish critical connections between rnn and ode  the temporal  dynamics of rnn architectures can be represented by a specific numerical integration  method for a set of odes of a given order. our result elucidates a fundamental tradeoff between training stability  temporal dynamics  and architectural complexity of rnn   network s stability and complexity of temporal dynamics can be increased by increasing the length of temporal memory  koutnik et al.        and the degree of temporal  non linearity  which on the other hand demands more non local composition rules and  thus higher complexity of the network architecture as predicted by the corresponding  ode integration method. this insight has practical implications when applying rnns  to real world problems. on the one hand  additional information about the training  data  such as its temporal correlation length obtained by lower level prepossessing  can  be valuable for the choice of rnn architectures. on the other hand  one can design  unconventional rnn architectures inspired by ode counterparts for increased ability to represent complex temporal dynamics. for example  as opposed to autonomous  odes which do not explicitly depend on the physical time of the incoming temporal  data  rnns based on non autonomous dynamical odes have weight matrices specifically dependent on the input at each iteration after the training  or more concisely are  dynamical. this captures and generalizes now common extensions to traditional rnn  structures  such as in the neural turing machine  graves et al.        which adds a  write and read gated function to the data independent network to facilitate the learn           ing of arbitrary procedures. illustrating the potential of this direction  we provide here  one such dynamical weight rnn construction  inspired by a quantum algorithm for  realizing universal quantum computation through the preparation of the ground state  of a  clock hamiltonian   aharonov et al.       . a clock hamiltonian represents the  dynamical map between input and output of each temporal update  and is therefore  specifically input dependent.  using this specific correspondence between general ode integration methods and  rnn composition rules  we also identify a new property about rnn architecture  the  order of non linearity in its represented temporal dynamics. traditionally  the stability  of rnn is considered only with respect to its memory scale represented by the order of  time derivatives in discrete odes. however  we realized that the range of connectivity  between hidden layers in rnn can also affect the order of temporal non linearity reflected in different stage of runge kutta recursion. this offers us an additional insight  to the inner workings of rnns that is helpful for designing more suitable architectures  given partial knowledge of the underlying physical dynamics of the data.  the structure of the paper are summarized as follows. in sec.   we identify rnn s  temporal memory scale t and the degree of non linearity n to its underlying architecture through an explicit correspondence to odes  by analyzing the discrete odes integration method that matches the rnn composition rules between hidden variables.  we show that t and n are respectively determined by the order of time derivative and  the order of non linearity of the odes that represent the rnn s dynamics. we also  provide sufficient condition for training stability for any n t odernn. existing rnn  architectures can thus be comprehended on the same ground according to their mem           lstm  gru  urnn  cw rnn  qunn    l odernn   l odernn   l odernn l l odernn l l odernn    table    categorization of rnn architectures according to their temporal memory scale  and their order of non linearity. lstm  hochreiter and schmidhuber         long term  short term memory rnn with l hidden layers. gru  cho et al.         gate model  recurrent neural network with l hidden layers. urnn  arjovsky et al.      b   unitary evolution recurrent neural network with l hidden layers. cw rnn  koutnik et al.          clockwork recurrent neural network with l hidden layers. qunn  quantum  universal computing recurrent neural network with l hidden layers. n t odernn   recurrent neural network whose temporal dynamics is represented by a discrete integration of tth order ode recursion relation using nth order recursion methods.  ory scale and the non linearity of their dynamics  table.   . in sec.   we provide an  example of constructing new rnn architectures by choosing the appropriate underlying ode dynamics first  quantum inspired universal computing recurrent neural network  qunn . qunn is unconventional in its specific time dependence in the weight  matrix construction. the number of training parameters in qunn grows linearly with  the temporal correlation length between input data which is otherwise independent of  the dimension of data itself. we discuss the implication of our results in sec.  .         stable recurrent neural network    the success of supervised machine learning techniques depends on the stability  the  representability and the computational overhead associated with the proposed training  architecture and training data. careful engineering of network s architectures are necessary to realize these desirable properties since a generic neural network without any  structure is susceptible to exploding or vanishing gradients  bengio et al.        pascanu et al.       .  the groundbreaking work by haber et. al.  haber and ruthotto        provides an          elegant solution to guaranteeing the stability of deep neural networks  understand the  neural network forward and backward propagation as a form of integration of discrete  odes. as an example  we look at a type of resnet proposed in  haber and ruthotto        . let lth layer of hidden variable be yl   rs p and bias be bl   rs p   to ensure  the stability of propagation. they introduce a conjugate variable zl      rs p as a       intermediate step such that the propagation of neural network is described by    zl      zl      hl   wl  yl   bl    yl     yl     wl zl      bl  .                        the dynamics of the above discrete ode is stable regardless of the form of weight  matrix wl  haber and ruthotto       .  if rnn can be trained to represent temporal structures of physical data  its stability  and complexity should also be understandable through the physical dynamics represented by odes. we generalized the method by haber and ruthotto        introduced  above to include higher order non linearity and higher order time derivative odes and  to apply it larger family of neural network that include existing architectures of rnns  as special cases. we define an ode recurrent neural network with nth order in nonlinearity and tth order in time derivative  n t odernn  according to its propagation  rule  the update of n t odernn can be mapped to a generalized n stage rungekutta  integration with t coupled variables. the specific choice of runge kutta method is  not essential to such generalization  and can be replaced by other integration method  that provides different architecture ansatz. such generalization help us to provide a  sufficient condition for the stability criteria for any n t odernn. we then categorize          several existing rnn architectures into different n t odernn according to their temporal memory scale and degree of non linearity. lastly  we define the n   odernn  with anti hermitian weight matrices as n arnn and prove the stability of   arnn  and   arnn.     .     nth order ode recurrent neural network    definition  . an ode recurrent neural network of nth order in non linearity tth order  in gradient  n t odernn   with integers n  t     and k    n   is described by the  update rule between input state value yl   rs at time step l  the hidden variables of the  k th layer as kk   rp with     k   n  and output state value yl     rs as    k        w  yl   b       kq    q kq      q  q         wq yl tq     bq   h    n  x        q k kk                k      yl      n   yl    n    q   wn   yl tn     bn     h    n  x        k k k           k      where     q   n  the time corresponding to each hidden layer obeys tk   bt nk c with  the number of inputs in time coupled by the n hidden layers being t  the point wise  activation function          rp   rp at each layer is a nonlinear map that preserves  the dimension of the input  the weight matrix at each layer is represented by wq    rq   q  where q  is the dimension of the input variable and q  is the dimension of the  output variable of that layer  the scalar constant h changes the rate of update of hidden  variables between layers  the vector bq is the bias for the qth hidden layer   k    q    q and   q k   rp p are matrices served to rescale and rotate the hidden variables.        to facilitate later discussions based on definition    we name  q k the rungekutta  matrix and   k   the runge kutta weight matrices following similar notation in the  numerical integration of odes. lastly  we define burrage butcher tensor q named  after its first inventors with each element defined by    qi j    i  ij    j  ji    i  j            which will be an important quantity in the stability analysis of the represented dynamics  to be discussed in theorem  .  example     odernn . a     odernn is a four layer rnn with two hidden layers  that obey the composition rules     k        w  yl   b                k       k           w  yl     b    h     k       yl        yl         w  yl     b    h       x               k k k           k      which is illustrated in fig.  . in comparison the connectivity for lstm is represented  in fig.    where the connection between input and output in fig.   is replaced by the  output gate ot   the first hidden layer k  is replaced by forget gate  and the second hidden  layer k  is replaced by input gate and memory cell.  definition   can be compared with the explicit or implicit runge kutta method for  solving    d   y  dt      f   y   t  where the vector  y is an unknown function of time that ode    solution provides. recall that a tth order ode can be mapped to the first order ode           yl         yl      k   k     r  figure    diagrammatic representation of     odernn  where the sign inside a circle represents the nonlinear activation function  and the   represents the time delayed  feed forward. each arrow represent the multiplication  by a re scaling factor     for the  r  arrow from yl to yl        for the arrow  r from sign to k       for the arrow from k  to  k    and  k for the arrow from kk to sign.    figure    diagrammatic  representation of peehole lstm taken from wikipedia  r          where the sign inside a circle represents the nonlinear activation function   and the   represents the time delayed feed forward.    with  y of length t  each element of which is proportional to different order of discrete  time derivative of the original variable. the n stage explicit or implicit rugge kutta  method can be generalized to the following form for the solution to the ode at the  discrete time tk with time step tk   tk       given the solution  yk   at the previous  time step through the following iteration      yk    yk          n  x    ei ki           i       kq   f  dq  yk        n  x  j             aqj kj   tk     cq                where aq j   ei   cq   dq are square matrices and determine the corresponding integration  method. for example  if we set aq j     for all q   j  it gives us an explicit rungekutta method  otherwise it corresponds to an implicit runge kutta method. the difference between the two methods is in the additional requirement of solving the linear  dependence of   kq   in each iteration of implicit method  which lower the requirements  on f    for the numerical stability of the integration. if we treat the kth hidden layer  from the odernn as the kth stage of integration method above  and choose the matrix  dk to pick out the kth derivative which in discrete time step corresponds to the variable  separated by k   the order of time derivative and the order of functional non linearity of  n t odernn becomes self evident.  with this explicit connection  we can directly apply the stability analysis of rungekutta method to the odernn with theorem    which utilizes the notion of bnstability specified below in definition  . bn stability was first proposed by  dahlquist         to investigate stability of numerical schemes applied to nonlinear systems satisfying a monotonicity condition  which is a generalization of the  a  stability for linear  systems and widely used in analyzing the stability of high order runge kutta methods.  definition  .the integration method for solving the nonlinear discretized ode system  of equations  y    f   y   t  is bn stable if it satisfies the following requirements. it is  monotonic  the inner product between the variable vector and the function vector is  non negative hf   x  t  f   y   t    x   y i     for t       x   y   rs   and a small perturbation  at the initial state y        y       does not amplify as step size increases  for any k       yk     y   k           yk   y   k   .                   based on definition    we are ready to provide a stability guarantee for the odernns  in the following theorem.  theorem  . an n t odernn given by eq.     is bn stable if it satisfies the following  conditions   i. the burrage butcher tensor q is positive semi definite.  ii. for any k    n   the matrix  k is positive semi definite.  proof   since the composition rule of n t odernn can be mapped to that of an n  stage general implicit runge kutta method  the bn stability proof for theorem  .   from spijker        directly applies. the monotonicity requirement  dahlquist         in definition   is not sensitive to the gradient of the function and can also be replaced  by hf   x  t    f   y   t   where we use  x and  y to represent the input and output of each  hidden layer of rnn  which obeys h x    y i     for t       x   y   rs   which is naturally  satisfied by the rectified linear function or tanh.     .     categorization of existing rnns    we apply this odernn framework to analyze some of the most widely used rnn  architectures in regard to their non linearity and memory scale of their underlying dynamics.  definition  . a lstm with l non trivial hidden layers  ktl    with l    l   at time  step t obeys the following propagation rules between hidden variables. each ktl is of           traditional rnn  lstm   physical rnn  odernn   yl  input at time step l  state variable at time step l  kl j  j th hidden layer  j th order increment of the gradient slope   lj  forget gate activation  energy dissipation rate   i j weight matrix for hidden variable  weight of ith increment in j th order slope   lj  input gate s activation  re scale factor of normalized gradient function   lj activation function of j th hidden layer  gradient function    table    comparison of the lstm architecture and nth order odernn structure.  dimension n and is updated at each time step as follows                   i                                   f         l              l  k t             w                        o         l  kt                                g                clt   f   clt     i   g            ktl   o   tanh clt              where w l is of dimension  n    n    represents the element wise product  and the four  vectors  i  f  o  g each of dimension n  with the first three controlling which input will  be saved to influence the future output according to the above update rules in eq.       and     .  claim  . for any lstm with l hidden layers in definition    there exists a   lodernn that realizes the same input output relation.  proof   for one layer rnn  we have the update rule for lstm  hochreiter and           schmidhuber        as     kt   ft   kt     it       wc yt     uc kt     bc               yt   ot       kt              with vector coefficient determined by    ft     wf yt     uf kt     bf               it     wi yt     ui kt     bi    ot     wo yt     uo kt     bo              which is equivalent to setting n      t           d ft         d it    w    wc   b     bc   h      uc and              ot in n t odernn. notice that the weight matrix  wq in odernn can depend on time and is therefore able to include the memory dependency from kt   . we use d a  to represent a p   p diagonal matrix with each  diagonal element equal to each element of the vector a of length p. this is because the  element wise product between two vectors can be re written as diagonal matrix matrix  multiplication with the second vector  a   b   d a b.  for multi layer lstm with l hidden layers  the only change is that the diagonal  matrices d ft    d it   and d ot   are generalized to d ftl    d ilt   and d olt    which not  only depend on the hidden variable of the same layer from the previous time step  but           also the hidden variable of the same time step from a previous layer     l    blf     ftl     wf ktl     uf kt              l  ilt     wi ktl     ui kt      bli               l  olt     wo ktl     uo kt      blo              where kt    yt     and thus the non linearity of the ode increases by one when the  number of hidden layers increases by one  thus gives l   odernn for a l layer architecture  q. e. d..  definition  . a gated recurrent unit  gru  with l non trivial hidden layers  ktl     with l    l   at time step t obey the following propagation rules between hidden variables. each ktl is of dimension n and is updated at each time step according to                           l     r        l  kt                w                   l  kt       z            l  l  ktl        z    kt      z   tanh wxl ktl     wgl r   kt                 where weight matrix w l is of dimension  n    n  and weight matrices wxl and wgl are  both of dimension n   n    represents a given point wise non linearity.  claim  . for any gru with l hidden layers as in definition    there exists a   lodernn that realizes the same input output relation between each layer of hidden  variables.           for one layer gru  cho et al.         we have the update rule as     yt        z    yt     z   tanh  wt yt     wg r   yt                  with z     wlz yt      r     wlr yt                      we can rewrite r   yt   as      wlq yt      rewrite   wlz y     tanh wt y   as    wt l y    and thus simplify the update rule to    yt        z    yt                       wt l yt     wt l  wg      wlq yt                which is equivalent to setting n   t           d      z            w    wt l   b            h     wt l  wg in n t odernn. this can be similarly generalized to multi layer    gru with l total hidden layers by defining the weight matrices d      z   and d r   with               l  l  z     wlz yt      wlz ytl      r     wlr yt      wlr ytl                which for lth layer it corresponds to l   odernn. q. e. d..  definition  . a unitary evolution recurrent neural network  urnn  with l nontrivial hidden layers  ktl    with l    l   at time step t given input data at time step xt  obey the following propagation rules between hidden variables. each ktl is of dimension n and is updated at each time step according to     ktl     wl ktl     vl xt                     where the weight matrix wl is of dimension n   n.  claim  . for any urnn  arjovsky et al.      b  in definition    there exists a   lodernn that realizes the same transformation of hidden variables.  proof   the propagation rule of urnn between the hidden variable at time step t of  the lth layer with     l   t can be realized by a   l odernn by setting  q      and wq     and choosing n   l and t     in eq.        . urnn thus belongs to    l odernn. q.e.d.  claim  . there exists a l l odernn that realizes clockwork rnn  koutnik et al.          with l clocks.  proof   the propagation rule for cw rnn between input yt at time step t  hidden layers  at the same time step kt as well as from the previous time step kt   and output yt   is  described by     kt    h  wh  t kt     wi  t yt     yt      o  wo kt              where the dynamical weight matrices wh  t  and wi  t  are structured to store memory  of previous time steps in into different blocks with increasing duration of time delays  such that effectively one can rewrite wh  t kt        p    j t           wj  jt j   wh  j kj   wi  j yj    contributions from all previous step iteratively  and so is the clock structure in wi  t yt  which contributes to all hidden layers after t. this is equivalent to setting n   t   l  and  q     in eq.        . cw rnn thus belongs to l l odernn. q.e.d.                 .     n t arnn    apart from the categorization of some of existing rnn architectures  this ode methodology can also be applied to design new kinds of rnn starting first by choosing the order of the odernn and the weight matrices. now we showcase the advantage of such  top down construction of rnn architecture  its length of temporal memory scale and  degree of temporal non linearity are determined by design. its application in reducing  the architectural complexity while guaranteeing the stability and representability will  be demonstrated in the next section.  definition  . the nth order ode anti hermitian recurrent neural network  n t arnn   corresponds n t odernn with anti hermitian weight matrices.  theorem  .     arnn with monotonic activation function          rn   rn and  purely imaginary anti hermitian weight matrix is stable for h that satisfies   h maxk n  k  w            where  k  w    represents the kth eigenvalue of w  .  proof   this will be proven in theorem    where the original complex anti hermitian  matrix is embedded into a hilbert space twice as large such that a purely imaginary antihermitian weight matrix guarantees the stability of the first order integration method.  q.e.d.  definition  . an integration method that correspond to a map     m   m for linear  space m is reversible with respect to a reversible differential equation that represent a  differential map   if   exists and the following holds                   .                   theorem  . both     arnn and     arnn are reversible.  proof   it is not hard to see that   arnn corresponds to the first order mid point  integration and   arnn corresponds to the symplectic euler integration. their reversibilities are guaranteed by the reversibility of these two integration schemes inside  the stble regime  haber and ruthotto       . q.e.d.  it is notable that the definition of n t odernn does not restrict weight matrices  to be independent of the input to each recursion step. this setup is less restrictive than  conventional definition of rnn and is indispensable for generalizing various architectures of rnn under the same framework. such generalization  however  is natural to its  ode counterparts  a generic ode can be non autonomous.  the unification of different rnn architectures through n t odernn paves the way  for tailoring the temporal memory scale and the degree of non linearity of rnn architecture towards the underlying data  while reducing the complexity overhead in the  learning architecture. we showcase one such application in rnn design in the next  section.         quantum inspired universal computing neural network    despite the complex structure of lstm  a simple   l odernn is able to represent the  same type of temporal dynamics as lstm while providing specific stability guarantees  through theorem  . it is thus tempting to construct rnn starting from choosing the  appropriate ode counterparts first. in this section  we provide such an example of           rnn design  qunn  by emulating the odes for quantum dynamics with the rnn  architectures.  for preparation  we bridge the gap between rnn dynamics and the quantum dynamics in its discrete realization in sec.  . . we illustate the application of ode rnn  correspondence in designing better rnn architectures in sec.  .   where we define a  qunn construction inspired by a universal computation scheme in quantum systems  first conceived by richard feynman  feynman       .     .     quantum dynamics in the eye of odes    the dynamics of a quantum system can be described by a first order ordinary differential equation  namely the schro dinger equation  where the quantum state represented by  the complex vector y  t  at time t obeys     d  y  t     ih t y  t    dt            where h t  is a hermitian matrix representing the system hamiltonian and determining the dynamical evolution of the quantum state. the hamiltonian matrix is nothing  more than the gradient with respect to time in the first order linear ode. despite such  fundamental linearity  the emergent phenomena in a sub system by tracing out certain  elements in the state parameter y  t  can be highly nonlinear.  the quantum dynamics represented by eq.      can perform universal computation as first suggested by richard feynman in  feynman         any boolean function  can be encoded into the reversible evolution of a quantum system under the a system           hamiltonian that contains independent hamiltonian terms polynomial in the number of  logical gates needed to describe the boolean function. this result is captured by the  theorem   in appendix a.  the universality and reversibility of quantum dynamics inspire us to harness previous results by kitaev and aharonov et. al. kitaev et al.        aharonov et al.         to propose a rnn ansatzs resembling the construction of temporal dynamics through  the addition of a clock hamiltonian. there  the time evolution of a generic quantum  system can be represented by the lowest energy eigenstate of a clock hamiltonian matrix defined on an enlarged system. this clock hamiltonian has two unique properties   it is constructed using the quantum state trajectory of the target system  for periodic  system it contains only a fixed number of hamiltonian terms proportional to the periodicity. based on our newly established connection between rnn and ode  we can  construct rnn architectures whose temporal dynamics emulate quantum evolution under the clock hamiltonian.     .     quantum inspired rnn    we propose an rnn ansatz called quantum inspired universal recurrent neural network  qunn . it reduces the number training parameters in temporal correlation length  from quadratic in traditional architectures to linear. we achieve this by constructing  weight matrices in hidden layers from the input data based on a quantum inspired  ansatz. such construction exploit the knowledge of temporal structure embodied by  the data itself. in comparison  traditional rnn weights are data independent and remain constant after the training. qunn is thus able to represent a larger family of           dynamical maps by utilizing the input to the network for the construction of dynamical  relations.  definition  . let the temporal correlation length of training data be n . quantuminspired universal recurrent neural network  qunn  with n hidden layers is defined  as  a recurrent neural network architecture that transform the input state yl   which could  be either a binary  real  or complex vector depending on the problem type  to the subsequent output yl   denoted by the integer index l          ....  n   according the the  following three stages.  in the first stage  the incoming data is boosted to a higher dimension by a tensor  product with a clock state vector  cl that marks the relative distance in the training data  sequence from the first input within the same data block of length l followed by a  nonlinear function  kl        yl    cl   . we use  i     here and henceforth to represent  the monotonic and continuously differentiable point wise nonlinear function of the ith  layer.  in the second step  the output of the first layer is passed into a l layer neural network  with a composition rule between the kth hidden layer klk given by       klk   sk klk        hl klk                hl   dlk wlk                        i    cl c   dlk        p   l  i   i c   p   l  i    cl k c   l  l k                     wlk        p   l  wlk     p   l   yl k yl        cl k c   cl c         l      yl yl k         l k      where we use sk to represent a weight matrix similar to that for resnet  haber and           ruthotto        in front of the output from the previous hidden layer  the dl consists of  identity matrix on both the original data space i and the clock space ic weighted by      p   l  and a re ordering operator weighted by a real scalar coefficient p          . notice  that  c  ctl   permutes the order of clock states which adds noise  lu et al.        as well  as correction to possibly temporally mislabeled training data. such a permutation step  is in product with wl   which records the flexible range of memory important for the  training  the scale factor p   l           determines the length of temporal memory of  past inputs.  in the third step  the state is mapped back to the original data dimension by projecting it onto the corresponding clock vector for the next sequence of the incoming data  string     yl      l   tr           l     i     cl   c   .  l     kl            finally reset l     if l   n since the temporal correlation only exists within the block  of size n   see fig.   for the illustration of qunn architecture.  notice that the qunn weight matrix changes dynamically according to eq.        meaning that they evolve with the input data within the active block of length n . this is  different from conventional definition of rnn where the weight matrix does not explicitly depend on the data  but such memory dependence is indirectly actuated through the  gate construction such as the forgetting unit fi in lstm  hochreiter and schmidhuber        . the multi layer construction of lstm facilitates longer temporal memory as the  hidden layer number n increases. the depth of the hidden layers in qunn can also be                yl             t   n               d    d      d    h            h            h         h  l      yl      w    w    w         figure    qunn arcitecture    represents tensor product of input with delays  oval  marked by t       n represents different values of delays  and    represents nonlinear  activation functions at different layers.  qunn  lstm urnn n t odernn  memory scale  n        t  order of nonlinearity  n  n  n  n  stability  yes     yes     depth  n    n  n  n  training parameters  o n   o n    o n        origin  schro dinger equation ad hoc unitarity  ode  table    a top down comparison between qunn  lstm  urnn and n t odernn  structure.  interpreted as both the order of the corresponding ode recursion relation and the order  of nonlinearity that corresponds to the stage of runge kutta method  as illustrated in  fig.  . a qunn with l hidden layers thus corresponds to a l l odernn. the the  general qunn stability can be ensured by choosing parameters in eq.      and       according to the requirements in theorem  .  our top down design of qunn is compared with some of existing rnn architectures in table.  . qunn have longer range of memory scale than both lstm and  urnn  i.e.  the order of time derivatives in the corresponding ode is higher in qunn.           this comes with a price of additional layers of embedding in rnn architecture seen in  the depth difference. the data dependent construction of qunn weight matrices can  also slow down the convergence of the training process. and certain pre processing  of data  such as calculating the autocorrelation function  is needed to give a good estimate of n to make qunn effective. but qunn can reduce the total number of  training paramters from lstm  offering potential advantage to its training efficiency.  this show cases the distinction between an ad hoc heuristic approach and physically  inspired approach to rnn designs.         conclusion    we propose an ode theoretical framework for understanding rnn architectures s order of non linearity  length of memory scales and training stability. we apply this analysis to many existing rnn architectures  including lstm  gru  urnn  cw rnn  and identify the improved nonlinearity obtained by cw rnn. examing rnn through  the eyes of odes help us to design new rnn architectures inspired by dynamics and  stability of different odes and associated integration methods. as an example  we  showcase the design of an rnn based on the odes of quantum dynamics of universal quantum computation. we show that in the case when the temporal correlation is  known and the input data comes in active blocks  qunn provides a quadratic reduction  in the number of training parameters as a function of temporal correlation length than  generic lstm architectures. our findings point to an exciting direction of developing  new machine learning tools by harnessing physical knowledge of both the data and the           classical  quantum    column vector row vector matrix inner product tensor product hadamard product  yl  yl    wl  zl  yl  yl   zl  yl   zl   yl i  hyl    w l  hzl  yl i   yl i    zl i  d  yl   zl i    table    comparison of the representation of linear algebra in quantum and in classical  literature.  neural network itself.  a parallel and recently published work on designing stable rnn based on odes  with antisymmetric weight matrices by chang et al.        came to our attention after  the completion of this work. there  a stable rnn architecture is proposed and implemented  which have important practical applications in improving the state of the art  rnn performance. in comparison  we have focused on theoretical analysis on general  rnn architectures  which include the rnn design from chang et al.        as a specific case of n t arnn defined in sec. .  by setting n     and t equals the number of  connected hidden layers. and our stability proof is applicable for any rnn designed  based on ode integration methods. we aim at establishing a firm theoretical ground  for a wider application of numerical toolbox in the study of neural networks instead  of providing a ready to use rnn architecture. but more heuristic testing remains to be  done to fully understand the practical use of tailoring temporal non linearity of rnns  defined in this work.    a    proof of theorem  .    theorem  . any boolean function from the uniform family f         n         n  can be mapped to a unique fixed point of ode evolution with its characteristic function  containing polynomial in n many parameters.           proof. by definition  any polynomial time uniform boolean function can be represented by a deterministic turing machine that runs in polynomial time lc and outputs  the circuit description cn if given  n as input. we only need to show that there exists  a one on one mapping between a deterministic turing machine of polynomial runtime  and a set of odes that represents quantum dynamical evolution. the read and write  process of a turing machine can be mapped a rotation in the hilbert space of input and  output r j   w    j   h at the jth time step as     uj   w j    rj               where our use of  wj i and the associated quantum notation henceforth is explained in  table  . to keep track of the update in its relative location within each active block  we  tensor product such matrix with a time step update matrix  j    ihj c which increase  the book keeping of time j by one. so the overall unitary takes the form     uj    j    ihj c    wj ihrj      j    ihj c .            moreover  we add the inverse process of such turing computing step to satisfy the  hermiticity of the overall matrix h j      h j    wj ihrj      j    ihj c    rj ihwj      jihj     c              whose eigenvalues are purely imaginary. this consists of one step of reversible formulation of any tm computation step. summing up all the corresponding steps for a tm           that halts at step lc   we obtain a hermitian matrix that encode both the forward and  backward computation process of a conventional tm. the ground state energy of this  hamiltonian  or the absolute value of the lowest eigenvalues  is not necessarily zero. if  we like to ensure that the ground state which corresponds to the process of tm computation step is the fixed point of the dynamical evolution  we need to add two additional  term to obtain the overall hamiltonian     h t m      lc  x     i    jihj c    wj ihrj      j    ihj c   rj ihwj      jihj     c   i    j    ihj     c      j      whose lowest eigenvalue is exactly zero aharonov et al.       .  it is shown in aharonov et al.        that the zero energy ground state of the above  hamiltonian is  lc    x      i       wj i    j    i  lc j              which is not only unique but also separated from other eigenstates in eigenvalues by  a gap. projecting the this ground state onto the clock state  lc ic gives the output of  the turing machine that describe the desired boolean function. since  a polynomial  runtime turing machine can be described by polynomial sized computational tap lc    the size of uj is also polynomial in the number of bits. since the total number of terms  inside this hamiltonian equals the number of time steps lc in the deterministic turing  machine of the uniform boolean functions  it is also polynomial in number of bits of  the boolean function input. q. e. d.           