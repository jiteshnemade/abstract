introduction    relation classification is to select the relation class  that implies the relation of the two nominals  e    e   in the given text. for instance  given the following sentence   the  e  phone  e   went  into the  e  washer  e  .   where  e       e     e      e   are position indicators that  represent the starting and ending positions of nominals  the goal is to find the actual relation entitydestination of phone and washer. the task is important because the results can be utilized in other  natural language processing  nlp  applications  like question answering and information retrieval.  recently  neural network  nn  approaches to  relation classification have been spotlighted since  they do not need any handcrafted features but even  obtain better performances than traditional models. such nns can be simply classified into cnnbased and rnn based models  and they capture    jong hyeok lee  computer science and engineering   pohang university of science and  technology  postech   pohang  republic of korea  jhlee postech.ac.kr  slightly different features to predict a relation  class.  in general  cnn based models can only capture local features while rnn based models are  expected to capture global features as well  but  the performances of cnn based models are better  than rnn based models. that could be thought  that most of relation related terms are not scattered  but intensively positioned as short expressions on  a given sentence  and further even if rnns are expected to learn such information automatically  it  cannot be easily done contrary to our expectation.  to overcome the limitation of rnns  most of the  recent work using rnns have used additional linguistic information like shortest dependency path   sdp   which can reduce the effect of noise words  when predicting a relation.  in this paper  we propose a simple rnn based  model that strongly pays attention to nominalrelated and relation related parts with multiple  range restricted rnn variants called gated recurrent units  grus   cho et al.        and attention. on the semeval      task   dataset  hendrickx et al.         our model with only pretrained word embeddings achieved the f  score  of   .    which is comparable with the state ofthe art cnn based and rnn based models that  use additional linguistic resources such as part ofspeech  pos  tags  wordnet and sdp. our contributions are summarized as follows     for relation classification  without any additional linguistic information  we suggest  modeling nominals and a relation in a sentence with specified range restriction standards and attention using rnns.    we show how effective abstracting nominal  parts  a relation part and both separately with  the restrictions is to relation classification.          related work     .     traditional approaches to relation classification  are to find important features of relations with  various linguistic processors and utilize them to  train classifiers. for instance  rink and harabagiu         uses nlp tools to extract linguistic features  and trains an svm model with the features.  recently  many deep learning approaches have  been proposed. zeng et al.        proposes a  model based on cnns to automatically learn important n gram features. dos santos et al.         proposes a ranking loss function to well distinguish between the real classes and other class.  to capture long distance patterns  rnn based   usually using long short term memory  lstm    approaches have also appeared  one of which is  zhang and wang       . the model simply feeds  on all words in a sentence  then captures important one through the max pooling operation. xu  et al.      b  and miwa and bansal        propose other rnn models using sdp to ignore noise  words in a sentence. in addition  liu et al.         and cai et al.        propose hybrid models of  rnn and cnn.  one of the most related work to ours is the  attention based bidirectional lstm  att blstm    zhou et al.       . the model uses bidirectional  lstm and attention techniques to abstract important parts. however  the att blstm does not distinguish roles of each part in a sentence  which  could not involve sensitive attention. another of  the most related work is by zheng et al.       .  they try to capture nominal related and relationrelated patterns with cnns and use neither restrictions nor attention mechanism.         the proposed model    figure   shows the architecture of the proposed  model  which will be described in the subsections.   .     word embeddings    our model first takes word embeddings to represent a sentence at the word level. given a sentence  s consisting of n words  it can be represented as  s    w    w    w   ...  wn  . we convert each onehot vector wt by multiplying with the word embedding matrix we   rde   v      e t   w e wt .           then  the sentence can be represented as se     e    e    ...  en  .    range restricted bidirectional grus    to capture information of two nominals and one  relation  our model consists of three bidirectional  gru layers with range restrictions. a gru is  a kind of rnn variant to alleviate the gradientvanishing problem like lstm  but it has fewer  weights than lstm. in a gru  the t th hidden ht  with reset gate rt and update gate zt is computed  as        rt     wr et   ur ht         zt     wz et   uz ht      h t   tanh w et   u  rt  ht   zt    ht          zt                   ht                 h t             where   is the logistic sigmoid function.  the range restrictions can be done by using  masking techniques to restrict the input range of  the three bidirectional grus. therefore  they  should be conducted under three separate standards  but because the standards for two nominals are the same  we introduce two kinds of standards. first  to capture each nominal information   only the pen   k positioned words are regarded  as input to the corresponding bidirectional gru  layer  where pen is the position of nominal e  or e   and k is a hyperparameter affecting their window  size. second  for the relation gru layer  the input range is set to  pe    pe    or  pe    pe    according  to the relative order of the nominals in a sentence   which means that the range is from the formerlyappearing nominal to the latterly appearing nominal.  after the sentence representation at word level  se is fed into the six gru layers  three gru layers in two directions  under the restrictions  various hidden units are finally generated from the  layers. we call the hidden units of each gru layer                                  h e    h e    h e    h e    h rel   h rel for convenience  in the next subsection.   .     sentence level representation    among the hidden units of the six range restricted  grus  the model selects important parts by using  direct selection from hidden layers and the attention mechanism.  to extract e  and e  information  we propose  to directly select hidden units at each nominal position in the e  and e  bidirectional grus  and to     figure    multiple range restricted bidirectional grus with attention  k       sum them to construct ve    ve    rdh   respectively  as               ve    h e    h e                      ve    h e    h e          where each directional hen represents hidden units  at the en positions in the directional hen .  to abstract relation information  we adopt the  attention mechanism that has been widely used  in many areas  bahdanau et al.        hermann  et al.        chorowski et al.        xu et al.       a . we use the attention mechanism  zhou  et al.         but we apply it to each directional  gru layer independently to capture more informative parts with the flexibility. the forward di   rectional relation abstracted vector    v rel is com      puted as   v rel in the same way                m   tanh  h rel                             sof tmax    w t m                t        v rel   h rel           att               where    w att is a trained attention vector for the forward layer.        then  we sum    v rel and    v rel to make the  relation abstracted vector vrel   rdh          vrel      v rel      v rel .            lastly  the final representation vf in   r dh is  constructed by concatenating them   vf in   ve    vrel   ve     where   is a concatenation operator.             .     classification    our model uses scores of how similar the vf in is to  each class embedding to predict the actual relation   dos santos et al.       . concretely  we propose a  feed forward layer in which a weight matrix wc    rclasses  dh and a bias vector bc   rclasses can  be regarded as a set of the class embeddings. in  other words  the inner product of each row vector  in wc with vf in represents the similarity between  them in vector space  so the class score vector sc    rclasses is just computed as   sc   wc vf in   bc .            then  the model chooses the max valued index  that represents the most probable class label c  except that every value in the sc is negative. in the  exceptional case  c  is chosen as other  dos santos  et al.       .   .     training objectives    we adopt the ranking loss function  dos santos  et al.        to train the networks. let scy  the  score of the c   and scc  the competitive score that  is the best score excluding scy  for convenience.  then  the loss is computed as   l   log     exp   m    scy         log     exp   m    scc             where m  and m  represent margins and   is  a factor that magnifies the gap between the score  and the margin.     model  sdp lstm   xu et al.      b   depnn   liu et al.         sptree   miwa and bansal         mixcnn cnn   zheng et al.         att blstm   zhou et al.         our model  att bgru   our model  relation only   our model  nominals only   our model  nominals and relation     additional features  except word embeddings     f       pos  wordnet  dependency parse  grammar relation      .       ner  dependency parse      .       pos  dependency parse      .       none      .       none      .       none    none    none    none      .     .     .     .     table    comparison with the results of the state of the art models         experiments    for the experiments  we implement our model  in python using theano  theano development  team        and use the model with the following descriptions.   .     datasets and settings    we conduct the experiments with semeval       task   dataset  hendrickx et al.         which  contains       sentences as the training dataset   and       sentences as the test dataset. a sentence consists of two nominals  e   e    and a  relation between them. ten relation types are  considered  nine specific types  cause effect   component whole  content container  entitydestination  entity origin  instrument agency   member collection  message topic and productproducer   and the other class. the specific types  have directionality  so a total of                 relation classes exist.  we use    fold cross validation to tune the hyperparameters. we adopt the     dimensional  word vectors trained by pennington et al.         as initial word embeddings and select the hidden  layer dimension dn of      the learning rate of   .  and the batch size of   . adadelta  zeiler         is used as the learning optimizer. also  we  adapt the dropout  hinton et al.        to the word  embeddings  gru hidden units  and feed forward  layer with dropout rates of  .    .  and  .   respectively  and use the k of  . we adopt the position  indicator that regards  e      e     e   and    e   as single words  zhang and wang       .    we set m    m  and   to  .    .  and  .   respectively  dos santos et al.        and adopt the l   regularization with      . the official scorer is  used to evaluate our model in the macro averaged  f   excluding other .   .     results    in table    our results are compared with the other  state the art models. our model with only pretrained word embeddings achieved the f  score of    .    which is comparable to the state of the art  models.  furthermore  we investigated the effects of extracting relation  nominals and both of them.  attention based bidirectional grus with no restriction  att bgru  were also tested as a reimplementation of the att blstm. here  our finding is that the restricted version of the att bgru   the relation only model  is not significantly better   but by abstracting nominals together  the model  achieves higher f  score. that indicates even if  the ranges are slightly overlapped  they capture  distinct features and improve the performance.         conclusion    this paper proposed a novel model based on multiple range restricted rnns with attention. the  proposed model achieved a comparable performance to the state of the art models without any  additional linguistic information.     