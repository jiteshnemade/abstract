introduction    recent deep neural network successes rekindled  classic debates on their natural language processing abilities  e.g.  kirov and cotterell        mccoy et al.        pater       . lake and baroni        and loula et al.        proposed the  scan challenge to directly assess the ability of  sequence to sequence networks to perform systematic  compositional generalization of linguistic  rules. their results  and those of bastings et al.          have shown that modern recurrent networks  gated rnns  such as lstms and grus   generalize well to new sequences that resemble  those encountered in training  but achieve very low  performance when generalization must be supported by a systematic compositional rule  such as   to x twice you x and x   e.g.  to jump twice  you  jump and jump again .  non recurrent models  such as convolutional  neural networks  cnns  kalchbrenner et al.         gehring et al.              and selfattentive models  vaswani et al.        chen et al.         have recently reached comparable or better  performance than rnns on machine translation    marco baroni  icrea  facebook ai research  mbaroni fb.com    and other benchmarks. their linguistic properties are however still generally poorly understood.  tang et al.        have shown that rnns and selfattentive models are better than cnns at capturing long distance agreement  while self attentive  networks excel at word sense disambiguation. in  an extensive comparison  bai et al.        showed  that cnns generally outperform rnns  although  the differences were typically not huge. we evaluate here an out of the box cnn on the most challenging scan tasks  and we uncover the surprising fact that cnns are dramatically better than  rnns at compositional generalization. as they  are more cumbersome to train  we leave testing of  self attentive networks to future work.         scan    scan studies compositionality in a simple command execution environment framed as a supervised sequence to sequence task. the neural network receives word sequences as input  and has  to produce the correspondence action sequence.  examples are given in table  . lake and baroni        originally introduced   train test splits   of which we consider  .  in the random split   the training set includes     of randomly selected distinct scan commands  with the remaining     in the test set. this requires generalization  as no test command is encountered in training  but there is no systematic difference between  the commands in the two sets. in the jump split      we also tested our cnns on scan s length split  where  test commands require systematically longer actions than the  training ones. accuracy was near     as the learned positional embeddings of our cnn architecture do not generalize  beyond training lengths. we leave the investigation of more  flexible positional encodings  as in  e.g.  vaswani et al.         to future work. we also experimented with scan s turn left  split  obtaining near perfect generalization. as rnns were  already performing very well in this split  we focus in the  paper on the more challenging jump case.     split  random    jump    around right    train  command  walk  opposite  left  turn left  twice and look  jump  turn left  twice after look  jump  around left  turn  opposite right  twice    test command  walk and jump  right twice  run  and run thrice  turn left twice  after jump  run  twice and jump  walk  around right   look around right  and jump left    table    training and test examples for the three splits  used in our experiments.    the jump command is only seen in isolation during  training  and the test set consists of all composite commands with jump. a system able to extract compositional rules  such as  x twice means  to x and x   should have no problem generalizing them to a new verb  as in this split. loula  et al.        proposed a set of new scan splits   the most challenging one being the around right  split. the training partition contains examples of  around and right  but never in combination. the  test set contains all possible around right commands. loula and colleagues want to test  secondorder modification   as models must learn how  to compositionally apply the around function to  right  which is in turn a first order function modifying simple action verbs.         experimental setup    model we use the fully convolutional encoderdecoder model of gehring et al.        out of the  box  using version  . .  of the fairseq toolkit.   the model uses convolutional filters and gated  linear units  dauphin et al.        along with  an attention mechanism that connects the encoder  and the decoder. attention is computed separately  for each encoder layer  and produces weighted  sums over encoder input embeddings and encoder  outputs. see the original paper for details.  training the shift in distribution between training and test splits makes scan unsuitable for  validation set tuning. instead  following lake and  baroni        and loula et al.         we train on     k random samples with replacement from the  training command set. we explore different batch  sizes  in terms of number of tokens per batch                                 learning rates   .         https   github.com pytorch fairseq    lstm  gru  cnn    random    .      .   .      .   .     jump   .     .   .     .   .     around right   .   .        .    .     table    test accuracy     on scan splits  means  across   seeds  with standard deviation if available .  top lstm results from lake and baroni        loula  et al.         gru from bastings et al.       .     .     .      layer dimensionalities                   layer numbers    to      convolutional kernel width           and amount of dropout used       .     .  . for all other hyperparameters  we  accept recommended default fairseq values. each  configuration is run with   seeds  and we report  means and standard deviations.         results    our main results are in table  . cnns  like rnns   succeed in the random split  and achieve much  higher accuracy  albeit still far from being perfect   in the challenging jump and around right splits.  the scan tasks should be easy for a system  that learned the right composition rules. perhaps   cnns do not achieve      accuracy because they  only learned a subset of the necessary rules. for  example  they might correctly interpret the new  expression jump twice because they induced a x  twice rule at training time  but fail jump thrice because they missed the corresponding x thrice rule.  since scan semantic composition rules are associated with single words in input commands  we  can check this hypothesis by looking at error distribution across input words. it turns out  fig.     that errors are not associated to specific input commands. error proportion is instead relatively stable across command words. direct inspection reveals no traces of systematicity  errors cut across  composition rules. indeed  we often find minimal  pairs in which changing one action verb with another  distributionally equivalent in scan  turns  a correctly executed command into a failed one.  for example  in the jump split  the cnn correctly  executes jump left after walk  but fails jump left after run  jumping is forgotten . analogously  in the  around right split  run around right is correctly  executed  but  walk around right  is not  the network stops too early .  robustness fig.   shows a big difference in stability between random and the other splits across  top hyperparameter configurations. the random     figure    proportion of commands with a certain  command word  over total commands with that word   wrongly executed by best cnns.    results are very stable. jump accuracy is relatively stable across hyperparameters  but has large  variance across initialization seeds. for the most  challenging around right split  we observe instability both across seeds and hyperparameters  although even the lowest end of the reported accuracies is well above best rnn performance in the  corresponding experiments . another question is  whether the best configurations are shared  or each  split requires an ad hoc hyperparameter choice.  we find that there are configurations that achieve  good performance across the splits. in particular  the best overall configuration  found by minimizing ranks across splits  has  .   learning rate      tokens batch size   .   dropout    layers       layer dimensionality  and kernels of width  . such  model was   th best  of about  . k explored  on  the random split  with mean cross seed accuracy  of   .     off by  .    from top configuration      th on the jump split    .    mean accuracy  off  by  .      and  nd in the around right split  mean    .    accuracy  off by  .    .  kernel width one important difference between recurrent and convolutional architectures is  that cnn kernel width imposes a strong prior on  the window of elements to be processed together.  we conjecture that relatively wide encoder and  decoder widths  by pushing the network to keep  wider contexts into account  might favour the acquisition of template based generalizations  and  hence better compositionality. to investigate this     figure    accuracies     of top    models on random   jump and around right. arrows denote standard deviations  dashed lines average accuracy across top   .    we varied encoder and decoder widths of the bestoverall model between   and  .   fig.   shows that the random split confirms our  expectations  as both wider encoder and decoder  windows improve performance. the jump results  follow the same trend  although in a less clear cut  way. still  the narrowest encoder decoder combination has the worst performance  and the widest  one the top one. for the around right split  it is  also better to use the widest encoder  but top performance is achieved with the narrowest decoder   width   . indeed  with the narrow decoder we  obtain around right accuracies that are even above  the absolute best jump split performance. since  the novel output templates in the around right split  are by construction long  they involve executing  an around command that requires repeating an  action   times   we would have rather expected  models keeping track of a larger decoding window to fare better  particularly in this case. we  tried to gain some insight on the attested behaviour  by looking at performance distribution in function  of input and output length  failing to detect different patterns in the wide decoder jump model  vs. the narrow decoder around right model  analysis not reported here for space reasons . looking qualitatively at the errors  we note that  for  both splits  the narrower decoder tends to skip trajectory sub chunks  e.g.  executing  jump around  right  with   instead of   right turns followed by     at least on the encoder side  larger widths seem excessive  as the longest commands are   word long.     figure    mean accuracies     across   seeds  in function of decoder  x axis  and encoder  colors  kernel widths.  arrows denote standard deviations. best viewed in color.    jumps   whereas the wider kernel is more likely  to substitute actions  e.g.  turning left instead of  right  than undershooting the length. this impressionistic observation is supported by the fact that   for both splits  the narrow kernel errors have considerably larger variance than the wide kernel errors with respect to ground truth length  indicating  that  with narrow decoder kernel  the model is less  stable in terms of output sequence length. this   however  only confirms our conjecture that a wider  decoder kernel helps length management. we still  have no insight on why the narrower kernel should  be better on the around right split.  multi layer attention the fairseq cnn has attention from all layers of the decoder. is the possibility to focus on different aspects of the input  while decoding from different layers crucial to its  better generalization skills  fig.   reports accuracies when applying attention from a subset of the    layers only. the random split differences are minimal  but ablating attentions greatly affects performance on the compositional splits  although  in  both cases  there is a single ablated configuration  that is as good as the full setup .         conclusion    compared to the rnns previously tested in the  literature  the out of the box fairseq cnn architecture reaches dramatically better performance  on the scan compositional generalization tasks.  the cnn is however not learning rule like compositional generalizations  as its mistakes are nonsystematic and they are evenly spread across different commands. thus  the cnn achieved a con     figure    accuracy     of overall best model with attention only from first layer  bottom    first two layers   bottom    . . .   last two layers  top    top layer only   top  . means and standard deviations across   seeds.  dashed lines show full multi layer attention results.    siderable degree of generalization  even on an explicitly compositional benchmark  without something akin to rule based reasoning. fully understanding generalization of deep seq seq models  might require a less clear cut view of the divide  between statistical pattern matching and symbolic  composition. in future work  we would like to further our insights on the cnn aspects that are crucial for the task  our preliminary analyses of kernel  width and attention.  concerning the comparison with rnns  the best  lstm architecture of lake and baroni has two      dimensional layers  and it is consequently  more parsimonious than our best cnn      of  parameters . in informal experiments  we found  shallow cnns incapable to handle even the sim      plest random split. on the other hand  it is hard to  train very deep lstms  and it is not clear that the  latter models need the same depth cnns require  to  view  long sequences. we leave a proper formulation of a tighter comparison to future work.    acknowledgements  we thank brenden lake  michael auli  myle ott   joa o loula  joost bastings and the reviewers for  comments and advice.    