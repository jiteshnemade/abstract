introduction    recurrent neural networks  rnns  have seen  rapid adoption in natural language processing applications. since these models are not equipped  with explicit linguistic representations such as dependency parses or logical forms  new methods  are needed to characterize the linguistic generalizations that they capture. one such method is  drawn from behavioral psychology  the network  is tested on cases that are carefully selected to be    informative as to the generalizations that the network has acquired.  linzen et al.        have recently applied this  methodology to evaluate how well a trained rnn  captures sentence structure  using the agreement  prediction task  bock and miller        elman        . the form of an english verb often depends on its subject. identifying the subject of a  given verb of requires sensitivity to sentence structure. consequently  testing an rnn on its ability  to choose the correct form of a verb in context can  shed light on the sophistication of its syntactic representations  see section  .  for details .  rnns trained specifically to perform the agreement task can achieve very good average performance on a corpus  with accuracy close to     . however  error rates increase substantially  on complex sentences  linzen et al.                suggesting that the syntactic knowledge acquired  by the rnn is imperfect. finally  when the rnn  is trained as a language model rather than specifically on the agreement task  its sensitivity to  subject verb agreement  measured as the relative  probability of the grammatical and ungrammatical  forms of the verb  degrades dramatically.  are the limitations that rnns showed in previous work inherent to their architecture  or can  these limitations be mitigated by stronger supervision  we address this question using multitask learning  where the same model is encouraged to develop representations that are simultaneously useful for multiple tasks. to provide the  rnn with an incentive to develop more sophisticated representations  we trained it to perform  one of two tasks  the first is combinatory categorical grammar  ccg  supertagging  bangalore and  joshi         a sequence labeling task likely to require robust syntactic representations  the second  task is language modeling.  we also investigate the inverse question  can     tasks such as supertagging benefit from joint training with the agreement task  this question is of  practical interest. large training sets for the agreement task are much easier to create than training  sets for supertagging  which are based on manually parsed sentences. if the training signal from  the agreement prediction task proves to be beneficial for supertagging  this could lead to improved supertagging  and therefore parsing  performance in languages in which we only have a  small amount of parsed training sentences.  we found that multi task learning  either with  lm or with ccg supertagging  improved the performance of the rnn on the agreement prediction  task. the benefits of combined training with supertagging can be quite large  accuracy in challenging relative clause sentences increased from    .   to   .  . this suggests that rnns are  in principle capable of acquiring much better syntactic representations than those they learned from  the corpus in linzen et al.       .  in the other direction  joint training on the  agreement prediction task did not improve overall language model perplexity  but made the model  more syntax aware  grammatically appropriate  verb forms had higher probability than grammatically inappropriate ones. when a limited amount  of ccg training data was available  joint training  on agreement prediction led to improved supertagging accuracy. these findings suggest that multitask training with auxiliary syntactic tasks such as  agreement prediction can lead to improved performance on standard nlp tasks.        .     background and related work           the number of men is not clear.  one attractor            the ratio of men to women is not clear.   two attractors            the ratio of men to women and children is  not clear.  three attractors     agreement prediction    english present tense third person verbs agree in  number with their subject  singular subjects require singular verbs  the boy smiles  and plural  subjects require plural verbs  the boys smile . subjects in english are not overtly marked  and complex sentences often have multiple subjects corresponding to different verbs. identifying the subject  of a particular verb can therefore be non trivial in  sentences that have multiple nouns          championship and building requires an understanding of the structure of the sentence.  in the agreement task  the learner is given the  words leading up to a verb  a  preamble    and is  instructed to predict whether that verb will take the  plural or singular form. this task is modeled after  a standard psycholinguistic task  which is used to  study syntactic representations in humans  bock  and miller        franck et al.        staub         bock and middleton       .  any english sentence with a third person  present tense verb can be used as a training example for this task  all we need is a tagger that can  identify such verbs and determine whether they  are plural or singular. as such  large amounts of  training data for this task can be obtained from a  corpus.  the agreement task can often be solved using  simple heuristics  such as copying the number of  the most recent noun. it can therefore be useful to  evaluate the model using sentences in which such  a heuristic would fail because one or more nouns  of the opposite number from the subject intervene  between the subject and the verb  such nouns  attract  the agreement away from the grammatical  subject. in general  the more such attractors there  are the more difficult the task is for a sequence  model that does not represent syntax  we focus on  sentences in which all of the nouns between the  subject and the verb are of the opposite number  from the subject      the only championship banners that are  currently displayed within the building are  for national or ncaa championships.    determining that the subject of the verb in boldface is banners rather than the singular nouns     .     ccg supertagging    combinatory categorial grammar  ccg  is a syntactic formalism that relies on a large inventory of  lexical categories  steedman       . these categories are known as supertags  and can be thought  of as a fine grained extension of the usual partof speech tags. for example  intransitive verbs   smile   transitive verbs  build  and raising verbs   seem  all have different tags  s np   s np  np  and  s np   s np   respectively.  ccg parsers typically rely on a supertagging  step where each word in a sentence is associated     with an appropriate tag. in fact  supertagging is  almost as difficult as finding the full ccg parse of  the sentence  once the supertags are determined   only a small number of parses are possible. at the  same time  supertagging is simple to set up as a  machine learning problem  since at each word it  amounts to a straightforward classification problem  bangalore and joshi       . rnns have  shown excellent performance on this task  at least  in english  xu et al.        lewis et al.         vaswani et al.       .  in contrast with the agreement task  training  data for supertagging needs to be obtained from  parsed sentences which require expert annotation   hockenmaier and steedman         the amount  of training data is therefore limited even in english  and much more sparse in other languages.   .     language modeling    the goal of a language model is to learn the distribution p  wj  w    . . .   wj     of the j th word in  a sentence given the j     words preceding it. we  seek to minimize the mean negative log likelihood  of all sentences si   wi   . . . wi ni in our data   n ni    xx  l p        log p  wi j  wi   j          z  i   j      p  where z   n  i   ni . language modeling performance is often quantified using the perplexity   l p   . the effectiveness of rnns in language  modeling  in particular lstms  has been demonstrated in numerous studies  mikolov et al.         sundermeyer et al.        jozefowicz et al.       .   .     multitask learning    the benefits of multi task learning in neural networks are straightforward. neural networks often  require a large amount of training data to achieve  good performance on a task. even with a significant amount of training data  the signal may be too  sparse for them to pick it up given their weak inductive biases. by training a network on a simple  task for which large quantities of data are available  we can encourage it to evolve representations  that would help its performance on the primary  task  caruana        bakker and heskes       .  this logic has been applied to various nlp tasks   with generally encouraging results  collobert and  weston        hashimoto et al.        s gaard    and goldberg        mart nez alonso and plank         bingel and s gaard       .         methods     .     datasets    we used two training datasets. the first is the corpus of approximately  .  million sentences from  the english wikipedia compiled by linzen et al.        . all sentences had at most    words and  contained at least one third person present tense  agreement dependency. following linzen et al.          we replaced rare words by their part ofspeech tags  using the penn treebank tag set  marcus et al.       .   the second data set we used is the ccg bank   hockenmaier and steedman         a ccg version of the penn treebank. this corpus contained       english sentences        of which  include a present tense third person verb agreement dependency. a negligible number of sentences longer than    words were removed. we  applied the traditional split where sections       are used for training and section    for testing         and      sentences respectively .  out  of the      different supertags that occur in the  corpus  we only attempted to predict the     supertags that occurred at least ten times  we replaced the rest   .   of the tokens  by a dummy  value.   .     model    the model in all of our experiments was a standard  single layer lstm.  the first layer was a vector embedding of word tokens into d dimensional  space. the second was a d dimensional lstm.  the following layers depended on the task. for  agreement  the output layers consisted of a linear  layer with a one dimensional output and a sigmoid  activation  for language modeling  a linear layer  with an n  dimensional output  where n is the size  of the lexicon  and a softmax activation  and for  supertagging  a linear layer with an s dimensional       in the lm experiments  we restricted ourselves to        words  amounting to   .   of the all occurrences. in the  ccg supertagging experiments  we used those         words  that occurred more than     times  amounting to   .   of  the total number of occurrences.     for experiments using this corpus  we use       words  occurring at least four times  amounting to   .   of occurrences  and replace other words by their pos tags.     our code and data are available at https   github.  com emengd multitask agreement.     output  where s is the number of possible tags   followed by a softmax activation.  the language modeling loss is the mean negative log likelihood of the data given in equation      the loss for agreement is the mean binary  cross entropy of the classifier   lagr          x  log  q  num s  s vb      s   s s    where q  is the estimated distribution of verb numbers  s the set of sentences  num s  the correct  verb number in s and s vb the sentence up to the  verb. the loss for ccg supertagging is the mean  cross entropy of the classifiers   lst     p         xx    s  s  s s wj  s    log r  tag wj   s wj           where r  is the estimated distribution of ccg supertags  tag wj   is the correct tag of word wj in s   and s wj is the sentence s up to and including wj .  we had at most two tasks in any given experiment. we considered two separate setups for  learning from those two tasks  joint training and  pre training.  joint training  in this setup we had parallel output layers for each task. both output layers received the shared lstm representations as their  input. we define the global loss l as follows      r  l     l          r    r  where l  and l  are the losses associated with  each task  and r is the weighting ratio of task    relative to task  . this means that r is a hyperparameter that needs to be tuned. note that sample  averaging occurs before formula     is applied.  l     pre training  in this setup  we first trained the  network on one of the tasks  we then used the  weights learned by the network for the embedding  layer and the lstm layer as the initial weights of  a new network which we then trained on the second task.   .     training    all neural networks were implemented in keras   chollet        and theano  theano development  team       . we use the adagrad optimizer.  we use batch training with batch sizes     for  language modeling experiments and     for supertagging experiments on supertagging.         agreement and supertagging    for the supertagging experiments we used the full  ccg corpus as well as     of the wikipedia corpus for the agreement task      for training and      for testing . we trained the model for     epochs. the accuracy figures we report are averaged across three runs. we set the size of the  network d to     hidden units.  we ran a single  pre training experiment in each direction  as well  as four joint training experiments  with the weight  r of the agreement task set to  .         or    .  we considered two baselines for the agreement  task  the last noun baseline predicts the number of  the verb based on the number of the most recent  noun  and the majority baseline always predicts  a singular verb  singular verbs are more common  than plural ones in our corpus . our baseline for  supertagging was a majority baseline that predicts  for each word its most common supertag.  the agreement task predicts the number of the  verb based only on its left context  the preamble .  we trained our supertagging model in the same  setup. since our model did not have access to the  right context of a word when determining its supertag  we could not expect to compete with stateof the art taggers that use right context lookahead   xu et al.        or even bidirectional rnns that  read the entire sentence from right to left  vaswani  et al.        lewis et al.         we therefore did  not compare our accuracy to these taggers.   .     overall results    figure   shows the overall results of the experiment. multi task training with supertagging significantly improved overall accuracy on the agreement task  figure  a   either with pre training or  joint training  compared to the single task setup   the agreement error rate decreased by up to      in relative terms  from  .    to  .    . conversely  multi task training with agreement did not  improve supertagging accuracy  either in the pretraining or in the joint training regime  supertagging accuracy decreased the higher the weight of  the agreement task  figure  b .  comparing the two multi task learning regimes   the pre training setup performed about as well as  the joint training setup with the optimal r. in the  following supertagging experiments we dispensed  with the joint training setup  which is time con   in initial experiments d      yielded supertagging results inferior to a majority choice baseline.      .    agreement prediction accuracy     .      agreement prediction accuracy    pre training with tagging   .      single task baseline    joint training   .     .      last noun baseline     .     .     .       .     .     .     .       .      .     weight r of agreement task      .        .     single task agreement  pos pre training  ccg pre training      a           l  greem    agreem   agreeme    agreem ast noun ba  nt       ent      ent      ent      s      c      c     cc     cc eline  cg  cg  g  g     a  agreement     a  agreement   .       .     .      pre training with agreement     .      joint training     .      single task baseline    majority baseline     .      ccg classification accuracy    ccg classification accuracy     .       .     .     .     .     .     .     .       .     .     .      single task ccg  agreement pre training     .       .       .      .     weight r of agreement task      .        .         a             m  greem    agreem   agreeme    agreem ajority base  n  ent      e  e      c nt        c t        cc nt       cc line  cg  cg  g  g     b  supertagging     b  supertagging    figure    overall results of supertagging   agreement multi task training.  suming since it requires trying multiple values of  r  and focused only on the pre training setup.   .     effect of corpus size    to further investigate the relative contribution of  the two supervision signals  we conducted a series of follow up experiments in the pre training  setup  using subsets of varying size of both corpora. we also included pos tagging as an auxiliary task to determine to what extent the full  parse of the sentence  approximated by supertags   is crucial to the improvements we have seen in the  agreement task. since pos tags contain less syntactic information than ccg supertags  we expect  them to be less helpful as an auxiliary task. penn  treebank pos tags distinguish singular and plural  nouns and verbs  but ccg supertags do not  to put  the two tasks on equal footing we removed number information from the pos tags. we trained for     epochs and averaged our results over   runs.  the results for the agreement task are shown  in figure  a  baseline values are always calculated over the full corpora . the figure confirms    figure    the effect of corpus size on agreement  and supertagging accuracy in multi task settings.  the beneficial effect of supertagging pre training   note that the scale starts at  .   not  .  as in figure  a . this effect was amplified when we used  less training data for the agreement task. pretraining on pos tagging yielded a similar though  slightly weaker effect. this suggests that much of  the improvement in syntactic representations due  to pre training on supertagging can also be gained  from pre training on pos tagging.  finally  figure  b shows that pre training on  the agreement task improved supertagging accuracy when we only used     of the ccg corpus   increase in accuracy from   .   to   .     however  even with agreement pre training supertagging accuracy is lower than when the model is  trained on the full ccg corpus  where accuracy  was   .   .  in summary  the data for each task can be used  to supplement the data for the other  but there  is a large imbalance in the amount of information provided by each task. this is not surprising given that the ccg supertagging data is much  richer than the agreement data for any individual  sentence. still  we showed that the syntactic sig       .   agreement prediction accuracy    agreement prediction accuracy     .    .    .    .    .    .      with ccg        with pos        single task        with ccg       with pos       single task                 number of attractors              figure    agreement accuracy as a function of the  number of attractors intervening between the subject and the verb  for two different subsets of the  agreement corpus      and    of the corpus .  nal from the agreement prediction task can help  improve parsing performance when ccg training data is sparse  this weak but widely available  source of syntactic supervision may therefore have  a practical use in languages with smaller treebanks  than english.   .     attraction errors    most sentences are syntactically simple and do not  pose particular challenges to the models  the accuracy of the last noun baseline in figure  a was  close to    . to investigate the behavior of the  model on more difficult sentences  we next break  down our test sentences by the number of agreement attractors  see section  .  .  our results  shown in figure    confirm that attractors make the agreement task more difficult   and that pre training helps overcome this difficulty. this effect is amplified when we only use  a small subset of the agreement corpus. in this  scenario  the accuracy of the single task model on  sentences with four attractors is only   .  . pretraining makes it possible to overcome this difficulty to a significant extent  though not entirely    increasing the accuracy to   .   in the case of  pos tagging and   .   in the case of supertagging. this suggests that a network that has developed sophisticated syntactic representations can  transfer its knowledge to a new syntactic task using only a moderate amount of data.   .     relative clauses    in linzen et al.         attraction errors were particularly severe when the attractor was inside a rel      .    .    .    .    .     single task agreement  ccg pre training  prepo  prepo  relat  ive    sition  sition  sp  al   s  al   p  s  p    relat  ive    ps    figure    accuracy on sentences from bock and  cutting       . error bars indicate standard deviation across runs.    ative clause. to gain a more precise understanding  of the errors and the extent to which pre training  can mitigate them  we turn to two sets of carefully constructed sentences from the psycholinguistic literature  linzen et al.       . bock and  cutting        compared preambles with prepositional phrase modifiers to closely matched relative  clause modifiers          p repositional   the demo tape s  from  the popular rock singer s ...           r elative   the demo tape s  that promoted the popular rock singer s ...    they constructed    such sentence pairs. each  of the sentences in each pair has four versions   with all possible combinations of the number of  the subject and the attractor. we refer to them  as ss for singular singular  tape  singer   sp for  singular plural  tape  singers   and likewise ps  and pp. we replaced out of vocabulary words with  their pos  and further streamlined the materials by  always using that as the relativizer.  we retrained the single task and pre trained  models on     of the wikipedia corpus. like humans  neither model had any issues with ss and  pp sentences  which do not have an attractor. the  results for sp and ps sentences are shown in figure  . the comparison between prepositional and  relative modifiers shows that the single task model  was much more likely to make errors when the attractor was in a relative clause  whereas humans  are not sensitive to this distinction . this asymmetry was substantially mitigated  though not completely eliminated  by ccg pre training.     e mbedded verb    coach es ...    the player s  the           m ain clause verb   the player s  the  coach es  like the best...    in the first preamble  the verb is expected to agree  with the embedded clause subject  the coach es     whereas in the second one it is expected to agree  with the main clause subject  the player s  .  figure   shows that both models made very  few errors predicting the embedded clause verb   and more errors predicting the main clause verb.  the relative improvement of the pre trained model  compared to the single task one is more modest in  these sentences  possibly because the single task  model does better to begin with on these sentences  than on the bock and cutting        ones. this  in turn may be because the attractor immediately  precedes the verb in bock and cutting        but  not in wagers et al.         and an immediately  adjacent noun may be a stronger attractor. the  appendix contains additional figures tracking the  predictions of the network as it processes a sample  of sentences with relative clauses  it also illustrates  the activation of particular units over the course of  such a sentence.         agreement and language modeling    we now turn our attention to the language modeling task. the previous experiments confirmed that    agreement prediction accuracy     .    .    .      .     single task agreement  ccg pre training  embe  embe  main  main  claus  claus  dded  dded  e   sp  e   ps    sp    ps    figure    accuracy on sentences based on wagers  et al.       . error bars indicate standard deviation across runs.     .    .     single task baseline  joint training     .     last noun baseline     .     majority baseline     .    .    .       .      .     weight r of agreement task      .        .      a  agreement            joint training                     .      single task baseline   .      .     weight r of agreement task      .        .      b  language modeling    figure    overall results of language modeling    agreement multi task training  trained only on  sentences with an intervening noun .    agreement in sentences without attractors is easy  to predict. we therefore limited ourselves in the  language modeling experiments to sentences with  potential attractors. concretely  within the subset  of     of the wikipedia corpus  we trained our  language model only on sentences with at least  one noun  of any number  between the subject  and the verb. there were       sentences in the  training set. we averaged our results over three  runs. training was stopped after    epochs  and  the number of hidden units was set to d     .   .      .    .     agreement prediction accuracy            .     lm perplexity    our second set of sentences was based on the  experimental materials of wagers et al.       .  we adapted them by deleting the relativizer and  creating two preambles from each sentence in the  original experiment     overall results    the overall results are shown in figure  . joint  training with the lm task improves the performance of the agreement task to a significant extent  bringing accuracy up from   .   to   .    a  relative reduction of     in error rate . this may  be due to the higher quality of the word representations that can be learned from the language modeling signal  which in turn help the model make  more accurate syntactic predictions.      .     grammaticality of lm predictions    to evaluate the syntactic abilities of an rnn  trained as a language model  linzen et al.         proposed to perform the agreement task by comparing the probability under the learned lm of  the correct and incorrect verb forms  under the assumption that all other things being equal a grammatical sequence should have a higher probability than an ungrammatical one  lau et al.         le godais et al.       . for instance  if the sentence starts with the dogs  we compute   p  w    are w      the dogs   p  w    are  . . .     p  w    is  . . .         the prediction for the agreement task is derived by  thresholding p correct at  . .  is the lm learned in the joint training setup with  high r more aware of subject verb agreement than  a single task lm  note that this is not a circular  question  we are not asking whether the explicit  agreement prediction output layer can perform the  agreement task   that would be unsurprising    but whether joint training with this task rearranges  the probability distributions that the lm defines  over the entire vocabulary in a way that is more  consistent with english grammar.  as the method outlined in equation   may be  sensitive to the idiosyncrasies of the particular  verb being predicted  we also explored an unlexicalized way of performing the task. recall that  since we replace uncommon words by their pos  p correct       .   agreement prediction accuracy    in the other direction  we do not obtain clear improvements in perplexity from jointly training the  lm with agreement. surprisingly  visual inspection of figure  b suggests that the jointly trained  lm may achieve somewhat better performance  than the single task baseline for small values of r   that is  when the agreement task has a small effect  on the overall training loss . to assess the statistical significance of this difference  we repeated  the experiment with r    .   with    random  initializations. the standard deviation in lm loss  was about  .     yielding a standard deviation of   .    for three run averages under gaussian assumptions. since the difference of  .    between  the mean lm losses of the single task and joint  training setups is of comparable magnitude  we  conclude that there is no clear evidence that joint  training reduces perplexity.     .    .    .    .    .     single task lm  joint training  lm predic  lm predic  agreemen  last noun  tion  verb  ti  t  baseline  form  on  pos tag  model    figure    language model agreement evaluation. red bars indicate the results obtained on the  single task lm model  blue bars those obtained in  a joint training setup with r      .    tags  pos tags are part of our lexicon. we can  use this fact to compare the lm probabilities of  the pos tags for the correct and incorrect verb  forms  in the example of the preamble the dogs   the correct pos would be vbp and the incorrect  one vbz.  the results can be seen in figure  . the accuracy of the lm predictions from the jointly trained  models is almost as high as that obtained through  the agreement model itself. conversely  the  single task model trained only on language modeling performed only slightly better than chance   and worse than our last noun baseline  recall that  the dataset only included sentences with an intervening noun between the subject and the verb   though possibly of the same number as the subject . predictions based on pos tags are somewhat worse than predictions based on the specific  verb. in summary  while joint training with the explicit agreement task does not noticeably reduce  language model perplexity  it does help the lm  capture syntactic dependencies  the ranking of upcoming words is more consistent with the constraints of english syntax.         conclusions    previous work has shown that the syntactic representations developed by rnns that are trained  on the agreement prediction task are sufficient for  the majority of sentences  but break down in more  complex sentences  linzen et al.             .  these deficiencies could be due to fundamental  limitations of the architecture  which can only be  addressed by switching to more expressive archi      tectures  socher        grefenstette et al.         dyer et al.       . alternatively  they could be  due to insufficient supervision signal in the agreement prediction task  for example because relative  clauses with agreement attractors are infrequent in  a natural corpus.  we showed that additional supervision from  pre training on syntactic tagging tasks such as  ccg supertagging can help the rnn develop  more effective syntactic representations which  substantially improve its performance on complex  sentences  supporting the second hypothesis.  the syntactic representations developed by the  rnns were still not perfect even in the multitask setting  suggesting that stronger inductive biases expressed as richer representational assumptions may lead to further improvements in syntactic performance. the weaker performance on  complex sentences in the single task setting indicates that the inductive bias inherent in rnns  is insufficient for learning adequate syntactic representations from unannotated strings  improvements due to a stronger inductive bias are therefore likely to be particularly pronounced in languages for which parsed corpora are small or unavailable. finally  the strong syntactic supervision required to promote sophisticated syntactic  representations in rnns may limit their viability as models of language acquisition in children   though children may have sources of supervision  that were not available to our models .  we also explored whether multi task training  with the agreement task can improve performance  on more standard nlp tasks. we found that it  can indeed lead to improved supertagging accuracy when there is a limited amount of training  data for that task  this form of weak syntactic supervision can be used to improve parsers for lowresource languages for which only small treebanks  are available.  finally  for language modeling  multi task  training with the agreement task did not reduce  perplexity  but did improve the grammaticality  of the predictions of the language model  as  measured by the relative ranking of grammatical  and ungrammatical verb forms   such a language  model that favors grammatical sentences may produce more natural sounding text.    acknowledgments  we thank emmanuel dupoux for discussion. this  research was supported by the european research  council  grant erc      adg        bootphon   the agence nationale pour la recherche   grants anr    idex         psl and anr   labx      iec  and the israeli science  foundation  grant number         .    