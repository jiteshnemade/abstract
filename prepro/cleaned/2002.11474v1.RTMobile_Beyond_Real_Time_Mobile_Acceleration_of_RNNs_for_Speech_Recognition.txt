introduction  deep neural network  dnn  has evolved to the stateof the art technique due to its high prediction accuracy in  many artificial intelligence tasks  such as image recognition  and characterization          speech recognition           and  recommender system     . among various dnn architectures   recurrent neural networks  rnns  are widely used for speech  recognition tasks because they can contain cycles to carry  information across neurons when reading inputs. for instance   gated recurrent unit  gru        the most recent representative popular type of rnns  achieve great success in automatic  speech recognition. in recent years  executing dnns on mobile  platforms has become more and more popular because many  high end mobile devices are emerging. several recent studies  have proposed techniques to accelerate large scale dnns in  mobile environment. however  due to fairly high computation  complexity and memory consumption when executing rnns   it is very challenging to deploy rnns on current embedded  processors and mobile devices to achieve real time inference.  dnn model compression provides an effective way to  mitigate the computation and memory challenges bringing  by dnns     . many model compression techniques have  corresponding author  dingwen tao  department of computer science   the university of alabama  tuscaloosa  al        usa.    been studied for recent years. for example  weight pruning  can provide a notable reduction ratio in the model size.  early work      on non structured weight pruning eliminates  weights at arbitrary location  which leads to the pruned model  to be stored in a sparse matrix format  such as compressed  sparse column  csc  format. non structured weight pruning   however  hurts processing throughput because the indices  in the compressed weight representation result in stalls or  complex workloads on highly parallel architectures  such as  gpus and fpgas. on the other hand  structured weight  pruning      is more hardware friendly. by exploiting filter  pruning      and channel pruning       the pruned model is  more regular in terms of the shape  which can eliminate storing  the weight indices. however  structured pruning hurts accuracy  more than non structured pruning. moreover  state of the art  model compression based rnn acceleration techniques such  as ese      and c lstm      still suffer from limited inference accuracy and processing throughput  which prevents them  to be implemented on mobile devices. furthermore  existing  dnn acceleration frameworks for mobile devices such as  tvm      do not even support rnn. therefore  in order to  achieve the real time inference for rnns on mobile devices   it is necessary to develop an end to end rnn acceleration  framework that can achieve both high inference accuracy and  high computational efficiency.  in this paper  we propose a real time rnn acceleration  framework for mobile devices named rtmobile. rtmobile  is composed of two main components  block based structured  pruning and compiler assisted performance optimization. unlike traditional structured pruning methods used on dnns   our novel block based structured pruning approach that can  provide a finer pruning granularity to maintain high inference  accuracy while significantly reducing the rnn model size. we  also propose several compiler based optimization techniques  to determine the block size and generate the optimal code on  mobiles. our contributions are summarized as follows.       we propose a novel rnn acceleration framework for  mobile devices  namely  rtmobile. to the best of our  knowledge  rtmobile is the first work that achieves       lter width  x   lter height  x  number of channels     wn m   wn m      wn m k    ...    ...    w  m k    ...    ...    ...  ...     b     ...  ...  ...    as a representative technique in dnn model compression   dnn weight pruning removes the redundant or less important  weights to reduce the storage and computational costs for  the inference phase. there exist two mainstreams of weight  pruning  i.e.  non structured pruning and structured pruning.  a  non structured pruning  non structured weight pruning is fine grained and prunes weights at arbitrary locations.  the early work proposed by han et al.      leverages a heuristic method to iteratively prune weights with small magnitudes.  with the successful applications of the powerful admm    filter pruning    wn   k     a   column pruning    ...  ...  ...    b. dnn model compression techniques    ...    the gated recurrent unit  gru  is a variation from the  lstm  proposed by cho et al.     . it combines the forget  and input gates into a single  update gate . it also merges the  cell state and hidden state  and makes some other changes.  the resulting gru model is simpler than standard lstm  models  and has been growing increasingly popular. fig.    shows a single gru  whose functionality is derived by using  the following equations iteratively from t     to t   where  e h are respectively the update gate  output  symbols z  r  h   gate  cell state  and cell output. as gru is a more advanced  version of rnn than lstm  we mainly focus on gru model  in this work.    wn   k wn     wn        ...    a. gated recurrent unit    ...    in this section  we present some background information  about gru  dnn model compression  and dnn mobile  acceleration framework  and discuss our research motivation.    w  m   w  m      ...    ii. background and m otivation    ...         ...         w    k    ...         real time rnn inference on mobile devices.  we propose a fine grained block based structured  pruning algorithm  bsp  for both high inference accuracy  and high computational efficiency.  we develop a series of compiler based optimization  techniques to further accelerate rnn inference on mobile platforms  including matrix reorder  load redundant  elimination  and a new compact data format for pruned  model storage  called bspc  i.e.  block based structured  pruning compact format .  we compare rtmobile with multiple state of the art  methods based on a representative rnn  gru  using a  well known speech recognition dataset. evaluation results  demonstrate that rtmobile is the first work that can  compress the gru model by over   x without losing  accuracy. experiments also illustrate that rtmobile can  obtain about   x energy efficiency improvement over  prior work with the same inference time.    w    k w      w         ...    ...    wn     wn        fig.    a single gru model.    ...    w      w         ...    num of   lters    channel m  w  m   w  m   w  m k    ...    channel    channel    w    k w      w       w    k  w      w         channel pruning    fig.     a  to support gemm computation  the weight tensor representation of a conv layer is transformed into the weight matrix representation.  b  how different structured weight pruning  schemes are implemented on the weight matrix representation.    optimization framework  existing research works             achieve a very high weight reduction ratio while maintaining  promising accuracy. however  non structured methods lead to  sparse and irregular weight matrices  which require indices to  be stored in a compressed format. though saving the storage  cost  the decoding of each stored index requires a search  over the whole activation vector. consequently  it suffers from  limited acceleration in actual hardware implementation     .  b  structured pruning  to overcome the limitations of  non structured pruning  recent works                  considered to incorporate regularity in weight pruning with a main  focus on convolutional  conv  layers of dnns. previous  works mainly focus on two types of structured pruning  filter  pruning and channel pruning. filter pruning  also known  as row pruning  removes the entire filter s   while channel  pruning removes the whole channel s . figure   illustrates  the example of transforming convolutional computation into  general matrix multiplication  gemm  by converting weight  tensors and feature map tensors to matrices     . in general   structured pruning directly reduces the dimension of a weight  matrix and preserves a full matrix format  thereby facilitating hardware implementations. on the downside  the coarsegrained nature of structured pruning hurts the accuracy more  significantly.  c. dnn acceleration on mobile devices  many efforts target accelerating dnn execution on mobile devices in the past few years  including mcdnn        deepmon       tflite       tvm       and alibaba mobile  neural network     . however  most of them do not deeply  exploit model compression techniques as rtmobile. in particular  none of the existing frameworks can even support rnn  acceleration on mobile devices.  d. research motivation  based on the survey of recent research works  we conclude  the following insights   i  non structured pruning has the  advantage of very high compression ratio but is typically not  compatible with gpu acceleration for inference   ii  structured     pruning facilitates hardware implementations but is often subjected to accuracy degradation  especially when it is applied  to time based rnns. to overcome the limitations of current  methods  a more flexible and fine grained pruning policy is  needed. this work specifically focuses on rnn models that  have not been extensively studied.    where n is the total number of weight tensor in recurrent  neural network  f  w  b  is the loss function  and g w   is an  indicator function that is zero when the constraint s     the  number of nonzero weights is less than certain threshold   is  satisfied  but    otherwise.  the augmented lagrangian formation of problem     is  lp   minimize    iii. r elated w ork     wi       many existing studies have implemented model compression algorithms for rnn acceleration on fpgas                             . however  the majority of these works focus on  constructing new rnn architectures      rather than software  and hardware co design framework. instead  our rtmobile  proposes architecture designs in both software and hardware  level. in this work  we mainly discuss and compare rtmobile  with two most recent and related approaches  i.e.  ese       and c lstm       which not only address the rnn model  compression problem on algorithm software but also take into  account the hardware efficiency on hardware  i.e.  fpgas .    n    x   i  kwi   zi   ui k f    f  wi   bi  n     i       i           where  i is a penalty value  zi is pruning mask and ui is  dual variable.  the admm algorithm      is to iteratively update the  indicated pruning mask and retrain the neural network under  this mask  until a good mask and neural network converge. it  proceed by repeating iteration k         . . . as following   wik      arg min lp   wi     zki     uki      zk       arg min lp   wik       zi     uki      i    ese proposes an optimized lstm compression framework  on fpga  which sparses the model through parameter pruning            . compared with both cpu  and gpu based implementations  ese achieves higher energy efficiency on fpga.  however  the design of ese has three main limitations       ese s irregular pruning method used for model compression  causes large overhead when performing read write operations  on hardware      the irregularity of weight matrix storage  in ese results in inefficient implementations of indices that  consume extra storage cost  thus the computing power of  the fpga is not fully exerted  and     ese only marginally  improves compression ratio taking into account indices.  b. c lstm  in order to solve the problem caused by irregular pruning   wang et al.      propose an approach  called c lstm   to employ a structured compression technique using blockcirculant matrices to compress the lstm model. with regular  structure of the block circulant matrices  c lstm can further  reduces both computational and storage complexity compared  with ese. however  the coarse grained nature of structured  pruning also cause relatively significant degradation on the  model accuracy. moreover  the advanced admm based neural  network pruning method  which can effectively handle both  model compression and accuracy  is not supported in the clstm training because it requires the most advanced optimizer in stochastic gradient decent  e.g.  adam optimizer .  c. admm  the pruning problem can be formulated as the minimization  of f  w  b    g w   by following   minimize       n    f  wi   bi  n  i     g  wi  i        subject to    wi   si   i      . . .   n      wi                    zi    .     uki   wik     zk    uk    i  i    a. ese           wi           the pruning mask can be trained by algorithm  .  iv. p roposed rtm obile f ramework  in this section  we describe in detail rtmobile  our proposed mobile acceleration framework for rnns.  a. block based structured pruning  to better facilitate the compression ratio and ensure the  structured model architecture for hardware implementations   we propose block based structured pruning  bsp  algorithm.  in general  training a bsp compressed model can be separated  into two main steps  step    row based column block pruning  and step    column based row pruning.  the training process starts with splitting the whole weight  matrix w into n umr rows horizontally. for each row  we  divide it into n umc blocks and then perform the structured  pruning using admm method  discussed in section iii c .  then  we perform column based row pruning over the entire  weight matrix w in the step  . given the constraint of block  number after dividing by n umc and n umr   the pruned model  can achieve a satisfactory performance overhead on hardware.  the training process continues iteratively until all the blocks  are pruned. we identify that by doing so  the training performance is stable  and the whole weight matrix after pruning  is decentralized. our bsp training approach is summarized in  algorithm  .  b. compiler assisted rnn acceleration framework  after block based structured pruning  rtmobile relies on  a compiler assisted rnn acceleration framework to achieve  efficient rnn inference on mobile devices. this compiler  framework consists of three key optimizations that work on  each rnn layer  as shown in figure     matrix reorder  load  redundancy elimination  and a compact data storage format  for pruned rnn matrices  bspc  i.e.  block based structured     fig.    systematic overview of rtmobile acceleration framework.    achieve balanced processing.  b  redundant load elimination  within a group  each  thread processes multiple continuous rows  offering us an  opportunity of eliminating the redundant memory load operations. this optimization is specifically enabled by our blockbased structured pruning  because after such pruning  the  preserved weights in two neighbor rows may share the same  pattern and require the same data in the input feature maps. it  is difficult to explore this optimization opportunity for existing  unstructured weight pruning due to its irregularity.  c  bspc format  our proposed block based structured  pruning also guides us to design a more compact data structure  than traditional csr format  called bspc format  to store  rnn weight matrices. this is because within each block the  preserved weights only exist in certain rows and columns   enabling to further compact the index array in csr. the bspc  format also includes the matrix reorder information to match  the corresponding input feature map with the weight matrix.  the bspc format significantly reduces the memory footprint  thus alleviating the memory bound issue in rnn execution.  in addition to above optimizations  our compiler framework  also includes an auto tuning component to perform an offline  search of the best execution configurations like the matrix  tiling size  unrolling size  memory placement  etc. in particular  we employ it to find the best block size that results in an  optimal combination of accuracy and performance.  v. e xperimental e valuation  pruning compact format . these optimizations aim to address  three key challenges in pruned rnn execution  thread divergence and load imbalance among threads  redundant memory  access  and unnecessary zero storage.  a  matrix reorder  the matrix is executed by multiple  cpu gpu threads simultaneously. without a further reorder   these threads may execute rows with significantly divergent  computations  causing severe load imbalance issue that hurts  thread level parallelism. therefore  rtmobile introduces a  matrix reorder optimization to group the rows with the same   or similar  computation patterns together. after this reorder   the rows in each group are assigned to multiple threads to    in this section  we evaluate rtmobile by comparing it with  several state of the art methods. there are three evaluation  objectives     comparing rtmobile with other model compression methods and demonstrating that our method outperforms  others in both compression rate and accuracy     showing  rtmobile has both higher computational efficiency and energy  efficiency than a well known deployment on fpga  ese           and    studying the relationship between compression  rate and inference execution time.    we compare rtmobile on mobile with ese on fpga because     none  of the existing rnn acceleration works supports mobile device  and     ese  provides one of the highest inference accuracy among prior works.     table i  results of different model compression methods on gru using timit dataset  per is phone error rate  the lower the  better. baseline per is for dense  non pruned  models and pruned per is for pruned compressed models. per degrad. represents for the  per degradation  i.e.  p erpruned   p erbaseline . the rest columns show the column compression rate  row compression rate  the number  of preserved parameters  and the overall compression rate  respectively.  method  ese       c lstm       c lstm       bbs       wang       e rnn       bsp  ours   bsp  ours   bsp  ours   bsp  ours   bsp  ours   bsp  ours   bsp  ours   bsp  ours   bsp  ours   bsp  ours     per       baseline   pruned     .       .      .       .      .       .      .       .         .       .      .    w o pruning     .       .      .       .      .       .      .       .      .       .      .       .      .       .      .       .      .       .      per degrad.    column compress. rate    row compress. rate    para. no.    overall compress. rate     .     .     .     .     .     .           .     .     .     .     .     .     .     .                                                                                          .                                .  m   .  m   .  m   .  m   .  m   .  m   . m   .  m   .  m   .  m   .  m   .  m   .  m   .  m   .  m   .  m                                                                                    table ii  performance and energy evaluation on mobile gpu and cpu  gop refers to giga operations. gpu cpu energy efficiency  is calculated as inf erencef rames  p ower   inf erencet ime   i.e.  the number of frames inferred per unit energy consumption. this  table normalizes our method s gpu cpu energy efficiency by the ese fpga implementation s. as our compression rate reaches        our gpu inference time becomes slightly faster than ese s    . us . our gpu implementation uses    bit floating point.  compression rate      baseline                                                      gop    gpu time   frame  us     gpu gop s     .       .       .       .       .       .       .       .       .       .            .       .       .       .       .       .       .      .      .      .         .       .       .      .      .      .      .      .      .      .      gpu energy efficiency   normalized with ese    .     .      .      .      .      .     .      .     .      .      cpu time   frame  us     cpu gop s        .        .       .       .       .       .       .       .       .       .        .      .      .      .      .      .      .      .      .      .      cpu energy efficiency   normalized with ese    .     .     .     .     .     .     .      .      .      .       . m overall number of parameters.  evaluation dataset. we conduct our experiments on the  timit dataset       which is widely adopted for evaluating  automatic speech recognition systems. the timit dataset  contains broadband recordings from     speakers reading ten  phonetically rich sentences in eight major dialects of american  english  each reading ten phonetically rich sentences.  b. evaluation results and discussion    fig.    speedup using rtmobile with different compression rates on  mobile platform.    a. experiment setup  experimental platform. we conduct our experiments using a  samsung galaxy s   with the latest qualcomm snapdragon      mobile platform  which consists of a qualcomm kryo      octa core cpu and a qualcomm adreno     gpu.  model architecture. we evaluate rtmobile and compare it  with the state of the art methods on the popular gru rnn  model  which has been widely used in previous studies                  . the gru model contains   gru layers and about    compression rate and accuracy. table i illustrates the  results  including phone error rate and number of preserved  parameters  of rtmobile with different compression rates and  the comparison with other state of the art methods  including  ese       c lstm       bbs       wang      and e rnn      . for a fair comparison  we train all models using the  same timit dataset     . benefit from the most advanced  pytorch kaldi speech recognition toolkit       the baseline  gru model for our rtmobile can achieve higher recognition  accuracy than the other methods before pruning  e.g.  our per  is  .    lower than c lstm s    .    v.s.   .    .  we observe that our proposed bsp method can guarantee  no accuracy degradation when the compression rate is not  higher than      which is superior than ese and c lstm  from both compression rate and inference accuracy. we also  observe that bsp can stably keep a high accuracy compared     to the other methods when the compression rate is relatively  high. for instance  when the compression rate is       the  bsp pruned model can even outperform the c lstm baseline  model in terms of both compression rate and accuracy. the clstm baseline model  with  .  m parameters  has     more  parameters than our bsp pruned model  but its per is  .     higher than ours    .    vs.   .    . in addition  we use bsp  to further prune the model until the rate of      and observe  that our method can well adapt to ultra high compression rate  scenario. for example  our model with      compression rate  can still maintain the same level per as the c lstm baseline  model    .    vs.   .     and reduce the parameter number  by over       .  m vs.  .  m .  inference time and energy efficiency. table ii presents the  evaluation results of rtmobile s inference time  giga operations per second  gop s   and energy efficiency  normalized  with ese method  on mobile gpu and cpu  respectively.  the table illustrates that  when the compression rate is higher  than       rtmobile can outperform in energy efficiency  by about     compared with ese while maintaining the  same inference time  ese s inference time is   .  us  on  the mobile gpu  ese uses a large fpga platform of   w  power  and thus it is easier to achieve higher energy efficiency  than speed . please note that this is a clear feat  as it is  typically perceived that fpga is more energy efficient than  general purpose computing devices. this is because of two  main reasons. first  comparing to ese s activation calculation by look up tables that results in limited parallelization  and irregular memory accesses  two key performance factors  on fpga   rtmobile s compiler optimizations significantly  improve both the parallelization and memory performance.  second  rtmobile has a much better compression rate  with  a negligible accuracy loss   resulting in a more significant  computation reduction. although our compression rates are  significant  we must emphasize that the inefficiency in fpga  implementation in ese  especially activation  plays an equally  important  if not more  role. as can be seen from the table  our  gpu energy efficiency  frames in unit energy  is almost the  same as ese  which uses compression  even when we do not  have any pruning. with increase in the compression rate  the  computation pattern becomes i o and memory bounded  the  memory access pattern becomes more irregular  which leads  to lower cpu gpu gop s.  relationship between compression rate and inference  time. figure   further illustrates the relationship between  inference time and compression rate. the inference time is in  the form of speedups over our own dense cpu gpu baselines   respectively. the speedup grows as compression rate increases.  the speedup becomes stable when compression rate reaches to  a certain range  e.g.  compression rate reaches      . when  the compression rate is       our inference time on mobile  gpu is the same to ese s on fpga.  vi. c onclusion  in this paper  we propose the first rnn acceleration framework for mobiles  called rtmobile. we develop a novel block     based pruning algorithm and three compiler optimizations to  achieve real time inference without any accuracy degradation.  experimental results demonstrate that rtmobile significantly  outperforms the existing rnn hardware acceleration methods  in terms of compression rate  inference accuracy  execution  time  and energy efficiency.  