introduction    rnns are highly effective in learning sequential data. simple rnns maintain memory through  hidden states that evolve over time. keeping memory in this simple  transient manner has  among  others  two shortcomings. first  memory capacity scales linearly with the dimensionality of recurrent representations  limited for complex tasks. second  it is difficult to support memory at diverse  time scales  particularly challenging for tasks that require information from variably distant past.  numerous differentiable memory mechanisms have been proposed to overcome the limitations of  deep rnns. some of these mechanisms  e.g. attention  have become a universal practice in realworld applications such as machine translation  bahdanau et al.        daniluk et al.        vaswani  et al.       . one type of memory augmentation of rnns includes mechanisms that employ longterm  generic key value storages  graves et al.        weston et al.        kaiser et al.       .  another kind of memory mechanisms  inspired by early work on fast weights  hinton   plaut         schmidhuber         uses auto associative  recurrently adaptive weights for short term memory storage  ba et al.      a  zhang   zhou        schlag   schmidhuber       . associative  memory considerably ameliorates limitations of rnns. first  it liberates memory capacity from the  linear scaling with respect to hidden state dimensions  in the case of auto associative memory like  fast weights  the scaling is quadratic  ba et al.      a . neural turing machine  ntm  style generic  storage can support memory access at arbitrary temporal displacements  whereas fast weight style  memory has its own recurrent dynamics  potentially learnable as well  zhang   zhou       . finally  if architected and parameterized carefully  some associative memory dynamics can also alleviate the vanishing exploding gradient problem  dangovski et al.       .  besides memory augmentation  another entirely distinct approach to overcoming regular rnns   drawbacks is by clever design of recurrent network architecture. the earliest but most effective and  widely adopted one is gated rnn cells such as long short term memory  lstm   hochreiter    schmidhuber       . recent work has proposed ever more complex topologies involving hierarchy  and nesting  e.g. chung et al.         zilly et al.         ruben et al.       .  how do gated rnns such as lstm interact with associative memory mechanisms like fast weights   are they redundant  synergistic  or rather competitive to each other  this remains an open question  since all fast weight networks reported so far are based on regular  instead of gated  rnns. here we  answer this question by revealing a strong synergy between fast weight and lstm.         r elated w ork    our present work builds upon results reported by ba et al.      a   using the same fast weight mechanism. a number of studies subsequent to ba et al.      a   though not applied to gated rnns  pro      posed interesting mechanisms directly extending or closely related to fast weights. weinet  zhang    zhou        parameterized the fast weight update rule and learned it jointly with the network.  gated fast weights  schlag   schmidhuber        used a separate network to produce fast weights  for the main rnn and the entire network was trained end to end. rotational unit of memory  dangovski et al.        is an associative memory mechanism related to yet distinct from fast weights.  its memory matrix is updated with a norm preserving operation between the input and a target.  danihelka et al.        proposed an lstm network augmented by an associative memory that leverages hyperdimensional vector arithmetic for key value storage and retrieval. this is an ntm style   non recurrent memory mechanism and hence different from the fast weight short term memory.         fast w eight lstm    our fast weight lstm  fw lstm  network is defined by the following update equations for the  cell states  hidden state  and fast weight matrix  figure   .     lstm    fw                      i t  wi ui    bi       f     w  u  h  b                  f  f  t      t     ln         f      o t    wo uo   xt  bo  w g ug  bg  g t         it   ft   ot   gt       i t      f t      o t    relu g t    at     at       gt gt   ct   ln  ft ct     it  ht   ot relu  ct                         relu  g t   at gt                figure    fw lstm diagram  here xt   rd   ht   vt   v t   bv   rh   wv   at   rh h and uv   rh d   where v    i  f   o  g    and t indexes time steps. denotes hadamard  element wise  product  ln     layer normalization   and       relu    are the sigmoind and rectified linear function applied element wise. we used  relu    in places of tanh    for efficiency  as it did not make a significant difference in practice.  our construction is identical to the standard lstm cell except for a fast weight memory at queried  by the input activation gt . since gt is a function of both the network output ht   and the new input  xt   this gives the network control over what to associate with each new input.         e xperiments    to study the performance of fw lstm in comparison with the original fast weight rnn  fwrnn  and lstm with layer normalization  ln lstm   we experimented with the associative retrieval task  art  described in ba et al.      a . input sequences are composed of k    key value  pairs followed by a separator     and then a query key  e.g. for k      an example sequence is  a b c d   b whose target answer is  . we experimented with sequence lengths much greater  than the original k      up to k      similar to zhang   zhou        and dangovski et al.       .  we further devised a modified art  mart  that is a re arrangement of input sequences in the  original art. in mart  all keys are presented first  then followed by all values in the corresponding  order  e.g. the mart equivalent of the above training example is abcd      b with target answer  of again  . in contrast to art  where the temporal distance is constantly   between associated pairs  and only average retrieval distance grows with k  in mart temporal distances of both association  and retrieval scales linearly with k. this renders the task more difficult to learn than the original  art  and k can be used to control the difficulty of memory associations.  in all experiments  we augmented the fw lstm cell with a learned     dimensional embedding  for the input xt . additionally  network output at the end of the sequence was processed by another     note that the placement of layer normalizations is slightly different from the method described in the  original paper  ba et al.      b  we find applying layer normalization to the hidden state and input activations  simultaneously  rather than separately as in the original model  worked better for this fast weight architecture.          validation accuracy                   fw lstm h     fw lstm h     fw rnn h     fw rnn h                                           epoch                         figure    validation accuracy during the course of training of mart k   for fw lstms and  fw rnns of    and    hidden units.  hidden layer with     relu units before the final softmax  identical to ba et al.      a . all  models were tuned as described in appendix and run for a minimum of     epochs.  the left half of table   shows performances of ln lstm  fw rnn    and our fw lstm trained on  art with different sequence lengths and numbers of hidden units. fw lstm has a slight advantage  when the number of hidden units is low  but otherwise both the fw rnn and fw lstm solve the  task perfectly.  the right half of table   shows performances of the same models trained on the mart. due to  significantly increased difficulty of the task  we instead show results for sequence lengths k        .  in learning mart  fw lstm outperformed fw rnn and ln lstm by a much greater margin  especially at high memory difficulty  k       and also converged much faster  figure   .  table    test accuracy     of associative retrieval task  art  and modified associative retrieval task   mart  for different sized models and sequence lengths k.  task    hidden  model  ln lstm  h       fw rnn  fw lstm  ln lstm  h       fw rnn  fw lstm  ln lstm  h        fw rnn  fw lstm         art  k     k         .     .     .     .     .     .     .     .      .      .      .      .     .     .      .      .      .      .     mart  k     k         .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .       parameters    k    k    k    k    k    k     k    k     k    c onclusions    we observed that fw lstm trained significantly faster and achieved lower test error in performing the original art. further  in learning the harder mart  when input sequences are longer  we  found that fw lstm could still perform the task highly accurately  while both fw rnn and lnlstm utterly failed. this was true even when fw lstm had fewer trainable parameters. these  results suggest that gated rnns equipped with fast weight memory is a promising combination for  associative learning of sequences.       the parameters   and   used for fw rnn here are different than those in zhang   zhou         resulting  in an improved performance. the values used are listed in appendix.          