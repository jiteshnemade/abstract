introduction    recurrent neural network  rnns  emerge as  very strong learners of sequential data. a famous  result by siegelmann and sontag                and its extension in  siegelmann         demonstrates that an elman rnn  elman        with a  sigmoid activation function  rational weights and  infinite precision states can simulate a turingmachine in real time  making rnns turingcomplete. recently  chen et al        extended  the result to the relu activation function. however  these constructions  a  assume reading the  entire input into the rnn state and only then performing the computation  using unbounded time   and  b  rely on having infinite precision in the  network states. as argued by chen et al          this is not the model of rnn computation used in  nlp applications. instead  rnns are often used  by feeding an input sequence into the rnn one  item at a time  each immediately returning a state     vector that corresponds to a prefix of the sequence  and which can be passed as input for a subsequent feed forward prediction network operating  in constant time. the amount of tape used by a  turing machine under this restriction is linear in  the input length  reducing its power to recognition of context sensitive language. more importantly  computation is often performed on gpus  with   bit floating point computation  and there is  increasing evidence that competitive performance  can be achieved also for quantized networks with    bit weights or fixed point arithmetics  hubara  et al.       . the construction of  siegelmann         implements pushing   into a binary stack by  the operation g   g        . this allows pushing roughly    zeros before reaching the limit of  the   bit floating point precision. finally  rnn  solutions that rely on carefully orchestrated mathematical constructions are unlikely to be found using backpropagation based training.  in this work we restrict ourselves to inputbound recurrent neural networks with finiteprecision states  ibfp rnn   trained using backpropagation. this class of networks is likely to coincide with the networks one can expect to obtain  when training rnns for nlp applications. an  ibfp elman rnn is finite state. but what about  other rnn variants  in particular  we consider the  elman rnn  srnn   elman        with squashing and with relu activations  the long shortterm memory  lstm   hochreiter and schmidhuber        and the gated recurrent unit  gru    cho et al.        chung et al.       .  the common wisdom is that the lstm and  gru introduce additional gating components that  handle the vanishing gradients problem of training srnns  thus stabilizing training and making  it more robust. the lstm and gru are often  considered as almost equivalent variants of each  other.     accepted as a short paper in acl          a  an bn  lstm on a     b         b  an bn cn  lstm on a    b    c        c  an bn  gru on a     b         d  an bn cn  gru on a    b    c       figure    activations   c for lstm and h for gru   for networks trained on an bn and an bn cn . the  lstm has clearly learned to use an explicit counting mechanism  in contrast with the gru.  we show that in the input bound  finiteprecision case  there is a real difference between  the computational capacities of the lstm and the  gru  the lstm can easily perform unbounded  counting  while the gru  and the srnn  cannot. this makes the lstm a variant of a k counter  machine  fischer et al.         while the gru remains finite state. interestingly  the srnn with  relu activation followed by an mlp classifier  also has power similar to a k counter machine.  these results suggest there is a class of formal  languages that can be recognized by lstms but  not by grus. in section    we demonstrate that  for at least two such languages  the lstm manages to learn the desired concept classes using  back propagation  while using the hypothesized  control structure. figure   shows the activations  of    d lstm and gru trained to recognize the  languages an bn and an bn cn . it is clear that the  lstm learned to dedicate specific dimensions for  counting  in contrast to the gru.      is the ability to perform unbounded counting relevant to   real world  nlp tasks  in some cases it might be. for example  processing linearized parse trees  vinyals et al.         choe and charniak        aharoni and goldberg        requires counting brackets and nesting levels. indeed  previous  works that process linearized parse trees report using lstms  and not grus for this purpose. our work here suggests that  this may not be a coincidence.         the rnn models    an rnn is a parameterized function r that takes  as input an input vector xt and a state vector ht    and returns a state vector ht    ht   r xt   ht               the rnn is applied to a sequence x    ...  xn  by starting with an initial vector h   often the    vector  and applying r repeatedly according to  equation    . let   be an input vocabulary  alphabet   and assume a mapping e from every vocabulary item to a vector x  achieved through a  hot encoding  an embedding layer  or some other  means . let rn n  x    ...  xn   denote the state  vector h resulting from the application of r to  the sequence e x     ...  e xn  . an rnn recognizer  or rnn acceptor  has an additional function f mapping states h to     . typically  f is a  log linear classifier or multi layer perceptron. we  say that an rnn recognizes a language l      if f  rn n  w   returns   for all and only words  w   x    ...  xn   l.  elman rnn  srnn  in the elman rnn  elman         also called the simple rnn  srnn    the function r takes the form of an affine transform followed by a tanh nonlinearity      accepted as a short paper in acl            ht   tanh w xt   u ht     b            elman rnns are known to be at least finitestate. siegelmann        proved that the tanh can  be replaced by any other squashing function without sacrificing computational power.  irnn the irnn model  explored by  le et al.          replaces the tanh activation with a nonsquashing relu   ht   max     w xt   u ht     b             the computational power of such rnns  given infinite precision  is explored in  chen et al.       .  gated recurrent unit  gru  in the gru   cho et al.         the function r incorporates a  gating mechanism  taking the form   zt     w z xt   u z ht     bz    r    r           r    rt     w xt   u ht     b    h           h    h    h t   tanh w xt   u  rt   ht       b       ht   zt   ht          zt     h t           where   is the sigmoid function and   is the  hadamard product  element wise product .  long short term memory  lstm  in the  lstm  hochreiter and schmidhuber         r  uses a different gating component configuration          it     w i xt   u i ht     bi             ot     w o xt   u o ht     bo    c    power beyond finite state can be obtained by introducing counters. counting languages and kcounter machines are discussed in depth in  fischer et al.       . when unbounded computation is allowed  a   counter machine has turing  power. however  for computation bound by input length  real time  there is a more interesting  hierarchy. in particular  real time counting languages cut across the traditional chomsky hierarchy  real time k counter machines can recognize  at least one context free language  an bn    and at  least one context sensitive one  an bn cn  . however  they cannot recognize the context free language given by the grammar s   x asa bsb   palindromes .  skcm for our purposes  we consider a simplified variant of k counter machines  skcm .  a counter is a device which can be incremented  by a fixed amount  i nc   decremented by a fixed  amount  d ec  or compared to    c omp   . informally   an skcm is a finite state automaton  extended with k counters  where at each step of  the computation each counter can be incremented   decremented or ignored in an input dependent  way  and state transitions and accept reject decisions can inspect the counters  states using  c omp  . the results for the three languages discussed above hold for the skcm variant as well   with proofs provided in appendix a.         ft     w f xt   u f ht     bf      c    power of counting          c    c t   tanh w xt   u ht     b         ct   ft   ct     it   c t            ht   ot   g ct              where g can be either tanh or the identity.  equivalences the gru and lstm are at least  as strong as the srnn  by setting the gates of the  gru to zt     and rt     we obtain the srnn  computation. similarly by setting the lstm gates  to it     ot      and ft    . this is easily  achieved by setting the matrices w and u to     and the biases b to the  constant  desired gate values.  thus  all the above rnns can recognize finitestate languages.    rnns as skcms    in what follows  we consider the effect on the  state update equations on a single dimension   ht  j . we omit the index  j  for readability.  lstm the lstm acts as an skcm by designating k dimensions of the memory cell ct as  counters. in non counting steps  set it      ft      through equations      . in counting steps  the  counter direction     or     is set in c t  equation      based on the input xt and state ht   . the  counting itself is performed in equation       after setting it   ft    . the counter can be reset  to   by setting it   ft    .  finally  the counter values are exposed through  ht   ot g ct    making it trivial to compare the  counter s value to  .        formal definition is given in appendix a.  some further remarks on the lstm  lstm supports  both increment and decrement in a single dimension. the  counting dimensions in ct are exposed through a function        accepted as a short paper in acl         we note that this implementation of the skcm  operations is achieved by saturating the activations to their boundaries  making it relatively easy  to reach and maintain in practice.  srnn the finite precision srnn cannot designate unbounded counting dimensions.  the srnn update equation is   ht   tanh w x   u ht     b   ht  i    tanh     dx  x  j      wij x j       dh  x    uij ht    j    b i      j      by properly setting u and w  one can get certain dimensions of h to update according to the  value of x  by ht  i    tanh ht    i    wi x   b i  .  however  this counting behavior is within a tanh  activation. theoretically  this means unbounded  counting cannot be achieved without infinite precision. practically  this makes the counting behavior inherently unstable  and bounded to a relatively narrow region. while the network could  adapt to set w to be small enough such that counting works for the needed range seen in training  without overflowing the tanh  attempting to count  to larger n will quickly leave this safe region and  diverge.  irnn finite precision irnns can perform unbounded counting conditioned on input symbols.  this requires representing each counter as two dimensions  and implementing i nc as incrementing  one dimension  d ec as incrementing the other   and c omp   as comparing their difference to  . indeed  appendix a in  chen et al.        provides  concrete irnns for recognizing the languages  an bn and an bn cn . this makes ibfp rnn with  relu activation more powerful than ibfp rnn  with a squashing activation. practically  reluactivated rnns are known to be notoriously hard  g. for both g x    x and g x    tanh x   it is trivial  to do compare  . another operation of interest is comparing two counters  for example  checking the difference between them . this cannot be reliably achieved with g x     tanh x   due to the non linearity and saturation properties  of the tanh function  but is possible in the g x    x case.  lstm can also easily set the value of a counter to   in one  step. the ability to set the counter to   gives slightly more  power for real time recognition  as discussed by fischer et al.        .  relation to known architectural variants  adding peephole connections  gers and schmidhuber        essentially  sets g x    x and allows comparing counters in a stable  way. coupling the input and the forget gates  it       ft     greff et al.        removes the single dimension unbounded  counting ability  as discussed for the gru.    to train because of the exploding gradient problem.  gru finite precision grus cannot implement  unbounded counting on a given dimension. the  tanh in equation     combined with the interpolation  tying zt and     zt   in equation     restricts  the range of values in h to between    and    precluding unbounded counting with finite precision.  practically  the gru can learn to count up to some  bound m seen in training  but will not generalize  well beyond that.  moreover  simulating forms of  counting behavior in equation     require consistently setting the gates zt   rt and the proposal h t  to precise  non saturated values  making it much  harder to find and maintain stable solutions.  summary we show that lstm and irnn  can implement unbounded counting in dedicated  counting dimensions  while the gru and srnn  cannot. this makes the lstm and irnn at least  as strong as skcms  and strictly stronger than the  srnn and the gru.          experimental results    can the lstm indeed learn to behave as a kcounter machine when trained using backpropagation  we show empirically that    . lstms can be trained to recognize an bn and  an bn cn .   . these lstms generalize to much higher n  than seen in the training set  though not infinitely so .   . the trained lstm learn to use the perdimension counting mechanism.   . the gru can also be trained to recognize  an bn and an bn cn   but they do not have clear  counting dimensions  and they generalize to  much smaller n than the lstms  often failing to generalize correctly even for n within  their training domain.       one such mechanism could be to divide a given dimension by k     at each symbol encounter  by setting zt     k  and h t    . note that the inverse operation would not be implementable  and counting down would have to be realized  with a second counter.     one can argue that other counting mechanisms   involving several dimensions are also possible. intuitively   such mechanisms cannot be trained to perform unbounded  counting based on a finite sample as the model has no means  of generalizing the counting behavior to dimensions beyond  those seen in training. we discuss this more in depth in appendix b  where we also prove that an srnn cannot represent a binary counter.     accepted as a short paper in acl          . trained lstm networks outperform trained  gru networks on random test sets for the  languages an bn and an bn cn .  similar empirical observations regarding the  ability of the lstm to learn to recognize an bn and  an bn cn are described also in  gers and schmidhuber       .  we train    dimension    layer lstm and  gru networks to recognize an bn and an bn cn . for  an bn the training samples went up to n       and  for an bn cn up to n     .   results on an bn   the lstm generalizes well up  to n        after which it accumulates a deviation making it reject an bn but recognize an bn    for a while  until the deviation grows.  the gru  does not capture the desired concept even within  its training domain  accepting an bn   for n        and also accepting an bn   for n     . it stops  accepting an bn for n      .  on an bn cn the lstm recognizes well until n       . it then starts accepting also an bn   cn . at  n       it stops accepting an bn cn and switches to  accepting an bn   cn   until at some point the deviation grows. the gru accepts already a  b   c      and stops accepting an bn cn for n     .  figure  a plots the activations of the    dimensions of the an bn  lstm for the input a     b     .  while the lstm misclassifies this example  the  use of the counting mechanism is clear. figure  b plots the activation for the an bn cn lstm  on a    b    c    . here  again  the two counting  dimensions are clearly identified indicating the  lstm learned the canonical   counter solution   although the slightly imprecise counting also  starts to show. in contrast  figures  c and  d  show the state values of the gru networks. the  gru behavior is much less interpretable than the  lstm. in the an bn case  some dimensions may be  performing counting within a bounded range  but  move to erratic behavior at around t         the       implementation in dynet  using the sgd optimizer.  positive examples are generated by sampling n in the desired  range. for negative examples we sample   or   n values independently  and ensuring at least one of them differs from  the others. we dedicate a portion of the examples as the dev  set  and train up to      dev set accuracy.     these fluctuations occur as the networks do not fully saturate their gates  meaning the lstm implements an imperfect counter that accumulates small deviations during computation  e.g.  increasing the counting dimension by  .   but  decreasing only by  .  . despite this  we see that the its solution remains much more robust than that found by the gru    the lstm has learned the essence of the counting based  solution  but its implementation is imprecise.    network starts to misclassify on sequences much  shorter than that . the an bn cn state dynamics are  even less interpretable.  finally  we created      sample test sets for  each of the languages. for an bn we used words  with the form an i bn j where n   rand          and i  j   rand         and for an bn cn we use  words of the form an i bn j cn k where n    rand         and i  j  k   rand       . the  lstm s accuracy was      and   .   on an bn  and an bn cn respectively  as opposed to the gru s    .   and   .    also respectively.  all of this empirically supports our result   showing that ibfp lstms can not only theoretically implement  unbounded  counters  but also  learn to do so in practice  although not perfectly    while ibfp grus do not manage to learn proper  counting behavior  even when allowing floating  point computations.         conclusions    we show that the ibfp lstm can model a realtime skcm  both in theory and in practice. this  makes it more powerful than the ibfp srnn  and the ibfp gru  which cannot implement unbounded counting and are hence restricted to recognizing regular languages. the ibfp irnn can  also perform input dependent counting  and is  thus more powerful than the ibfp srnn.  we note that in addition to theoretical distinctions between architectures  it is important to consider also the practicality of different solutions   how easy it is for a given architecture to discover  and maintain a stable behavior in practice. we  leave further exploration of this question for future work.    acknowledgments  the research leading to the results presented  in this paper is supported by the european  union s seventh framework programme  fp    under grant agreement no.         prime    the israeli science foundation  grant number            and the allen institute for artificial intelligence.     accepted as a short paper in acl         appendix  a    simplified k counter machines    we use a simplified variant of the k counter machines  skcm  defined in  fischer et al.          which has no autonomous states and makes classification decisions based on a combination of its  current state and counter values. this variant consumes input sequences on a symbol by symbol basis  updating at each step its state and its counters  the latter of which may be manipulated by  increment  decrement  zero  or no ops alone  and  observed only by checking equivalence to zero.  to define the transitions of this model its accepting configurations  we will introduce the following notations   notations we define z   zk         k as follows  for every n   zk   for every     i    k  z n i     iff ni      this function masks  a set of integers such that only their zero ness  is observed . for a vector of operations  o                    k   we denote by o n  the pointwise application of the operations to the vector  n   zk   e.g. for o                 o                        .  we now define the model. an skcm is a tuple  m   h   q  qo   k     u  f i containing    .   .   .   .   .    a finite input alphabet    a finite state set q  an initial state q    q  k   n  the number of counters  a state transition function      q             k   q     . a counter update function   u                       k   . a set of accepting masked  configurations  f   q         k  the set of configurations of an skcm is the set  c   q   zk   and the initial configuration is c      q         i.e.  the counters are initiated to zero . the    transitions of an skcm are as follows  given a  configuration ct    q  n   n   zk   and input wt       the next configuration of the skcm is ct         q  wt   z n    u wt   n  .  the language recognized by a k counter machine is the set of words w for which the machine  reaches an accepting configuration   a configuration c    q  n  for which  q  z n     f .  note that while the counters can and are increased to various non zero values  the transition  function   and the accept reject classification of  the configurations observe only their zero ness.  a.     computational power of skcms    we show that the skcm model can recognize  the context free and context sensitive languages  an bn and an bn cn   but not the context free language of palindromes  meaning its computational  power differs from the language classes defined  in the chomsky hierarchy. similar proofs appear  in  fischer et al.        for their variant of the kcounter machine.  an bn   we define the following skcm over the  alphabet  a  b    q    qa   qb   qr    q    qa  k    u a        u b        for any z              qa   a  z    qa      qa   b  z    qb      qb   a  z    qr      qb   b  z    qb    qr   a  z    qr      qr   b  z    qr   . c     qb           .   .   .   .   .    the state qr is a rejecting sink state  and the states  qa and qb keep track of whether the sequence is  currently in the  a  or  b  phase. if an a is seen  after moving to the b phase  the machine moves  to  and stays in  the rejecting state. the counter is  increased on input a and decreased on input b  and  the machine accepts only sequences that reach the  state qb with counter value zero  i.e.  that have increased and decreased the counter an equal number of times  without switching from b to a. it follows easily that this machine recognizes exactly  the language an bn .         we note that in this definition  the counter update function depends only on the input symbol. in practice we see  that the lstm is not limited in this way  and can also update  according to some state input combinations   as can be seen  when it it is taught  for instance  the language an ban we do  not explore this here however  leaving a more complete characterization of the learnable models to future work.     i.e.  counters are observed only by zero ness.    an bn cn   we define the following skcm over  the alphabet  a  b . as its state transition function  ignores the counter values  we use the shorthand    q     for   q     z   for all z           .   . q    qa   qb   qc   qr       accepted as a short paper in acl          . q    qa   . k       . u a              u b               u c              . for any z              qa   a    qa     qa   b    qb     qa   c     qr      qb   a    qr     qb   b    qb     qb   c     qc      qc   a    qr     qc   b    qr     qc   c     qc      qr   a    qr     qr   b    qr     qr   c    qr   . c     qc           by similar reasoning as that for an bn   we see  that this machine recognizes exactly the language  an bn cn . we note that this construction can be extended to build an skcm for any language of the  sort an  an  ...anm   using k   m     counters and  k     states.  palindromes  we prove that no skcm can recognize the language of palindromes defined over  the alphabet  a  b  x  by the grammar s    x asa bsb. the intuition is that in order to correctly recognize this language in an one way setting  one must be able to reach a unique configuration for every possible input sequence over  a  b    requiring an exponential number of reachable  configurations   whereas for any skcm  the number of reachable configurations is always polynomial in the input length.    let m be an skcm with k counters. as its  counters are only manipulated by steps of   or resets  the maximum and minimum values that each  counter can attain on any input w      are   w   and   w   and in particular the total number of  possible values a counter could reach at the end  of input w is   w     . this means that the total  number of possible configurations m could reach  on input of length n is c n     q      n     k .  c n  is polynomial in n  and so there exists a  value m for which the number of input sequences  of length m over  a  b     m   is greater than  c m . it follows by the pigeonhole principle that  there exist two input sequences w     w      a  b m for which m reaches the same configuration. this means that for any suffix w         and in particular for w   x   w    where w    is  the reverse of w    m classifies w    w and w    w      this will hold even if the counter update function can  rely on any state input combination.    identically despite the fact that w    x   w    is in  the language and w    x   w    is not. this means  that m necessarily does not recognize this palindrome language  and ultimately that no such m  exists.  note that this proof can be easily generalized to  any palindrome grammar over   or more characters  with or without a clear  midpoint  marker.    b    impossibility of counting in binary    while we have seen that the srnn and gru cannot allocate individual counting dimensions  the  question remains whether they can count using a  more elaborate mechanism  perhaps over several  dimensions. we show here that one such mechanism   a binary counter   is not implementable  in the srnn.  for the purposes of this discussion  we first define a binary counter in an rnn.  binary interpretation in an rnn with hidden  state values in the range          the binary interpretation of a sequence of dimensions d    ...  dn  of its hidden state is the binary number obtained  by replacing each positive hidden value in the sequence with a     and each negative value with  a    . for instance  the binary interpretation of  the dimensions       in the hidden state vector    .     .    .    .   is      i.e.   .  binary counting we say that the dimensions  d    d    ...  dn in an rnn s hidden state implement  a binary counter in the rnn if  in every transition  their binary interpretation either increases   decreases  resets to    or doesn t change.    a similar pair of definitions can be made for  state values in the range       .  we first note intuitively that an srnn would  not generalize binary counting to a counter with  dimensions beyond those seen in training   as it  would have no reason to learn the  carry  behavior between the untrained dimensions. we prove  further that we cannot reasonably implement such  counters regardless.  we now present a proof sketch that a singlelayer srnn with hidden size n     cannot implement an n dimensional binary counter that will  consistently increase on one of its input symbols.  after this  we will prove that even with helper      we note that the skcms presented here are more restricted in their relation between counter action and transition  but prefer here to give a general definition. our proof  will be relevant even within the restrictions.     accepted as a short paper in acl         dimensions  we cannot implement a counter that  will consistently increase on one input token and  decrease on another   as we might want in order  to classify the language of all words w for which   a  w     b  w .    consistently increasing counter  the proof relies on the linearity of the affine transform w x    u h   b  and the fact that  carry  is a non linear  operation. we work with state values in the range           but the proof can easily be adapted to         by rewriting h as h     .   where h     h    .  is a vector with values in the range     .    .  .  suppose we have a single layer srnn with  hidden size n      such that its entire hidden  state represents a binary counter that increases  every time it receives the input symbol a. we  denote by xa the embedding of a  and assume  w.l.o.g. that the hidden state dimensions are ordered from msb to lsb  e.g. the hidden state  vector            represents the number      .  recall that the binary interpretation of the hidden state relies only on the signs of its values. we  use p and n to denote  some  positive or negative  value  respectively. then the number   can be represented by any state vector  p  p  n .  recall also that the srnn state transition is     p  n  n    b    b     p  n  p   i.e. for some assignment to each p and n   b      p  n  n     p  n  p   and in particular b        .  similarly  for             and             we  obtain  u                n  p  n    b   u               p  p  p    b   i.e.   n  p  n    b    b     p  p  p     or  b     p  p  p     n  p  n   and in particular that  b          leading to a contradiction and proving  that such an srnn cannot exist. the argument  trivially extends to n      by padding from the  msb .  we note that this proof does not extend to the  case where additional  non counting dimensions  are added to the rnn   at least not without further assumptions  such as the assumption that the  counter behave correctly for all values of these dimensions  reachable and unreachable. one may  argue then that  with enough dimensions  it could  be possible to implement a consistently increasing  binary counter on a subset of the srnn s state.    we now show a counting mechanism that cannot  be implemented even with such  helper  dimensions.  ht   tanh w xt   u ht     b   bi directional counter  we show that for n       no srnn can implement an n dimensional biand consider the state vectors            and  nary counter that increases for one token   up   and               which represent   and   respectively.  decreases for another   down . as before  we show  denoting b    w xa   b  we find that the constants  the proof explicitly for n      and note that it can  u and b  must satisfy   be simply expanded to any n     by padding.  assume by contradiction we have such an  tanh u              b      p  n  n   srnn  with m     dimensions  and assume  tanh u               b      p  n  p   w.l.o.g. that a counter is encoded along the first    of these. we use the shorthand  v    v    v  c  as tanh is sign preserving  this simplifies to   to show the values of the counter dimensions  explicitly while abstracting the remaining state  u               p  n  n    b   dimensions  e.g. we write the hidden state  u                p  n  p    b      .    .            as    .    .     c where c          .  noting the linearity of matrix multiplication and  let xup and xdown be the embeddings of  up  that                            we obtain   and  down   and as before denote bup   w xup   b  u              u                   u             and bdown   w xdown   b. then for some reachable state h    r where the counter value is      of course a counter could also be  decreased  by in   e.g.  the state reached on the input sequence  crementing a parallel   negative  counter  and implementing  compare to zero as a comparison between these two. as in up        we find that the constants u  bdown   and  tuitively no rnn could generalize binary counting behavior to dimensions not used in training  this approach could  quickly find both counters outside of their learned range even  on a sequence where the difference between them is never  larger than in training.           by storing processing information on the additional    helper  dimensions        or whichever appropriate sequence if the counter is not  initiated to zero.      accepted as a short paper in acl         bup must satisfy   tanh u h    bup      n  p  n c   tanh u h    bdown      n  n  n c    i.e.   up increases the counter and updates the additional dimensions to the values c    while  down  decreases and updates to c  .  removing the signpreserving function tanh we obtain the constraints  u h    bup    n  p  n sign c     u h    bdown    n  n  n sign c     i.e.  bup   bdown             n  p  n     n  n  n    and in particular  bup   bdown         . now consider a reachable state h  for which the counter  value is  . similarly to before  we now obtain  u h    bup    p  n  n sign c     u h    bdown    n  p  n sign c     from which we get  bup   bdown              p  n  n     n  p  n   and in particular  bup    bdown           a contradiction to the previous  statement. again we conclude that no such srnn  can exist.    