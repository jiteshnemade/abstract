introduction  hyperspectral image  hsi  has attracted considerable attention in the remote sensing community and been widely used  in various areas    . with the rich spectral information in hsi   different land cover categories can potentially be differentiated  precisely.  in recent years  deep learning has been widely used in  various fields  including hsi classification    . convolutional  neural networks  cnns  and residual networks  resnets  have  obtained a successful result for hsi classification         .  recurrent neural networks  rnns  are also applied in hsi  classification    .  because of the ability to extract the spatial contextual  information  cnns and resnets can achieve a high accuracy in the classification task. however  cnns and resnets  consider spectra as orderless vectors in d dimensional feature  space where d represents the number of bands. however   spectra can be seen as orderly and continuing sequences in  the spectral space. in other words  cnns and resnets ignore  the continuity of spectra    .  rnns have proved effective in solving many challenging  problems involving sequential data  such as natural language    the    sources are now available at   https   github.com coderimoe dl for rsis tree master stsspgru    processing  nlp      and prediction of time series    . considering the spectrum as a sequential sequence  the application  of rnns is reasonable as it can take full advantage of the high  spectral resolution characteristics of hsi. however  for a longsequence task  rnns is not as effective as we expected. long  distance dependence  gradient vanish and overfitting are prone  to occur    . even if the long short term memory network   lstm      is used to solve the long distance dependence  problem  rnns is still hard for training and easily overfitting  in a long sequence task.  in previous work   d cnn is applied in hsi classification and obtained a good behavior           . for rnns   convolutional lstm  clstm       also achieved a good  performance in hsi classification     .  d cnns and clstm  consider both spatial contextual information and spectral continuity  which result in a high accuracy. nevertheless  it takes  a long time to train these two models.  in      lstm and its variant  gru       are applied in  his classification  and it is proved that gru has a better  performance in his classification. to solve the problem that  rnns are easily over fitting and difficult for training        proposed band group lstm  which can effectively make  training easier by reducing the number of timestep in lstm.  in this study  a shorten spatial spectral rnn with parallelgru  st ss pgru  is proposed. this study contributes to the  literature in   major respects       a shorten rnn with gru is applied in his classification. the model is more efficient and easier for  training than band by band rnn. by combining converlusion layer  an advanced model shorten spatialspectral rnn with gru is proposed. the model  considers not only spectral but also spatial feature   which leads to a better performance.      an architecture named parallel gru is proposed  and the model with this architecture has a better  performance and is more robust.  the remainder of this paper is organized as follows. in the  methodology section  firstly the structure of traditional rnn   lstm and gru are introduced and then the architecture  of the proposed models are described. in the experimental  section  the network setup  the experimental results  and the  comparison of different models are provided. finally  the  conclusion section concludes the paper.     ii. m ethodology  a. recurrent neural networks  rnn   different from artificial neural network  ann   rnn      a  neural network with recurrent unit  has a better performance in  solving many challenging problems involving sequential data  analysis. the state of each time step of the recurrent unit is not  only related to the input of the current step  but also related to  the state of the previous step. thus  the state of the preceding  step can effectively influence the next step.         given a sequence sample x   x      x      ...  x m    in  which x t  is the data at tth timestep. for the tth recurrent  unit  its hidden state can be described as      h     t     t   h             t      t   h h   x   t      where h    is the initial state of the recurrent unit  h is a  nonlinear function. normally  h    is set as a zero vector.  optionally  in tth timestep  the recurrent unit may have  an output y t  . for some task  the rnn model will finally         have an output vector y   y      y      ...  y m    while for  classification tasks  only one output is needed. generally  the  last output is adopted   y t    y h t              the recurrent unit in a traditional rnn is shown in fig.   . in the traditional rnn model  the update rule of the  recurrent hidden state and output in eq.     and     is usually  implemented as follows    t       h h     t       x       wh x     t      t         uh h      bh       y h t      wy h t    by      by using long short term memory  lstm        the problems have been solved. as fig.   shows  lstm contains a  forget gate  an input gate and an output gate.  gate  structure  is actually a logistic regression model so that part of the  information is filtered selectively  while the rest is reserved  and passes through the gate. lstm can simulate the process  of forgetting and memory and calculate the probability of  forgetting and memory  so information flow could be preserved  in long distance propagation. the structure of lstm can be  described as   f  t      wf x t    uf h t      bf              i t      wi x t    ui h t      bi              o     t         wo x     t      t         uo h      bo              c  t    tanh wc x t    uc h t      bc              c t    i t    c  t    f  t    c t                 t     h     o     t      t       tanh c               where eq.          and     represent forget gate  input gate and  output gate. wf   wi   wo   wc   uf   ui   uo and uc are the  weight matrices. bf   bi   bo and bc are the bias vectors.    refers to sigmoid function and tanh refers to the hyperbolic  tangent function     x                 e    x     tanh x       ex   e    x      ex   e    x                               where wh   uh and wy are the weight matrices. bh and by  are the bias vectors  and   is an activation function  such as  the sigmoid function or the hyperbolic tangent function.    fig.  . graphic model of lstm.    fig.  . graphic model of traditional recurrent unit.    b. long short term memory  lstm   the traditional rnn has the problem of long distance dependence    . the rnn has the capability to connect different  timesteps related information. however  when the sequence  is too long  the rnn becomes unable to connect related  information as the distance increases  because the information  losses when propagating through multi time step recurrent  units.    c. gated recurrent unit  gru   over the years  there have been many variants of lstm  but  there is no evidence to show that there is not a superior variant.  any variant may have advantages in a particular problem     .  gru      is a variant of lstm. with fewer parameters  it  is much easier for training than lstm  and usually achieves  the same performance as lstm in some tasks     . it is  considered that using gru in a hsi classification task is more  appropriate than using lstm    .  the main difference between lstm and gru is that an  update gate and a reset gate are adopted in gru  instead of     using a forget gate  an input gate and an output gate. the  structure of the gru is shown in fig.    which can be defined  as follows   z t      wz x t    uz h t      bz               r t      wr x t    ur h t      br               h  t    tanh wh x t    uh  r t    h t        bh     h t         z t   h t      z t  h  t                     where eq.      and      represent update gate and reset gate.  wz   wr   wh   uz   ur and uh are the weight matrices. bz    br and bh are the bias vectors.    fig.  . st ss gru      the first row shows a flowchart of the network. fc  refers to fully connected layer and conv refers to convolutional layers.      the second row illustrates the shapes of input and output tensors of each layer  and their connection.     n is the number of filters in the  d convolutional  layer  d is the number of bands in the input image  t is the number of gru  timestep  and h is the number of neurons in hidden layer in a gru. for the  pavia university dataset  d      and in the experiment the hyperparameters  are set as  n     t    h    .    fig.  . graphic model of gru.    d. the proposed model     shorten spatial spectral rnn with gru st ss gru   a  shorten spatial spectral rnn with gru  st ss gru  model  for hsi classification is shown in fig.  . for each pixel  a  square subgraph composed of     pixels centered on it is  used as a training sample.  the first part of st ss gru is actually a  d convolutional  layer but both the depth and stride of the kernels are  .  three different convolution kernels              were used to  convolve different bands. the output of this part is a sequence  with the same length as the original input. the output sequence  is a  spectra  with the spatial contextual feature. every timestep  of the sequence is a feature vector.  the second part is a shorten rnn with gru  st gru . the  structure of st gru is shown in fig.  . the  d converlusion  layer before gru is used to reduce the number of timesteps  so that the network is easier for training.     parallel gru architecture  in order to make the model  more robust  a parallel gru  pgru  architecture is proposed.  the architecture of shorten parallel gru  st pgru  is shown  in fig.  . the architecture is actually a combination of several  gru units. the output of the architecture is the summation  of every unit.  the shorten spatial spectral rnn with parallel gru  stss pgru  is similar to st ss gru  except that st gru is  replaced by st ss pgru.    fig.  . st gru and st pgru      the first two rows show the architecture  of st gru and st pgru  conv refers to convolutional layers.     the third  row illustrates the shapes of input and output tensors of each layer and  their connection.     d is the number of bands in the input image  n is  the dimension of the vector in each band of input  m is the number of filters  in the  d convolutional layer  t is the number of gru timestep  and h is the  number of neurons in hidden layer in a gru. l and s  which are determined  by t  refer to the size and stride of filters in the  d convolutional layer. for  the pavia university dataset  d      and in the experiment the hyperparameter  is set as  n m     t    h    .    iii. e xperiment  a. data  in the experiment  two hsi datasets  including the pavia  university and indian pines  are used to evaluate the performance of the proposed model.  the pavia university dataset was acquired by the reflective optics system imaging spectrometer rosis  sensor over  pavia  northern italy in     . the corrected data  with a spatial  resolution of  .  m per pixel  contains     spectral bands  ranging from  .   to  .    m. the image  with          pixels  is differentiated into   ground truth classes. table i  provides information about all classes of the dataset with their  corresponding training and test sample.     the indian pines dataset was acquired by the tairborne  visible infrared imaging spectrometer  aviris  sensor over  the indian pines test site in north western indiana in     .  the corrected data with a moderate spatial resolution of   m  contains     spectral bands ranging from  .  to  .   m. the  image consists of         pixels  which are differentiated  into    ground truth classes. table ii provides information  about all classes of the dataset with their corresponding  training and test sample.  table i  n umber of t raining and t est s amples u sed in the pavia  u niversity dataset  no.                               class name  asphalt  meadows  gravel  trees  metal sheet  bare soil  bitumen  bricks  shadows  total    training samples                                                       class name  alfalfa  corn notill  corn min  corn  grass pasture  grass trees  grass pasture mowed  hay windrowed  oats  soybean notill  soybean mintill  soybean clean  wheat  woods  building grass trees  stone stell towers  total     b  ground truth     c  gru   .         e  st gru   .         f  st ss gru   .         g  st ss pgru   .        test samples                                                                table ii  n umber of t raining and t est s amples u sed in the i ndian p ines  dataset  no.                                                            a  false color map    training samples                                                                                     test samples                                                                                      b. result  table iii and iv list the results obtained by the experiment   and fig.   and   show the classification maps on the pavia  university dataset and the indian pines dataset. note that  the accuracies list in table iii and iv are overall accuracies   oa  along with the standard deviation  from    independent  runs on each dataset. the experiment is implemented with an  intel i      k  .  ghz processor with   gb of ram and  an nvidia gtx    ti graphic card under python .  with  tensorflow . . .  first of all  for all the datasets  gru outperforms lstm.  in addition  it is observed that lstm is difficult to converge    fig.  . the classification maps of the pavia university dataset.     a  false color map     b  ground truth     c  gru   .         e  st gru   .         f  st ss gru   .         g  st ss pgru   .        fig.  . the classification maps of the indian pines dataset.    in the experiment  while gru is not. thus  it is reasonable to  indicate that gru is a better choice for a hsi classification  task.  furthermore  it is apparent that st gru increases the accuracy significantly by  .    and  .    in the pavia university dataset and the indian pines correspondingly. with  converlusion layers  st ss gru has a better than st gru. the  accuracy of st ss gru is  .    and  .    higher than that  in st gru. after parallel gru is adopted  the model gains  the best performance in this experiment. the accuracy of st      ss pgru is  .    and  .    higher than st ss gru. what  is more  the standard deviation of st ss pgru is smaller than  other models  which indicate that st ss pgru is more robust.  comparing the processing time of different methods  stgru is significantly faster in training than band by band  gru. st ss gru and st ss pgru are as slow as lstm and  gru in training  but they have higher accuracies than lstm  and gru.  table iii  c lassification accuracies and t raining t ime for the pavia  u niversity dataset  model  overall accuracy  training time  s   lstm    .    .        .      .    .        .    gru  st gru    .    .      .     st ss gru    .    .        .      .    .         .    st ss pgru    the best performance in each column is shown in bold.    table iv  c lassification accuracies and t raining t ime for the i ndian  p ines dataset  model  overall accuracy  training time  s   lstm    .    .        .    gru    .    .        .    st gru    .    .      .     st ss gru    .    .        .      .    .         .    st ss pgru    the best performance in each column is shown in bold.    iv. c onclusion  in the study  a st ss pgru model is proposed for hsi  classification. what is more  an architecture named parallelgru is proposed to promote the performance and robustness.  then an experiment is conducted to compare the performance  of different models. from the experiment  it is confirmed that  gru performs better than lstm in hsi classification task.  moreover  it is apparent that the proposed models are a lot  more accurate  more robust and faster than the traditional  gru network. specifically  st gru effectively reduced the  training time and promoted the accuracy. st ss gru needs  more time for training but gains a better performance than stgru. the proposed architecture parallel gru also provided  a satisfactory result in the experiment.  