introduction    recurrent neural networks  rumelhart et al.        werbos        williams   zipser        are  a powerful class of neural networks that are naturally suited to modeling sequential data. for example  in recent years alone  rnns have achieved state of the art performance on tasks as diverse  as machine translation  wu et al.         speech recognition  miao et al.         generative image  modeling  oord et al.         and surgical activity recognition  dipietro et al.       .  these successes  and the vast majority of other rnn successes  rely on a mechanism introduced by  long short term memory  hochreiter   schmidhuber        gers et al.         which was designed  to alleviate the so called vanishing gradient problem  hochreiter        bengio et al.       . the  problem is that gradient contributions from events at time t     to a loss at time t diminish exponentially fast with     thus making it extremely difficult to learn from distant events  see figures   and   .  lstm alleviates the problem using nearly additive connections between adjacent states  which help  push the base of the exponential decay toward  . however lstm in no way solves the problem  and  in many cases still fails to learn long term dependencies  see  e.g.   arjovsky et al.        .  narx  rnns  lin et al.        offer an orthogonal mechanism for dealing with the vanishing gradient problem  by allowing direct connections  or delays  from the distant past. however narx  rnns have received much less attention in literature than lstm  which we believe is for two reasons. first  as previously introduced  narx rnns have only a small effect on vanishing gradients   as they reduce the exponent of the decay by only a factor of nd   the number of delays. second  as  previously introduced  narx rnns are extremely inefficient  as both parameter counts and computation counts grow by the same factor nd .  in this paper  we introduce mixed history rnns  mist rnns   a novel narx rnn architecture  which    exhibits superior vanishing gradient properties in comparison to lstm and previouslyproposed narx rnns     improves performance substantially over lstm on tasks requiring very  long term dependencies  and    remains efficient in parameters and computation  requiring even          work done while visiting technische universita t mu nchen  munich  germany.  the acronym narx stems from nonlinear autoregressive models with exogeneous inputs.          workshop track   iclr          a  simple rnns     b  lstm     c  simple narx rnns  nd          d  narx rnns with exponential delays  nd         figure    direct connections  dashed  to a single time step t and example shortest paths  solid  from  time t     to time t for various architectures. typical rnn connections  blue  impede gradient flow  through matrix multiplications and nonlinearities. lstm facilitates gradient flow through additional  paths between adjacent time steps with less resistance  orange . narx rnns facilitate gradient  flow through additional paths that span multiple time steps.  fewer than lstm for a fixed number of hidden units. importantly  mist rnns reduce the decay s  exponent by a factor of  nd      see figure  .         background and r elated w ork    recurrent neural networks  as commonly described in literature  take on the general form  ht   f  ht     xt                which compute a new state ht in terms of the previous state ht     the current input xt   and some  parameters    which are shared over time .  one of the earliest variants  now known to be especially vulnerable to the vanishing gradient problem  is that of simple rnns  elman         described by  ht   tanh wh ht     wx xt   b            in this equation and elsewhere in this paper  all weight matrices w and biases b collectively form  the parameters   to be learned  and tanh is always written explicitly.  long short term memory  hochreiter   schmidhuber        gers et al.         the most widelyused rnn architecture to date  was specifically introduced to address the vanishing gradient problem. the term lstm is often overloaded  we refer to the variant with forget gates and without  peephole connections  which performs similarly to more complex variants  greff et al.          ft  it  ot  c t  ct  ht        wf h ht     wf x xt   bf        wih ht     wix xt   bi        woh ht     wox xt   bo      tanh wch ht     wcx xt   bc      f t ct     it c t    ot tanh ct                                      here      denotes the element wise sigmoid function and denotes element wise multiplication. f t    it   and ot are referred as the forget  input  and output gates  which can be interpreted as controlling  how much we reset  write to  and read from the memory cell ct . lstm has better gradient properties  than simple rnns  see figure    because of the mechanism in equation    which introduces a path  between ct   and ct which is modulated only by the forget gate. we also remark that gated recurrent  units  cho et al.        alleviate the vanishing gradient problem using this exact same idea.  narx rnns  lin et al.        also address the vanishing gradient problem  but using a mechanism  that is orthogonal to  and possibly complementary to  that of lstm. this is done by allowing delays         workshop track   iclr         gradient norm               simple rnn  lstm  clockwork rnn  mist rnn                                                                                           lt  figure    gradient norms k  h  k averaged over a batch of examples during permuted mnist  t    training. unlike clockwork rnns and mist rnns  simple rnns and lstm capture essentially  no learning signal from inputs that are far from the loss.    or direct connections from the past. narx rnns in their general form are described by  ht   f  ht     ht     . . .   xt   xt     . . .       but literature typically assumes the specific variant explored in  lin et al.           n        d  x  ht   tanh  wd ht d   wx xt   b                   d      which we refer to as simple narx rnns.  note that simple narx rnns require approximately nd as much computation and nd as many  parameters as their simple rnn counterpart  with nd       which greatly hinders their applicability  in practice. to our knowledge  this drawback holds for all narx rnn variants before mist  rnns. for example  in  soltani   jiang         higher order recurrent neural networks  hornns   are defined precisely as simple narx rnns  and every variant in the paper suffers from this exact  same problem. and  in  zhang et al.         a simple narx rnn architecture is defined that is  limited to having precisely two delays with non zero weights. this way  at the expense of having  fewer  longer paths to the past  parameter and computation counts are only doubled.  the previous work that is most similar to ours is that of clockwork rnns  koutnik et al.          which split weights and hidden units into partitions  each with a distinct period. when it s not a  partition s time to tick  its hidden units are passed through unchanged  and so clockwork rnns  in some ways mimic narx rnns. however clockwork rnns differ in two key ways. first   clockwork rnns sever high frequency to low frequency paths  thus making it difficult to learn  long term behavior that must be detected at high frequency  for example  learning to depend on  quick motions from the past for activity recognition . second  clockwork rnns require hidden  units to be partitioned a priori  which in practice is difficult to do in any meaningful way. narx  rnns  and in particular mist rnns  suffer from neither of these drawbacks.  many other approaches have also been proposed to capture long term dependencies. notable approaches include maintaining a generative model over inputs and learning to process only unexpected inputs  schmidhuber         operating explicitly at multiple time scales  el hihi   bengio          hessian free optimization  martens   sutskever         using associative or explicit memory  plate        danihelka et al.        graves et al.        weston et al.         and initializing or  restricting weight matrices to be orthogonal  arjovsky et al.        henaff et al.       .         t he vanishing g radient p roblem in the c ontext of narx rnn s    in  bengio et al.        pascanu et al.         gradient decompositions and sufficient conditions for  vanishing gradients are presented for simple rnns  which contain one path between times t     and  t. here  we use the chain rule for ordered derivatives  werbos        to connect gradient components to paths and edges  which in turn provides a simple extension of the results from  bengio et al.         pascanu et al.        to general narx rnns. we remark that we rely on slightly overloaded  notation for clarity  as otherwise notation becomes cumbersome  see  werbos        .        workshop track   iclr          f  we begin by disambiguating notation  as the symbol  x  is routinely overloaded in literature. con    x   sider the jacobian of f  x  u x   with respect to x. we let   xf denote  f x    a collection of full   f  x u    f  derivatives  and we let  x denote  x   a collection of partial derivatives. this lets us write the      f     x   f     u  ordinary chain rule as   xf    x   x    u  x . note that this notation is consistent with  werbos                but is the exact opposite of the convention used in  pascanu et al.       .     .     t he c hain rule for o rdered d erivatives    consider an ordered system of n vectors v    v    . . .   vn   where each is a function of all previous   vi   vi  vi     vi     . . .   v          i n    the chain rule for ordered derivatives expresses the full derivatives  the full derivatives that relate vi to all previous vk    x     vi  vk      vi       j i   vj   vk  vj           vi   vj          for any j   i in terms of            i k j     .     g radient d ecomposition for g eneral narx rnn s    consider narx rnns in their general form  equation     which we remark encompasses other  rnns such as lstm as special cases. also  for simplicity  consider the situation that is most often  encountered in practice  where the loss at time t is defined in terms of the current state ht and its  own parameters   l  which are independent of   .  lt   fl  ht     l               this is in not necessary  but we proceed this way to make the connection with rnns in practice  evident. for example  fl may be a linear transformation with parameters   l followed by squarederror loss.  then the jacobian  or transposed gradient  with respect to   can be written as   fl     ht      lt          ht                     fl     l  because the additional term     is  . now  by letting v       v    x    v    x    and so on in  l     equations    and     we immediately obtain  t      x     ht  ht        ht          ht             because all of the partials     xt                  are  .    equations    and    extend equations   and   of  pascanu et al.        to general narx rnns      which encompass simple rnns  lstm  etc.  as special cases. this decomposition breaks     ht into      ht  its temporal components  making it clear that the spectral norm of  h  plays a major role in how  t         t           ht  ht   affects the final gradient    lt . in particular  if the norm of  h  is extremely small  then  t    ht   has only a negligible effect on the final gradient  which in turn makes it extremely difficult to  learn from events that occurred at t     .     .     c onnecting g radient c omponents to paths and e dges    equations    and     along with the chain rule for ordered derivatives  let us connect gradient  components to paths and edges  which is useful for a  gaining insights into various architectures and  b  solidifying intuitions from backpropagation through time which suggest that short paths between  t     and t facilitate gradient flow. here we provide an overview of the main idea  please see the  appendix for a full derivation.         ht  in equation     we obtain a sum  by applying the chain rule for ordered derivatives to expand  h  t    over   terms. however each term involves a partial derivative between ht and a prior hidden state           workshop track   iclr         and thus all of these terms are   with the exception of those states that share an edge with ht . now   for each term  we can repeat this process. this then yields non zero terms only for hidden states  which can be connected to ht through two edges. we can then continue to apply the chain rule for  ordered derivatives repeatedly  until only partial derivatives remain.  upon completion  we have a sum over gradient components  with each component corresponding  to exactly one path from t     to t and being a product over its path s edges. the spectral norm  corresponding to any particular path  t       t    t             t  can then bounded as   ht   ht    ht   ht                   ne         ht         ht     ht         ht    where   is the maximum spectral norm of any factor and ne is the number of edges on the path.  terms with       diminish exponentially fast  and when all        shortest paths dominate  .         m ixed h istory r ecurrent n eural n etworks    viewing gradient components as paths  with each component being a product with one factor per  edge along the path  gives us useful insight into various rnn architectures. when relating a loss  at time t to events at time t       simple rnns and lstm contain shortest paths of length     while  simple narx rnns contain shortest paths of length    nd   where nd is the number of delays.  one can envision many narx rnn architectures with non contiguous delays that reduce these  shortest paths further. in this section we introduce one such architecture using base   exponential  delays. in this case  for all      nd      shortest paths exist with only log    edges  and for       nd      shortest paths exist with only     nd    edges  see figure   . finally we must avoid the  parameter and computation growth of simple narx rnns. we achieve this by sharing weights  over delays  instead using an attention like mechanism  bahdanau et al.        over delays and a  reset mechanism from gated recurrent units  cho et al.       .  the proposed architecture  which we call mixed history rnns  mist rnns   is described by  at   softmax wah ht     wax xt   ba          rt     wrh ht     wrx xt   br                   nx      d  ht   tanh wh rt  ati ht  i   wx xt   b        i      here  at is a learned vector of nd convex combination coefficients and rt is a reset gate. at each time  step  a convex combination of delayed states is formed according to at   units of this combination  are reset according to rt   and finally the typical linear layer and nonlinearity are applied.         e xperiments    here we compare mist rnns to simple rnns  lstm  and clockwork rnns. we begin with  the sequential permuted mnist task and the copy problem  synthetic tasks that were introduced to  explicitly test rnns for their ability to learn long term dependencies  hochreiter   schmidhuber         martens   sutskever        le et al.        arjovsky et al.        henaff et al.        danihelka et al.       . next we move on to   tasks for which it is plausible that very long term dependencies play a role  recognizing surgical maneuvers from robot kinematics  recognizing phonemes  from speech  and classifying activities from smartphone motion data. we note that for all architectures involved  many variations can be applied  variational dropout  layer normalization  zoneout   etc. . we keep experiments manageable by comparing architectures without such variations.   .     s equential p mnist c lassification    the sequential mnist task  le et al.        consists of classifying   x   mnist images  lecun  et al.        as one of    digits  by scanning pixel by pixel   left to right  top to bottom   and emitting     we remark that it is also possible for gradient contributions to explode exponentially fast  however this  problem can be remedied in practice with gradient clipping. none of the architectures discussed in this work   including lstm  address the exploding gradient problem.          workshop track   iclr         table    test set error rates for sequential pmnist classification. hidden unit counts  nh   vary to  match parameter counts with lstm  approx.        parameters   except models marked with     which have more parameters .    denotes the optimal learning rate according to validation error.  nh    log         error rate        simple rnns  lstm  clockwork rnns  mist rnns                            .      .      .      .      .      .      .      .        .     .     .     .     .     .    .     .     lstm   lstm   mist rnns                        .      .      .      .      .      .       .     .    .     .    .     .     a label upon completion. sequential pmnist  le et al.        is a challenging variant where a  random permutation of pixels is chosen and applied to all images before classification. lstm with      hidden units is used as a baseline  with hidden unit counts for other architectures chosen to  match the number of parameters. means and standard deviations are computed using the top    randomized trials out of     ranked according to performance on the validation set   with random  learning rates and initializations. additional experimental details can be found in the appendix.  test error rates are shown in table  . here  mist rnns outperform simple rnns  lstm  and  clockwork rnns by a large margin. we remark that our lstm error rates are consistent with best  previously reported values  such as the error rates of  .   in  cooijmans et al.        and     in   arjovsky et al.         which also use     hidden units. one may also wonder if the difference  in performance is due to hidden unit counts. to test this we also increased the lstm hidden unit  count to      to match mist rnns   and continued to increase the capacity of each model further.  mist rnns significantly outperform lstm in all cases.  we also used this task to visualize gradient magnitudes as a function of    the distance from the loss  which occurs at time t       . gradient norms for all methods were averaged over a batch of      random examples early in training  see figure  . here we can see that simple rnns and lstm  capture essentially no learning signal from steps that are far from the loss. to validate this claim  further  we repeated the     unit lstm and mist rnn experiments  but using only the last      permuted pixels  rather than all     . lstm performance remains the same   .   error  within    standard deviation  whereas mist rnn performance drops by    standard deviations   .   error .   .     t he c opy p roblem    simple rnn  lstm  clockwork rnn  mist rnn     .      error rate    error rate    the copy problem is a synthetic task that explicitly challenges a network to store and reproduce  information from the past. our setup follows  arjovsky et al.         which is in turn based on     .     .                                                .       .       .                               .       .       .                                                                                              iteration    error rate    error rate    iteration                       iteration     .       .       .                                      iteration    figure    validation curves for the copy problem with copy delays of     upper left        upper  right        lower left   and      lower right .        workshop track   iclr         table    error rates for surgical maneuver recognition. hyperparameters were copied from  dipietro  et al.         where they were tuned for lstm with peephole connections. our lstm does not  include peephole connections  see section   .  nh    error rate        lstm  dipietro et al.                     .     .     simple rnns  lstm  clockwork rnns  mist rnns                                .     .     .     .     .     .     .     .      hochreiter   schmidhuber       . an input sequence begins with l relevant symbols to be copied   is followed by a delay of d     special blank symbols and   special go symbol  and ends with l  additional blank symbols. the corresponding target sequence begins with l   d blank symbols  and ends with a copy of the relevant symbols from the inputs  in the same order . we run experiments with copy delays of d                 and    . lstm with     hidden units is used as a  baseline  with hidden unit counts for other architectures chosen to match the number of parameters.  additional experimental details can be found in the appendix.  results are shown in figure    showing validation curves of the top   randomized trials out of      with random learning rates and initializations. with a short copy delay of d       we can see  that all methods other than clockwork rnns can solve the task in a reasonable amount of time.  however  as the copy delay d is increased  we can see that simple rnns and lstm become unable  to learn a solution  whereas mist rnns are relatively unaffected. we also note that our lstm  results are consistent with those in  arjovsky et al.        henaff et al.       .  note that clockwork rnns are expected to fail for large delays  for example  the second symbol can  only be seen by the highest frequency partition  so learning to copy this symbol will fail for precisely  the same reason that simple rnns fail . however  here they also fail for short delays  which is  surprising because the high speed partition resembles a simple rnn. we hypothesized that this  failure is due to hidden unit counts   parameter counts  here  the high frequency partition is allocated  only              hidden units. to test this hypothesis  we reran the clockwork rnn experiments  with      hidden units  so that     are allocated to the high frequency partition. indeed  under this  configuration  with   x as many parameters   clockwork rnns do solve the task for a delay of  d      and fail to solve the task for all higher delays  thus behaving like simple rnns.   .     s urgical m aneuver r ecognition    here we consider the task of online surgical maneuver recognition using the mistic sl dataset   gao et al.        dipietro et al.       . maneuvers are fairly long  high level activities  examples  include suture throw and knot tying. the dataset was collected using a da vinci  and the goal is to  map robot kinematics over time  e.g.  x  y  z  to gestures over time  which are densely labeled as    of   maneuvers on a per frame basis . we follow  dipietro et al.         which achieves state ofthe art performance on this task  as closely as possible  using the same kinematic inputs  test setup   and hyperparameters  details can be found in the original work or in the appendix. the primary  difference is that we replace their lstm layer with our layers. results are shown in table  . here  mist rnns match lstm performance  with half the number of parameters .   .     p honeme r ecognition    here we consider the task of online framewise phoneme recognition using the timit corpus  garofolo et al.       . each frame is originally labeled as   of    phonemes. we follow common practice  and collapse these into a smaller set of    phonemes  lee   hon         and we include glottal stops  to yield    classes in total. we follow  greff et al.        for data preprocessing and  halberstadt         for training  validation  and test splits. lstm with     hidden units is used as a baseline   with hidden unit counts for other architectures chosen to match the number of parameters. means  and standard deviations are computed using the top   randomized trials out of     ranked according        workshop track   iclr         table    test set error rates for timit phoneme recognition. hidden unit counts  nh   vary to match  parameter counts with lstm  approx.        parameters .    denotes the optimal learning rate  according to validation error.    simple rnns  lstm  clockwork rnns  mist rnns    nh    log         error rate                                .      .      .      .      .      .      .      .        .     .     .     .     .     .     .     .     table    test set error rates for mobiact activity classification. hidden unit counts  nh   vary to  match parameter counts with lstm  approx.        parameters   with the exception of lstm    approx.        parameters .    denotes the optimal learning rate according to validation error.    simple rnns  lstm  lstm   clockwork rnns  mist rnns    nh    log         error rate                                     .      .      .      .      .      .      .      .      .      .        .     .     .     .     .     .     .     .     .     .     to performance on the validation set   with random learning rates and initializations. other experimental details can be found in the appendix. table   shows that lstm and mist rnns perform  nearly identically  which both outperform simple rnns and clockwork rnns.   .     activity r ecognition from s martphones    here we consider the task of sequence classification from smartphones using the mobiact  v .    dataset  chatzaki et al.       . the goal is to classify each sequence as jogging  running  sitting  down  etc.  using smartphone motion data over time. approximately       sequences were collected  from    different subjects. we use the first    subjects for training  the next    for validation  and  the final    for testing. means and standard deviations are computed using the top   randomized  trials out of     ranked according to performance on the validation set   with random learning rates  and initializations. other experimental details can be found in the appendix. results are shown in  table  . here  mist rnns outperform all other methods  including lstm and lstm    a variant  with the same number of hidden units and twice as many parameters.         c onclusions and f uture w ork    in this work we analyzed narx rnns and introduced a variant which we call mist rnns  which     exhibit superior vanishing gradient properties in comparison to lstm and previously proposed  narx rnns     improve performance substantially over lstm on tasks requiring very long term  dependencies  and    require even fewer parameters and computation than lstm. one obvious  direction for future work is the exploration of other narx rnn architectures with non contiguous  delays. in addition  many recent techniques that have focused on lstm are immediately transferable  to narx rnns  such as variational dropout  gal   ghahramani         layer normalization  ba  et al.         and zoneout  krueger et al.         and it will be interesting to see if such enhancements  can improve mist rnn performance further.  acknowledgments  this work was supported by the technische universita t mu nchen   institute for advanced study   funded by the german excellence initiative and the european union seventh framework pro      workshop track   iclr         gramme under grant agreement         and by the national institutes of health  grant r  de      .    r eferences  martin arjovsky  shah amar  and yoshua bengio. unitary evolution recurrent neural networks.  international conference on machine learning  icml       .  jimmy lei ba  jamie ryan kiros  and geoffrey e hinton. layer normalization. arxiv preprint  arxiv     .           .  dzmitry bahdanau  kyunghyun cho  and yoshua bengio. neural machine translation by jointly  learning to align and translate. iclr      .  yoshua bengio  patrice simard  and paolo frasconi. learning long term dependencies with gradient  descent is difficult. ieee transactions on neural networks                    .  charikleia chatzaki  matthew pediaditis  george vavoulas  and manolis tsiknakis. human daily  activity and fall recognition using a smartphone s acceleration sensor. international conference  on information and communication technologies for ageing well and e health      .  kyunghyun cho  bart van merrie nboer  c ag lar gu lc ehre  dzmitry bahdanau  fethi bougares  holger schwenk  and yoshua bengio. learning phrase representations using rnn encoder decoder  for statistical machine translation. emnlp      .  tim cooijmans  nicolas ballas  ce sar laurent  and aaron courville. recurrent batch normalization.  arxiv preprint arxiv     .           .  ivo danihelka  greg wayne  benigno uria  nal kalchbrenner  and alex graves. associative long  short term memory. international conference on machine learning  icml       .  robert dipietro  colin lea  anand malpani  narges ahmidi  s swaroop vedula  gyusung i lee   mija r lee  and gregory d hager. recognizing surgical activities with recurrent neural networks.  international conference on medical image computing and computer assisted intervention  pp.               .  salah el hihi and yoshua bengio. hierarchical recurrent neural networks for long term dependencies. advances in neural information processing systems  nips       .  jeffrey l elman. finding structure in time. cognitive science                     .  yarin gal and zoubin ghahramani. a theoretically grounded application of dropout in recurrent  neural networks. in advances in neural information processing systems  pp.                .  yixin gao  s. swaroop vedula  carol e. reiley  narges ahmidi  balakrishnan varadarajan   henry c. lin  lingling tao  luca zappella  benjamn bejar  david d. yuh  chi chiung grace  chen  rene vidal  sanjeev khudanpur  and gregory d. hager. language of surgery  a surgical gesture dataset for human motion modeling. modeling and monitoring of computer assisted  interventions  m cai            .  john s garofolo  lori f lamel  william m fisher  jonathon g fiscus  and david s pallett. darpa  timit acoustic phonetic continous speech corpus cd rom. nist speech disc    . . nasa  sti recon technical report      .  felix a gers  ju rgen schmidhuber  and fred cummins. learning to forget  continual prediction  with lstm. neural computation                        .  alex graves  greg wayne  and ivo danihelka.  arxiv     .          .    neural turing machines.    arxiv preprint    k. greff  r. k. srivastava  j. koutn  k  b. r. steunebrink  and j. schmidhuber. lstm  a search  space odyssey. ieee transactions on neural networks and learning systems      .        workshop track   iclr         andrew k halberstadt. heterogeneous acoustic measurements and multiple classifiers for speech  recognition. phd thesis  massachusetts institute of technology      .  mikael henaff  arthur szlam  and yann lecun. orthogonal rnns and long memory tasks. international conference on machine learning  icml       .  sepp hochreiter. untersuchungen zu dynamischen neuronalen netzen. diploma  technische universita t mu nchen  pp.         .  sepp hochreiter and ju rgen schmidhuber. long short term memory. neural computation                        .  rafal jozefowicz  wojciech zaremba  and ilya sutskever. an empirical exploration of recurrent  network architectures. international conference on machine learning  icml       .  jan koutnik  klaus greff  faustino gomez  and juergen schmidhuber. a clockwork rnn. international conference on machine learning  icml   pp.                .  david krueger  tegan maharaj  ja nos krama r  mohammad pezeshki  nicolas ballas  nan rosemary ke  anirudh goyal  yoshua bengio  hugo larochelle  aaron courville  et al. zoneout   regularizing rnns by randomly preserving hidden activations. arxiv preprint arxiv     .            .  quoc v le  navdeep jaitly  and geoffrey e hinton. a simple way to initialize recurrent networks  of rectified linear units. arxiv preprint arxiv     .           .  yann lecun  le on bottou  yoshua bengio  and patrick haffner. gradient based learning applied to  document recognition. proceedings of the ieee                        .  k f lee and h w hon. speaker independent phone recognition using hidden markov models. ieee  transactions on acoustics  speech  and signal processing                        .  tsungnan lin  bill g horne  peter tino  and c lee giles. learning long term dependencies in  narx recurrent neural networks. ieee transactions on neural networks                       .  james martens and ilya sutskever. learning recurrent neural networks with hessian free optimization. in international conference on machine learning  icml       .  yajie miao  mohammad gowayyed  and florian metze. eesen  end to end speech recognition  using deep rnn models and wfst based decoding. automatic speech recognition and understanding  asru   pp.              .  aaron van den oord  nal kalchbrenner  and koray kavukcuoglu. pixel recurrent neural networks.  international conference on machine learning  icml       .  razvan pascanu  tomas mikolov  and yoshua bengio. on the difficulty of training recurrent neural  networks. international conference on machine learning  icml                     .  tony a. plate. holographic recurrent networks. advances in neural information processing systems   nips       .  david e rumelhart  geoffrey e hinton  and ronald j williams. learning representations by backpropagating errors. nature                         .  ju rgen schmidhuber. learning complex  extended sequences using the principle of history compression. neural computation                    .  rohollah soltani and hui jiang.  arxiv     .           .    higher order recurrent neural networks.    arxiv preprint    paul j werbos. generalization of backpropagation with application to a recurrent gas market model.  neural networks                    .         workshop track   iclr         paul j werbos. maximizing long term gas industry profits in two minutes in lotus using neural  network methods. ieee transactions on systems  man  and cybernetics                     .  paul j werbos. backpropagation through time  what it does and how to do it. proceedings of the  ieee                        .  jason weston  sumit chopra  and antoine bordes. memory networks. international conference on  learning representations  iclr       .  ronald j williams and david zipser. a learning algorithm for continually running fully recurrent  neural networks. neural computation                    .  yonghui wu  mike schuster  zhifeng chen  quoc v le  mohammad norouzi  wolfgang macherey   maxim krikun  yuan cao  qin gao  klaus macherey  et al. google s neural machine translation system  bridging the gap between human and machine translation. arxiv preprint  arxiv     .           .  saizheng zhang  yuhuai wu  tong che  zhouhan lin  roland memisevic  ruslan salakhutdinov   and yoshua bengio. architectural complexity measures of recurrent neural networks. advances  in neural information processing systems  nips       .           workshop track   iclr              a ppendix   g radient c omponents as paths    here we will apply equation    repeatedly to associate gradient components with paths connecting  t     to t  beginning with equation    and handling simple rnns and simple narx rnns in order.      ht  applying equation    to expand  h    we obtain  t        ht      ht     . .     x  t t   t          ht  ht    ht   ht              s imple rnn s    for simple rnns  by examining equation    we can immediately see that all partials  except for the one satisfying t    t        . this yields      ht      ht  ht           ht     ht       ht       ht    ht      are                        now  by applying equation    again to  h t  ht     and then to  h t  ht     and so on  we trace out a path  from t     to t  as shown in figure    finally resulting the single term   ht   ht       ht             ht     ht       ht              which is associated with the only path from t     to t  with one factor for each edge that is encountered along the path.   . .     s imple narx rnn s and g eneral narx rnn s    next we consider simple narx rnns  again by expanding equation   . from equation     we   ht   is nonzero if  can see that up to nd partials are now nonzero  and that any particular partial  h  t             and only if t   t     and t and t     share an edge. collecting these t as the set vt      t    t     t     and  t      t      e   we can write  x     ht  ht       ht      ht     ht   ht                 t  vt      we can then apply this exact same process to each  t  and  t    t       e  for all t    we can write  x      ht      ht             ht   ht       by defining vt     t     t        x     ht  ht    ht    ht    ht   ht                  t  vt   t  vt     by continuing this process until only partials remain  we obtain a summation over all possible paths  from t     to t. each term in the sum is a product over factors  one per edge    ht   ht    ht         ht         ht   ht              the analysis is nearly identical for general narx rnns  with the only difference being the specific  sets of edges that are considered.        .     a ppendix   e xperimental d etails  g eneral e xperimental s etup    everything in this section holds for all experiments except surgical maneuver recognition  as in that  case we mimicked dipietro et al.        as closely as possible  as described above.         workshop track   iclr         all weight matrices  are initialized using a normal distribution with a mean of   and a standard     deviation of    nh   where nh is the number of hidden units. all initial hidden states  for t       are initialized to  . for optimization  gradients are computed using full backpropagation through  time  and we use stochastic gradient descent with a momentum of  .   with gradient clipping as  described by pascanu et al.        at    and with a minibatch size of    . biases are generally  initialized to    but we follow best practice for lstm by initializing the forget gate bias to   gers  et al.         jozefowicz et al.       . for clockwork rnns    exponential periods are used  as in  the original paper. for mist rnns    delays are used. we avoid manual learning rate tuning in  its entirety. instead we run    trials for each experimental configuration. in each trial  the learning  rate is drawn uniformly at random in log space between      and       and initial weight matrices  are also redrawn at random. we report results over the top     of trials according to validationset error.  an alternative option is to report results over all trials. however  because the majority  of trials yields bad performance for all methods  this simply blurs comparisons. see for example  figure   of greff et al.         which compares these two options.    .     s equential p mnist c lassification    data preprocessing is kept minimal  with each input image individually shifted and scaled to have  mean   and variance  . we split the official training set into two parts  the first        used for  training and the last       used for validation. our test set is the same as the official test set   consisting of        images. training is carried out by minimizing cross entropy loss.   .     c opy p roblem   e xperimental d etails    in our experiments  the l relevant symbols are drawn at random  with replacement  from the set         . . .       d is always a multiple of     and l is chosen to be d   . this way the simplest  baseline of always predicting the blank symbol yields a constant error rate for all experiments. no  input preprocessing of any kind is performed. in each case  we generate         examples for  training and       examples for validation. training is carried out by minimizing cross entropy  loss.   .     s urgical activity r ecognition   e xperimental d etails    we use the same experimental setup as dipietro et al.         which currently holds state of the art  performance on these tasks. for kinematic inputs we use positions  velocities  and gripper angles for  both hands. we also use their leave one user out teset setup  with   users in the case of jigsaws  and    users in the case of mistic sl. finally we use the same hyperparameters    hidden layer  of      units  dropout with p    .      epochs of training with a learning rate of  .  for the first     epochs and having the learning rate every   epochs for the rest of training. as mentioned in the main  paper  the primary difference is that we replaced their lstm layer with our simple rnn  lstm  or  mist rnn layer. training is carried out by minimizing cross entropy loss.   .     p honeme r ecognition   e xperimental d etails    we follow greff et al.        and extract    mel frequency cepstral coefficients plus energy every    ms using   ms hamming windows and a pre emphasis coefficient of  .  . however we do not use  derivatives  resulting in    inputs per frame. each input sequence is individually shifted and scaled  to have mean   and variance   over each dimension. we form our splits according to halberstadt          resulting in      sequences for training      sequences for validation  and     sequences  for testing. training is carried out by minimizing cross entropy loss. means and standard deviations  are computed using the top   randomized trials out of     ranked according to performance on the  validation set .   .     activity r ecognition from s martphones    in  chatzaki et al.         emphasis was placed on hand crafted features  and each subject was  included during both training and testing  with no official test set defined . we instead operate on         workshop track   iclr         the raw sequence data  with no preprocessing other than sequence wise centering and scaling of  inputs  and we define train  val  test splits so that subjects are disjoint among the three groups.           