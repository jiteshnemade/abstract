introduction  processing  modeling and predicting sequential data of variable length is a major challenge in the  field of machine learning. in recent years  recurrent neural networks  rnns                   have  been the most popular tool to approach this challenge. rnns have been successfully applied to  improve state of the art results in complex tasks like language modeling and speech recognition. a  popular variation of rnns are long short term memories  lstms        which have been proposed  to address the vanishing gradient problem            . lstms maintain constant error flow and thus  are more suitable to learn long term dependencies compared to standard rnns.  our work contributes to the ongoing debate on how to interconnect several rnn cells with the  goals of promoting the learning of long term dependencies  favoring efficient hierarchical representations of information  exploiting the computational advantages of deep over shallow networks and  increasing computational efficiency of training and testing. in deep rnn architectures  rnns or  lstms are stacked layer wise on top of each other            . the additional layers enable the  network to learn complex input to output relations and encourage a efficient hierarchical representation of information. in multiscale rnn architectures                 the operation on different  timescales is enforced by updating the higher layers less frequently  which further encourages an  efficient hierarchical representation of information. the slower update rate of higher layers leads    st conference on neural information processing systems  nips        long beach  ca  usa.     to computationally efficient implementations and gives rise to short gradient paths that favor the  learning of long term dependencies. in deep transition rnn architectures  intermediate sequentially  connected layers are interposed between two consecutive hidden states in order to increase the depth  of the transition function from one time step to the next  as for example in deep transition networks       or recurrent highway networks  rhn      . the intermediate layers enable the network to  learn complex non linear transition functions. thus  the model exploits the fact that deep models  can represent some functions exponentially more efficiently than shallow models    . we interpret  these networks as shallow networks that share the hidden state  rather than a single deep network.  despite being the same in practice  this interpretation makes it trivial to convert any rnn cell to a  deep rnn by connecting the cells sequentially  see figure  b.  here  we propose the fast slow rnn  fs rnn  architecture  a novel way of interconnecting rnn  cells  that combines advantages of multiscale rnns and deep transition rnns. in its simplest  form the architecture consists of two sequentially connected  fast operating rnn cells in the lower  hierarchical layer and a slow operating rnn cell in the higher hierarchical layer  see figure   and  section  . we evaluate the fs rnn on two standard character level language modeling data sets   namely penn treebank and hutter prize wikipedia. additionally  following       we present an  empirical analysis that reveals advantages of the fs rnn architecture over other rnn architectures.  the main contributions of this paper are     we propose the fs rnn as a novel rnn architecture.    we improve state of the art results on the penn treebank and hutter prize wikipedia data  sets.    we surpass the bpc performance of the best known text compression algorithm evaluated  on hutter prize wikipedia by using an ensemble of two fs rnns.    we show empirically that the fs rnn incorporates strengths of both multiscale rnns and  deep transition rnns  as it stores long term dependencies efficiently and it adapts quickly  to unexpected input.    we provide our code in the following url https   github.com amujika fast slow lstm.      related work  in the following  we review the work that relates to our approach in more detail. first  we focus  on deep transition rnns and multiscale rnns since these two architectures are the main sources  of inspiration for the fs rnn architecture. then  we discuss how our approach differs from these  two architectures. finally  we review other approaches that address the issue of learning long term  dependencies when processing sequential data.  pascanu et al.      investigated how a rnn can be converted into a deep rnn. in standard rnns   the transition function from one hidden state to the next is shallow  that is  the function can be  written as one linear transformation concatenated with a point wise non linearity. the authors added  intermediate layers to increase the depth of the transition function  and they found empirically that  such deeper architectures boost performance. since deeper architectures are more difficult to train   they equip the network with skip connections  which give rise to shorter gradient paths  dt s rnn  see      . following a similar line of research  zilly et al.      further increased the transition  depth between two consecutive hidden states. they used highway layers      to address the issue  of training deep architectures. the resulting rhn      achieved state of the art results on the penn  treebank and hutter prize wikipedia data sets. furthermore  a vague similarity to deep transition  networks can be seen in adaptive computation       where an lstm cell learns how many times it  should update its state after receiving the input to produce the next output.  multiscale rnns are obtained by stacking multiple rnns with decreasing order of update frequencies on top of each other. early attempts proposed such architectures for sequential data compression        where the higher layer is only updated in case of prediction errors of the lower layer  and for  sequence classification      where the higher layers are updated with a fixed smaller frequency. more  recently  koutnik et al.      proposed the clockwork rnn  in which the hidden units are divided into  several modules  of which the i th module is only updated every  i  th time step. general advantages  of this multiscale rnn architecture are improved computational efficiency  efficient propagation of        hst      k  hf  t      hst    fs  f f        hf  t    f f        hf  t           f    ht k      f fk    k  hf  t    yt    xt    figure    diagram of a fast slow rnn with k fast cells. observe that only the second fast cell  receives the input from the slow cell.    long term dependencies and flexibility in allocating resources  units  to the hierarchical layers. multiscale rnns have been applied for speech recognition in      where the slower operating rnn  pools information over time and the timescales are fixed hyperparameters as in clockwork rnns.  in       multiscale rnns are applied to make context aware query suggestions. in this case  explicit  hierarchical boundary information is provided. chung et al.     presented a hierarchical multiscale  rnn  hm rnn  that discovers the latent hierarchical structure of the sequence without explicitly  given boundary information. if a parametrized boundary detector indicates the end of a segment   then a summarized representation of the segment is fed to the upper layer and the state of the lower  layer is reset    .  our fs rnn architectures borrows elements from both deep transition rnns and multiscale rnns.  the major difference to multiscale rnns is that our lower hierarchical layer zooms in in time  that  is  it operates faster than the timescale that is naturally given by the input sequence. the major  difference to deep transition rnns is our approach to facilitate long term dependencies  namely  we  employ a rnn operating on a slow timescale.  many approaches aim at solving the problem of learning long term dependencies in sequential data.  a very popular one is to use external memory cells that can be accessed and modified by the network   see neural turing machines       memory networks      and differentiable neural computer     .  other approaches focus on different optimization techniques rather than network architectures. one  attempt is hessian free optimization       a second order training method that achieved good results  on rnns. the use of different optimization techniques can improve learning in a wide range of  rnn architectures and therefore  the fs rnn may also benefit from it.      fast slow rnn  we propose the fs rnn architecture  see figure  . it consists of k sequentially connected rnn  cells f    . . .   fk on the lower hierarchical layer and one rnn cell s on the higher hierarchical layer.  we call f    . . .   fk the fast cells  s the slow cell and the corresponding hierarchical layers the fast  and slow layer  respectively. s receives input from f  and feeds its state to f  . f  receives the  sequential input data xt   and fk outputs the predicted probability distribution yt of the next element  of the sequence.  intuitively  the fast cells are able to learn complex transition functions from one time step to the  next one. the slow cell gives rise to shorter gradient paths between sequential inputs that are distant  in time  and thus  it facilitates the learning of long term dependencies. therefore  the fs rnn  architecture incorporates advantages of deep transition rnns and of multiscale rnns  see section   .  since any kind of rnn cell can be used as building block for the fs rnn architecture  we state  the formal update rules of the fs rnn for arbitrary rnn cells. we define a rnn cell q to be a  differentiable function f q  h  x  that maps a hidden state h and an additional input x to a new hidden  state. note that x can be input data or input from a cell in a higher or lower hierarchical layer. if a  cell does not receive an additional input  then we will omit x. the following equations define the  fs rnn architecture for arbitrary rnn cells f    . . .   fk and s.        f      k  hf   hf  t  f  t     xt       hst   f s  hst     hf  t      f   s        hf   hf  t  f  t   ht    f    i    fi  i    for     i   k  hf  t   f  ht  k  the output yt is computed as an affine transformation of hf  t . it is possible to extend the fs rnn  architecture in order to further facilitate the learning of long term dependencies by adding hierarchical layers  each of which operates on a slower timescale than the ones below  resembling clockwork  rnns     . however  for the tasks considered in section    we observed that this led to overfitting  the training data even when applying regularization techniques and reduced the performance at test  time. therefore  we will not further investigate this extension of the model in this paper  even though  it might be beneficial for other tasks or larger data sets.    in the experiments in section    we use lstm cells as building blocks for the fs rnn architecture.  for completeness  we state the update function f q for an lstm q. the state of an lstm is a pair   ht   ct    consisting of the hidden state and the cell state. the function f q maps the previous state  and input  ht     ct     xt   to the next state  ht   ct   according to       ft    it    q  q  q   o     wh ht     wx xt   b  t  gt  ct     ft     ct       it     tanh gt    ht     ot     tanh ct      where ft   it and ot are commonly referred to as forget  input and output gates  and gt are the new  candidate cell states. moreover  whq   wxq and bq are the learnable parameters    denotes the  sigmoid function  and   denotes the element wise multiplication.      experiments  for the experiments  we consider the fast slow lstm  fs lstm  that is a fs rnn  where each  rnn cell is a lstm cell. the fs lstm is evaluated on two character level language modeling data  sets  namely penn treebank and hutter prize wikipedia  which will be referred to as enwik  in this  section. the task consists of predicting the probability distribution of the next character given all the  previous ones. in section  .   we compare the performance of the fs lstm with other approaches.  in section  .   we empirically compare the network dynamics of different rnn architectures and  show the fs lstm combines the benefits of both  deep transition rnns and multiscale rnns.   .  performance on penn treebank and hutter prize wikipedia  the fs lstm achieves  .   bpc and  .   bpc on the penn treebank and enwik  data sets  respectively. these results are compared to other approaches in table   and table    the baseline  lstm results without citations are taken from      for penn treebank and from      for enwik  .  for the penn treebank  the fs lstm outperforms all previous approaches with significantly less  parameters than the previous top approaches. we did not observe any improvement when increasing  the model size  probably due to overfitting. in the enwik  data set  the fs lstm surpasses all other  neural approaches. following       we compare the results with text compression algorithms using  the bpc measure. an ensemble of two fs lstm models   .   bpc  outperforms cmix   .   bpc         the current best text compression algorithm on enwik      . however  a fair comparison is  difficult. compression algorithms are usually evaluated by the final size of the compressed data  set including the decompressor size. for character prediction models  the network size is usually  not taken into account and the performance is measured on the test set. we remark that as the fslstm is evaluated on the test set  it should achieve similar performance on any part of the english  wikipedia.        table    bpc on penn treebank  model    bpc    param count    zoneout lstm        layers lstm  hm lstm      hyperlstm   small       hyperlstm       nascell   small       nascell          .     .      .     .      .      .      .        . m   . m    . m   . m    . m    fs lstm    ours   fs lstm    ours      .      .        . m   . m    the fs lstm   and fs lstm   model consist of two and four cells in the fast layer  respectively.  the fs lstm   model outperforms the fs lstm   model  but its processing time for one time  step is     higher than the one of the fs lstm  . adding more cells to the fast layer could  further improve the performance as observed for rhn       but would increase the processing time   because the cell states are computed sequentially. therefore  we did not further increase the number  of fast cells.  the model is trained to minimize the cross entropy loss between  pn the predictions and the training  data. formally  the loss function is defined as l     n  i   log p   xi  x    . . .   xi      where  p   xi  x    . . .   xi     is the probability that a model with parameters   assigns to the next character xi given all the previous ones. the model is evaluated by the bpc measure  which uses the  binary logarithm instead of the natural logarithm in the loss function. all the hyperparameters used  for the experiments are summarized in table  . we regularize the fs lstm with dropout     . in  each time step  a different dropout mask is applied for the non recurrent connections       and zoneout     is applied for the recurrent connections. the network is trained with minibatch gradient  descent using the adam optimizer     . if the gradients have norm larger than   they are normalized to  . truncated backpropagation through time  tbptt           is used to approximate the  gradients  and the final hidden state is passed to the next sequence. the learning rate is divided by  a factor    for the last    epochs in the penn treebank experiments  and it is divided by a factor     whenever the validation error does not improve in two consecutive epochs in the enwik  experiments. the forget bias of every lstm cell is initialized to    and all weight matrices are initialized  to orthogonal matrices. layer normalization     is applied to the cell and to each gate separately.  the network with the smallest validation error is evaluated on the test set. the two data sets that we  use for evaluation are   penn treebank      the dataset is a collection of wall street journal articles written in english.  it only contains       different words  all written in lower case  and rare words are replaced with     unk   . following       we split the data set into train  validation and test sets consisting of   . m     k and    k characters  respectively.  hutter prize wikipedia      this dataset is also known as enwik  and it consists of  raw   wikipedia data  that is  english articles  tables  xml data  hyperlinks and special characters. the  data set contains    m characters with     unique tokens. following      we split the data set into  train  validation and test sets consisting of   m   m and  m characters  respectively.   .  comparison of network dynamics of different architectures  we compare the fs lstm architecture with the stacked lstm and the sequential lstm architectures  depicted in figure    by investigating the network dynamics. in order to conduct a fair  comparison we chose the number of parameters to roughly be the same for all three models. the  fs lstm consists of one slow and four fast lstm cells of     units each. the stacked lstm  consists of five lstm cells stacked on top of each other consisting of     units each  which will be        table    bpc on enwik   model    bpc    param count    lstm       units  layer norm lstm       units  hyperlstm       hm lstm      surprisal driven zoneout       rhn   depth         rhn   depth          large rhn   depth             .      .      .      .     .     .     .     .        m    m    m    m    m    m    m    m    fs lstm    ours   fs lstm    ours   large fs lstm    ours       large fs lstm    ours      .      .      .      .         m    m    m        m    cmix v            .            table    hyperparameters for the character level language model experiments.  penn treebank    enwik   large    fs lstm      fs lstm      fs lstm      fs lstm      non recurrent dropout  cell zoneout  hidden zoneout     .     .    .      .     .    .      .    .    .       .    .    .       .     .    .      fast cell size  slow cell size                                                                                    .                            .                            .                           .                           .           tbptt length  minibatch size  input embedding size  initial learning rate  epochs    fs lstm      referred to as stacked    ...   stacked    from bottom to top. the sequential lstm consists of five  sequentially connected lstm cells of     units each. all three models require roughly the same  time to process one time step. the models are trained on enwik  for    epochs with minibatch  gradient descent using the adam optimizer      without any regularization  but layer normalization      is applied on the cell states of the lstms. the hyperparameters are not optimized for any of the  three models.  the experiments suggest that the fs lstm architecture favors the learning of long term dependencies  figure     enforces hidden cell states to change at different rates  figure    and facilitates a  quick adaptation to unexpected inputs  figure   . moreover  the fs lstm achieves  .   bpc and  outperforms the stacked lstm   .   bpc  and the sequential lstm   .   bpc .  in figure    we asses the ability to capture long term dependencies by investigating the effect of the  cell state on the loss at later time points  following    . we measure the effect of the cell state at time  t  t   k on the loss at time t by the gradient k  c l  k. this gradient is the largest for the slow lstm   t k  and it is small and steeply decaying as k increases for the fast lstm. evidently  the slow cell  captures long term dependencies  whereas the fast cell only stores short term information. in the  stacked lstm  the gradients decrease from the top layer to the bottom layer  which can be explained        yt  f     h t      h t    ..  .  f     h t      h t    f     ht      f     f            yt    xt    xt    ht     b  sequential     a  stacked    figure    diagram of  a  stacked lstm and  b  sequential lstm with   cells each.     .    .     gradient norm     .    .     stacked    stacked    stacked    stacked    stacked      fs fast  fs slow  sequential  stacked       .    .    .    .    .                      k                                     k                     t  figure    long term effect of the cell states on the loss function. the average value of  c l     t k  which is the effect of the cell state at time t   k on the loss function at time t  is plotted against k  for the different layers in the three rnn architectures. for the sequential lstm only the first cell  is considered.    by the vanishing gradient problem. the small  steeply decaying gradients of the sequential lstm  indicate that it is less capable to learn long term dependencies than the other two models.  figure   gives further evidence that the fs lstm stores long term dependencies efficiently in the  slow lstm cell. it shows that among all the layers of the three rnn architectures  the cell states  of the slow lstm change the least from one time step to the next. the highest change is observed  for the cells of the sequential model followed by the fast lstm cells.  in figure    we investigate whether the fs lstm quickly adapts to unexpected characters  that is   whether it performs well on the subsequent ones. in text modeling  the initial character of a word  has the highest entropy  whereas later characters in a word are usually less ambiguous     . since  the first character of a word is the most difficult one to predict  the performance at the following  positions should reflect the ability to adapt to unexpected inputs. while the prediction qualities at  the first position are rather close for all three models  the fs lstm outperforms the stacked lstm  and sequential lstm significantly on subsequent positions. it is possible that new information is  incorporated quickly in the fast layer  because it only stores short term information  see figure  .         .     sequential  fs fast  fs slow  stacked    stacked    stacked    stacked    stacked      rate of change     .    .    .    .    .    .   sequential    fast slow    stacked    p  figure    rate of change of the cell states from one time step to the next. we plot n  ni    ct i    ct   i    averaged over all time steps  where ct i is the value of the ith unit at time step t  for the  different layers of the three rnn architectures. for the sequential lstm only the first cell is considered.   .      .      fast slow  sequential  stacked     .    .      .     .      relative loss    bpc     .    .    .      .     .     .       .      .       .      .       .                                        character position               .                                               character position                    figure    bits per character at each character position. the left panel shows the average bits percharacter at each character positions in the test set. the right panel shows the average relative loss  with respect to the stacked lstm at each character position. for this figure  a word is considered  to be a sequence of lower case letters of length at least   in between two spaces.      conclusion  in this paper  we have proposed the fs rnn architecture. up to our knowledge  it is the first  architecture that incorporates ideas of both multiscale and deep transition rnns. the fs rnn  architecture improved state of the art results on character level language modeling evaluated on  the penn treebank and hutter prize wikipedia data sets. an ensemble of two fs rnns achieves  better bpc performance than the best known compression algorithm. further experiments provided  evidence that the slow cell enables the network to learn long term dependencies  while the fast cells  enable the network to quickly adapt to unexpected inputs and learn complex transition functions  from one time step to the next.  our fs rnn architecture provides a general framework for connecting rnn cells as any type of  rnn cell can be used as building block. thus  there is a lot of flexibility in applying the architecture to different tasks. for instance using rnn cells with good long term memory  like eurnns       or narx rnns          for the slow cell might boost the long term memory of the fs rnn  architecture. therefore  the fs rnn architecture might improve performance in many different  applications.        acknowledgments  we thank julian zilly for many helpful discussions.    