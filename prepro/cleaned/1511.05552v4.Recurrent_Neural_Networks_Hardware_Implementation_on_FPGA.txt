introduction  as humanity progresses into the digital era  more and  more data is produced and distributed across the world. deep  neural networks  dnn  provides a method for computers  to learn from this mass of data. this unlocks a new set of  possibilities in computer vision  speech recognition  natural  language processing and more. however  dnns are computationally expensive  such that general processors consume  large amounts of power to deliver desired performance. this  limits the application of dnns in the embedded world. thus   a custom architecture optimized for dnns provides superior  performance per power and brings us a step closer to selflearning mobile devices.  recurrent neural networks  rnns  are becoming an  increasingly popular way to learn sequences of data           and it has been shown to be successful in various applications  such as speech recognition      machine translation      and scene analysis    . a combination of a convolutional  neural network  cnn  with a rnn can lead to fascinating  results such as image caption generation        .  due to the recurrent nature of rnns  it is sometimes  hard to parallelize all its computations on conventional  hardware. general purposes cpus do not currently offer  large parallelism  while small rnn models do not get full  benefit from gpus. thus  an optimized hardware architecture is necessary for executing rnns models on embedded  systems.    long short term memory  or lstm      is a specific  rnn architecture that implements a learned memory controller for avoiding vanishing or exploding gradients     .  the purpose of this paper is to present a lstm hardware  module implemented on the zynq      fpga from xilinx      . figure   shows an overview of the system. as proof  of concept  the hardware was tested with a character level  language model made with   lstm layers and     hidden  units. the next following sections present the background for  lstm  related work  implementation details of the hardware  and driver software  the experimental setup and the obtained  results.  ii. lstm background  one main feature of rnns are that they can learn from  previous information. but the question is how far should a  model remember  and what to remember. standard rnn can  retain and use recent past information     . but it fails to  learn long term dependencies. vanilla rnns are hard to train  for long sequences due to vanishing or exploding gradients      . this is where lstm comes into play. lstm is an  rnn architecture that explicitly adds memory controllers to  decide when to remember  forget and output. this makes the  training procedure much more stable and allows the model  to learn long term dependencies    .  there are some variations on the lstm architecture. one  variant is the lstm with peephole introduced by     . in  this variation  the cell memory influences the input  forget     ht    wxo    x    sigmoid    who    ot    tanh    c t      memory cell c  t    wxi  input gate    x    ht      x         xt    sigmoid    ft         wxf    forget gate  sigmoid         x         xt  ht      output gate    it    tanh    xt  h t      w    hf       ct    whi    x  x    candidate memory    one needs to train the model to get the parameters that  will give the desired output. in simple terms  training is an  iterating process in which training data is fed in and the  output is compared with a target. then the model needs to  backpropagate the error derivatives to update new parameters  that minimize the error. this cycle repeats until the error is  small enough     . models can become fairly complex as  more layers and more different functions are added. for the  lstm case  each module has four gates and some elementwise operations. a deep lstm network would have multiple  lstm modules cascaded in a way that the output of one  layer is the input of the following layer.       wxc    whc    x    x    xt    h t      iii. r elated w ork    figure  .  the vanilla lstm architecture that was implemented in  hardware.   represents matrix vector multiplication and is element wise  multiplication.    and output gates. conceptually  the model peeps into the  memory cell before deciding whether to memorize or forget.  in      input and forget gate are merged together into one  gate. there are many other variations such as the ones  presented in      and     . all those variations have similar  performance as shown in     .  the lstm hardware module that was implemented focuses on the lstm version that does not have peepholes   which is shown in figure  . this is the vanilla lstm        which is characterized by the following equations   it     wxi xt   whi ht     bi             ft     wxf xt   whf ht     bf             ot     wxo xt   who ht     bo             c t   tanh wxc xt   whc ht     bc             ct   ft    ct     it    ht   ot    tanh ct      c t                where   is the logistic sigmoid function   is element  wise multiplication  x is the input vector of the layer  w is  the model parameters  c is memory cell activation  c t is the  candidate memory cell gate  h is the layer output vector. the  subscript t     means results from the previous time step.  the i  f and o are respectively input  forget and output gate.  conceptually  these gates decide when to remember or forget  an input sequence  and when to respond with an output. the  combination of two matrix vector multiplications and a nonlinear function  f  wx xt  wh ht    b   extracts information  from the input and previous output vectors. this operation  is referred as gate.    co processors for accelerating computer vision algorithms  and cnns have been implemented on fpgas. a system  that can perform recognition on mega pixel images in realtime is presented in     . a similar architecture for general  purpose vision algorithms called neuflow is described in      . neuflow is a scalable architecture composed by a  grid of operation modules connected with an optimized data  streaming network. this system can achieve speedups up to       in end to end applications.  an accelerator called nn x for deep neural networks is  described in          . nn x is a high performance coprocessor implemented on fpga. the design is based on  computational elements called collections that are capable  of performing convolution  non linear functions and pooling.  the accelerator efficiently pipelines the collections achieving  up to     g op s.  rnns are different from cnns in the context that they  require a different arrangement of computation modules.  this allows different hardware optimization strategies that  should be exploited. a lstm learning algorithm using simultaneous perturbation stochastic approximation  spsa   for hardware friendly implementation was described in     .  the paper focuses on transformation of the learning phase  of lstm for fpga.  another fpga implementation that focus on standard  rnn is described by     . their approach was to unfold the  rnn model into a fixed number of timesteps b and compute  them in parallel. the hardware architecture is composed of  a hidden layer module and duplicated output layer modules.  first  the hidden layer serially processes the input x for  b timesteps. then  with the results of the hidden layer   the duplicated logic computes output h for b timesteps in  parallel.  this work presents a different approach of implementing  rnn in fpga  focusing the lstm architecture. it is different than       in the sense that it uses a single module that  consumes input x and previous output ht   simultaneously.     ewise module    gate module    sync    sync  xt    wx  ht    wh    mac  mac         tanh  sigmoid    it     ct    ct  sync       ft    tanh    ht    c t    ot    figure  . the main hardware module that implements the lstm gates.  the non linear module can be configured to be a tanh or logistic sigmoid.    figure  .  the gates.    the module that computes the ct and ht from the results of  is element wise multiplication.    lstm    iv. i mplementation    router    a. hardware  the main operations to be implemented in hardware  are matrix vector multiplications and non linear functions   hyperbolic tangent and logistic sigmoid . both are modifications of the modules presented in     . for this design  the  number format of choice is q .  fixed point. the matrixvector multiplication is computed by a multiply accumulate  mac  unit  which takes two streams  vector stream  and weight matrix row stream. the same vector stream  is multiplied and accumulated with each weight matrix  row to produce an output vector with same size of the  weight s height. the mac is reset after computing each  output element to avoid accumulating previous matrix rows  computations. the bias b can be added in the multiply  accumulate by adding the bias vector to the last column  of the weight matrix and adding an extra vector element set  to unity. this way there is no need to add extra input ports  for the bias nor add extra pre configuration step to the mac  unit. the results from the mac units are added together. the  adder s output goes to an element wise non linear function   which is implemented with linear mapping.  the non linear function is segmented into lines y   ax    b  with x limited to a particular range. the values of a  b  and x range are stored in configuration registers during the  configuration stage. each line segment is implemented with  a mac unit and a comparator. the mac multiplies a and x  and accumulates with b. the comparison between the input  value with the line range decides whether to process the  input or pass it to the next line segment module. the nonlinear functions were segmented into    lines  thus the nonlinear module contains    pipelined line segment modules.  the main building block of the implemented design is the  gate module as shown in figure  .  the implemented module uses direct memory access   dma  ports to stream data in and out. the dma ports  use valid and ready handshake. because the dma ports  are independent  the input streams are not synchronized  even when the module activates the ports at same the time.  therefore  a stream synchronizing module is needed. the  sync block is a buffer that caches some streaming data until    gate  sigmoid    axis  dma    gate  tanh  gate  sigmoid    main  memory    arm  processor    fifos  it     ct  ft  ot    ewise    configuration  registers    figure  . the lstm module block diagram. it is mainly composed of  three gates and one ewise stage module.    all ports are streaming. when the last port starts streaming   the sync block starts to output synchronized streams. this  ensures that vector and matrix row elements that goes to  mac units are aligned.  the gate module in figure   also contains a rescale block  that converts    bit values to    bit values. the mac units  perform    bit multiplication that results into    bit values.  the addition is performed using    bit values to preserve  accuracy.  all that is left are some element wise operations to  calculate ct and ht in equations   and  . to do this  extra  multipliers and adders were added into a separate module  shown in figure  .  the lstm module uses three blocks from figure   and  one from figure  . the gates are pre configured to have a  non linear function  tanh or sigmoid . the lstm module is  shown in figure  .  the internal blocks are controlled by a state machine to  perform a sequence of operations. the implemented design  uses four    bit dma ports. since the operations are done in     bit  each dma port can transmit two    bit streams. the     weights wx and wh are concatenated in the main memory to  exploit this feature. the streams are then routed to different  modules depending on the operation to be performed. with  this setup  the lstm computation was separated into three  sequential stages      compute it and c t .     compute ft and ot .     compute ct and ht .  in the first and second stage  two gate modules    mac  units  are running in parallel to generate two internal vectors   it   c t   ft and ot    which are stored into a first in first out   fifo  for the next stages. the ewise module consumes  the fifo vectors to output the ht and ct back to main  memory. after that  the module waits for new weights and  new vectors  which can be for the next layer or next time  step. the hardware also implements an extra matrix vector  multiplication to generate the final output. this is only used  when the last lstm layer has finished its computation.  this architecture was implemented on the zedboard        which contains the zynq      soc xc z   . the chip  contains dual arm cortex a  mpcore  which is used for  running the lstm driver c code and timing comparisons.  the hardware utilization is shown in table i. the module  runs at     mhz and the total on chip power is  .    w.  table i  fpga hardware resource utilization for z ynq zc    .    components  ff  lut  memory lut  bram  dsp    bufg    utilization                                       utilization        .      .     .      .      .     .      b. driving software  the control and testing software was implemented with c  code. the software populates the main memory with weight  values and input vectors  and it controls the hardware module  with a set of configuration registers.  the weight matrix have an extra element containing the  bias value in the end of each row. the input vector contains  an extra unity value so that the matrix vector multiplication  will only add the last element of the matrix row  bias  addition . usually the input vector x size can be different  from the output vector h size. zero padding was used to  match both the matrix row size and vector size  which makes  stream synchronization easier.  due to the recurrent nature of lstm  ct and ht becomes  the ct   and ht   for the next time step. therefore  the  input memory location for ct   and ht   is the same for  the output ct and ht . each time step c and h are overwritten.    this is done to minimize the number of memory copies done  by the cpu. to implement a multi layer lstm  the output  of the previous layer ht was copied to the xt location of  the next layer  so that ht is preserved in between layers for  error measurements. this feature was removed for profiling  time. the control software also needs to change the weights  for different layers by setting different memory locations in  the control registers.  v. e xperiments  the training script by andrej karpathy of the character  level language model was written in torch . the code can be  downloaded from github  . additional functions were written  to transfer the trained parameters from the torch  code to  the control software.  the torch  code implements a character level language  model  which predicts the next character given a previous  character. character by character  the model generates a text  that looks like the training data set  which can be a book  or a large internet corpora with more than   mb of words.  for this experiment  the model was trained on a subset of  shakespeare s work. the batch size was     the training  sequence was    and learning rate was  .   . the model  is expected to output shakespeare look like text.  the torch  code implements a   layer lstm with hidden  layer size      weight matrix height . the character input  and output is a    sized vector one hot encoded. the  character that the vector represents is the index of the only  unity element. the predicted character from last layer is fed  back to input xt of first layer for following time step.  for profiling time  the torch  code was ran on other  embedded platforms to compare the execution time between  them. one platform is the tegra k  development board   which contains quad core arm cortex a   cpu and kepler gpu     cores. the tegra s cpu was clocked at  maximum frequency of     .  mhz. the gpu was clocked  at maximum of     mhz. the gpu memory was running  at     mhz.  another platform used is the odroid xu   which has the  exynos     with four high performance cortex a   cores  and four low power cortex a  cores  arm big.little  technology . the low power cortex a  cores was clocked  at      mhz and the high performance cortex a   cores  was running at      mhz.  the c code lstm implementation was ran on zedboard s  dual arm cortex a  processor clocked at     mhz. finally  the hardware was ran on zedboard s fpga clocked  at     mhz.  vi. r esults  a. accuracy  the number of weights of some models can be very large.  even our small model used almost     kb of weights. thus    https   github.com karpathy char rnn     ezwhan i have s ll the soul of thee  that i may be the sun to the state   that we may be the bear the state to see   that is the man that should be so far o the world.       ebut they ld not kindle perpide d thee this   so shall you shall be gratuly not dost thereof enring          king edward iv   why  then i see thee to the common sons  that we may be a countering thee there were  the sea for the most cause to the common sons   that we may be the boy of the state   and then the world that was the state to thee   that is the sea for the most contrary.  king richard ii   then we shall be a surper in the seas  of the statue of my sons and therefore with the statue  to the sea of men with the storesy.    king edward iv   steep that we do desire. near me in seeming here   hastings   and i am coming to pray you.    lstm hw  x     .       lstm hw  this work      .       .       zynq zc     cpu   .       exynos      cortex a     bianca   he shall be you both so  get your lord  i ll sea tay.  the law  how both his sake  let him not only smiles  aad my sons but was  lend that common me within   i mean the case of me chooser.     .       exynos      cortex a       .       tegra tk  gpu     .       tegra tk  cpu    lucio   and of this feelen clus on what i was forward.  yer  as it is trudment.                         queen margaret   and then the world that was the state to thee   that is the man that should be so far o the worl       petruchio   suggion your shape   than all the morn unknoward   which  good how be thy jus ce  which he will dead   speak  let me took no more pitch  the bloody dealest heart would show it  to death.  and if those caius shalt go alone  befe ence  or lie  the love fell d with mine   then  yet so  fair of him wall  graces  be curse i wish thee  cake for my heart  and eat and ever                      execution time s          duke vicentio   i cannot say the prince that hath a sun  to the prince and the season of the state   and then the world that was the state to thee   that is the sea for the most cause to the common sons   that we may be the sun to the state to thee   that is the man that should be so far o the world.    figure  . execution time of feedforward lstm character level language  model on different embedded platforms  the lower the better .              figure  . on the left side is the output text from the lstm hardware. on  the right side is text from cpu implementation. the model predict each  next character based on the previous characters. the model only gets the  first character  seed  and generates the entire text character by character.           lstm hw  x     .     lstm hw  this work   zynq zc     cpu     .      exynos      cortex a   .    exynos      cortex a       .      tegra tk  gpu     .      tegra tk  cpu    it makes sense to compress those weights into different  number formats for a throughput versus accuracy trade  off. the use of fixed point q .  data format certainly  introduces rounding errors. then one may raise the question  of how much these errors can propagate to the final output.  comparing the results from the torch  code with the lstm  module s output for same xt sequence  the average percentage error for the ct was  .   and for ht was  .  . those  values are average error of all time steps. the best was  .    and the worse was  .  . the recurrent nature of lstm did  not accumulate the errors and on average it stabilized at a  low percentage.  the text generated by sampling      characters  timestep  t     to       is shown in figure  . on the left is text output  from fpga and on the right is text from the cpu implementation. the result shows that the lstm model was able to  generate personage dialog  just like in one of shakespeare s  book. both implementations displayed different texts  but  same behavior.  b. memory bandwidth  the zedboard zynq zc     platform has   advanced  extensible interface  axi  dma ports available. each is  ran at     mhz and send packages of    bits. this allows  aggregate bandwidth up to  .  gb s full duplex transfer  between fpga and external ddr .  at     mhz  one lstm module is capable of computing     .  m ops s and uses simultaneously   axi dma ports  for streaming weight and vector values. during the peak  memory usage  the module requires  .    gb s of memory  bandwidth. the high memory bandwidth requirements poses      .                                                  performace per watt  mop s w     figure  . performance per unit power of different embedded platforms   the higher the better .    a limit to the number of lstm modules that can be ran on  parallel. to replicated lstm module  it is required higher  memory bandwidth or to introduce internal memory to lower  requirements of external ddr  memory usage.  c. performance  figure   shows the timing results. one can observe that  the implemented hardware lstm was significantly faster  than other platforms  even running at lower clock frequency  of     mhz  zynq zc     cpu uses     mhz . scaling  the implemented design by replicating the number of lstm  modules running in parallel will provide faster speed up.  using   lstm cells in parallel can be     faster than  exynos     on quad core arm cortex a .  in one lstm layer of size      there are    .  kops. multiplying this by number of samples and number  of layers for the experimental application        gives the  total number of operations    .  m ops. the execution time  divided by number of operations gives performance  ops s .  the power consumption of each platform was measured.  figure   shows the performance per unit power.  the gpu performance was slower because of the following reasons. the model is too small for getting benefit  from gpu  since the software needs to do memory copies.  this is confirmed by running the same torch  code on a     macbook pro     . the cpu of the macbook pro       executed the torch  code for character level language model  in  .    s  whereas the macbook pro      s gpu executed  the same test in  .    s.  vii. c onclusion  recurrent neural networks have recently gained popularity due to the success from the use of long short term  memory architecture in many applications  such as speech  recognition  machine translation  scene analysis and image  caption generation.  this work presented a hardware implementation of lstm  module. the hardware successfully produced shakespearelike text using a character level model. furthermore  the  implemented hardware showed to be significantly faster than  other mobile platforms. this work can potentially evolve  to a rnn co processor for future devices  although further  work needs to be done. the main future work is to optimize  the design to allow parallel computation of the gates. this  involves designing a parallel mac unit configuration to  perform the matrix vector multiplication.  acknowledgments  this work is supported by office  of naval research  onr  grants   pr         p       and muri n             and national council for the  improvement of higher education  capes  through brazil  scientific mobility program  bsmp . we would like to  thank vinayak gokhale for the discussion on implementation  and hardware architecture and also thank alfredo canziani   aysegul dundar and jonghoon jin for the support. we  gratefully appreciate the support of nvidia corporation  with the donation of gpus used for this research.  