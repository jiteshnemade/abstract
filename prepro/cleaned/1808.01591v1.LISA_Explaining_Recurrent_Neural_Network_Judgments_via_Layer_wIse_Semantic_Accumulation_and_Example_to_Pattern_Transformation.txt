introduction    the interpretability of systems based on deep neural network is required to be able to explain the  reasoning behind the network prediction s   that  offers to     verify that the network works as expected and identify the cause of incorrect decision s      understand the network in order to improve data or model with or without human intervention. there is a long line of research in  techniques of interpretability of deep neural networks  dnns  via different aspects  such as explaining network decisions  data generation  etc.  erhan et al.         hinton         simonyan et al.         and nguyen et al.        focused on model    aspects to interpret neural networks via activation maximization approach by finding inputs that  maximize activations of given neurons. goodfellow et al.        interprets by generating adversarial examples. however  baehrens et al.        and  bach et al.         montavon et al.        explain  neural network predictions by sensitivity analysis  to different input features and decomposition of  decision functions  respectively.  recurrent neural networks  rnns   elman         are temporal networks and cumulative in  nature to effectively model sequential data such  as text or speech. rnns and their variants such  as lstm  hochreiter and schmidhuber         have shown success in several natural language  processing  nlp  tasks  such as entity extraction   lample et al.        ma and hovy         relation extraction  vu et al.      a  miwa and bansal         gupta et al.            c   language modeling  mikolov et al.        peters et al.          slot filling  mesnil et al.        vu et al.      b    machine translation  bahdanau et al.         sentiment analysis  wang et al.        tang et al.          semantic textual similarity  mueller and  thyagarajan        gupta et al.      a  and dynamic topic modeling  gupta et al.      d .  past works  zeiler and fergus        dosovitskiy and brox        have mostly analyzed deep  neural network  especially cnn in the field of  computer vision to study and visualize the features  learned by neurons. recent studies have investigated visualization of rnn and its variants. tang  et al.        visualized the memory vectors to understand the behavior of lstm and gated recurrent unit  gru  in speech recognition task. for  given words in a sentence  li et al.        employed heat maps to study sensitivity and meaning composition in recurrent networks. ming et al.         proposed a tool  rnnvis to visualize hidden states based on rnn s expected response to       e    ub  hb    wb    wbi         wf    wf    uf  demolition    uf    e      hb         hbi       wf    hf  uf         the    was  ub  wb    hb     hbi    wf    hf    uf    was    wbi         wf    hf    wb         wbi    hbi       hf    wb    hb  wbi    hbi    the  ub    ub    wb         wbi    hbi    hf     e      hb    c brnn    cause    ub    wb         wf  uf    hb  wbi    hbi    of    ub    wb            hf     e      hb       hbi    backward direction    terror  ub    wf    hf  uf  of    forward direction    hb    demolition  wb       hbi         uf  cause    wbi      e    ub    wbi    wf    hf  uf   e      hb    wb       hbi          e      ub    wbi    wf  uf    terror    causeeffect   e  e      hb    r  e  l  a  t  i  o  n       hbi       hf    ub    why       hf  uf    softmax  layer      e      figure    connectionist bi directional recurrent neural network  c brnn   vu et al.      a     inputs. peters et al.        studied the internal states of deep bidirectional language model to  learn contextualized word representations and observed that the higher level hidden states capture  word semantics  while lower level states capture  syntactical aspects. despite the possibility of visualizing hidden state activations and performancebased analysis  there still remains a challenge for  humans to interpret hidden behavior of the black  box  networks that raised questions in the nlp  community as to verify that the network behaves  as expected. in this aspect  we address the cumulative nature of rnn with the text input and  computed response to answer  how does it aggregate and build the semantic meaning of a sentence  word by word at each time point in the sequence  for each category in the data .  contribution  in this work  we analyze and interpret the cumulative nature of rnn via a proposed technique named as layer wise semanticaccumulation  lisa  for explaining decisions and  detecting the most likely  i.e.  saliency  patterns  that the network relies on while decision making.  we demonstrate     lisa   how an rnn accumulates or builds semantics during its sequential processing for a given text example and expected response      example pattern   how the saliency  patterns look like for each category in the data according to the network in decision making . we  analyse the sensitiveness of rnns about different  inputs to check the increase or decrease in prediction scores. for an example sentence that is classified correctly  we identify and extract a saliency    pattern  n grams of words in order learned by the  network  that contributes the most in prediction  score. therefore  the term example pattern transformation for each category in the data. we employ two relation classification datasets  semeval     task   and tac kbp slot filling  sf  shared  task  st  to explain rnn predictions via the proposed lisa and example pattern techniques.         connectionist bi directional rnn    we adopt the bi directional recurrent neural network architecture with ranking loss  proposed by  vu et al.      a . the network consists of three  parts  a forward pass which processes the original  sentence word by word  equation     a backward  pass which processes the reversed sentence word  by word  equation     and a combination of both   equation   . the forward and backward passes  are combined by adding their hidden layers. there  is also a connection to the previous combined hidden layer with weight wbi with a motivation to include all intermediate hidden layers into the final  decision of the network  see equation   . they  named the neural architecture as  connectionist  bi directional rnn   c brnn . figure   shows  the c brnn architecture  where all the three parts  are trained jointly.  hft   f  uf   wt   wf   hft               hbt   f  ub   wn t     wb   hbt               hbit   f  hft   hbt   wbi   hbit               where wt is the word vector of dimension d for  a word at time step t in a sentence of length n.     prediction probability for  relation index in softmax    lisa     .     .     .     .     .     .     .     .     .     .      demolition      e      was    the    cause     e      terror     .       .       .       .       .       .       .       .       .       .      c brnn    c brnn    c brnn    c brnn    c brnn    c brnn    c brnn    c brnn    c brnn    c brnn     e    softmax  desicion  layer    of      e       e       e   demolition   e   demolition   e     e   demolition   e   was   e   demolition   e   was the  subsequences of   e   demolition   e   was the cause  sentence s    e   demolition   e   was the cause of   e   demolition   e   was the cause of  e     e   demolition   e   was the cause of  e   terror   e   demolition   e   was the cause of  e   terror   e      figure    an illustration of layer wise semantic accumulation  lisa  in c brnn  where we compute    prediction score for a  known  relation type at each of the input subsequence. the highlighted indices in  the softmax layer signify one of the relation types  i.e.  cause effect e   e   in semeval   task   dataset.  the bold signifies the last word in the subsequence. note  each word is represented by n gram  n       or     therefore each input subsequence is a sequence of n grams. e.g.  the word  of     cause of   e    for n  . to avoid complexity in this illustration  each word is shown as a uni gram.  d is the hidden unit dimension. uf   rd d  and ub   rd d are the weight matrices between  hidden units and input wt in forward and backward networks  respectively  wf   rd d and  wb   rd d are the weights matrices connecting hidden units in forward and backward networks  respectively. wbi   rd d is the weight  matrix connecting the hidden vectors of the combined forward and backward network. following  gupta et al.        during model training  we use    gram and   gram representation of each word  wt at timestep t in the word sequence  where a  gram for wt is obtained by concatenating the corresponding word embeddings  i.e.  wt   wt wt   .  ranking objective  similar to santos et al.         and vu et al.      a   we applied the ranking loss function to train c brnn. the ranking  scheme offers to maximize the distance between  the true label y   and the best competitive label c   given a data point x. it is defined as     l   log     exp   m   s   x y         log     exp   m    s   x c                where s   x y  and s   x c  being the scores for  the classes y   and c    respectively. the parameter   controls the penalization of the prediction  errors and m  and m are margins for the correct  and incorrect classes. following vu et al.      a    we set        m     .  and m     . .    model training and features  we represent  each word by the concatenation of its word embedding and position feature vectors. we use  word vec  mikolov et al.        embeddings   that are updated during model training. as position features in relation classification experiments  we use position indicators  pi   zhang and  wang        in c brnn to annotate target entity nominals in the word sequence  without necessity to change the input vectors  while it increases  the length of the input word sequences  as four  independent words  as position indicators   e        e     e      e    around the relation arguments are introduced.  in our analysis and interpretation of recurrent  neural networks  we use the trained c brnn   figure     vu et al.      a  model.         lisa and example pattern in rnn    there are several aspects in interpreting the neural network  for instance via     data   which dimensions of the data are the most relevant for the  task      prediction or decision   explain why a  certain pattern  is classified in a certain way      model   how patterns belonging to each category  in the data look like according to the network .  in this work  we focus to explain rnn via decision and model aspects by finding the patterns  that explains  why  a model arrives at a particu      lar decision for each category in the data and verifies that model behaves as expected. to do so  we  propose a technique named as lisa that interprets  rnn about  how it accumulates and builds meaningful semantics of a sentence word by word  and   how the saliency patterns look like according to  the network  for each category in the data while  decision making. we extract the saliency patterns  via example pattern transformation.  lisa formulation  to explain the cumulative nature of recurrent neural networks  we show  how does it build semantic meaning of a sentence  word by word belonging to a particular category  in the data and compute prediction scores for the  expected category on different inputs  as shown in  figure  . the scheme also depicts the contribution of each word in the sequence towards the final  classification score  prediction probability .  at first  we compute different subsequences  of word s  for a given sequence of words  i.e.   sentence . consider a sequence s of words   w    w    ...  wk   ...  wn   for a given sentence s of  length n. we compute n number of subsequences   where each subsequence s k is a subvector of  words  w    ...wk    i.e.  s k consists of words preceding and including the word wk in the sequence  s. in context of this work  extending a subsequence by a word means appending the subsequence by the next word in the sequence. observe  that the number of subsequences  n is equal to the  total number of time steps in the c brnn.  next is to compute rnn prediction score for the  category r associated with sentence s. we compute the score via the autoregressive conditional  p  r s k   m  for each subsequence s k   asp  r s k   m    sof tmax why   hbik   by        using the trained c brnn  figure    model  m. for each k       n   we compute the network prediction  p  r s k   m  to demonstrate the  cumulative property of recurrent neural network  that builds meaningful semantics of the sequence  s by extending each subsequence s k word by  word. the internal state hbik  attached to softmax  layer as in figure    is involved in decision making  for each input subsequence s k with bias vector  by   rc and hidden to softmax weights matrix  why   rd c for c categories.  the lisa is illustrated in figure    where each  word in the sequence contributes to final classification score. it allows us to understand the network decisions via peaks in the prediction score    algorithm   example pattern transformation  input  sentence s  length n  category r   threshold     c brnn m  n gram size n  output  n gram saliency pattern patt     for k in   to n do      compute n gramk  eqn    of words in s  for k in   to n do  compute s k  eqn    of n grams      compute p  r s k   m  using eqn        if p  r s k   m      then      return patt   s k                   over different subsequences. the peaks signify  the saliency patterns  i.e.  sequence of words  that  the network has learned in order to make decision. for instance  the input word  of  following  the subsequence   e   demolition   e   was  the cause  introduces a sudden increase in prediction score for the relation type cause effect e    e  . it suggests that the c brnn collects the semantics layer wise via temporally organized subsequences. observe that the subsequence  ...cause  of  is salient enough in decision making  i.e.  prediction score  .     where the next subsequence   ...cause of  e    adds in the score to get  .  .  example pattern for saliency pattern  to  further interpret rnn  we seek to identify and extract the most likely input pattern  or phrases  for  a given class that is discriminating enough in decision making. therefore  each example input is  transformed into a saliency pattern that informs us  about the network learning. to do so  we first  compute n gram for each word wt in the sentence s. for instance  a   gram representation  of wt is given by wt     wt   wt   . therefore  an  n gram  for n    sequence s of words is represented as   wt     wt   wt    nt      where w  and  wn   are padding  zero  vectors of embedding  dimension.  following vu et al.      a   we use n grams   e.g.  tri grams  representation for each word in  each subsequence s k that is input to c brnn  to compute p  r s k    where the n gram  n     subsequence s k is given by   s k     p addin g  w    w        w    w    w       ...    wt     wt   wt    t   ...   wk     wk   wk    k         s k    tri    tri    ...  trit   ...trik             for k       n . observe that the   gram trik con       g  lisa for s      j  t sne visualization for training set    e     e      ca  stl    e   e        of    by  th  m e  aj  o    r  e  pr     od  uc    er   e           ci e    ga  re  tt    es   e        prediction probability  i    n  e  lo     ca  tio    n   e         h  lisa for s     product producer e   e       f  lisa for s     slot per location of birth       e   pe    rs  o    n   e      w  as  bo  rn        .    .    .    .    .    .    .    .    .        th       co e    ur  ty  ar    d   e        prediction probability     e      pl  an    t   e        th    e    t        .    .    .    .    .    .    .    .    .         e  lisa for s     prediction probability  instrument agency e   e         ci e    ga  re  tt    es   e      ar  e  us  ed  b    y  e   w    om  e    n   e        prediction probability     d  lisa for s     component whole e   e       c  lisa for s     entity origin e   e    le  f       e            .    .    .    .    .    .    .    .    .        ca    r   e        prediction probability    prediction probability  entity destination e   e      s d  l     rble    wa ppe into the e  bow  e     e ma   e        dro        .    .    .    .    .    .    .    .    .        th    e  bo e    m  bi  n    g   e           e  da     m  ag    e   e     ca    us  ed  by     b  lisa for s      a  lisa for s       .    .    .    .    .    .    .    .    .        cause effect e   e          .    .    .    .    .    .    .    .    .            .    .    .    .    .    .    .    .    .        slot per spouse     e      pe  rs  on      e      m  ar  rie  d     e      sp  ou  se      e            .    .    .    .    .    .    .    .    .        prediction probability    prediction probability  cause effect e   e      de  e  m     ol  iti    on   e      w  as  th  ca e  us  e    of  e      te  rr    or   e        prediction probability        .    .    .    .    .    .    .    .    .         i  lisa for s      k  t sne visualization for testing set    figure     a i  layer wise semantic accumulation  lisa  by c brnn for different relation types in    semeval   task   and tac kbp slot filling datasets. the square in red color signifies that the relation  is correctly detected with the input subsequence  enough in decision making .  j k  t sne visualization  of the last combined hidden unit  hbi   of c brnn computed using the semeval   train and test sets.     id  s   s   s   s   s   s   s   s   s     relation slot types  cause effect e   e    cause effect e   e    component whole e   e    entity destination e  e    entity origin e   e    product produce e   e    instrument agency e   e    per loc of birth e   e    per spouse e   e      example sentences   e   demolition   e   was the cause of  e   terror   e     e   damage   e   caused by the  e   bombing   e     e   countyard   e   of the  e   castle   e     e   marble   e   was dropped into the  e   bowl   e     e   car   e   left the  e   plant   e     e   cigarettes   e   by the major  e   producer   e     e   cigarettes   e   are used by  e   women   e     e   person   e   was born in  e   location   e     e   person   e   married  e   spouse   e      example pattern  cause of  e    damage   e   caused    e   of the  dropped into the  left the  e      e   by the    e   are used  born in  e      e   married  e      table    example sentences for lisa and example pattern illustrations. the sentences s  s  belong to  semeval   task   dataset and s  s  to tac kbp slot filling  sf  shared task dataset.    sists of the word wk     if k    n. to generalize for  i       bn  c   an n gramk of size n for word wk  in c brnn is given byn gramk    wk i   ...  wk   ...  wk i  k           algorithm   shows the transformation of an example sentence into pattern that is salient in decision making. for a given example sentence s with  its length n and category r  we extract the most  salient n gram  n      or    pattern patt  the last  n gram in the n gram subsequence s k   that contributes the most in detecting the relation type r.  the threshold parameter   signifies the probability of prediction for the category r by the model  m. for an input n gram sequence s k of sentence s  we extract the last n gram  e.g.  trik that  detects the relation r with prediction score above    . by manual inspection of patterns extracted at  different values   .    .    .    .   of     we found  that      .  generates the most salient and interpretable patterns. the saliency pattern detection  follows lisa as demonstrated in figure    except  that we use n gram  n       or    input to detect  and extract the key relationship patterns.         analysis  relation classification    given a sentence and two annotated nominals  the  task of binary relation classification is to predict  the semantic relations between the pairs of nominals. in most cases  the context in between the  two nominals define the relationship. however   vu et al.      a  has shown that the extended context helps. in this work  we focus on the building  semantics for a given sentence using relationship  contexts between the two nominals.  we analyse rnns for lisa and example pattern using two relation classification datsets      semeval   shared task    hendrickx    input word sequence to c brnn   e     e   demolition   e   demolition   e     e   demolition   e   was   e   demolition   e   was the   e   demolition   e   was the cause   e   demolition   e   was the cause of   e   demolition   e   was the cause of  e     e   demolition   e   was the cause of  e   terror   e   demolition   e   was the cause of  e   terror   e      pp   .     .     .     .     .     .     .     .     .     .      table    semantic accumulation and sensitivity    of c brnn over subsequences for sentence s .  bold indicates the last word in the subsequence.  pp  prediction probability in the softmax layer for  the relation type. the underline signifies that the  pp is sufficient enough      .    in detecting the  relation. saliency patterns  i.e.  n grams can be  extracted from the input subsequence that leads to  a sudden peak in pp  where pp     .  et al.            tac kbp slot filling  sf  shared  task   adel and schu tze       . we demonstrate the sensitiveness of rnn for different subsequences  figure     input in the same order as  in the original sentence. we explain its predictions  or judgments  and extract the salient relationship patterns learned for each category in the  two datasets.   .     semeval   shared task   dataset    the relation classification dataset of the semantic  evaluation       semeval    shared task    hendrickx et al.        consists of    relations    directed relations and one artificial class other          training and       testing sentences. we  split the training data into train   . k  and development   . k  sentences to optimize the c brnn     data from the slot filler classification component of the  slot filling pipeline  treated as relation classification     relation  causeeffect e  e      causeeffect e  e      contentcontainer e  e      productproduce e  e      whole e   e    component     entitydestination e  e      instrumentagency e  e        gram patterns    e   cause  e      e   caused a  that cause respiratory  which cause acne  leading causes of  caused due to  comes from the  arose from an  caused by the  radiated from a  in a  e    was inside a  contained in a  hidden in a  stored in a    e   released by    e   issued by    e   created by  by the  e    of the  e      e   of the  of the  e    part of the    e   of  e      e   on a  put into a  released into the    e   into the  moved into the  added to the    e   are used  used by  e      e   is used  set by the    e   set by      gram patterns  the leading causes of  e    the main causes of  e      e   leads to  e   inspiration    e   that results in  e      e   resulted in the  e      e   has been caused by    e   are caused by the    e   arose from an  e      e   caused due to  e    infection   e   results in an    e   was contained in a    e   was discovered inside a    e   were in a  e    is hidden in a  e      e   was contained in a    e   issued by the  e      e   was prepared by  e    was written by a  e      e   built by the  e      e   are made by  e      e   of the  e   device    e   was a part of    e   is part of the  is a basic element of    e   is part of a  have been moving into the  was dropped into the  e      e   moved into the  e    were released into the  e      e   have been exported to    e   assists the  e   eye    e   are used by  e      e   were used by some    e   with which the  e    readily associated with the  e        gram patterns  is one of the leading causes of  is one of the main causes of    e   that results in  e   hardening   e      e   resulted in the  e   loss   e     e   sadness   e   leads to  e   inspiration    e   is caused by a  e   comet    e   however has been caused by the    e   that has been caused by the  that has been caused by the  e     e   product   e   arose from an  e      e   was contained in a  e   box    e   was in a  e   suitcase   e      e   were in a  e   box   e      e   was inside a  e   box   e      e   was hidden in an  e   envelope   e   products   e   created by an  e      e   by an  e   artist   e   who    e   written by most of the  e    temple   e   has been built by  e      e   were founded by the  e   potter  the  e   timer   e   of the  e      e   was a part of the romulan    e   was the best part of the    e   is a basic element of the  are core components of the  e   solutions    e   have been moving back into  e      e   have been moving into the  e      e   have been dropped into the  e      e   have been released back into the  power   e   is exported to the  e    cigarettes   e   are used by  e   women   e   telescope   e   assists the  e   eye   e   practices   e   for  e   engineers   e    the best  e   tools   e   for  e     e   wire   e   with which the  e      table    semeval   task   dataset  n gram       and    saliency patterns extracted for different relation    types by c brnn with pi  network. for instance  an example sentence with  relation label is given bythe  e   demolition   e   was  the cause of  e   terror   e    and communal divide is just a way  of not letting truth prevail.    cause effect e  e    the terms demolition and terror are  the relation arguments or nominals  where the  phrase was the cause of is the relationship  context between the two arguments. table    shows the examples sentences  shortened to argument  relationship context argument   drawn  from the development and test sets that we employed to analyse the c brnn for semantic accumulation in our experiments. we use the similar  experimental setup as vu et al.      a .    lisa analysis  as discussed in section    we  interpret c brnn by explaining its predictions  via the semantic accumulation over the subsequences s k  figure    for each sentence s. we  select the example sentences s  s   table    for  which the network predicts the correct relation  type with high scores. for an example sentence  s   table   illustrates how different subsequences  are input to c brnn in order to compute prediction scores pp in the softmax layer for the relation  cause effect e   e  . we use tri gram   section    word representation for each word for  the examples s  s .  figures  a   b   c   d  e   f and  g demonstrate the cumulative nature and sensitiveness of  rnn via prediction probability  pp  about different inputs for sentences s  s   respectively. for     slots    perspouse e  e      perlocation of birth e  e      n gram patterns    e   wife of    e     wife    e   wife    e   married  e      e   marriages to  was born in  born in  e    a native of    e   from  e      e    s hometown    table    tac kbp sf dataset  tri gram saliency    patterns extracted for slots per spouse e   e   and  per location of birth e  e      instance in figure  a and table    the c brnn  builds meaning of the sentence s  word by word   where a sudden increase in pp is observed when  the input subsequence  e   demolition    e   was the cause is extended with the  next term of in the word sequence s. note that  the relationship context between the arguments  demolition and terror is sufficient enough  in detecting the relationship type. interestingly   we also observe that the prepositions  such as of   by  into  etc.  in combination with verbs are key  features in building the meaningful semantics.  saliency patterns via example pattern transformation  following the discussion in section    and algorithm    we transform each correctly  identified example into pattern by extracting the  most likely n gram in the input subsequence s .  in each of the figures  a   b   c   d  e   f and  g   the square box in red color signifies that the relation type is correctly identified  when      .   at  this particular subsequence input  without the remaining context in the sentence . we extract the  last n gram of such a subsequence.  table   shows the example pattern transformations for sentences s  s  in semeval   dataset   derived from figures  a  g  respectively with n     in the n grams . similarly  we extract the salient  patterns    gram    gram and   gram   table     for different relationships. we also observe that  the relation types content container e    e    and  instrument agency e    e   are mostly defined by smaller relationship contexts  e.g    gram   however  entity destination e  e   by larger  contexts    gram .     .     tac kbp slot filling dataset    we investigate another dataset from tac kbp  slot filling  sf  shared task  surdeanu          where we use the relation classification dataset by  adel et al.        in the context of slot filling. we  have selected the two slots  per loc of birth and  per spouse out of    types.  lisa analysis  following section  .   we analyse the c brnn for lisa using sentences s   and s   table   . figures  h and  i demonstrate  the cumulative nature of recurrent neural network   where we observe that the salient patterns born  in  e   and   e   married e  lead to  correct decision making for s  and s   respectively. interestingly for s   we see a decrease in  prediction score from  .   to  .   on including  terms in the subsequence  following the term in.  saliency patterns via example pattern transformation  following section   and algorithm     we demonstrate the example pattern transformation of sentences s  and s  in table   with tirgrams. in addition  table   shows the tri gram  salient patterns extracted for the two slots.         visualizing latent semantics    in this section  we attempt to visualize the hidden  state of each test  and train  example that has accumulated  or built  the meaningful semantics during sequential processing in c brnn. to do this   we compute the last hidden vector hbi of the combined network  e.g.  hbi attached to the softmax  layer in figure    for each test  and train  example and visualize  figure  k and  j  using t sne   maaten and hinton       . each color represents  a relation type. observe the distinctive clusters of  accumulated semantics in hidden states for each  category in the data  semeval   task   .         conclusion and future work    we have demonstrated the cumulative nature of  recurrent neural networks via sensitivity analysis  over different inputs  i.e.  lisa to understand how  they build meaningful semantics and explain predictions for each category in the data. we have  also detected a salient pattern in each of the example sentences  i.e.  example pattern transformation that the network learns in decision making.  we extract the salient patterns for different categories in two relation classification datasets.  in future work  it would be interesting to analyse the sensitiveness of rnns with corruption in     the salient patterns. one could also investigate  visualizing the dimensions of hidden states  activation maximization  and word embedding vectors with the network decisions over time. we  forsee to apply lisa and example pattern on different tasks such as document categorization  sentiment analysis  language modeling  etc. another interesting direction would be to analyze  the bag of word neural topic models such as docnade  larochelle and lauly        and idocnade  gupta et al.      b  to interpret their semantic accumulation during autoregressive computations in building document representation s .  we extract the saliency patterns for each category in the data that can be effectively used in  instantiating pattern based information extraction  systems  such as bootstrapping entity  gupta and  manning        and relation extractors  gupta  et al.      e .    acknowledgments  we thank heike adel for providing us with  the tac kbp dataset used in our experiments. we express appreciation for our colleagues bernt andrassy  florian buettner  ulli  waltinger  mark buckley  stefan langer  subbu  rajaram  yatin chaudhary  and anonymous reviewers for their in depth review comments. this  research was supported by bundeswirtschaftsministerium  bmwi.de   grant   md     a  smart  data web  at siemens ag  ct machine intelligence  munich germany.    