introduction   . recurren neural networks  rnns    . . rnn definition  a recurrent neural network  rnn      has a visible state xt at each time  step  and a corresponding hidden state ht . the  dynamics can be described in terms of two distributions p ht  ht     xt   and p  xt  ht  . typically the state of the    hidden  units ht is deterministic given xt   and ht     the values at the previous timestep  such that ht   f ht     xt   . taking  slight liberties with notation  we indicate the distribution of the hidden units given their parents as               p ht  ht     xt       ht   f ht     xt      p  h x       h   f  x   .       where x    x        xt   and h    h        ht   are the full trajectories over visible and hidden units. the initial hidden  vector h  is included as a model parameter.   . . rnn training  training is typically performed by maximizing the log likelihood of this model  computed from a data distribution over  trajectories q  x . note that this empirical data distribution q  x  is most often simply a collection of datapoints  i.e. dirac  delta peaks.        argmax  eq x   log p  x         f  ht     xt     p  xt  ht    f  ht    xt     p xt  ht           argmax  f  ht    xt     p xt  ht           argmax  f  ht    xt     p xt  ht           eq x  ep h x             x    t    log p x  h    t    eq x  e  h f  x         x  t    t             log p xt  h           t         .            . variational bayesian perspective  variational bayesian methods where both the generative and inference models are trained against each other have recently  proven very powerful for building probabilistic models of arbitrary distributions                      . here we show how an  rnn can be interpreted using a variational bayesian framework.     rnn vae equivalence     . . inference model  we take p  x  h  from section  .  to be the  generative  model  and introduce an  inference  model q  h x . we set the  inference model to be identical to the corresponding conditional distribution in the generative model         q ht  xt     ht     p ht  xt     ht         y     q  h x          q ht  xt     ht    t      p  h x  .            . . log likelihood bound  we now derive a variational bound k on the data log likelihood l   eq x   log p x   for these generative and inference  distributions   l   eq x   log p x      z       eq x  log dh p  h  x             p  h  x     eq x  log eq h x   q  h x             p  h  x     eq x  eq h x  log  q  h x                q  t t      ht   p  xt  ht    t p h  x  q    k   eq x  eq h x  log  t t     ht      t q  h  x               q  t t    t    t t  p  h   x     h  p   x   h       eq x  eq h x  log t q  t t     ht      t p  h  x            x     t t    eq x  eq h x   log p x  h                      jensen s inequality                         t           eq x  e  h f  x           x  t    log p xt  h           t    .            the bound in equation    is identical to the rnn training objective in equation  . therefore  for the choice of inference  model in equation    the variational bayesian training objective is identical to the standard log likelihood training objective.   . . optimality of noise free hidden dynamics  often  noise free dynamics is optimal w.r.t. k. if the latent dynamics p ht  ht     xt     is in the location scale family with  location   ht     xt     and scale    we can parameterize the latent variables as ht   f  ht     xt            t   where f is a  deterministic function and  t   p    is independent zero centered noise per timestep. equation      can be written in this  so called non centered form     as             x     t t  eq x  ep           log p x  h  t    t    the inserted noise   has the effect of removing information about previous states from the hidden state ht   such that xt  will be harder to predict. this contribution of the noise to ht can trivially be minimized by letting        i.e. by choosing  q h x      h   f  x    eq.      .     . discussion  from one perspective it is a trivial observation that if q  h x    p  h x   then the variational bayesian objective becomes  the true log likelihood objective. from another perspective  it is non obvious and interesting that due to its causal structure      rnn vae equivalence    a recurrent neural network can be viewed simultaneously as an inference and generative model  and that the inference  model can be made trivially identical to the posterior of the generative model.  note that the equivalence q  h x    pq h x  in equation   relies on the true posterior distribution p  h x  having the  causal  factorial  structure p  h x    t p ht  xt     ht      . this structure stems from the deterministic dynamics of the  rnn  and would not hold in general if p ht  ht     xt   were not a delta function. in this case the variational bound on  the log likelihood in equation    would continue to hold  but it would no longer be identical to the true log likelihood in  equation  .  this perspective on rnns as consisting of matching inference and generative models may suggest natural extensions to  the rnn framework  or novel model forms for variational bayesian methods. as one example  it suggests the use of  multiple inference particles in an rnn  which may allow more complex and multimodal distributions over visible units to  be captured by simpler and lower dimensional hidden representations.   . . multiple particles     rnns are often called upon to represent a multimodal distribution over p xt  x        xt    for instance  a distribution  over words in the context of language models . since the hidden state of an rnn is deterministic  it must capture this  multimodal distribution using a single high dimensional vector ht .  in variational inference  a multimodal posterior can be approximated using multiple samples from the inference model.  this raises the possibility of training an rnn with a multimodal distribution over hidden units. this has the potential to  reduce the required complexity of the rnn. rather than forcing a high dimensional unimodal distribution over hidden  units to represent a multimodal distribution over visible units  instead multiple modes in a lower dimensional hidden  representation can be made to correspond to the multiple modes over the visible units.  specifically  the hidden state can be extended to consist of l samples h    h    h            hl  . as shown in appendix a  these multiple samples can be averaged over in the variational bayesian framework. this leads to the following modification of the training objective from equation                 l  x      x  p xt  htl  .        l   k   eq x  e  h f  x    log  l  t  l      the multiple samples can be initialized with different  learned  initial vectors h l   allowing them to explore different modes  despite being governed by the same deterministic dynamics.     rnn vae equivalence    appendix  a. multiple particles in variational bayesian models  here we modify the derivation in section  .  to include multiple particles  leading to equation   .  l     x  l    z    dhl p  hl   x      x  l    z    dhl     x     l    z    y  p  hl   x   q  hl  x   dhl  q  hl  x        p  x       l    l         l    l    l              p  hl   x   q  hl  x   q  hl  x             l   l    z    dhl  q  hl   x     l z  y   x  p  hl   x   q  hl  x   q  hl   x   dh  l  q  hl  x   l    l  l    y  q  h x     q  hl  x                                l    l z   x  p  hl   x   dh q  h x   l  q  hl  x   l       z  l    x p  hl   x     dh q  h x   l  q  hl  x     p  x                       l      l   eq x   log p x              l  x                   p  hl   x   l  q  hl  x   l               l  x      x  t t     p x  hl  l   eq x  e  h f  x    log  l  t    eq x  log eq h x                     l      where the steps between equations    and    parallel those in section  . .    