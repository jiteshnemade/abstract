introduction    machine learning has been widely used in various commercial and non commercial products  and has  brought great convenience and profits to human beings. however  recent researches on adversarial  examples show that many machine learning algorithms are not robust at all when someone want to  crack them on purpose        . adding some small perturbations to original samples will make a  classifier unable to classify them correctly.  in some security related applications  attackers will try their best to attack any defensive systems to  spread their malicious products such as malware. existing machine learning based malware detection  algorithms mainly represent programs as feature vectors with fixed dimension and classify them  between benign programs and malware     . for example  a binary feature vector can be constructed  according to the presences or absences of system apis  i.e. application programming interfaces  in a  program     . grosse et al.     and hu et al.      have shown that fixed dimensional feature based  malware detection algorithms are very vulnerable under the attack of adversarial examples.  recently  as recurrent neural networks  rnn  become popular  some researchers have tried to use  rnn for malware detection and classification             . the api sequence invoked by a program  is used as the input of rnn. rnn will predict whether the program is benign or malware.  this paper tries to validate the security of a rnn based malware detection model when it is attacked  by adversarial examples. we proposed a novel algorithm to generate sequential adversarial examples.  existing researches on adversarial samples mainly focus on images. images are represented as  matrices with fixed dimensions  and the values of the matrices are continuous. api sequences       prof. ying tan is the corresponding author.     consist of discrete symbols with variable lengths. therefore  generating adversarial examples for api  sequences will become quite different from generating adversarial examples for images.  to generate adversarial examples from api sequences we only consider to insert some irreverent  apis into the original sequences. removing an api from the api sequence may make the program  unable to work. how to insert irreverent apis into the sequence will be the key to generate adversarial  examples.  we propose a generative rnn based approach to generate irreverent apis and insert them into the  original api sequences. a substitute rnn is trained to fit the victim rnn. gumbel softmax      is  used to smooth the api symbols and deliver gradient information between the generative rnn and  the substitute rnn.         adversarial examples    adversarial examples are usually generated by adding some perturbations to the original samples.  szegedy et al. used a box constrained l bfgs to search for an appropriate perturbation which can  make a neural network misclassify an image     . they found that adversarial examples are able to  transfer among different neural networks. goodfellow et al. proposed the  fast gradient sign method   where added perturbations are determined by the gradients of the cost function with respect to inputs     . an iterative algorithm to generate adversarial examples was proposed by papernot et al.     . at  each iteration the algorithm only modifies one pixel or two pixels of the image.  grosse et al. used the iterative algorithm proposed by papernot et al.      to add some adversarial  perturbations to android malware on about     thousand binary features    . for the best three  malware detection models used in their experiments  about     to     malware will become  undetected after their adversarial attacks.  previous algorithms to generate adversarial examples mainly focused on attacking feed forward  neural networks. papernot et al. migrated these algorithms to attack rnn     . rnn is unrolled  along time and existing algorithms for feed forward neural networks are used to generate adversarial  examples for rnn. the limitation of their algorithm is that the perturbations are not truly sequential.  for examples  if they want to generate adversarial examples from sentences  they can only replace  existing words with others words  but cannot insert words to the original sentences or delete words  form the original sentences.  sometimes it is hard for the attackers to know the structures and parameters of the victim machine  learning models. for example  many machine learning models are deployed in remote servers or  compiled into binary executables. to attack a black box victim neural network  papernot et al. first  got the outputs from the victim neural network on their training data  and then trained a substitute  neural network to fit the victim neural network     . adversarial examples are generated from the  substitute neural network. they also showed that other kinds of machine learning models such as  decision trees can also be attacked by using the substitute network to fit them     .  besides substitute network based approaches  several direct algorithms for black box attacks have  been proposed recently. narodytska et al. adopted a greedy local search to find a small set of pixels  by observing the probability outputs of the victim network after applying perturbations     . liu et  al. used an ensemble based algorithm to generate adversarial examples and the adversarial examples  are able to attack other black box models due to the transferability of adversarial examples      .  several defensive algorithms against adversarial examples have been proposed  such as feature selection       defensive distillation      and retraining     . however  it is found that the effectiveness of  these defensive algorithms is limited  especially under repeated attacks          .         rnn for malware detection    in this section we will show how to use rnn to detect malware. malware detection is regarded as  a sequential classification problem             . rnn is used to classify whether an api sequence  comes from a benign program or malware.        we will also introduce some variants of rnn in this section. malware detection model is usually a  black box to malware authors  and they need to take the potential variants into consideration when  developing attacking algorithms.  each api is represented as a one hot vector. assuming there are m apis in total  these apis are  numbered from   to m    . the feature vector x of an api is an m  dimensional binary vector. if  the api s number is i  the i th dimension of x is    and other dimensions are all zeros.  an api sequence is represented as x    x    ...  xt   where t is the length of the sequence. after  feeding the input to rnn  the hidden states of rnn can be represented as h    h    ...  ht .  in the basic version of rnn  the hidden state of the last time step ht is used as the representation  of the api sequence. the output layer uses ht to compute the probability distribution over the two  classes. then cross entropy is used as the loss function of api sequence classification.  the first variant of the rnn model introduced here is average pooling      which uses the average  states across h  to ht as the representation of the sequence  instead of the last state ht .  attention mechanism     is another variant  which uses weighted average of the hidden states to  represent the sequence. attention mechanism is inspired by the selective nature of human perception.  for example  when faced with a picture human beings will focus on some meaningful objects in  it  rather than every details of it. attention mechanism in deep learning makes the model to focus  on meaningful parts of inputs. it has shown to be very useful in machine translation     and image  caption     .  an attention function a is defined to map the hidden state to a scalar value  which indicates the  importance of the corresponding time step. the attention function is usually a feed forward neural  network. the attention function values across the whole sequence are then normalized according to  t  p  the formula  t   exp a ht       exp a hs     where  t is the final weight of time step t.  s      the above rnn models only process the sequence in the forward direction  while some sequential  patterns may lie in the backward direction. bidirectional rnn tries to learn patterns from both  directions     . in bidirectional rnn  an additional backward rnn is used to process the reversed  sequence  i.e. from xt to x  . the concatenation of the hidden states from both directions is used to  calculate the output probability.         attacking rnn based malware detection algorithms    papernot et al.      migrated the adversarial example generation algorithms for feed forward neural  networks to attack rnn by unrolling rnn along time and regarding it as a special kind of feedforward neural network. however  such model can only replace existing elements in the sequence  with other elements  since the perturbations are not truly sequential. this algorithm cannot insert  irreverent apis to the original sequences. the main contribution of this paper is that we proposed  a generative rnn based approach to generate sequential adversarial examples  which is able to  effectively mine the vulnerabilities in the sequential patterns.  the proposed algorithm consists of a generative rnn and a substitute rnn  as shown in figure    and figure  . the generative model is based on a modified version of the sequence to sequence  model       which takes malware s api sequence as input and generates an adversarial api sequence.  the substitute rnn is trained on benign sequences and the gumbel softmax      outputs of the  generative rnn  in order to fit the black box victim rnn. the gumbel softmax enables the gradient  to back propagate from the substitute rnn to the generative rnn.   .     the generative rnn    the input of the generative rnn is a malware api sequence  and the output is the generated sequential  adversarial example for the input malware. the generative rnn generates a small piece of api  sequence after each api and tries to insert the sequence piece after the api.  for the input sequence x    x    ...  xt   the hidden states of the recurrent layer are h    h    ...  ht . at  time step t  a small sequence of gumbel softmax output g t    g t    ...  g tl with length l is generated  based on ht   where l is a hyper parameter.        classification         attention  subtitute rnn    bidirectional  layer  gumbel softmax  sampling    decoder layer  generative rnn    recurrent layer    malware input    inserting  adversarial example    black box victim rnn    cross entropy    figure    the architecture of the proposed model when trained on malware.    sequence decoder     is used to generate the small sequence. the decoder rnn uses the formula  d  d  d  d  hd      dec x    h       to update hidden states  where x  is the input and h  is the hidden state of  the decoder rnn which is initialized with zero.  formula   is used to get the hidden state when generating g t  .  d  hd      dec ht   h      .           when generating the first element at time step t  the input is the hidden state ht .  then a softmax layer is followed to generate the api. besides the m apis  we introduce a special  null api into the api set. if the null api is generated at time step     no api will be inserted to the  original sequence at that moment. if we do not use the null api  too many generated apis will be  inserted into the sequence and the resulting sequence will become too long. allowing null api will  make the final sequence shorter. since the m valid apis have been numbered from   to m      the  null api is numbered as m .  the softmax layer will have m     elements  which is calculated as   t    sof tmax w s hd        where w s is the weights to map the hidden state to the output layer.  then we can sample an api from   t  . let the one hot representation of the sampled api be at  .  the sampled api is a discrete symbol. if we give at  to the substitute rnn  we are unable to get the  gradients from the substitute rnn and thus unable to train the generative rnn.  gumbel softmax is recently proposed to approximate one hot vectors with differentiable representations     . the gumbel softmax output g t  has the same dimension with   t  . the i th element of  g t  is calculated by formula  .  g it       exp  log   it      zi   temp      m  p  j  exp  log   t      zj   temp     j                   where zi is a random number sampled from the gumbel distribution     and temp is the temperature  of gumbel softmax. in this paper we use a superscript to index the element in a vector.  to generate the    th api at time step t when   is greater than    the decoder rnn uses formula   to  update the hidden state.  d  hd      dec w g g t         h      .           the decoder rnn takes the previous gumbel softmax output as input. w g is used to map g t        to a space with the same dimension as ht   in order to make the input dimension of the decoder rnn  compatible with formula  .  calculating gumbel softmax for       can use the same way as        i.e. formula   . therefore   we omit the formula here.  after generating small sequences from t     to t and inserting the generated sequences to the  original sequence  we obtained two kinds of results.  the first kind of result is the one hot representation of the final adversarial sequence sadv      sadv   removen ull x    a     a     ...  a l   x    a     a     ...  a l   ......  xt   at     at     ...  at l  .       the generated null apis should be removed from the one hot sequence.  the second kind of result uses gumbel softmax outputs to replace one hot representations   sgumbel   x    g      g      ...  g  l   x    g      g      ...  g  l   ......  xt   g t     g t     ...  g t l .           the null apis  gumbel softmax outputs are reserved in the sequence  in order to connect the gradients  of loss function with null apis. the loss function will be defined in the following sections.   .     the substitute rnn    malware authors usually do not know the detailed structure of the victim rnn. they do not know  whether the victim rnn uses bidirectional connection  average pooling and the attention mechanism.  the weights of the victim rnn is also unavailable to malware authors.  to fit such victim rnn with unknown structure and weights  a neural network with strong representation ability should be used. therefore  the substitute rnn uses bidirectional rnn with attention  mechanism since it is able to learn complex sequential patterns. bidirectional connection contains  both the forward connection and the backward connection  and therefore it has the ability to represent  the unidirectional connection. the attention mechanism is able to focus on different positions of  the sequence. therefore  rnn with attention mechanism can represent the cases without attention  mechanism such as average pooling and the using of the last state to represent the sequence.  to fit the victim rnn  the substitute rnn should regard the output labels of the victim rnn on the  training data as the target labels. the training data should contain both malware and benign programs.  as shown in figure   and the previous section  for malware input two kinds of outputs are generated  from the generative rnn  i.e. the one hot adversarial example sadv and the gumbel softmax output  sgumbel .  we use the victim rnn to detect the one hot adversarial example  and get the resulting label v. v is a  binary value where   represents benign label and   represents malware.  then the substitute rnn is used to classify the gumbel softmax output sgumbel   and outputs the  malicious probability ps .  cross entropy is used as the loss function  as shown in formula  .  ls    v log ps          v log     ps  .               for a benign input sequence  it is directly fed into the victim rnn and the substitute rnn  as shown  in figure  . the outputs of the two rnns v and ps are used to calculate the loss function in the same  way as formula  .    classification         attention  subtitute rnn  bidirectional  layer    benign input    black box victim rnn    cross entropy    figure    the architecture of the proposed model when trained on benign programs.     .     training    the training objective of the generative rnn is to minimize the predicted malicious probability  ps on sgumbel . we also add a regularization term to restrict the number of inserted apis in the  adversarial sequence by maximizing the null api s expectation probability. the final loss function of  the generative rnn is defined in formula  .  lg   log ps      et   t      l   m  t              where   is the regularization coefficient and m is the index of the null api.  the training process of the proposed model is summarized in algorithm  .  algorithm   training the proposed model     while terminal condition not satisfied do      sample a minibatch of data  which contains malware and benign programs.      calculate the outputs of the generative rnn for malware.      get the outputs of the substitute rnn on benign programs and the gumbel softmax output  of malware.      get the outputs of the victim rnn on the adversarial examples and benign programs.      minimize ls on both benign and malware data by updating the substitute rnn s weights.      minimize lg on malware data by updating the generative rnn s weights.     end while         experiments    adam      was used to train all of the models. lstm unit was used for all of the rnns presented in  the experiments due to its good performance in processing long sequences        .   .     dataset    we crawled     programs with corresponding behavior reports from a website for malware analysis   https   malwr.com  . on the website users can upload their programs and the website will execute        the programs in virtual machines. then the api sequences called by the uploaded programs will be  posted on the website.     of the crawled programs are malware.  in real world applications  the adversarial example generation model and the victim rnn should  be trained by malware authors and anti virus vendors respectively. the datasets that they collected  cannot be the same. therefore  we use different training sets for the two models. we selected     of  our dataset as the training set of the adversarial example generation model  i.e. the generative rnn  and the substitute rnn   and selected     as the validation set of the adversarial example generation  model. then we selected another     and     as the training set and the validation set of the victim  rnn respectively. the remaining     of our dataset was regarded as the test set.   .     the victim rnns    to validate the representation ability of the substitute rnn  we used the several different structures for  the black box victim rnn  as shown in the first column of table  . in table    the first lstm model  uses the last hidden state as the representation of the sequence. bilstm represents bidirectional  lstm. the suffixes  average  and  attention  in the last four rows indicate the use of average  pooling and attention mechanism to represent the sequence.  we first tuned the hyper parameters of bilstm attention on the validation set. the final learning  rate was set to  .   . the number of recurrent hidden layers and the number of attention hidden layers  were both set to one and the layer sizes were both set to    . we directly used the resulting hyperparameters to other victim models. we have tried to separately tune the hyper parameters for other  victim rnns but the performance did not improve much compared with using bilstm attention s  hyper parameters.  table   gives the area under curve  auc  of these victim rnns before adversarial attacks.  table    auc of different victim rnns before attacks.  algorhthm    training set    test set    lstm  bilstm  lstm average  bilstm average  lstm attention  bilstm attention      .       .       .       .       .       .         .       .       .       .       .       .       overall  the attention mechanism works better than non attention approaches  since attention mechanism is able to learn the relative importance of different parts in sequences. lstm and bilstm  only use the last hidden state  and therefore the information delivered to the output layer is limited.  in this case bidirectional connection delivers more information than unidirectional connection  and  auc of bilstm is higher than lstm. for average pooling and attention mechanism  bidirectional  lstm does not outperform unidirectional lstm in auc. average pooling and attention mechanism  are able to capture the information of the whole api sequence. unidirectional lstm is enough to  learn the sequential patterns. compared with unidirectional lstm  bidirectional lstm has more  parameters  which makes the learn process more difficult. therefore  the bidirectional connection  does not improve the performance for average pooling and attention mechanism.   .     experimental results of the proposed model    the hyper parameters of the generative rnn and the substitute rnn were tuned separately for each  black box victim rnn. the learning rate and the regularization coefficient were chosen by line  search along the direction  .     .     et al.. the gumbel softmax temperature was searched in the  range         . actually  the decoder length l in the generative rnn is also a kind of regularization  coefficient. a large l will make the generative rnn have strong representation ability  but the whole  adversarial sequences will become too long  and the generative rnn s size may exceed the capacity  of the gpu memory. therefore  in our experiments we set l to  .  the experimental results are show in table  .        table    detection rate on original samples and adversarial examples.  adver.  represents adversarial  examples.    lstm  bilstm  lstm average  bilstm average  lstm attention  bilstm attention    training set  original  adver.    test set  original    adver.      .       .       .       .       .       .         .       .       .       .       .       .         .      .      .      .      .      .         .      .      .      .      .      .       after adversarial attacks  all the victim rnns fails to detect most of the malware. for different  victim rnns  the detection rates on adversarial examples range from  .    to   .     while before  adversarial attacks the detection rates range from   .    to   .   . that is to say  about      malware will bypass the detection algorithms under our proposed attack model.  except lstm  the detection rates on adversarial examples of all the victim models are smaller than   .     which means that victim rnns are almost unable to detect any malware. the victim model  lstm have detection rates of   .    and   .    on the training set and the test set respectively   which are higher than other victim rnns. we can see that for the lstm model the substitute rnn  does not fit the victim rnn very well on the training data.  the differences in adversarial examples  detection rates are very small between the training set and  the test set for these victim rnns. the generalization ability of the proposed model is quite well for  unseen malware examples. the proposed adversarial example generation algorithm can be applied to  both existing malware and unseen malware.  it can be seen that even if the adversarial example generation algorithm and the victim rnn use  different rnn models and different training set  most of the adversarial examples are still able to  attack the victim rnn successfully. the adversarial examples can transfer among different models  and different training sets. the transferability makes it very easy for malware authors to attack rnn  based malware detection algorithms.         conclusions and future works    a novel algorithm of generating sequential adversarial examples for malware is proposed in this  paper. the generative network is based on the sequence to sequence model. a substitute rnn is  trained to fit the black box victim rnn. we use gumbel softmax to approximate the generated  discrete apis  which is able to propagate the gradients from the substitute rnn to the generative  rnn. the proposed model has successfully made most of the generated adversarial examples able to  bypass several black box victim rnns with different structures.  previous researches on adversarial examples mainly focused on images which have fixed input  dimension. we have shown that the sequential machine models are also not safe under adversarial  attacks. the problem of adversarial examples becomes more serious when it comes to malware  detection. robust defensive models are needed to deal with adversarial attacks.  in future works we will use the proposed model to attack convolutional neural network  cnn  based  malware detection algorithms  since many researchers have begun to use cnn to process sequential  data recently         . we will validate whether a substitute rnn has enough capacity to fit a victim  cnn  and whether a substitute cnn has enough capacity to fit a victim rnn. the research on the  transferability of adversarial examples between rnn and cnn is very important to the practicability  of sequential malware detection algorithms.    