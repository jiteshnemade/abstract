introduction    in many language modeling applications  the speech  or text is associated with some metadata or contextual information. for example  in speech recognition  if a user is speaking to a personal assistant then  the system might know the time of day or the identity of the task that the user is trying to accomplish.  if the user takes a picture of a sign to translate it with  their smart phone  the system would have contextual  information related to the geographic location and  the user s preferred language. the context aware  language model targets these types of applications  with a model that can adapt its predictions based on  the provided contextual information.  there has been much work on using context information to adapt language models. here  we are interested in contexts described by metadata  vs. word    history or related documents  and in neural network  approaches due to their flexibility for representing  diverse types of contexts. specifically  we focus  on recurrent neural networks  rnns  due to their  widespread use.  the standard approach to adapt an rnn language  model is to concatenate the context representation  with the word embedding at the input to the rnn   mikolov and zweig       . optionally  the context  embedding is also concatenated with the output from  the recurrent layer to adapt the softmax layer. this  basic strategy has been adopted for various types of  adaptation such as for lm personalization  wen et  al.        li et al.         adapting to television  show genres  chen et al.         adapting to long  range dependencies in a document  ji et al.          etc.  we propose a more powerful mechanism for using a context vector  which we call the factorcell.  rather than simply using context as an additional  input  it is used to control a factored  low rank   transformation of the recurrent layer weight matrix.  the motivation is that allowing a greater fraction of  the model parameters to be adjusted in response to  the input context will produce a model that is more  adaptable and responsive to that context.  we evaluate the resulting models in terms of  context dependent perplexity and context classification accuracy on six tasks reflecting different types  of context variables  comparing to baselines that represent the most popular methods for using context  in neural models. we choose tasks where context  is specified by metadata  rather than text samples  as used in many prior studies. the combination     of experiments on a variety of data sources provides strong evidence for the utility of the factorcell model  but the results show that it can be useful  to consider more than just perplexity in training a  language model.  the remainder proceeds as follows. in section     we introduce the factorcell model and show how it  differs mathematically from alternative approaches.  next  section   describes the six datasets used to  probe the performance of different models. experiments and analyses contrasting perplexity and  classification results for a variety of context variables are provided in section    demonstrating consistent improvements in both criteria for the factorcell model but also confirming that perplexity is  not correlated with classification performance for all  models. analyses explore the effectiveness of the  model for characterizing high dimensional context  spaces. the model is compared to related work in  section  . finally  section   summarizes contributions and open questions.         context aware rnn    our model uses adaptation in both the recurrent  layer and in the bias vector of the output layer. in  this section we describe how we represent context  as an embedding and methods for adapting the recurrent layer and the softmax layer  showing that our  proposed model is a generalization of prior methods.  the novelty of our model is that instead of using  context as an additional input to the model  it uses  the context information to transform the weights of  the recurrent layer. this is accomplished using a  low rank decomposition in order to control the extent of parameter sharing between contexts  which  is important for handling high dimensional  sparse  contexts.   .     context representation    we assume the availability of contextual information  metadata or other side information   that is represented as a set of context variables  f  n   f    f    . . . fn   from which we produce a kdimensional representation in the form of an embedding  c   rk . each of the context variables  fi   represents some type of information or metadata about  the sequence and can be either categorical or numer     ical. the embeddings can either be learned off line  using a topic model  mikolov and zweig        or  end to end as part of the adapted lm  tang et al.        . here  we use end to end learning  where the  context embedding is the output of a feed forward  network with a relu activation function. the resulting embedding  c  is used for adapting both the  recurrent layer and the output layer of the rnn.   .     adapting the recurrent layer    the basic operation of the recurrent layer is to use a  matrix w to transform the concatenation of a word  embedding  wt   re   with the hidden state from the  previous time step  ht     rd   and produce a new  hidden state  ht   as given by equation     ht     w  wt   w  ht     b       w wt   ht       b .           the size of w is d    e   d . for simplicity  our  equations assume a simple rnn. appendix a shows  how the equations can be adjusted to work with an  lstm.  the standard approach to recurrent layer adaptation is to include  via concatenation  the context embedding as an additional input to the recurrent layer   mikolov and zweig       . when the context embedding is constant across the whole sequence  it is  easy to show that this concatenation is equivalent to  using a context dependent bias at the recurrent layer   ht     w  wt   ht     c    b       w wt   ht       vc   b                     w wt   ht       b     where w     w v  and b    vc   b is the contextdependent bias  formed by adding a linear projection  of the context embedding. we refer to this adaptation approach as the concatcell model.  our proposed model extends the concatcell  by using a context dependent weight matrix  w    w   a  in place of the generic weight matrix w.  we refer to w as generic because it is  shared across all context settings.  the adaptation  matrix  a  is generated by taking the product of the  context embedding vector against a set of left and  right basis tensors to produce a rank r matrix. the  left and right adaptation basis tensors are given as     zl   rk  e d  r and zr   rr d k . the two  bases tensors together can be thought of as holding  k different rank r matrices  aj   zl j zr j   each  the size of w. by taking the product between c and  the corresponding tensor modes of zl and zr  using  i to denote the mode i tensor product  i.e.  the  product with the i th dimension of the tensor   the  context determines the weighted combination of the  k matrices   a    c    zl   zr    c   .           the number of degrees of freedom of a is controlled  by the dimension k of the context vector and the rank  r of the k weight matrices. the rank is treated as a  hyperparameter and controls the extent to which the  model relies on the generic weight matrix w versus  behaves in a more context specific manner.  we call this model the factorcell because the  weight matrix has been adapted by adding a factored  component. the concatcell model is a special case  of the factorcell where zl and zr are set to zero.  in summary  the proposed model is given by   ht     w   wt   ht       b     w    w    c    zl   zr    c                 b   vc   b.  if the context is known in advance  w  can be precomputed  in which case applying the rnn at test  time requires no more computation than using an unadapted rnn of the same size. this means that for  a fixed sized recurrent layer  the factorcell model  can have many more parameters than the concatcell model but hardly any increase in computational  cost.   .     adapting the softmax bias    the last layer of the model predicts the probability  of the next symbol in the sequence using the output  from the recurrent layer using the softmax function  to create a normalized probability distribution. the  output probabilities are given by  yt   softmax elht   bout              where e   r v   e is the matrix of word embeddings  l   re d is a linear projection to match the  dimension of the recurrent layer  when e    d   and    bout   r v   is the softmax bias vector. we tie the  word embeddings in the input layer with the ones in  the output layer  press and wolf        inan et al.        .  if sj is the indicator row vector for the jth word  in the vocabulary  p then p wt  w  t       st yt and  log p w  t     t log swt yt .  adapting the softmax bias alters the unigram distribution. there are two ways to accomplish this.  when the values that the context can take are categorical with low cardinality then context dependent  softmax bias vectors can be learned directly. this is  equivalent to replacing c with a one hot vector. otherwise  a projection of the context embedding  qc  where q   r v   k   can be used to adapt the bias  vector as in  yt   softmax elht   qc   bout  .           the projection can be thought of as a low rank approximation to using the one hot context vector.  both strategies are explored  depending on the nature of the original context space.  as noted in section    adaptation of the softmax  bias has been used in other studies. as we show in  the experimental work  it is useful for representing  phenomena where unigram statistics are important.         data    our experiments make use of six datasets  four targeting word level sequences  and two targeting character sequences. the character studies are motivated by the growing interest in character level models in both speech recognition and machine translation  hannun et al.        chung et al.       . by  using multiple datasets with different types of context  we hope to learn more about what makes a  dataset amenable to adaptation. the datasets range  in size from over     million words of training data  to   million characters of training data for the smallest one. when using a word based vocabulary  we  preprocess the data by lowercasing  tokenizing and  removing most punctuation. we also truncate sentences to be shorter than a maximum length of     words for agnews and dbpedia and     to     tokens for the remaining datasets. summary information is provided in table    including the training   development  and test data sizes in terms of number     name  agnews  dbpedia  tripadvisor  yelp  eurotwitter   geotwitter     train   . m    . m     . m    . m   . m    . m    dev   . m   . m   . m   . m   . m   . m    test   . m   . m   . m   . m   . m   . m    vocab                                              docs.     k     k     k     k    k     k    context    newspaper sections     entity categories   . k hotels   sentiment    sentiment    languages  latitude   longitude    table    dataset statistics  dataset size in words    or characters  of train  dev and test sets  vocabulary size  number  of training documents  and context variables.    of tokens  vocabulary size  number of training documents  i.e. context samples   and the context variables  f  n  . the largest dataset  tripadvisor  has  over     thousand hotel review documents  which  adds up to over     million words of training data.  the first three datasets  agnews  dbpedia  and  yelp  have previously been used for text classification  zhang et al.       . these consist of newspaper headlines  encyclopedia entries  and restaurant  and business reviews  respectively. the context variables associated with these correspond to the newspaper section  world  sports  business  sci   tech   for each headline  the page category on dbpedia   out of    options such as actor  athlete  building   etc.   and the star rating on yelp  from one to five .  for agnews  dbpedia  and yelp we use the same  test data as in previous work. our fourth dataset   from tripadvisor  was previously used for language  modeling and consists of two relevant context variables  an identifier for the hotel and a sentiment  score from one to five stars  tang et al.       . some  of the reviews are written in french or german but  most are in english. there are       different hotels  but we group all the ones that do not occur at least     times in the training data into a single entity  leaving  us with around      . these four datasets use wordbased vocabularies.  we also experiment on two twitter datasets  eurotwitter and geotwitter. eurotwitter consists  of    thousand tweets labeled with one of nine  languages   english  spanish  galician  catalan   basque  portuguese  french  german  and italian .  the corpus was created by combining portions of  multiple published datasets for language identification including twitter    jaech et al.         tweetlid  zubiaga et al.         and the monolingual  portion of tweets from a code switching detection    workshop  molina et al.       . the geotwitter  data contains tweets with latitude and longitude  information from england  spain  and the united  states.  the latitude and longitude coordinates are  given as numerical inputs. this is different from  the other five datasets that all use categorical context variables.         experiments with different contexts    the goal of our experiments is to show that the  factorcell model can deliver improved performance  over current approaches for multiple language  model applications and a variety of types of contexts. specifically  results are reported for contextconditioned perplexity and generative model text  classification accuracy  using contexts that capture  a range of phenomena and dimensionalities.  test set perplexity is the most widely accepted  method for evaluating language models  both for use  in recognition translation applications and generation. it has the advantage that it is easy to measure  and is widely used as a criteria for model fit  but the  limitation that it is not directly matched to most tasks  that language models are directly used for. text classification using the model in a generative classifier is  a simple application of bayes rule        arg max p w  t    p                   where w  t is the text sequence  p    is the class  prior  which we assume to be uniform. classification accuracy provides additional information about  the power of a model  even if it is not being designed  explicitly for text classification. further  it allows  us to be able to directly compare our model perfor     data was accessed from http   followthehashtag.com.     mance against previously published text classification benchmarks.  note that the use of classification accuracy for  evaluation here involves counting errors associated  with applying the generative model to independent  test samples. this differs from the accuracy criterion  used for evaluating context sensitive language models for text generation based on a separate discriminative classifier trained on generated text  ficler and  goldberg        hu et al.       . we discuss this  further in section  .  the experiments compare the factorcell model   equations   and    to two popular alternatives   which we refer to as concatcell  equations   and     and softmaxbias  equation   . as noted earlier   the softmaxbias method is a simplification of the  concatcell model  which is in turn a simplification  of the factorcell model. the softmaxbias method  impacts only the output layer and thus only unigram statistics. since bag of word models provide  strong baselines in many text classification tasks  we  hypothesize that the softmaxbias model will capture much of the relative improvement over the unadapted model for word based tasks. however  in  small vocabulary character based models  the unigram distribution is unlikely to carry much information about the context  so adapting the recurrent  layer should become more important in characterlevel models. we expect that performance gains will  be greatest for the factorcell model for sources that  have sufficient structure and data to support learning  the extra degrees of freedom.  another possible baseline would use models independently trained on the subset of data for each  context. this is the  independent component  case  in  yogatama et al.       . this will fail when a  context variable takes on many values  or continuous values  or when training data is limited  because  it makes poor use of the training data  as shown in  that study. while we do have some datasets where  this approach is plausible  we feel that its limitations  have been clearly established.   .     implementation details    the rnn variant that we use is an lstm with coupled input and forget gates  melis et al.       . the    different model variants are implemented  using the  tensorflow library. the model is trained with the  standard negative log likelihood loss function  i.e.  minimizing cross entropy. dropout is used as a regularizer in the recurrent connections as described in  semeniuta et al.       . training is done using the  adam optimizer with a learning rate of  .   . for  the models with word based vocabularies  a sampled  softmax loss is used with a unigram proposal distribution and sampling     words at each time step   jean et al.       . the classification experiments  use a sampled softmax loss with a sample size of        words. this is an order of magnitude faster to  compute with a minimal effect on accuracy.  hyperparameter tuning was done based on minimizing perplexity on the development set and using a random search. hyperparameters included  word embedding size e  recurrent state size d  context embedding size k  and weight adaptation matrix rank r  the number of training steps  recurrent  dropout probability  and random initialization seed.  the selected hyperparameter values are listed in table   for any fixed lstm size  the factorcell has a  higher count of learned parameters compared to the  concatcell. however  during evaluation both models use approximately the same number of floatingpoint operations because w  only needs to be computed once per sentence. because of this  we believe  limiting the recurrent layer cell size is a fair way to  compare between the factorcell and the concatcell.   .     word based models    perplexities and classification accuracies for the four  word based datasets are presented in table  . in  each of the four datasets  the factorcell model gives  the best perplexity. for classification accuracy  there  is a bigger difference between the models  and the  factorcell model is the most accurate on three out of  four datasets and tied with the softmaxbias model  on agnews. for dbpedia and tripadvisor  most  of the improvement in perplexity relative to the unadapted case is achieved by the softmaxbias model  with smaller relative improvements coming from the  increased power of the concatcell and factorcell  models. for yelp  the perplexity improvements are  small  the factorcell model is just  .   better than       code available at http   github.com ajaech calm.     word embed  lstm dim  steps  dropout  ctx. embed  rank    agnews             .   . k   .            dbpedia                     .   . k   .              eurotwitter               .   . k   .    .              geotwitter               .    . k   .    .                tripadvisor             .   . k   .    .                 yelp             .   . k   .              table    selected hyperparameters for each dataset. when a range is listed it means that a different values were  selected for the factorcell  concatcell  softmaxbias or unadapted models.    model  unadapted  softmaxbias  concatcell  factorcell    agnews  ppl acc    .        .    .     .    .     .    .     dbpedia  ppl acc    .        .    .     .    .     .    .     tripadvisor  ppl acc    .        .     .     .     .     .     .     yelp  ppl acc    .        .    .     .    .     .    .     table    perplexity and classification accuracy on the test set for the four word based datasets.    the unadapted model.  from yogatama et al.         we see that for agnews  much more so than for other datasets  the unigram statistics capture the discriminating information  and it is the only dataset in that work where a  naive bayes classifier is competitive with the generative lstm for the full range of training data. the  fact that the softmaxbias model gets the same accuracy as the factorcell model on this task suggests  that topic classification tasks may benefit less from  adapting the recurrent layer.    perplexity on the x axis with accuracy on the y axis  with both metrics computed on the development set.  each point in this figure represents a different instance of the model trained with random hyperparameter settings and the best results are in the upper right corner of each plot. the color shape differences of the points correspond to the three classes of  models  factorcell  concatcell  and softmaxbias.    for the dbpedia and yelp datasets  the factorcell model beats previously reported classification  accuracies for generative models  yogatama et al.        . however  it is not competitive with state ofthe art discriminative models on these tasks with the  full training set. with less training data  it probably  would be  based on the results in  yogatama et al.        .  the numbers in table   do not adequately convey  the fact that there are hyperparameters whose effect  on perplexity is greater than the sometimes small  relative differences between models. even the seed  for the random weight initialization can have a  major impact  on the final performance of an lstm   reimers and gurevych       . we use figure   to  show how the three classes of models perform across  a range of hyperparameters. the figure compares    figure    accuracy vs. perplexity for different classes of  models on the four word based datasets.     within the same model class but across different  hyperparameter settings  there is much more variation in perplexity than in accuracy. the lstm cell  size is mainly responsible for this  it has a much bigger impact on perplexity than on accuracy. it is also  apparent that the models with the lowest perplexity  are not always the ones with the highest accuracy.  see section  .  for further analysis.  figure   is a visualization of the per word log  likelihood ratios between a model assuming a   star  review and the same model assuming a   star review.  likelihoods were computed using an ensemble of  three models to reduce variance. the analysis is repeated for each class of model. words highlighted  in blue are given a higher likelihood under the   star  assumption.  unigrams with strong sentiment such as  lovely   and  friendly  are well represented by all three  models. the reader may not consider the tokens   craziness  or     pm  to be strong indicators of a  positive review but the way they are used in this review is representative of how they are typically used  across the corpus.  as expected  the concatcell and factorcell  model capture the sentiment of multi token phrases.  as an example  the unigram  enough  is    more  likely to occur in a   star review than in a   star review. however   do enough  is    times more likely  to appear in a   star review than in a   star review.  in this example  the factorcell model does a better  job of handling the word  enough.    .     character based models    next  we evaluate the eurotwitter and geotwitter  models using both perplexity and a classification  task. for eurotwitter  the classification task is to  identify the language. with geotwitter  it is less obvious what the classification task should be because  the context values are continuous and not categorical. we selected six cities and then assigned each  sentence the label of the closest city in that list while  still retaining the exact coordinates of the tweet.  there are two cities from each country  manchester   london  madrid  barcelona  new york city  and  los angeles. tweets from locations further than      km from the nearest city in the list were discarded  when evaluating the classification accuracy.  perplexities and classification accuracies are pre     softmaxbias    concatcell    factorcell    figure    log likelihood ratio between a model that assumes a   star review and the same model that assumes  a   star review. blue indicates a higher   star likelihood  and red is a higher likelihood for the   star condition.    sented in table  . the factorcell model has the  lowest perplexity and the highest accuracy for both  datasets. again  the factorcell model clearly improves on the concatcell as measured by classification accuracy. consistent with our hypothesis   adapting the softmax bias is not effective for these  small vocabulary character based tasks. the softmaxbias model has small perplexity improvements         and low classification accuracies.    model  unadapted  softmaxbias  concatcell  factorcell    eurotwitter  ppl acc   .        .      .    .      .    .      .     geotwitter  ppl acc   .        .     .    .     .    .     .     table    perplexity and classification accuracies for the  eurotwitter and geotwitter datasets.    figure   compares perplexity and classification  accuracy for different hyperparameter settings of the  character based models. again  we see that it is pos      figure    accuracy vs. perplexity for different classes of  models on the two character based datasets.    sible to trade off some perplexity for gains in classification accuracy. for eurotwitter  if tuning is done  on accuracy rather than perplexity then the accuracy  of the best model is as high as    .  sometimes there can be little to no perplexity improvement between the unadapted model and the  factorcell model. this can be explained if the provided context variables are mostly redundant given  the previous tokens in the sequence. to investigate  this further  we trained a logistic regression classifier to predict the language using the state from  the lstm at the last time step on the unadapted  model as a feature vector. using just    labeled examples per class it is possible to get   .   accuracy. furthermore  we find that a single dimension  in the hidden state of the unadapted model is often  enough to distinguish between different languages  even though the model was not given any supervision signal. this finding is consistent with previous work that showed that individual dimensions of  lstm hidden states can be strong indicators of concepts like sentiment  karpathy et al.        radford  et al.       .  figure   visualizes the value of the dimension  of the hidden layer that is the strongest indicator  of spanish on three different code switched tweets.  code switching is not a part of the training data but  it provides a compelling visualization of the ability of the unsupervised model to quickly recognize  the language. the fact that it is so easy for the  unadapted model to pick up on the identity of the  contextual variable fits with our explanation for the  small relative gain in perplexity from the adapted  models in these two tasks.    figure    the value of the dimension of the lstm hidden  state in an unadapted model that is the strongest indicator  for spanish text for three different code switched tweets.     .     hyperparameter analysis    the hyperparameter with the strongest effect on perplexity is the size of the lstm. this was consistent across all six datasets. the effect on classification accuracy of increasing the lstm size was  mixed. increasing the context embedding size generally helped with accuracy on all datasets  but it  had a more neutral effect on tripadvisor and yelp  and increased perplexity on the two character based  datasets. for the factorcell model  increasing the  rank of the adaptation matrix tended to lead to increased classification accuracy on all datasets and  seemed to help with perplexity on agnews  dbpedia  and tripadvisor.    figure    comparison of the effect of lstm parameter  count and factorcell rank hyperparameters on perplexity  for dbpedia.    figure   compares the effect on perplexity of the  lstm parameter count and the factorcell rank hyperparameters. each point in those plots represents  a separate instance of the model with varied hy      perparameters. in the right subplot of figure     we see that increasing the rank hyperparameter improves perplexity. this is consistent with our hypothesis that increasing the rank can let the model  adapt more. the variance is large because differences in other hyperparameters  such as hidden state  size  also have an impact.  in the left subplot we compare the performance of  the factorcell with the concatcell as the size of the  word embeddings and recurrent state change. the  x axis is the size of the w recurrent weight matrix   specifically   e   d d for an lstm with   gates.  since the adapted weights can be precomputed  the  computational cost is roughly the same for points  with the same x value. for a fixed size hidden state   the factorcell model has a better perplexity than the  concatcell.  since performance can be improved both by increasing the recurrent state dimension and or by increasing rank  we examined the relative benefits of  each. the perplexity of a factorcell model with an  lstm size of    k will improve by    when the  rank is increased from   to   . to get the same decrease in perplexity by changing the size of the hidden state would require    k parameters  resulting  in a significant computational advantage for the factorcell model.  using a one hot vector for adapting the softmax  bias layer in place of the context embedding when  adapting the softmax bias vector tended to have a  large positive effect on accuracy leaving perplexity  mostly unchanged. recall from section  .  that if  the number of values that a context variable can take  on is small then we can allow the model to choose  between using the low dimensional context embedding or a one hot vector. this option is not available for the tripadvisor and the geotwitter datasets  because the dimensionality of their one hot vectors  would be too large. the method of adapting the softmax bias is the main explanation for why some concatcell models performed significantly above below  the trendline for dbpedia in figure  .  we experimented with an additional hyperparameter on the yelp dataset  namely the inclusion of  layer normalization  ba et al.       .  we had ruledout using layer normalization in preliminary work  on the agnews data before we understood that agnews is not representative  so only one task was    explored here.  layer normalization significantly  helped the perplexity on yelp       relative improvement  and all of the top performing models on  the held out development data had it enabled.   .     analysis for sparse contexts    the tripadvisor data is an interesting case because  the original context space is high dimensional        hotels     user ratings  and sparse. since the model  applies end to end learning  we can investigate what  the context embeddings learn. in particular  we  looked at location  hotels are from    cities in the  united states  and class of hotel  neither of which  are input to the model. all of what it learns about  these concepts come from extracting information  from the text of the reviews.  to visualize the embedding  we used a  dimensional pca projection of the embeddings of  the      hotels. we found that the model learns  to group the hotels based on geographic region   the projected embeddings for the largest cities are  shown in figure    plotting the  .   ellipsoid of the  gaussian distribution of the points.  actual points  are not shown to avoid clutter.  not only are hotels  from the same city grouped together  cities that are  close geographically appear close to each other in  the embedding space. cities in the southwest appear on the left of the figure  the west coast is on top  and the east coast and midwest is on the right side.  this is likely due in part to the impact of the region  on activities that guests may mention  but there also  appears to be a geographic sampling bias in the hotel  class that may impact language use.  class is a rating from an independent agency that  indicates the level of service and amenities that customers can expect to receive at a hotel. whereas  the  star rating is the average score given to each establishment by the customers who reviewed it. hotel  class does not determine star rating although they  are correlated  r    .   . the dataset does not  contain a uniform sample of hotel classes from each  city. the hotels included from boston  chicago  and  philly are almost exclusively high class and the ones  from l.a. and san diego happen to be low class  so  the embedding distributions also reflect hotel class   lower class hotels towards the top left and higher  class hotels towards the bottom right. the visualization for the concatcell and softmaxbias models     figure    distribution of a pca projection of hotel embeddings pca from the tripadvisor factorcell model  showing the grouping of the hotels by city.    are similar.  another way of understanding what the context  embeddings represent is to compute the softmax bias  projection qc and examine the words that experience the biggest increase in probability. we show  three examples in table  . in each case  the top  words are strongly related to geography and include  names of neighborhoods  local attractions  and other  hotels in the same city. the top boosted words are  relatively unaffected by changing the rating.  recall  that the hotel identifier and the user rating are the  only two inputs used to create the context embedding.  this table combined with the other visualizations indicates that location effects tend to dominate  in the output layer  which may explain why the two  models adapting the recurrent network seem to have  a bigger impact on classification performance.         prior work    there have been many studies of neural language  models that can be dynamically adapted based on  context. methods have been referred to as contextdependent models  mikolov and zweig          context aware models  tang et al.         conditioned models  ficler and goldberg         and controllable text generation  hu et al.       . these  models have been used in scoring word sequences   such as for speech recognition or machine translation   for text classification  and for generation.  in some work  context corresponds to the previous    word history. here  we instead consider known factors such as user  location and domain metadata   though the framework could be used with historybased context.  the studies that most directly relate to our work  are neural models that correspond to special cases of  the more general factorcell model  including those  that leverage what we call the softmaxbias model   dieng et al.        tang et al.        yogatama et  al.        ficler and goldberg        and others that  use the concatcell approach  mikolov and zweig         wen et al.        chen et al.        ghosh et  al.       . one study  ji et al.        compares the  two approaches  which they refer to as ccdclm and  codclm. they find that both approaches give similar perplexities  but their concatcell style model  does better at an auxiliary sentence ordering task.  this is consistent with our finding that adapting at  the recurrent layer can benefit certain tasks while  having only a minor impact on perplexity. they  do not test any models that adapt both the recurrent  and output layers. hoang et al.        also consider  adapting at the hidden layer vs. at the softmax layer   but their architecture does not fit cleanly into the  framework of the softmaxbias model because they  use an extra perceptron layer  thus  it is difficult to  compare the experimental findings with ours.  the factorcell model is distinguished by having an additive  factored  context dependent transformation of the recurrent layer weight matrix. a  related additive context dependent transformation  has been proposed for log bilinear sequence models  eisenstein et al.        hutchinson et al.          but these are less powerful than the rnn. a somewhat different use of low rank factorization has previously been used to reduce the parameter count in  an lstm lm  kuchaiev and ginsburg         finding that the reduced number of parameters leads to  faster training.  there is a long history of adapting n gram language models.  see demori and federico        or  bellegarda        for a survey.  one recent example  is chelba and shazeer        where a     relative  improvement in perplexity was obtained when using geographic features for adaptation. we hypothesize that the impressive improvement in perplexity  is possible because the language in their dataset of  google mobile search queries is particularly sensi      hotel  amalfi    city  chicago    class   .     rating       blvd hotel suites    los angeles     .          four points sheraton    seattle     .          top boosted words  amalfi  chicago  allegro  burnham  sable  michigan  acme  conrad  talbott  wrigley  hollywood  kodak  highland  universal  reseda   griffith  grauman s  beverly  ventura  seattle  pike  watertown  deca  needle  pikes   pike s monorail  uw  safeco    table    the top boosted words in the softmax bias layer for different context settings in a factorcell model.    tive to location. compared to n gram based lms   our model has two advantages in the way that it handles context. first  as we showed in our geotwitter  experiments  we can adapt to geography using gps  coordinates as input without using predefined geographic regions as in chelba and shazeer. second   our model supports the joint effect of multiple contextual variables. neural models have an advantage  over discrete models as the number of context variables increases.  much of the work on context adaptive neural language models has focused on incorporating document or topic information  mikolov and zweig         ji et al.        ghosh et al.        dieng  et al.         where context is defined in terms  of word or n gram statistics. our work differs  from these studies in that the context is defined  by a variety of sources  including discrete and or  continuous metadata  which is mapped to a context vector in end to end training. context sensitive  language models for text generation tend to involve other forms of context similar to the objective of our work  including speaker characteristics  luan et al.        li et al.         dialog act   wen et al.         sentiment and other factors   tang et al.        hu et al.         and style  ficler  and goldberg       . as noted earlier  some of  this work has used discriminative text classification  to evaluate generation. in preliminary experiments  with the yelp data set  we found that the generative  classifier accuracy of our model is highly correlated  with discriminative classfier accuracy  r    .   .  thus  by this measure  we anticipate that the model  would be useful for generation applications. anecdotally  we find that the model gives more coherent  generation results for dbpedia data  but further validation with human ratings is necessary to confirm  the benefits on more sources.         conclusions    in summary  this paper has introduced a new model  for adapting  or controlling  a language model depending on contextual metadata. the factorcell  model extends prior work with context dependent  rnns by using the context vector to generate a lowrank  factored  additive transformation of the recurrent cell weight matrix. experiments with six tasks  show that the factorcell model matches or exceeds  performance of alternative methods in both perplexity and text classification accuracy. findings hold  for a variety of types of context  including highdimensional contexts  and the adaptation of the recurrent layer is particularly important for characterlevel models. for many contexts  the benefit of  the factorcell model comes with essentially no additional computational cost at test time  since the  transformations can be pre computed. analyses of a  dataset with a high dimensional sparse context vector show that the model learns context similarities to  facilitate parameter sharing.  in all six tasks that are explored here  all context  factors are available for all training and testing samples. in some scenarios  it may be possible for some  context factors to be missing. a simple solution for  handling this is to use the expected value for the  missing variable s   since this is equivalent to using  a weighted combination of the adaptation matrices  for the different possible values of the missing variables.  in this work  the experiment scenarios all used  metadata to specify context  since this type of context can be more sensitive to data sparsity and has  been less studied. in contrast  in many prior studies  of language model adaptation  context is specified  in terms of text samples  such as prior user queries   prior sentences in a dialog  other documents related     in terms of topic or style  etc. the factorcell framework introduced here is also applicable to this type  of context  but the best encoding of the text into an  embedding  e.g. using bag of words  sequence models  etc.  is likely to vary with the application. the  factorcell can also be used with online learning of  context vectors  e.g. to take advantage of previous  text from a particular author  or speaker   jaech and  ostendorf       .  the models evaluated here were tuned to minimize perplexity  as is typical for language modeling. in analyses of performance with different hyperparameter settings  we find that perplexity is not  always positively correlated with accuracy  but the  criteria are more often correlated for approaches that  adapt the recurrent layer. while not surprising  the  results raise concerns about using perplexity as the  sole evaluation metric for context aware language  models. more work is needed to understand the relative utility of these objectives for language model  design.    