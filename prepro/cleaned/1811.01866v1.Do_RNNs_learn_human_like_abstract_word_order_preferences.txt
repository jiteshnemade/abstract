introduction  the best performing models for many natural  language processing tasks in recent years have  been recurrent neural networks  rnns   elman         sutskever et al.        goldberg         but  the black box nature of these models makes it hard  to know exactly what generalizations they have  learned about their linguistic input  have they  learned generalizations stated over hierarchical  structures  or only dependencies among relatively  local groups of words  linzen et al.        gulordava et al.        futrell et al.         do they  represent structures analogous to syntactic dependency trees  williams et al.         and can they  represent complex relationships such as filler gap  dependencies  chowdhury and zamparelli         wilcox et al.         in order to make progress    with rnns  it is crucial to determine what rnns  actually learn given currently standard practices   then we can design network architectures  objective functions  and training practices to build on  strengths and alleviate weaknesses  linzen       .  in this work  we investigate whether rnns  trained on a language modeling objective learn  certain syntactic preferences exhibited by humans   especially those involving word order. we draw  on a rich literature from quantitative linguistics  that has investigated these preferences in corpora  and experiments  e.g.  mcdonald et al.         stallings et al.        bresnan et al.        rosenbach       .  word order preferences are a key aspect of human linguistic knowledge. in many cases  they  can be captured using local co occurrence statistics  for example  the preference for subject verb   object word order in english can often be captured  directly in short word strings  as in the dramatic  preference for i ate apples over i apples ate. however  some word order preferences are more abstract and can only be stated in terms of higherorder linguistic units and abstract features. for example  humans exhibit a general preference for  word orders in which words linked in syntactic dependencies are close to each other  such sentences  are produced more frequently and comprehended  more easily  hawkins        futrell et al.         temperley and gildea       .  we are interested in whether rnns learn abstract word order preferences as a way of probing  their syntactic knowledge. if rnns exhibit these  preferences for appropriately controlled stimuli   then on some level they have learned the abstractions required to state them.  knowing whether rnns show human like  word order preferences also bears on their suitability as language generation systems. white and  rajkumar        have shown that language gener      ation systems produce better output when humanlike word order preferecences are built in  it may  turn out that rnn language models reproduce  such preferences such that they do not need to be  built in explicitly.  as part of this work  we validate and quantify these word order preferences for humans by  collecting acceptability ratings for english sentences with different word orders. to our knowledge  this is the first experimental acceptabilityjudgment study of these word order preferences  using fully controlled stimuli  previous experimental work has used naturalistic stimuli derived  from corpora  in which the predictors of word  order are not directly manipulated  rosenbach         bresnan       .  alternations studied  we study four syntactic alternations in english   particle shift  in which a verbal particle can appear directly after the verb or later  e.g.  give up  the habit vs. give the habit up   heavy np shift   in which a verb is followed by an np and a pp  with order np pp or pp np  the dative alternation  e.g. give a book to tom vs. give tom a book    and the genitive alternation  e.g. the movie s title  vs. the title of the movie .  in all these alternations  three common factors  influencing word order preferences are evident   short constituents go before long constituents   words which are definite go earlier  and words referring to animate entities go earlier. in fact these  preferences are very general patterns across languages  and in some languages constitute hard  constraints  bresnan et al.       .         methods    we investigate the learned word order preferences  of rnns by studying the total probability they assign to sentences with various word order properties. specifically  we create sentences by hand  which can appear in a number of configurations   and study how these manipulations affect the sur prisal value assigned by an rnn to a sentence.  surprisal is the negative log probability   n  n  s xi          log  p xi       n          log  p xi  xi    j       i      n is a sequence of n words forming a senwhere xi    tence and the conditional probability p xi  xi    j     is    calculated as the rnn s normalized softmax activation for xi given its hidden state after consuming  xi    j   .  surprisal has a number of interpretations that  make it convenient as a dependent variable for examining language model behavior. first  surprisal  is equivalent to the contribution of a sentence to a  language model s cross entropy loss  effectively   our rnn language models are trained with the  sole objective of minimizing the average surprisal  of training sentences  so surprisal is directly related to the model s performance. second  wordby word surprisal has been found to be an effective predictor of human comprehension difficulty  hale        levy        smith and levy          interpreting surprisal as metric of  difficulty  allows us to analyze rnn behavior analogously to human processing behavior  van schijndel and linzen        futrell et al.       . third   surprisal more generally reflects the dispreference  or markedness of a sequence according to a language model. high surprisal for a sentence corresponds to a relative dispreference for that sentence. when the logarithm is taken to base    surprisal is equivalent to the bits of information required to encode a sentence under a model.  in the studies below  we test hypotheses statistically using maximal linear mixed effects models   baayen et al.        barr et al.        fit to predict  surprisals given experimental conditions.   .     models tested    we study the behavior of two lstms trained  on a language modeling objective over english  text  the one presented in jozefowicz et al.         as  big lstm cnn inputs   which we call   jrnn   which was trained on the one billion  word benchmark  chelba et al.        with two  hidden layers of      units and cnn character  embeddings as input  and the one presented in gulordava et al.         which we call  grnn   with  two hidden layers of     units  trained on    million tokens of english wikipedia.  as a control  we also study surprisals assigned  by an n gram model trained on the one billion  word benchmark  a   gram model with modified kneser ney interpolation  fit by kenlm with  default parameters   heafield et al.       . the  n gram surprisals tell us to what extent the patterns under study can be learned purely from cooccurrence statistics with a small context window     grnn    human   .         .          preference for order v np pp    without any generalization over words. to the extent that lstms yield more humanlike performance than the n gram model  this indicates one  of two things. either they have learned generalizations that are formulated in terms of more abstract  linguistic features  or they have learned generalizations that can span larger distances than the ngram window.     .           .              .   jrnn    n gram      .           .       .      .     human acceptability ratings    we also compare rnn surprisals against human  preferences on our experimental items. we collected acceptability judgments on a scale of     least acceptable  to    most acceptable  over  amazon mechanical turk.  for the studies of  heavy np shift  the dative alternation  and the genitive alternation  we collected data from    participants  filtering out participants who were not native english speakers or who could not correctly  answer     of simple comprehension questions  about the experimental items. after filtering  we  had data from    participants. for the study of particle shift  we used data from a previous  unpublished  acceptability rating experiment with      subjects  and the same filtering criteria. after filtering  we had data from     participants.         heavy np shift    h eavy np shift describes a scenario where constituent weight preferences become so strong that  an order which would otherwise be unacceptable  becomes more acceptable  as shown in example    .         a.    the publisher announced a book on  thursday.  b.  the publisher announced on thursday a book.  c. the publisher announced a new book  from a famous author who always  produced bestsellers on thursday.  d. the publisher announced on thursday a new book from a famous author  who always produced bestsellers.    in these examples  the verb announced is followed  by a noun phrase  a  new  book...  and a temporal pp adjunct  on thursday . the usual order for  these elements is to put the np before the pp  but  when the np becomes very heavy  the pp might    preregistered    pdf.    at https   aspredicted.org sh zf.          .    .        short    long    short    long    np length    figure    mean preference for standard word order by  np length. in this and other figures  for computational  models  preference is measured as total sentence surprisal for verb np pp order minus total sentence surprisal for verb pp np order  error bars represent      confidence intervals of the contrasts between conditions  computed by subtracting out the by item means  before calculating the intervals  masson and loftus        . for the human data  preference is measured as  the difference in mean acceptability for verb pp np  minus verb np pp  and error bars represent     confidence intervals of the contrasts between conditions after subtracting out by item and by subject means.    be placed closer to the verb  which case the word  order is called shifted. heavy np shift is the  primary example of locality effects in word order  preferences  in that it creates shorter dependencies  from the verb to the np and the pp.  we tested whether rnns show length based  preferences similar to example    .  we adapted     items from stallings et al.        which consist of a verb followed by an np and a temporal  pp adjunct  where the order of the np and the pp  and the length of the np are manipulated. if the  networks show human like word ordering preferences  there should be a penalty for pp np order  when the np is short  but this penalty should be  smaller or nonexistent when the np is long.  figure   shows the models  preference for  the standard word order  verb np pp  over the  shifted word order  verb pp np   calculated as  the surprisal of sentences in shifted word order minus their surprisal in standard word order.  also included are the human acceptability ratings  where the preference for the order verb np   pp is calculated as the average acceptability difference between verb np pp and verb pp np    the preregistration for this experiment can be viewed at  https   aspredicted.org ea m .pdf.     across items. in all cases  we see that the shifted  order becomes more preferred when the np is  long  although it never becomes the most preferred  order.  our experimental design allows us to control for  the effect of sentence length on rnn surprisals.  the sentences with long nps are naturally expected to have higher rnn surprisal than the ones  with short noun phrases  since they have more  words and thus higher information content. however  the verb np pp preference is quantified as  the difference between surprisal of order verb   pp np and surprisal of the order verb np pp  for  both the long and short np conditions. the long  np conditions may have higher surprisal overall   but this will be cancelled out in the difference. the  crucial question is then whether this preference is  larger in the long np case than in the short np  case whether the red and blue values in figure    are significantly different across items. the crucial  statistic for each item i is given by the interaction  ii    ii   si  short  verb np pp    si  short  verb pp np       si  long  verb np pp    si  long  verb pp np     where si is the surprisal for the ith item in the  given condition. if ii is significantly positive across  items  then we have evidence that np length  causes a preference for verb pp np order even  when controlling for the intrinsic effects of length  and of the particular words in each item. the same  logic is applied to the analysis of the acceptability ratings data. all studies in this paper apply  this same design and analysis. similar designs are  common in psycholinguistics  and are applied to  rnn surprisal data in futrell et al.        and  wilcox et al.       .  to test the significance of the interaction  we  use mixed effects modeling with random intercepts and slopes by item. we find that the interaction is statistically significant in jrnn  interaction size  .  bits  p    .     and grnn    .  bits  p    .     but not in the n gram baseline   .  bits  p    .   . the interaction in jrnn  is significantly stronger than in the n gram baseline  p    .     but the interaction in grnn is  not significantly stronger than the n gram baseline   p    .   . none of the models show the effect as  strongly as the human acceptability judgments.  thus we find that both jrnn and grnn exhibit human like word order biases for heavy np    shift  but do not find evidence for such a bias in  the n gram baseline. the result suggests that the  lstm models have learned a higher order generalization that is not trivially present in n gram  statistics.         phrasal verbs and particle shift    another domain of word order variation similar to  heavy np shift is phrasal verbs  which consist of  a verb and a particle  such as give up. the object  np of a transitive phrasal verb can appear in two  positions  it can be shifted  after the particle  or  unshifted  before the particle . as in heavy np  shift  the shifted order is generally preferred when  the np is long          a.  b.  c.    d.    kim gave up the habit.  shifted   kim gave the habit up.  unshifted   kim gave up the habit that was preventing success in the workplace.   shifted   kim gave the habit that was preventing success in the workplace up.  unshifted     the fact that both word orders are possible is  called particle shift. particle shift provides another arena to test whether rnns have learned the  basic short before long constituent ordering preference in english. furthermore  particle shift is  also affected by the animacy of the object np  in  that the unshifted order is preferred when the object np is animate  gries         so we can use  this construction to test order preferences involving both length and animacy.  we designed    experimental items consisting  of sentences with phrasal verbs as in example       where each item could occur with either a long or a  short object np. long nps were created by adding  adjectives and postmodifiers to short nps. half of  the items had inanimate objects  half had animate  objects. all nps were definite. we tested the effects of np length  np animacy  and word order  on language model surprisal.   figure   shows the average preference for  shifted word order according to each model  calculated as the surprisal of the shifted order minus the surprisal of the unshifted order. in general  we see that when the object np is long  the  shifted order is relatively preferred  the effect is    the preregistration for this experiment can be viewed at  https   aspredicted.org uu am.pdf.     grnn         human      .     dative alternation     .     .     preference for order v part np    the dative alternation refers to the fact that  in many cases the following forms are substitutable      .      .    .      .      .      .      .   jrnn    n gram                     a.                b.                    short    long    short    long    np length  np animacy    inanim    anim    figure    preference for shifted word order  total  sentence surprisal for verb particle np order minus  verb np particle order  by np length  np animacy   and model.    strongest in jrnn. in regressions  we found that  the interaction of np length and word order is significant in jrnn    .  bits  p    .      grnn    .  bits  p    .      and the n gram baseline   .   bits  p    .    . however  the interaction in the  n gram baseline is significantly smaller than in  jrnn  p    .     and grnn  p    .   .  the effects of animacy are unexpectedly intricate. numerically  grnn and the n gram baseline  show the expected effect  an animate np favors  unshifted order. however  in the human ratings we  find that the expected animacy effect for short nps  actually reverses for long nps  and this is reflected  numerically in jrnn. no effects of animacy are  significant in any model. in the human data  animacy has a significant interaction favoring the unshifted word order for short nps  p    .     with  an interaction that reverses the effect for long nps   p    .    . this reversal is surprising given what  was previously known about word order in phrasal  verbs.  our investigation of particle shift has shown  that lstm models learn short before long length  preferences in regard to word order in phrasal verb  constructions  n gram models show these preferences as well  though weaker. we do not find evidence that the models learned human word order  preferences based on np animacy in this case  but  the experimental results suggest that the effects of  animacy on this alternation might be more complex than previously believed.    the man gave the woman the book.   double object  do  construction   the man gave the book to the woman.   prepositional object  po  construction     the dative alternation is one of the most studied  topics in syntax. the two constructions have been  argued to convey subtly different meanings  with  the do construction indicating caused possession  and the po construction indicating caused motion   green        oehrle        gropen et al.         levin       . however  the semantic preference  of each construction appears to be only one factor  among many when it comes to determining which  form will be used in any given instance. other factors include the animacy  definiteness  and length  of the theme  the book in example      and the  recipient  the woman   bresnan et al.       .  the human preferences in the dative alternation  work out such that the np which is more animate   definite  and short goes earlier. an extreme case is  exemplified in      the sentences marked with   are  relatively dispreferred by native english speakers.         a.  b.    c.    d.    the man gave the woman a very old  book that was about historical topics.   the man gave a very old book that  was about historical topics to the  woman.  the man gave the book to a woman  who was waiting patiently in the hallway.   the man gave a woman who was  waiting patiently in the hallway a  book.    in order to examine whether lstms show humanlike preferences in the dative alternation  we designed    items on the pattern of      with   verbs  of caused possession  such as give  and   verbs  of caused motion  such as throw .  in all items   the theme was inanimate and the recipient was  animate. we manipulated the definiteness of the    the preregistration for this experiment can be viewed at  https   aspredicted.org ky ne.pdf.     grnn    human    grnn    human     .          .    .    .      .     preference for po construction  bits     preference for po construction           .              .     jrnn    n gram                           .    .    .      .      .      .   jrnn    n gram                                          short    long    short    long    indefinite    theme length  recipient length    short    definite    indefinite    definite    theme definiteness  recipient definiteness    long    indefinite    definite    figure    average po preference by length of theme  and recipient.    figure    average po preference by definiteness of  theme and recipient.    theme and the recipient using the articles the and  a  and the length of the theme and the recipient by  adding relative clauses to either or both.  figure   shows the strength of the models  preference for the po construction as a function of  the length of the theme and recipient. the lstms  have an overall preference for the po form  which  is mirrored in the human data. in addition  we see  that a long recipient is strongly associated with a  stronger preference for the po form  and a long  theme is strongly associated with a relative preference for the do form  in line with human preferences. the n gram baseline shows these effects but  with a smaller magnitude  and without the overall  po preference shown by humans.  all interactions of length and word order are  significant at p   .    in all models  with the exception of the effect of recipient length in the ngram model  where p    .  . the effects of recipient and theme length are significantly weaker in  the n gram baseline than in jrnn  p    .     for  grnn  the effect of recipient definiteness is significantly stronger than the n gram baseline  p     .    but the effect of theme definiteness is not significantly stronger than in the n gram baseline. in  human data  the interaction of recipient length and  word order is significant at p   .    and the interaction of theme length and word order is significant at p   .  .  now we turn to word order preferences based  on np definiteness. figure   shows the po preference by the definiteness of the theme and recipient. in line with the linguistic literature  the po  preference is numerically smaller for definite recipients in both lstm models and in human data     but not in the n gram model. the interaction of recipient definiteness and word order is significant  in the expected direction in jrnn at p    .     and grnn at p    .  . theme definiteness has  a small positive interaction with word order  at  p    .    for jrnn  favoring the po construction.  these results are broadly in line with the linguistic  literature  but they are not reflected in the human  data for these experimental items  in the human  ratings data  there are no significant interactions  of definiteness and word order.  overall  we find evidence for humanlike ordering preferences in the dative alternation with  respect to length and definiteness of theme and  recipient. the strongest effects which are most  in line with the linguistic literature come from  jrnn.         genitive alternation    similarly to the dative alternation  the genitive  alternation involves two constructions with  opposite word orders expressing similar meanings          a.  b.  c.  d.    the woman s house  s genitive  definite possessor   the house of the woman  of  genitive   definite possessor   a woman s house  s genitive  indefinite possessor   the house of a woman  of  genitive   indefinite possessor     as in the dative alternation  whatever semantic difference exists between the two constructions is  only one factor conditioning which form is used     grnn    human    grnn    human      .       .      .         .      .      .    .          preference for of construction    preference for of construction     .    .           .     .   jrnn    n gram                             .    .   jrnn    long    short                              long        inanim    possessor length  possessor definiteness    n gram                    short     .      .     indef    anim    inanim    anim    possessum animacy  possessor animacy    def    inanim    anim    figure    average of  genitive preference by length and  definiteness of possessor.    figure    average of  genitive preference by animacy  of possessor and possessum.    in each particular case. the other factors are the  usual suspects  animacy  definiteness  and length  of the possessor  the woman in      and pos sessum  the house in       kreyer        rosenbach              shih et al.       .  in order to study the genitive alternation in  rnns  we designed    items on the pattern of    .  we varied the definiteness and length of the possessor as in the dative alternation.  we also varied  the animacy of the possessor and possessum  between items.   figure   shows the rnns  preferences for the  of  genitive form based on the definiteness and  length of the possessor. in all models and in human data  we see that the of  genitive is preferred  generally when the possessor is long  and the sgenitive when it is short. the interaction of possessor length with word order is significant in models  and human data  p    .    in all cases . turning  to possessor definiteness  we see that it relatively  favors the s genitive in human data and in the ngram baseline  in line with the linguistic literature   but no such effect is found in the rnn models.  however  the interaction of definiteness with word  order is not significant in our data  p   .   in human data and higher for the language models .  now we turn to effects of animacy. figure    shows the of  genitive preference by the animacy  of the possessor and possessum. in all models  in  line with human preferences  we see that possessor animacy favors the s genitive. the interaction    of possessor animacy and word order is significant in jrnn  p    .    and grnn  p    .      but not in the n gram baseline  p    .     the effect is significantly stronger in jrnn than in the  n gram baseline  p    .    but not significantly  stronger in grnn. the effect of possessum animacy is more complex  it seems to favor the sgenitive in grnn  but the of  genitive in the other  models and in human preferences  in any case  the  effect is small  and the interaction is not significant  in any of the data collected here.  overall  it appears that the lstms tested show  humanlike order preferences in the genitive alternation when it comes to possessor length and animacy  they show more evidence for effects of possessor animacy than an n gram baseline. they do  not appear to pick up on definiteness preferences   but based on human experimental data these preferences might be weak in the first place.      for both constructions to be legitimate syntactic options     the possesssum must be definite and unmodified by relative  clauses.    the preregistration for this experiment can be viewed at  https   aspredicted.org f sk .pdf.         discussion    we have explored rnn language models  ability to represent soft word order preferences whose  formulation requires abstract features such as animacy  definiteness  and length. we found that  rnn language models generally do so  and  outperform an n gram baseline  indicating that  they learn generalizations which are not trivially  present in local co occurrence statistics of words.  the extent to which rnns learn such preferences  varies  effects of length are strongly and consistently represented  with weaker evidence for effects of animacy and definiteness.  much recent work has focused on whether  rnns can learn to represent discrete syntactic     structures such as long distance number agreement  linzen et al.         wh dependencies  mccoy et al.        chowdhury and zamparelli         wilcox et al.         anaphora  negative polarity item licensing  and garden path sentences   van schijndel and linzen        marvin and  linzen        futrell et al.       . the current  work focuses on soft preferences which have been  studied in quantitative syntax  and the abstract features that have been discovered to underly these  preferences  finding that rnns are able to represent many of the required features. the same features underlying these soft preferences in english  often play a role in hard constraints in other languages  bresnan et al.         thus our findings indicate that rnns can learn crosslinguistically useful abstractions.  our results also demonstrate that some of the  key features underlying syntactic alternations can  be learned from text data alone  without any particular innate bias toward such features. qualifying this point  note that the language models we  studied here were exposed to many more tokens  of linguistic input than a typical child learner.  in addition to the results about rnns  our work  provides human data from directly controlled experimental manipulations of animacy  definiteness  and length in particle shift  genitive  and dative alternations. our human acceptability ratings  experiments have revealed some unexpected patterns  such as the sign reversal in the effect of animacy for long nps in particle shift  section      which should be investigated in more detail in future work.  an interesting question which has been studied  in the functional linguistic literature is why these  particular word order preferences exist across languages. the preferences are often explained in  terms of cognitive pressures on language comprehension and production. the short before long  preference is most likely a manifestation of the  pressure for short dependencies  wasow          which is motivated by working memory limitations in sentence processing  gibson         this  word order preference is reversed in predominantly head final languages such as japanese   where there is a general preference for long constituents to come before short ones  yamashita and  chang       . the biases for animate and definite  nouns to come early are usually linked to biases in  the human language production process whereby    words and constituents which are easier to produce  come earlier  bock       .  it is possible that these same cognitively motivated biases might also be present in rnns. for  example  futrell and levy        have argued that  there should be a preference for short dependencies in any system that predicts words incrementally given lossy representations of the preceding  context  since rnns represent context using fixedlength vectors  their context representations must  be lossy in this way. furthermore  chang         has shown that the preference to place animate  words earlier can arise in simple recurrent networks without this bias being present in training  data  suggesting that rnns may be subject to similar pressures to produce certain kinds of words  earlier.  more generally  we have treated rnn language  models essentially as human subjects delivering  acceptability judgments  observing their behavior on carefully controlled linguistic stimuli rather  than examining their internals. by using controlled  experimental designs  we are able to control for  factors such as sentence length and the particular lexical items in each sentence  cf. lau et al.        . we believe this approach will allow us to  derive initial insight into the limits of what rnns  can do  and will guide work that explains the behavior we document here in terms of network internals.    acknowledgments  this work was supported in part by a gift from  the nvidia corporation. rpl gratefully acknowledges support from the mit ibm ai research laboratory. all code and data is available  at https   github.com langprocgroup rnn   soft constraints.    