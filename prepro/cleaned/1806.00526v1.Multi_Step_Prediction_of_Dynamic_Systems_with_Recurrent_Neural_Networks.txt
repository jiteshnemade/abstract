introduction    m    ulti step prediction of dynamic motion remains  a challenging problem. the simplifying assumptions  required in the process of modeling dynamic systems lead to  unmodeled dynamics  which in turn lead to repeatable and  systematic prediction errors. when the prediction is required  over very short periods into the future  the unmodeled dynamics may cause negligible error  however  using the model  over longer prediction horizons  the unmodeled dynamics can  lead to drastic growth in the prediction error over time. in  the discrete time domain  a prediction required for one time  step into the future is referred to as a single step prediction   and a prediction many steps into the future is referred to as a  multi step prediction.  in this paper  the multi step prediction of mobile robotic  systems is considered. recurrent neural networks  rnns  are  mainly employed to learn the dynamics of two aerial vehicles   a helicopter and a quadrotor  from experimental data. the  helicopter dataset belongs to the stanford helicopter platform   which has been used in apprenticeship learning     and  single step prediction and modeling of the platform    . the  quadrotor dataset  however  has been specifically collected for  this work.  multi step prediction has many applications in state estimation  simulation and control         . for example  model based    http   heli.stanford.edu dataset     control schemes  such as model predictive control  mpc        can extensively benefit from an accurate long term prediction.  accurate multi step predictions allow for a slower update  rate of the mpc  reducing the overall computational burden  while maintaining smoothness and accuracy of the resulting  control system response. as another example  in a moving  vehicle when some measurements  such as gps readings  are  temporarily unavailable  a multi step prediction can account  for the missing measurements and approximate the system  position and speed over the blackout period.  the classic method of modeling  i.e.  modeling from first  principles  or white box methods   suffer from two major  difficulties. first  the developed model will contain many  parameters which describe the system physical characteristics   mass  drag coefficient  etc.  and must be properly identified  prior to using the model. second  many properties of the  system might be too difficult to model explicitly  such as  the vortex ring effect on a quadrotor vehicle    . identifying  the parameters of a model can be expensive. for instance   measuring the blade drag coefficient of a quadrotor needs  a wind tunnel. moreover  by changing the system physical  properties slightly  the model should be adapted accordingly   which may involve new measurements and cumbersome tests.  on the other hand  learning based  or black box  modeling  relies on the observations from the system to decipher complex  nonlinearities acting on the system. there are several blackbox architectures  such as polynomial models  the wiener  model  voltera series  etc.   fuzzy models          and neural  networks          . regardless of the method  a black box  model has many degrees of freedom  dof   depicted as parameters or weights  that should be found based on a set of  input output observations from the system. the search to find  the appropriate values for the model parameters is usually done  through an optimization process and  since many black box  models are machine learning  ml  methods  the parameter  optimization process is frequently referred to as a learning  process.  in recent years  ml has been going through rapid development. deep learning  dl             is revolutionizing the  ml field and solving problems that could not have been  solved before  such as speech recognition      and object  detection and classification in images           . three main  components propel this revolution  i  the newly available  hardware capable of performing efficient parallel processing  on large numerical models  ii  methods to train these models  and iii  availability of large scale datasets. because of the third  reason  the applications of deep learning have been mainly  focused on natural language processing and image classifica      multi step prediction of dynamic systems with rnns  n. mohajerin  s. l. waslander    tion for which such datasets are readily available. however   dl methods can be also used in modeling and control of  robotic systems. in fact  in mobile robotics  where the robot  is deployed in an unstructured environment with noisy and  uncertain sensory information  learning from observation can  deal with problems that are either too difficult or too expensive  to handle with classical methods. this work demonstrates  some of the capabilities and applicability of ml methods in  such applications. particularly  in this work we show how to  employ rnns to accurately predict the behavior of the two  robotic systems using input information only  many steps into  the future.  using rnns in multi step prediction requires a proper state  initialization  that is  assigning proper initial values to the  neuron outputs at the first step of prediction. the underlying  reason is that the rnn recursively updates itself throughout  the prediction horizon and therefore  exhibits dynamics. like  any other dynamic system  the  transient  response of an rnn  in general depends heavily on its initial condition. for an rnn  with hidden layers  the common approach is to initialize the  hidden neuron outputs to zero  or random  values and run the  network until the effect of the initial values washes out            . some of the drawbacks in the washout method are     the washout period is not fixed and hard to determine  a priori  during which the network does not produce a  reliable prediction.    the network may become unstable during the washout  period  leading to prolonged or even failed training sessions.    in the training process  the input sequence used in  washout does not contribute to the learning process.  the third drawback listed above is more severe when the  dataset collection process is expensive  which is often the case  with experimental data acquisition in the robotics domain.  in this work  the rnn state initialization problem is carefully studied. an nn based initialization method  previously  proposed in       is revisited and expanded. it is shown that  the network initialized with the nn based method significantly  outperforms the same network initialized by the washout  method. after establishing the effectiveness and efficiency  of our nn based initialization method  the results of blackbox modeling of the two aerial vehicles are demonstrated and  studied. to further improve the quadrotor model  the hybrid  model in      is revisited. this work embodies the capability  of neural networks in learning unmodeled dynamics of a real  and challenging robotic system for multi step prediction  for  the first time  and may serve as a basis for future development  and application of more sophisticated learning based methods  in modeling  prediction and control of such systems.  ii. background    and    l iterature r eview    feed forward neural networks  ffnns  have been used  extensively in modeling and control of dynamic systems            . in a control problem  they may be employed as a  modelling part of a controller in a lyapunov design approach.  in this method using a lyapunov function  the equations for  evolution of the neural network weights are extracted so that         the closed loop controller stabilizes the system          .  since ffnns lack exhibiting dynamics  they are mainly used  as single step predictor or compensator.  rnns possess dynamics and are universal approximators  for reconstructing state space trajectories of dynamic systems            which make them suitable candidate models  for multi step prediction problem. in       it is shown that any  finite time trajectory of a dynamic system can be approximated  by some rnns to any desired level of accuracy given a proper  initial state. this result is extended to discrete rnns in     .  this is another aspect to reinforce the importance of a proper  state initialization for rnn in modeling dynamic systems.  nonlinear auto regressive  nar  models are classic tools  to model dynamic systems                 . in       narendra et al. devised methods to use multi layer perceptrons   mlps  in the non linear autoregressive exogenous  narx   framework. in a discrete time fashion  narx framework  implements a dynamic system whose output at any given time   yk   is a function of the input at that time  uk   and the system  output and input at previous time steps      yk   f yk     . . .   yk  y   uk   uk     . . .   uk  u    where the length of the memory buffers  i.e.   u and  y   are  usually given or determined through a hyper parameter optimization process. the function f  .  can be realised by various  methods. in       the function f  .  is realized by an mlp.  to avoid confusion  the method to implement this function  is added to the narx abbreviation as a suffix. for instance   if f  .  is realised by an mlp then the architecture will be  referred to as narx mlp. a narx mlp is essentially an  mlp equipped with buffers and feedback connections. hence   it can be classified as an rnn.  the narx mlp architectures are often trained via a seriesparallel      mode which uses the network target values  instead of the network past outputs as the delayed version s   of the network output. this method is also known as teacher  forcing     . clearly  this mode converts the narx architecture into a feedforward one which therefore loses the main  advantage of an rnn and limits the ability of the method to  represent dynamical systems accurately. on the other hand   training narx mlp in a closed loop form  parallel mode   to model dynamic systems can be difficult due to numerical  stability issues in the calculation of the gradient for learning  based optimization     .  one alternative to narx model is to define an internal  state  xk   and use a one step memory buffer      xk  f xk     yk     uk       yk  g xk .    this architecture is an example of a recurrent multilayer  perceptron  rmlp . an rmlp is made by one  or a few   locally recurrent layers of sigmoid neurons     . rmlps have  been used in a number of dynamic system identification and  modelling problems  such as a heat exchanger       engine  idle operation      and wind power prediction     .  it is not clear whether using rmlps is more advantageous  than narx mlps. however  in      it is shown that narxmlps  in a single step prediction scenario  outperform rmlps     multi step prediction of dynamic systems with rnns  n. mohajerin  s. l. waslander    in modelling a real helicopter. narx rnns have been extensively studied            and used in various modelling  and identification problems          . in       a radial basis  function  rbf  network in a narx architecture  i.e.  narxrbf  is used to model and control a quadrotor with three  horizontal and one vertical propeller. in       another form  of narx rbf is employed and trained using levenbergmarquardt  lm  optimization algorithm to model a small scale  helicopter. both approaches employ teacher forcing.  recently  abbeel et al. used rectified linear units neural  networks to model dynamics of a real helicopter    . although  they have not used rnns  their dataset is also used in this  work to assess the performance of rnns. in       a deep  neural network  trained by a model predictive control  mpc   policy search  is used as a policy learner to control a quadrotor.  the network generates one step policies and has a feedforward architecture. in       a few nn architectures  such as  mlps  rmlps  long short term memory  lstm  and gated  recurrent unit cells are compared against each other in single  step prediction of a few small robotic datasets. in       a hybrid  of recurrent and feed forward architectures is used to learn  the latent features for mpc of a robotic end effector to cut     types of fruits and vegetables. although the authors use  recurrent structure  they also state that using their transforming recurrent units  trus  in a multi step prediction scheme  results in  instability in the predicted values   so they use  their proposed network as a one step predictor. however  the  recurrent latent state helps to improve the predictions.  iii. p roblem f ormulation  in this section  the multi step prediction problem is defined  for a general non linear dynamic system. as discussed earlier   regardless of the system to be modeled by rnns  a proper state  initialization is required and therefore  the state initialization  problem is also defined. note that for practical purposes we  are working in the discrete time domain.  a. multi step prediction with recurrent neural networks  snm    consider a dynamic system  with m input and n output  dimensions. the system input and output at a time instance   k  is denoted by uk   rm and yk   rn   respectively. we  assume that both the input and output are measurable at all  ks. consider an input sequence of length t starting at a time  instance k       uk     t   rm   rt               uk     t   uk     uk     . . . uk   t .    the system response to this input is an output sequence  denoted by yk     t   rn   rt               yk     t   yk     yk     . . . yk   t .    the multi step prediction problem seeks an accurate estimate  of the system output over the same time horizon  y k     t    rn   rt               y k     t   y k     y k     . . . y k   t           which minimizes a sum of squares error measure  sse  over  the prediction length  t    l     kx     t    e   k ek           k k        ek   yk   y k .           an rnn with m input and n output  rm  n   is a dynamic  system. at each time instance k  feeding the input element  uk to the rnn  it evolves through two major steps     state  update and    output generation        a   xk      f xk        uk         b   y k      g xk      uk      where y k     and xk     are the rnn output vector and  state vector  respectively  at time k. the vector     rq  encompasses the network weights and q depends on the  architecture of the rnn. the function f  .  is defined either  explicitly  rmlps       or implicitly  lstms       as are  discussed in section v. for g .   however  we choose a  linear map because modeling a dynamic system by rnn is  a regression problem. note that the network states and or  output may also depend on a history of the right hand side  values in the equations    .  in the discrete domain  feedback connections require some  form of a memory buffer. in an rnn  states are the buffered  values since they determine the network output knowing the  input and network functions. depending on the feedback  source  there are two types of states     output states  ok    which encompasses feedbacks from the network output  and     internal states  sk   which encompasses feedbacks from within  the network. therefore the rnn state vector is        o       xk   k   rs .  sk  where s is the states count. therefore  we can rewrite equations     using single step buffers   ok      y k             a     sk      f  xk        uk     y k      g xk      uk  .      b     c     using rnns to address multi step prediction problem  we  seek an rnn which given any input sequence uk     t    produces an output sequence y k     t     which minimizes  the mean sse  msse  cost over the prediction interval   k       k    t     l            t    kx     t    ek      ek               k k        ek       yk   y k    .            where yk is the system output at time k    k       k    t   to  the input uk   uk     t . therefore  the solution to the multistep prediction problem is an rnn which minimizes l for all  possible input output sequences            arg min l                 multi step prediction of dynamic systems with rnns  n. mohajerin  s. l. waslander    the optimization in equation      is practically impossible because there can be infinite input output sequences. in practice   a dataset is collected by measuring the system input and output  and a numerical optimization is carried out to find a minimum  of the total cost   lpred          d    x  li      d i      d k   t    x x  ei k      ei k          t d i              where d is the dataset size. the datasets employed in the  optimization process are comprised of time series samples in  the form of input output tuples   n   o        d   si   ui  t    yi  t   .  where t indicates that all samples are of the same length.  there are many tricks and tweaks to enhance rnn training  which some of them are used in this paper and will be  mentioned in section vi. for detailed discussions refer to       and     .  b. state initialization in recurrent neural networks  given an initial state  xk    we can rewrite the rnn inputoutput equation as a sequence to sequence map            y k     t   f xk    uk     t .    the function f   rs   rm   rt   rn   rt   symbolizes the  operations taking place sequentially inside the rnn  defined  by    .  from       it is evident that the initial states play a key  role in the immediate response of the rnn. therefore  to  have an accurate estimate one should properly initialize the  rnn  i.e.  set the initial states of the rnn  xk   to values that       is minimized. note that minimizing      also requires  training the rnn. therefore  rnn state initialization should  be considered as a part of rnn training which should be  repeatable in the testing  generalization tests  as well. we will  explain this insight in more details in section iv.  to    rnn    according to      at each time instance k    k       k    t    the states  xk   must be updated prior to generating the output   y k   which requires the knowledge of x k     . based on  the universal approximation property  there exists an rnn   which can approximate the system to be modelled with  k  error accuracy  where  k is the prediction error at time k  and can be decreased infinitesimally for all k     . therefore   equation      for the rnn  becomes   y k     a s s k    a o o k    b  uk               and using the prediction error  k       k k        iv. s olutions         yk     k    a s s k    a o ok       a o  k       b  uk  .       letting  k     for all k   a s s k       c               where   c    yk    a o yk       b  uk  .  note that the weights are known at the time of state initialization. however  the rnn  is not necessarily known. therefore   during training an rnn  the state initialization problem for  system identification boils down to minimizing the following  cost         lsi    as sk    c    with respect to sk  subject to a   sk    b. the constraints  should enforce the initial states to remain within the range of  the function which generates the hidden states. for example  if the states are generated by a tanh .  then a       b     .  ideally we want the lsi      however  since the initialization  has to be carried out in both during training and testing phases   the exact solution may lead to overfitting as will be described  later in this section.  in this section   three methods of state initialization in rnns  are studied. the first method  which is also referred to by the  term washout is described in      and perhaps is the most  commonly used. the second method is based on optimizing  the initial states along with the network weights     . the  third method employs nns and has been proposed previously  by the authors      and is expanded here.  a. rnn state initialization  washout    state initialization problem    in the context of dynamic system modeling with rnns  the  function that produces the network output  i.e.  g .  in      is a  linear mapping. for a prediction generated at the time instance  k  the output is   y k   axk   buk          where a   rn   rs and b   rn   rm are the output layer  weights and their elements are included in the weight vector     hence  we have dropped  . using     to expand      and  letting k   k  we have   y k    as sk    ao ok    buk           a   as ao .       a      b     the washout method is based on the idea that running an  rnn for a period of time steps attenuates  washes out  the  effect of the initial state values. therefore  one can set the  initial states to zero       or a random value       run the  rnn for a length of time until the effect of the initial values  wears off.  since there is no deterministic approach to obtain the  washout period  it has to be treated as a hyper parameter  and reduces the speed of the learning process. additionally   since rnns are dynamic systems  during the training process   they may temporarily experience instability. while over the  entire learning process the stability of rnns is chained to  the stability of the system being learned  running an unstable  instance of an rnn during training may result in extremely  large cost values which cause the learning curve to diverge.     multi step prediction of dynamic systems with rnns  n. mohajerin  s. l. waslander    to avoid such  blow ups   extra measures seem necessary. in  this paper we employ washout as it is stated in     .    uk     t         predictor y k     t uk     t predictor y k     t  rnn    uk     b. rnn state initialization  optimization  another proposed approach to initialize an rnn is to  augment the initial states sk to the weight vector   and carry  out the learning process with respect to the augmented weight  vector     . although this approach may address the state  initialization problem during the training phase  it does not  provide a mechanism to generate the initial state values in  general  after the training is finished. for special cases  for  instance in       it may be possible to obtain the initial  hidden state values by running an architecture or problem  specific optimization process. since a general approach that is  applicable to any rnn regardless of the architecture is more  appealing  this method is not desirable for our purposes.  c. rnn state initialization  nn based  consider the ideal rnn  rnn    where the network output  differs from the desired output infinitesimally. we can write      ok  yk      yk           a     sk  f  xk     uk  .       b     equations      govern the dynamics of the rnn  states.  to approximate this mapping  it is possible to employ nns.  in       we have proposed an auxiliary ffnn to produce  the rnn initial state values  receiving a short history of the  system input and output. to avoid confusion  the auxiliary  network will be referred to as the initializer and the rnn  which performs the prediction as the predictor.  the idea is to divide the data samples into two segments   the first segment is used as the input to the initializer  which  initializes the predictor states  and the second one is used to  train the whole network  i.e.  the initializer predictor pair. the  number of steps in the prediction and initialization segment  will be referred to as the prediction and initialization length  and denoted by t and     respectively  as illustrated in figure  .  the total length of the training sample is therefore ttot        t.   .    .      velocity  m s      .    .      y k   yk        ..  .    xk   xk   initializer    ..  .    mlp          uk        yk          initializer  rnn     a  mlp initializer     b  rnn initializer    fig.    the two proposed initializer predictor pairs for multistep prediction.    proposes a penalty on the initializer network output. therefore   the initializer predictor pair will be trained on the following  cost   ltot    lpred    lsi        where the prediction error  lpred and lsi are defined in       and       respectively  and the coefficients   and   can be used  to balance between the two costs. without loss of generality     and   are set to one in this work. nevertheless  they can  be treated as hyper parameters and tuned to achieve desired  performance  if necessary.  mlp initializer network  an mlp can be employed  as the initializer network  which receives a history of the  measurements from the system and produces the predictor  initial states.  xk      uk       uk          ...  uk        yk       yk          ...  yk  .            in figure  a the block diagram of this type of the initializerpredictor pair is illustrated. the underlying assumption in this  approach is that the dynamics of the rnn states  defined  in       over a fixed period  i.e.  the initialization length  can  be approximated by a static function. the initializer network  approximates that function.  recurrent initializer network  since the rnn states also  possess dynamic  it is also viable to employ an rnn to model  their dynamic. an rnn for initialization purpose can be a  sequence to sequence model    .   which sequentially receives  the system measurement history over the initialization length       and produces an output sequence  xk                   xk           uk         yk          however  only the last element of the output sequence of the  initializer network  xk    is used         xk         xk     xk        . . . xk  .           .    .         .      .      k         uk        rnn             k  k                           time    ms                           k    t          fig.    dividing a data sample into initialization  red  and prediction  black  segments. each small circle is one measurement  from the continuous signal. in this figure        and t     .    the desired values for the output of the initializer network   i.e.  the initial rnn state values  are unknown. however          figure  b illustrates the rnn rnn initializer predictor pair.  the initializer rnn states are set to zero. clearly  the length  of the initialization segment should be long enough to capture  the dynamics of the predictor states.  in the training process  the two networks are trained together  meaning that their weights are augmented and the  gradient of the total cost in      is calculated with respect  to the augmented weight vector.     multi step prediction of dynamic systems with rnns  n. mohajerin  s. l. waslander    v. m odel a rchitectures  two classes of models are considered. the first class  implements rnn based black box models. the high level  architectures of the black box models are depicted in figure  .  the predictors in this figure can either be a multi layer fullyconnected  mlfc  rnn  which essentially is an rmlp with  skip connections across the network  or long short termmemory      cells arranged in layers. the initializer networks   however  are either mlp or lstm.  the second class implements a hybrid of the black box  model and a simple physics based dynamic model of the system in a single end to end network. the physics based model  represents a priori knowledge about the system behaviour  derived from first principles while the black box model learns  the unmodeled dynamics. the hybrid model is trained for the  quadrotor vehicle in this work.    a. multi layer fully connected rnn  this architecture consists of sigmoid layers which are  connected in series and equipped with inter layer  skip  connections in feedforward and feedback directions. the presence  of skip connections helps to attenuate the effect of the structural vanishing gradient problem. this network is previously  presented and studied for multi step prediction of a quadrotor  vehicle in                 . for an mlfc with l layers  each  layer gl   where l      ...  l  has ml inputs and nl outputs   neurons . the equations governing the dynamics of gl are  l  l l  l  xlk   al yk        b uk   b  l  l  l  yk   f xk         l          l    where xlk   rn is the layer activation level  ykl   rn is  l  the layer output  ulk   rm is the layer input  all at time  l  l  step k. the matrix al   rn   rn is the feedback weight   l  l  l  bl   rn   rm is the input weight matrix  bl   rn is a bias  l  l  weight vector and f l  .    rn   rnf is the layer activation  function  f l   c    . for an mlfc with l layers the input to  gl at time k  i.e.  ulk is constructed as   h  i  l    l  ulk   uk   yk    . . .   ykl     yk      . . .   yk    .            b. long short term memory rnn  lstms  first introduced in       consist of cells which are  equipped with gates  and are referred to as gated rnns.  gates in lstms are sigmoid layers which are trained to let  information pass throughout the network in such a way that the  gradient of the information is preserved across time. there are  many versions of lstms     . the version described in        equipped with peephole connections  is used in this work.  peephole connections are connection from the cell  c .   to         the gates. the equations of the lstm we use in this study  are given by     gi k    wii uk   wim mk     wic ck     bi       gkf    wfi uk   wfm mk     wfc ck     bf       gko    woi uk   wom mk     woc ck   bo       ck  gki   f wci uk   wcm mk     bc   gkf   ck         mk  g ck   gko       yk  h wy mk   by   wy mk   by .        in this set of equations  indices i  f   o and c correspond to the  input gate  forget gate  output gate and cell. gate activation  functions are logistic sigmoid    .   while the cell activation  function g .  and the output activation function h .  are chosen  by the designer. since the problem at hand is regression   the activation functions h .  and g .  are set to identity and  tangent hyperbolic function  respectively.  note that in lstms  there are two types of states  cell states   ck   and hidden states  mk . in our initialization scheme  they  both are treated similar to the hidden states of mlfcs. that  is  they are both initialized by the initializer network.  c. hybrid model  it is possible to incorporate prior knowledge in modeling  a dynamic system with rnns to enhance both the training  convergence speed and the accuracy of the predictions. in the  context of system identification  white box models are one of  the most convenient ways to represent the prior knowledge.  depending on the simplifying assumptions taken in white box  modeling  as well as the intended usage for the final model   the definition of the states and input to the white box model  may vary. therefore  the input to the white box model is not  necessarily the measured input collected in the dataset. for  instance  it is difficult to measure thrust acting on a quadrotor  vehicle  and the dependency of the thrust on the motor speeds  is complex in general. however  it is possible to approximate  this dependency throughout the training process and employ  a white box model which receives thrust as input. therefore   the first level of learning is to approximate the required input  to the white box model from the measured input  which is  unsupervised in nature since it is already assumed that the  required input to the white box is not measured during dataset  collection.  the desired values for the white box model output  on  the other hand  are partially or entirely measured during  dataset collection. however  since the white box model does  not capture many of the complex nonlinearities acting on  the system  its output may be too inaccurate to be useful in  generating predictions of the measured output. therefore  a  second level of training is deemed necessary for compensating  for the error between the white box model prediction and the  actual output measurements. figure   illustrates our suggested  hybrid model based on the two level training discussed above.  it comprises three modules  an input model  im   a physics  model  pm  and an output model  om . the pm module  represents the white box model while the im and om modules     multi step prediction of dynamic systems with rnns  n. mohajerin  s. l. waslander    u    r    im    v n    v     pm    v n    om    w         p     v          can be found in                 . this configuration is illustrated in figure  . blue links represent feedback connections  and black links represent feedforward connections. the gains  w  and w   are set to the maximum values of the vehicle body  rates and velocity  table i .    w      vi. r esults and d iscussions    fig.    a suggested method to incorporate white box models  with rnns as black box models  hybrid or grey box .    are rnns  with initialization networks . the pm module  receives r  generated by the im module  as input and updates  the state vector  x    p   v  . the vector p  represents the part  of the states that can be updated using integrators only  e.g.   position. the vector v  is additionally dependent on the input to  the pm module. the main responsibility of the om module is  to compensate for the deviation of the v  vector from the output  measurements. the weights denoted by w normalize the input  to the om module for numerical stability. the om module  generates correction to the white box prediction  assuming  the white box prediction error can be compensated by an  additive term. the inclusion of the pm module output in the  hybrid model output ensures that the pm module is actively  engaged in the prediction. during the supervised training of  the hybrid model  if the pm module only receives the error  information through the om module  it is possible  and highly  likely  that the pm module is not effectively employed and the  om module  due to its many dofs  learns a mapping which  is too far from being a reliable and accurate model for the  system     .    two datasets are used for training rnns in the described  multi step prediction problem. the first is the stanford helicopter dataset  which is intended for apprenticeship learning    . the second dataset  which is collected for this  work  encompasses various flight regimes of a quadrotor  flying indoors. the quadrotor dataset is publicly available at  https   github.com wavelab pelican dataset.  a. quadrotor dataset  the quadrotor dataset consists of time series samples which  are recovered from post processing measurements of the states  of a flying quadrotor in a cubic indoor space. the total  recorded flight time is approximately   hours and    minutes   which in total corresponds to about  .  million time steps for  each time series. the vehicle states are measured using onboard sensors as well as a precise motion capture system. the  vehicle is operated by a human pilot in various flight regimes   such as hover  slight  moderate and aggressive maneuvers. the  maximum values for the   dofs of the vehicle are listed in  table i. for more details about the dataset refer to     .  x   m s     y   m s     z   m s     .  p  rad s     q  rad s     r  rad s      .         .         .         .         .         .        table i  maximum values for the pelican measurements.      w     b. helicopter dataset      w           u    im          mm            n                n    om       n         n         w            w        fig.    hybrid model of a quadrotor.    c. architectures and implementation    for modeling the quadrotor  the pm module implements  the vehicle motion model and  therefore  is referred to as  the motion model  mm  module. the mm module receives  torques and thrust and updates the vehicle states  x  which is   t    xt      t  t   t                p q r x y z x i y i z i  .    the helicopter data set was collected in august      as  a part of research for apprenticeship learning    . it has  also been used for a single step prediction system identification problem    . the flights are carried out in an outdoor  environment  however  the dataset does not provide a wind  measurement. the flight time is approximately    minutes and  there are    k samples for each quantity. for more information  about the dataset see http   heli.stanford.edu index.html.            where   is the euler angles    is the body angular rates in  the body frame  or body rates     is the position and    is the  velocity  both in the inertial frame. the details of the model    the predictor network can be either an mlfc  lstm or  lstm equipped with buffers. the buffers are called tapped  delay lines  tdls     . the tdl size is     throughout  the experiments in this work. each of the predictors may  be initialized in one of the three fashions  washout  with an  mlp initializer or with an rnn initializer. in case of an  rnn initializer  an lstm with one layer of lstm cells is  employed. in order to refer to each configuration  the following  notation is used     available    at http   heli.stanford.edu dataset      multi step prediction of dynamic systems with rnns  n. mohajerin  s. l. waslander    tpred    .  sec      hz          e    k     deg sec                                            where e    k is measured in meters per second  m s  and e   k  and e    k are measured in degrees per second  deg sec . the      and    correspond to the velocity  body rates  subscripts      and euler rates  respectively. note that each error is averaged  over the three perpendicular axes to give the average errors  on each component of each vector. all the three prediction  errors are calculated at each prediction step  k. note that using  absolute values weighs all errors equally  regardless of sign.  using other norms does not significantly alter the comparative  results.                            to compare the effect of our nn based initialization method  versus washout  simple configurations of mlfcs and lstms   initialized by either our method or washout  are trained and  studied. to save training time  the networks are trained on  three subsets of the helicopter dataset. each dataset belongs  to a multi input single output  miso  subsystem of the helicopter which maps the pilot commands to the euler rates. the  following architectures are trained and compared in figure                   mlfc         mlp         mlfc         washout      lstm         mlp         lstm         washout        from figure   it is observed that the nn based initialization  method has improved both the immediate and the overall prediction accuracy. when the prediction error is long  however   the nn based and washout initialized rnn predictors converge to almost the same error eventually. it is also evident that  an efficient washout period is difficult to determine  whereas in  our nn based initialization methods such a problem does not  exist. the results clearly show the superiority of our nn based  initialization scheme over the state of the art.                          tpred    .  sec      hz           tpred    .  sec      hz                                                                                                     time    ms                   time    ms     tpred    .  sec      hz    tpred    .  sec      hz                  e    k     deg sec       e    k     deg sec            time    ms                                                                                                                      time    ms   lstm   x   mlp    x                    time    ms   lstm   x   washout        mlfc   x   mlp    x      mlfc   x   washout        fig.    comparison between nn based  using mlp  and  washout initialization on the simplified helicopter dataset. from  top to bottom  roll rate  pitch rate and yaw rate.    f. mlfc vs. lstm  having established the superiority of nn based initialization scheme  next the predictor architecture is evaluated. the  following architectures are trained on the same subsets of the  helicopter dataset                 e. nn based initialization vs. washout          time    ms          to evaluate the network prediction accuracy  the mean and  distribution of the prediction error at each time step are  studied. the distributions will be illustrated using box plots  over the two aforementioned prediction lengths. note that  for the evaluation the test dataset is used  i.e.  each sample  for evaluation has not been used to train the network. the  following norms are used        e    k          ex i  k      ey i  k      ez i  k           e   k          ep k      eq k      er k                    e    k         e   k      e   k      e   k                         e    k     deg sec     d. evaluation metrics      e    k     deg sec     for example  an lstm predictor with   layers  each having      lstm cells initialized by an mlp with      neurons in  the hidden layer and an initialization length of    is referred  to by lstm        mlp         .  the initialization length is  . s  or    steps  for all of the  networks trained in this work. two prediction lengths are  considered  tpred    . s and tpred    . s  which correspond  to    steps and     steps of prediction  respectively.    tpred    .  sec      hz          e    k     deg sec      predictor    number of layers     size of each layer   initializer type    hidden layer size   initialization length               mlfc         mlp         mlfc         mlp          mlfc          mlp          lstm         mlp         lstm         mlp            as a comparative measure  the following root mean sum  of squared errors  rmsse  measure is calculated on the test  dataset   v  u  ng tpred  u  x  x     rm sse   t        e   i  k ei  k    tpred ng i    k         where ng is the size of the test dataset  g.  in figure   the rmsse measure versus the size of the networks  number of weights  are plotted for the aforementioned  architectures. in these graphs it is observed that the lstms  with mlp initialization outperform other methods. in fact   lstms with fewer weights perform better than mlfcs. as a  part of hyper parameter optimization  it is a reasonable choice  to conduct the remaining experiments with lstms initialized  by the nn based scheme.  g. mlp vs rnn initializers  in this section  variants of lstm networks initialized with  the two initializer networks are examined. the lstm networks are comprised of layers of lstm cells connected in     multi step prediction of dynamic systems with rnns  n. mohajerin  s. l. waslander                                         tpred    .  sec                         h. black box modeling of the helicopter                     number of weights                                number of weights            tpred    .  sec                        rmsse                                                tpred    .  sec                                                    number of weights            tpred    .  sec                   rmsse                      number of weights            to study the reliability and accuracy of the predictions it is  best to look at the distribution of the prediction error  across  the test datasets  throughout the prediction length.  figure   compares the mean of the error distributions over  the two prediction lengths. it would have been expected that  the prediction error increases monotonically throughout the  prediction length. this is more or less the case for tpred     . s. however  for longer prediction lengths the monotonic  increasing behaviour is no longer observed. instead  a peak  appears at the early stages of the prediction and it is attenuated  as we go forward in time. this is contrary to our expectation.                                                  number of weights                                number of weights    fig.    network size vs. rm sse and lstms vs. mlfcs   helicopter reduced dataset.    series. the last layer output is fed back to the first layer. for  some experiments  to provide the predictor with a truncated  history of the signals  tdls are placed at the input and  output of the networks. the networks map the pilot commands  to the euler rates    inputs and   outputs . the following  architectures are trained and assessed   lstm          mlp            lstm          rnn           lstm tdl          mlp            lstm tdl          rnn                           tpred    .  sec     .     e   e   k        m s          .    .    .                        .       .     .       .       .     .       .     .           .           .     lstm   x    mlp       x                       .      .      .          .      .            .      .       .      .     .        .      .     network weights                .       rmsse     .        .     lstm tdl   x    rnn      x             .                network weights          lstm tdl   x    mlp       x       .      rmsse    rmsse     .     lstm   x    rnn      x       .        .      .      network weights           .      .           .           .     network weights                  lstm   x    rnn      x      tpred    .  sec                                                              lstm tdl   x    mlp       x            lstm tdl   x    rnn      x      tpred    .  sec                                             time  x  ms     fig.    mean of the error distributions for the four black box  models of the helicopter. the top row plots correspond to the  velocity and the bottom row plots correspond to angular rates.     .       .     .     network weights                       .      rmsse     .      rmsse    rmsse         .       .     time  x  ms     lstm   x    mlp       x      time  x  ms            .     time  x  ms           .           tpred    .  sec     .     e   e    k     deg s         e   e   k        m s         e   e    k     deg s    rmsse    it can be seen that the trained models behave similarly and  therefore  we mainly focus on these four architectures.    lstm  x   mlp   x    lstm  x   mlp    x    mlfc  x   mlp   x    mlfc  x   mlp    x    mlfc  x    mlp    x              rmsse    tpred    .  sec                   rmsse    rmsse    tpred    .  sec                         .           .           .     network weights          fig.    comparisons of network sizes and types on learning  the helicopter angular rates from pilot commands.  top row   tpred    . s  bottom row  tpred    . s     figure   illustrates the total rmsse cost on the test dataset  for the helicopter angular rates over the previously mentioned  prediction lengths. the networks are quite large  therefore  a  few strategies were chosen to prevent overfitting  the network  weights were initialized to tiny numbers  weight decay regularizer and drop out method      were also employed. since  the measures are calculated over the test dataset  overfitting  did not occur. based on the results illustrated in figure       as the lstms are efficient in learning long term dependencies       the decrease in the error over late predictions  might be due to noise attenuation and accumulating more  information about the process by the lstms. also  the peaks  may be reduced if the initialization length increases. however   increasing the initialization length decreases the lengths to be  used for training the predictor networks. it is also observed  that the lstms  equipped by tdls and initialized by rnns  generally perform better for longer horizons  which reinforces  this hypothesis. however  for the euler rate predictions  the  behaviour of the mean error is not consistent. this inconsistency may be due to the size and quality of the helicopter  dataset. some of the drawbacks in using the helicopter dataset  for multi step prediction are      the input to the networks is the pilot command and  there are many levels of transformation which take  place before the commands affect the helicopter motion.  time synchronization can also become very difficult to  manage in such situations. to circumvent this  using  actual motor speeds as the inputs is likely to mitigate  effects of delay and command transformation.                                              e   e   k     deg s    tpred    .  sec    tpred    .  sec                            time  x  ms   lstm   x    mlp       x      lstm tdl   x    mlp       x      e   e   k        m s     .       .                            .     .     .     .                          time  x  ms      .                     tpred    .  sec    e   e   k        m s    tpred    .  sec                 time  x  ms                      lstm tdl   x    rnn      x      tpred    .  sec     .                  time  x  ms   lstm   x    rnn      x      tpred    .  sec    e   e   k        m s    i. black box modeling of the quadrotor  the quadrotor models in this study map a trajectory of the  motor speeds and a truncated history of the system states  to  initialize the rnns  to the vehicle velocity and body angular  rate vectors  or body rates . learning velocity directly from  motor speeds is difficult  because of the dependency of the  velocity on the vehicle attitude. a network whose output comprises of both the velocity and body rates is difficult to train   as the network output associated with the velocity diverges  during early stages of the training and prohibits the training to  converge. therefore  as proposed in       the velocity and body  rates are decoupled and learned separately  one network learns  to predict body rates from motor speeds and the other learnes  to predict the velocity from motor speeds and body rates.  as the training is offline  the actual body rates are used to  train the second network  which resembles the teacher forcing  method     . however  to use the two networks for multi step  prediction  the first network provides the second network with  the predicted body rates  in a cascaded architecture. we call  this mode practical.  figure   illustrates the mean of the error norms  measured  at each prediction step  for the aforementioned architectures   over the two prediction lengths. for the body rate prediction   the top row   the prediction accuracy on average remains  better than  .   deg sec  over  .  seconds prediction length   tpred    . s . for this length  similar to the helicopter  black box model  an initial increase in the prediction error  is observed. although the error improves as the prediction  proceeds  this initial increase is not favourable in a control  application. the later improvement of the prediction error is  due to the properties of the lstm network as discussed for  the helicopter model.  the plots in the middle row illustrate the average of the  norm of the velocity prediction errors in the teacher force  mode  i.e.  the samples in the test dataset include the measured  body rates as inputs. the accuracy of the predictions on  average remains better than   cm s over almost   seconds.  from the plots on the first and second rows  it is also  observed that tdls improve the prediction accuracy  which is  consistent with our observation from the helicopter dataset. it  is also observed that the lstms initialized with rnns  rnnrnn pairs  have better prediction accuracy over the longer  prediction lengths  a reinforcing observation on the argument  that the lstms efficiently employ information spread across  time.               e   e   k        m s       considering the complex dynamics of a helicopter  the  dataset is relatively small. better prediction performance  is expected if more data is collected in a variety of flight  regimes.     the helicopter is flown outdoors and is very likely  affected by wind. however  no information about the  wind is provided in the helicopter dataset. to obtain a  predictor for the vehicle dynamic a controlled environment is more desirable.  as we have considered the above drawbacks in collecting  the quadrotor dataset  it is expected that quadrotor dataset  better suits the multi step prediction problem at hand.    e   e   k     deg s    multi step prediction of dynamic systems with rnns  n. mohajerin  s. l. waslander                .                time x  ms                              time  x  ms     fig.    mean of the error norms for the black box model of the  quadrotor  over the two prediction lengths. from top to bottom   the y axis corresponds to body rates  velocity in the teacher  force and velocity in the practical mode.    the bottom row corresponds to the velocity prediction errors  in the practical mode. comparing the teacher force mode   the velocity prediction accuracy is degraded by a factor of  approximately   . additionally  it can be observed that the  networks with the rnn initializer suffer more from the error  in body rate prediction. in conclusion  the black box model  provides a reliable and accurate body rate prediction  however   the velocity prediction is far from desirable.  j. hybrid mmodel of the quadrotor  as opposed to the black box model  the hybrid model   figure    provides the velocity and body rate prediction  simultaneously. the im and om modules in the hybrid model  are initialized rnns. the proceeding results correspond to  the im and om modules configured as lstm          mlp          . in this section  the error distributions are studied to  further assess the model prediction accuracy.  in figures            and    we compare the error distributions of the black box model  in the practical mode  and the  hybrid model for the two prediction lengths. for the shortest  prediction length   . s  the hybrid model reduces the body rate  error by     and the velocity error by about     compared  to the black box model.  for the body rate prediction over tpred    . s  the blackbox initially performs worse  however  in the long run it performs better than the hybrid model. one possible explanation  for this behaviour is that the mm module in the hybrid model  limits the exploration capacity of the om module. although   the inclusion of the motion model contributes significantly to  the accuracy of the early prediction steps  it may also impose     multi step prediction of dynamic systems with rnns  n. mohajerin  s. l. waslander    distribution of   e   k        hybrid model   m sec      .     .    .                                                                       time  x  ms     distribution of   e   k        blackbox model  practical mode   m sec      .       .                                                                      time  x  ms     fig.     comparison of the velocity error distribution between  the hybrid  top  and the black box  bottom  models  tpred     . s .  distribution of   e   k     hybrid model         deg sec     restrictions on the search for the optimal weights. however  the  lstms in the black box model are free to explore the entire  weight space and they can accumulate relevant information  over longer time to perform better. for the velocity  we see that  black box still performs worse  however  a slight decrease is  seen on the later predictions which is similar to the behaviour  in the body rate prediction.  in figure     the mean of the prediction errors are illustrated  before compensation  the mm output  and after  for each  prediction lengths. this figure shows the importance of rnn  network initialization. the plots show that the mm module  prediction error starts from a non zero value which requires the  om module to immediately compensate for that. therefore   the output of the om module should start from a non zero  value which requires a proper state initialization for the rnns.  note that at each prediction step  the compensated states are  fed back into the mm module.                                                                                                time  x  ms     distribution of   e   k        hybrid model    distribution of   e   k     black box model         deg sec      m sec      .      .                                                             time  x  ms                                                          time  x  ms     distribution of   e   k        blackbox model  practical mode    fig.     comparison of the body rate error distribution between  the hybrid  top  and the black box  bottom  models  tpred     . s .     .      m sec              .                                                          time  x  ms     fig.     velocity error distribution of the hybrid  top  and  the black box  bottom  models  tpred    . s . note that the  prediction by the hybrid model is over an order of magnitude  better than the black box.    distribution of   e   k     hybrid model   deg sec           model. the accuracy of the trained hybrid model for multistep prediction is demonstrated experimentally. the prediction  provided by the hybrid model can be effectively employed in  a variety of control schemes. we hope that the provided study  and experiments in this work illustrate a number of efficient  ways to benefit from the ever rising power of machine learning  methods in modeling and control of robotic systems.         r eferences                                                         time  x  ms     distribution of   e   k     black box model   deg sec                                                                       time  x  ms     fig.     comparison of the body rate error distribution between  the hybrid  top  and the black box  bottom  models  tpred     . s .    vii. c onclusion  in this paper  we employ rnns to identify the dynamics of  two aerial vehicles. the importance of rnn state initialization   as well as the effectiveness of our proposed method to properly  initialize the states of rnns are demonstrated. the rnns with  our proposed state initialization method are used as black box  models to learn the model of a helicopter and a quadrotor  for  multi step prediction  from experimental dataset. to overcome  drawbacks from a pure black box model  a simplified model  of the quadrotor motion is embedded with rnns in a hybrid  