introduction  with the advent of deep neural networks  dnn  in recent  years  a mass of vision tasks have made great progress                       due to its superior representation capability for high dimensional data. on top of spatial represen  equal    contribution.  author. cewu lu is a member of department of computer science and engineering  shanghai jiao tong university  moe key  lab of artificial intelligence  ai institute  shanghai jiao tong university.    corresponding    tation  temporal features are valuable and crucial as well  when dealing with sequential inputs like videos  for which  recurrent neural networks  rnn  are designed. taking all  above into consideration  we are intended to build a deep  rnn architecture that combines the merits of both rnn  and dnn to extract more powerful temporal and representation features from visual sequential inputs.  a straightforward way to build rnn deeper is to simply  stack multiple rnn layers. however  this method is encountered with two problems. for one thing  in this deep  rnn structure  there exist two information flows   representation flow and temporal flow  along structural  spatial   depth and temporal depth respectively  however  these two  flows are often entangled with each other  making it hard  for models to be co adaptive to both of them. many specific rnn structures like lstm      and gru     are designed mainly to capture temporal information among long  sequences  yet there is no adaption that can effectively take  advantage of both the two flows. therefore  simply stacking  these rnn modules will lead to higher training error and  heavier training consumption. for another  the limitation  of computing resources greatly influences the feasibility of  this method. unlike deep cnn               deep rnn  needs to unfold as many times as the sequence length  resulting in more significant expansion of memory and computational complexity with the depth increasing  especially  for visual sequential inputs.  in this paper  we propose a new deep rnn architecture  including two principle techniques  namely  context bridge  module  cbm  and overlap coherence training scheme.  in cbm  we design two computing units taking charge of  representation flow and temporal flow respectively  forcing  these two flows relatively independent of each other with  the aim of making them focus on representation and temporal information separately to ease the training process. after  these two units  a merge unit is utilized to synthesize them.  by adjusting the synthesizing method  we can balance the  dominant degree of each direction to better adapt to the requirements of different tasks. furthermore  to make representation flow less influenced by temporal flow in the beginning of training  we design the temporal dropout  td      to interdict the back propagation of temporal information  across layers with a certain probability.  besides  the proposed overlap coherence training  scheme aims at reducing the training cost of deep rnn.  since the enormous training consumptions are largely due  to the long sequence  we introduce this training scheme that  randomly samples the long sequence with length l into several overlapping short clips with length n and leverages the  overlaps as the communication bridge between the adjacent  clips to smooth the information propagation among clips. in  this way  we simplify the original markov process of order  l into several ones of order n  n   l   which remarkably reduces the training complexity  and guarantees the temporal  information coherence among clips at the same time. based  on overlaps  we design overlap coherence loss that forces  the detached clips to generate coherent results in order to  strengthen the consistency of temporal information  which  makes the model not a strict markov process of order n  but  the complexity is still reduced.  we conduct comprehensive experiments on several tasks  to show the challenge of training deep rnn and evaluate  our proposed deep rnn framework. results reveal that      deep rnn can enjoy accuracy gains from the greatly  increased depth  substantially better than the shallow networks.    our cbm is more suitable for stacking deep compared with other rnn structures like lstm.    the overlap  coherence training scheme can effectively make many computer vision problems with high dimensional sequential inputs trainable on commonly used computing devices.  we evaluate our framework on several visual sequence  tasks  action recognition and anticipation on ucf            hmdb         and kinetics      auxiliary annotation   polygon rnn      on cityscapes      and video future prediction on kth     . for action recognition and anticipation tasks  our deep rnn framework achieves more than      relative improvements on all the datasets compared  with the shallow rnn models. for polygon rnn task  iou  value improves by   .   on cityscapes. for video future  prediction task  our deep rnn improves the performance  by  .   on psnr      and ssim      metrics.    features without considering order relations.    then  d  convolutional networks         appear  which treat temporal  dimension equal to spatial dimension with its cubic convolution kernel  while  d convolutional networks need to consume large amount of computing resources.    rnn          is designed to handle sequence problems  therefore it is  a natural idea to utilize rnn to encode temporal information after obtaining spatial features  which is adopted  in                 for video classification          for video  description         for auxiliary annotation and               for video future prediction. whereas  currently used rnn  is shallow  which may limit its performance.     . related work    deep model has exhibited superior performance in producing powerful features  and we hope sequence modeling  can enjoy the deep representation as well. to this end  we  introduce our deep rnn framework in this section  which  contains two parts  context bridge module  cbm  designed  to effectively capture temporal and representation information simultaneously  and the overlap coherence training  scheme to further simplify the training process.    methods for visual sequence tasks visual sequence  problems require models to extract hierarchical temporal  and representation features simultaneously. a slew of prior  arts have shed light on this tough problem     an inchoate  approach is pooling the spatial representation features of every item in the sequence  such as          when dealing with  video classification and          for action detection and localization. this approach can extract relative high quality  spatial representation features but is very weak for temporal ones because it treats the sequence as a set and simply  combines the spatial features of the set as global temporal    exploration on deep rnn in this paper  we focus on exploring appropriate deep structure for rnn model. there  are many previous works trying to address this problem.  in           the authors evaluate several ways to extend  rnn deeper  and results show that stacked rnn has relatively better performance and more importantly  stacking  method can synthesize temporal information in each layer  to extract hierarchical temporal spatial features instead of  plain temporal  deep spatial features.  the learning computational complexity of deep rnn  significantly increases with the depth growing  thus in        the authors propose a new rnn structure called lstmp  to reduce the complexity. in                   researchers  prove that deep rnns outperform associated shallow counterparts that employ the same number of parameters.       shows that each layer captures a different aspect of compositionality which reveals deep rnn s ability to extract hierarchical features  and a deep bidirectional rnn structure  is proposed in     . all these previous works prove the importance of rnn depth in nlp and speech area  while for  high dimensional inputs like videos in computer vision  it  is more challenging to tackle as we mentioned above. for  them  what we suppose to build is a deep rnn framework  which is easy to optimize even when inputs are large scale  and can achieve promising improvements on performance  at the same time.     . deep rnn framework     . . context bridge module  to model visual sequential inputs  we need to make sure  it can be trained efficiently when building deep. for this   we design a non shallow recurrent architecture to respec      representation flow    upper layer  oi t    oi t         ci t         t    r    t    ci t  r                   ci t      later seq                                                 temporal flow    oi   t    oi   t      td rate  .     tively capture temporal information from sequential inputs   e.g. a sequence of frames in a video  and representation  information from each individual one  e.g. one frame of  the sequence . these two information flows are oriented towards temporal depth and structural depth separately  and  we name them as temporal flow and representation flow.  challenge the straight forward design for deep rnn can  be a vertically stacked rnn architecture. however  in highdimensional visual tasks  parameters in rnn cell are hard  to be co adaptive to two flows simultaneously  resulting in  ineffective and inefficient training. extensive experiments  show this design is very hard to train. this is why we hardly  see stacked deep rnn in related literatures. in most cases   people adopt shallow rnn which takes extracted cnn features as inputs  though it is not an end to end pipeline.  our architecture therefore  we go down to consider  how to capture these two branches of information flows as  independently as possible  through which the training process can be much easier since the two relatively independent  branches can share the burden of learning and ease complex  co adaptations. specifically  for representation flow  we use  a computing unit  e.g. cnn structure  to extract features  of the individual input sample without recurrent operations   while temporal flow adopts a rnn structure.  as shown in fig.    in each cell  there is a  representation  unit r and a  temporal  unit t which act as a representation feature extractor on individual input sample and  a temporal information encoder on the sequential inputs respectively. here r can be seen as a context bridge over the  temporal information. intuitively  the representation information flow would be encouraged to mainly propagate by  this bridge  since it doesn t need to interwind with temporal information. therefore  we call this module as context  bridge module  cbm . by denoting oi   t as the input to  the module in ith layer at time stamp t  we have           e num paths     e num paths     figure  . structure of cbm. the blue lines represent representation flows  while red ones represent temporal flows. r  t and   denote representation unit  temporal unit and merge function respectively. the dashed line  td  means feeding forward is allowed but  back propagation is forbidden with a certain probability.                                             path length              seq len   model depth      td rate  .     td rate  .     td rate  .                                                                            path length    s          model depth       figure  . temporal flows adopting td. top  when setting the  td rate to  .   all the colorized lines  red   purple  of temporal  flow cannot propagate back  while if only drop the red node out   the gradients from red node s temporal unit cannot flow backward  through the red lines. bottom  expectation numbers of backpropagation paths with different lengths  paths from     to different     in top  when adopting different td rates. note that the  back propagation remains unchanged when setting td rate to  . .    o i t   r oi   t    i              where the representation unit r is designed as a conventional cnn layer  namely relu  conv      and  i is the  parameters of r in ith layer.  on the other hand  temporal flow is captured by t unit   which is written as  ci t   t  ci t     oi   t    i              where ci t is the memory state in ith layer at time stamp t   and  i is the parameters of t in ith layer. as a recurrent  architecture  t can be a sigmoid conv      as simple as  the conventional rnn  or lstm. in practice  we suggest  sigmoid conv     since it only consumes half of computing resources compared with lstm cell  which greatly contributes to building model deeper.  finally  to fuse the information flows from the two units   we introduce a merge unit   oi t     o i t   ci t              where   is the merge function  and we adopt element wise  production for   in our experiments.  temporal dropout to make training easier  we hope the  learning in representation flow direction less interwinds  with temporal flow in the beginning. after a desirable  neural representation is shaped  the learning in temporal  flow direction can be more efficient. to this end  we  introduce a temporal dropout  td  scheme  forbidding     original training scheme    simple sampling method    overlap coherence training scheme    figure  . four examples of feature maps from the representation  and temporal unit on the toy experiment.  f   denotes frame      r  denotes representation unit and  t  denotes temporal unit.    back propagation from t unit through the dashed line in  fig.   with a certain probability. just like dropout proposed  in       it can reduce complex co adaptations of two flows  and enhance model s generalization ability. specifically  we  begin with a high temporal dropout rate  forbidding with  a high probability  to isolate temporal information of each  layer. in this way  the representation unit can capture effective representation easily  since it largely shortens the backpropagation chain in temporal flow as shown in fig.   and  only gradients from r can back propagate to previous layers. that is  the workload of learning two flows  to some  extent  can be de coupled in different time by gradually decreasing the td rate to incorporate temporal information  with representation features as training goes. to verify the  effectiveness of this idea  several experiments are conducted  in section   and section  .  comparison with conventional rnn lstm as mentioned before  stacked rnn lstm is a solution for deep  recurrent architecture. actually  our proposed approach is  a general version of it. specifically  when we set the output of r unit as constant    our model degenerates into  stacked rnn lstm model  t unit can be lstm cell .  if we further set the depth of representation branch to     our model becomes a conventional shallow rnn lstm.  from another perspective  our model can be considered as  an extension of stacked rnn lstm with an extra context  bridge  namely the r unit.  discussion with a toy experiment to further provide an  intuitive perception for the function of context bridge module  we design a toy experiment. the experiment is a video  classification task that requires the model to learn which  object is in the video from spatial information and how it  moves from the temporal information  such as  a triangle is  moving left  or  a circle is moving right . we adopt a  layer cbm model with   channels and visualize the feature  maps of the final layer s representation unit and temporal  unit in fig.  . we can see the two computing units act as  expected that the representation one mainly focuses on the  spatial information while the temporal information is captured by the temporal unit.    figure  . deep rnn training schemes. first row  original training scheme for rnn that takes the whole sequence as input and  the information can flow forward and backward without interdicting. second row  simple sampling method that samples the input  long sequence into several short clips. third row  our overlap  coherence training scheme. note that every item in the sequence  can receive backward information  gradients  due to the existence  of overlaps. the red line represents the initialization of each clip  that is randomly chosen from the former clips with overlaps.     . . overlap coherence training scheme  challenge in practice  utilizing deep rnn to model highdimensional visual long sequences can be hard to achieve  because with the depth increasing  the computing resources  needed significantly expand. the deeper the model is  the  more dramatically computational complexity grows with  the increase of the sequence length  which can be regarded  as a contradiction between the structural depth and sequence length  temporal depth . recently  a widely used  method is to sample a few items from the long sequence   successive or scattered  and learn a truncated version of the  original rnn on them                to solve the contradiction. under this scheme  training on short samples instead  of the long sequence greatly reduces the training complexity  which is very practical for deep rnn. however  this  can be seen as a compromise for the depth  which may lead  to losing some key temporal information. considering two  short sampled clips that own overlaps  the outputs of the  overlap sections must be different due to the broken temporal information  which will never happen if we train the  whole sequence together  and this provides a clear evidence  for the weakness of this sampling method.  method in this paper  we also consider shortening the  long sequence to simplify the l order markov process into  several n order  n   l  ones  but we smooth the information propagation among short clips by introducing the overlap coherence training scheme. in training phase  we randomly sample clips that have random lengths and starting  points  which will naturally generate many overlaps  third  row in fig.   . the overlaps serve as the communication  bridge among the disconnected clips to make information  flow forward and backward smoothly throughout the whole        figure  .  cat   dog  experiment. the input sequence is images  of cat and dog  and the label of each image represents the distance  from the cat image  padding with    .              sequence. therefore  we introduce a new loss called overlap coherence loss to force the outputs of overlaps from different clips to be as closed  coherent  as possible. then  the  training objective function can be written as  n  x  i      x    lr  si          ld  v  u              v u       where si is the ith clip and   is the set of pairs which are the  outputs of overlap sections from different clips. lr and ld  denote the original loss for the specific task and our overlap coherence loss implemented by mse loss respectively   where   is the hyper parameter to adjust the weight of them.  additionally  our training scheme exhibits several highlights in practice. firstly  our random sampling mode serves  as a great data argumentation approach to enhance model s  generalization ability. secondly  the vanishing exploding  gradient problem can be solved to some extent since the  scheme will shorten the sequence adequately to train easily.  thirdly  the initial state of each clip is taken from other earlier trained clips by picking up their hidden states at corresponding time stamp  which further bridges the information  flow among clips to make it smoothly transfer throughout  the whole sequence. furthermore  initialized clips can be  computed together in parallel  which can effectively reduce  the training time  especially when the overlap rate is high.  moreover  to verify our training scheme can actually  transfer useful information flow throughout the whole sequence  we commit a toy experiment shown in fig.  . the  input sequence is a series of images  where there is only one  cat and the others are all dogs. we train a model with overlap coherence training scheme to learn how far the current  dog image is from the cat image appeared before. we find  that the model can correctly predict even if the cat image appearing    frames ago  where we set the clip length smaller  than   . this is because temporal information of the image  sequence is successfully captured among clips due to our  overlap coherence training scheme.     . experimental results  in this section  we evaluate our deep rnn framework  and compare it with conventional shallow rnn  we choose  the commonly used one  lstm  on several sequence tasks  to exhibit the superiority of our deep rnn framework over  the shallow ones on high dimensional inputs.    figure  . shallow and deep rnn architecture. the shallow version is implemented based on    . the deep one contains    rnn  layers and we add shortcuts along the depth  following     . different from the shallow one  the rnn kernel is convolutional to  maintain the spatial features instead of linear kernel.     . . video action recognition and anticipation  we first evaluate our method with action recognition and  anticipation tasks         on the ucf     dataset      and  hmdb    dataset      to compare our deep rnn with the  common shallow one with cnn backbones. then we remove the backbones  evaluate the standalone deep rnn  model on kinetics dataset     to compare it with several  excellent approaches  not merely the shallow rnn.  implementation the frames in videos are resized with  its shorter side into     and a           crop is randomly  sampled from the frame or its horizontal flip. color augmentation is used  where all the random augmentation parameters are shared among the frames of each video. we  adopt bn      after each convolutional layer  the same as      . the backbones  if needed  are pre trained on imagenet      and the rnn part is initialized by  xavier initialization  proposed in     . we use adam optimizer       with    mini batch for shallow net and    for deep one.  the learning rate starts from      and gradually decays.     table  . classification accuracy on ucf     and hmdb     both  on the first test split . for action recognition  the whole sequence is  taken as input  while for anticipation  only the first two frames are  used to do inference. note that  recg  denotes action recognition  task and  atcp  denotes action anticipation task.      layer lstm     layer convlstm    layer cbm     layer cbm    ucf      recg  atcp    .         .         .     .     .     .     .     .     hmdb     recg  atcp    .         .     .     .     .     .     .     .     table  . action recognition accuracy on ucf     first test split.  model    layer lstm with vgg        layer lstm with inceptionv          layer convlstm with inceptionv      layer cbm with inceptionv     recognition acc    .     .     .     .     besides  we adopt a weight decay of      and dropout of   .  and  .  for feature extractor and classifier respectively.  adopting conventional backbone supported structure  conventional rnn model     is stacking a   layer lstm  on the vgg      backbone. now we extend it to deeper  versions  shown in fig.     stacking a    layer convlstm      or a    layer cbm on the vgg backbone. for  td rate of cbm cell  we start from  .   decay to  .  after  two epochs  and finally to  .  after another two epochs. we  adopt our overlap coherence training scheme for both of the  two deep versions to make them feasible  fix the weighting factor      .  for overlap coherence loss  and keep the  overlap rate of sampling as    .  the results are shown in tab.  . for action recognition   the deep convlstm model has a lower accuracy compared  with the shallow model while for our deep cbm model   it obtains   .   relative improvements on ucf     and    .   on hmdb   . for action anticipation  both of the  two deep models gain improvements and our cbm version  possesses the best performance   achieving   .   relative  improvements on ucf     and   .   on hmdb   .  furthermore  we replace the vgg backbone with inceptionv  to validate the universality of our deep rnn framework on ucf     of action recognition. results are shown  in tab.    where our deep cbm model still outperforms the  shallow one  achieving  .   relative improvements.  adopting standalone rnn structure to reveal the excellent spatial representation ability of our deep rnn  framework  we remove the backbone  adopt a standalone  end to end deep rnn model to extract temporal and representation features simultaneously.  specifically  we utilize a deeper structure with    layer    table  . action recognition accuracy on kinetics  and end to end  fine tuning on ucf     and hmdb   . note that our deep cbm  model applies    layers of cbm.  bb  denotes backbone.  architecture  shallow lstm with bb      c d       two stream        d fused      deep cbm without bb    kinetics    .     .     .     .     .     ucf        .     .     .     .     .     hmdb       .     .     .     .     .     cbm  where the representation unit of each layer is set the  same as the corresponding layer in resnet         and the  same shortcuts are employed. other implementation details  are consistent with the above backbone supported version.  the action recognition results on kinetics     are shown  in tab.   and we also fine tune the model on ucf      and hmdb   . compared with the conventional shallow  lstm with the backbone  our deep cbm achieves great  improvements    .   on ucf        .   on hmdb        .   on kinetics  and the performance is competitive with  some excellent non recurrent models which are more powerful on this task.     . . polygon rnn on cityscapes  for auxiliary annotation task  similar with instance segmentation task       we build the model following polygonrnn      and evaluate it on cityscapes instance segmentation dataset     which contains eight object categories and  we use the same train test split as    .  to build our model  we only replace the rnn part in the  original polygon rnn model with our deep rnn framework which is a plain stacking of our cbm cell or convlstm      cell. unlike the deep architecture shown in  fig.    we do not use shortcuts in this experiment. inside  the cbm cell  we still choose the element wise production  as merge function and set the size of all convolutional kernels as      . for td rate  we start from  .   decay to   .  after the first epoch  and finally to  .  after another one  epoch. we evaluate our deep rnn framework with different layers  and tab.   summarizes the specific architectures.  implementation the size of input images is          .  we adopt bn      but with no dropout     . we initialize  the convolutional layers with  xavier initialization      .  models are trained with a mini batch size of    using adam  optimizer       and the learning rate starts from      and  gradually decays when meeting the loss plateaus. we train  deep models     and    layer ones  with the overlap coherence training scheme  where we set and keep      . .  results we compare the              layer rnn networks  with convlstm or cbm cell. the results are shown in  tab.  . compared with the original polygon rnn with the     table  . video prediction results on kth.  t   denotes the first frame to predict and  avg  denotes the average value.  method  convlstm       mcnet       ours    metric    t     psnr   .     t     t     t     t     t     t     t     t     t      t      t      t      t      t      t      t      t      t      t      avg      .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .     ssim  .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     psnr   .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .     ssim  .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     psnr   .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .     ssim  .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .       table  . structures of polygon rnn models with different depths.    filters    layer model    layer model    layers     layer model     layer model                                                                                shallow rnn  our deep cbm model achieves   .   relative improvements which is even competitive with polygonrnn   proposed in     which adopts many complex tricks   while the deep convlstm model suffers from higher training loss  leading to a bad performance.     . . video future prediction  for video future prediction  we evaluate our deep rnn  framework using the state of the art method  mcnet proposed in       which predicts    future frames based on the  observed    previous frames. we only replace the   layer  convlstm part of the motion encoder into our    layer  deep cbm model  where the td rate is finally set to  .   with similar process as the above. the detailed implementation settings are consistent with the original method in     .  we evaluate on the kth dataset      which contains      videos for   human actions  and we utilize psnr      and  ssim      as metrics. the results are shown in tab.   and  we can see that compared with the original method using  shallow rnn  our deep model achieves  .   improvements  on ssim and  .   on psnr for    frame prediction  and   .   on ssim and  .   on psnr for    frame prediction.  in this experiment  we do not adopt the overlap coherence training scheme since the sequence is not too long.     . analysis  the above visual applications demonstrate the superiority of our deep rnn framework and in this section we will  further verify the effectiveness of our detailed designs    the model depth  cbm for deep structure  the overlap coherence training scheme  merge function and td rate of cbm.  analysis on depth results of all above experiments have  already demonstrated that our deep rnn model remarkably  outperforms the shallow rnn one due to the stronger representation capability with the depth growing. we analyze the  experiments on polygon rnn to further explore the spe     table  . performance  iou in    on cityscapes validation set   used as test set in     . note that  polyg lstm  denotes the  original polygon rnn structure with convlstm cell and  polygcbm  denotes the polygon rnn structure with cbm cell.  model  original polygon rnn      residual polygon rnn      residual polygon rnn   attention   rl      residual polygon rnn   attention   rl   en      polygon rnn          layers    params of rnn  polyg lstm      .  m  polyg lstm      .  m       .  m  polyg lstm  polyg lstm        .  m  polyg cbm      .  m      .  m  polyg cbm  polyg cbm       .  m  polyg cbm       .  m    iou    .     .     .     .     .     .     .     .     .     .     .     .     .     cific relationships between the depth and the model performance  which is illustrated in fig.   a  and fig.   b .  from fig.   b   we can observe that utilizing cbm  the  deeper the model is built  the lower training loss and higher  iou value we will receive. moreover  it is worth noting that  the deep models converge as fast as the shallow ones.  analysis on cbm as results shown in tab.   and tab.     our deep cbm model achieves the best performance on action recognition task with two different backbones and action anticipation task  while deep convlstm model suffers  from lower accuracy on action recognition even compared  with the shallow one.  as we discussed above  building deep rnn models  needs to co adapt to both temporal and representation information  making it difficult to optimize over a long sequence.  therefore  for action recognition that takes the whole videos  as inputs  commonly used deep rnn models cannot benefit  from the increased depth  while for action anticipation that  predicts only based on the first two frames  deeper structure  brings better results. to resolve this problem caused by the  contradiction of the two information flows when stacking  deep  our cbm cell is right introduced to de couple these  two flows to make training more efficient  and receives best  results on both tasks.      .     iou     .    .    .    .            k       k       k       k       k       k     .    .    .    .    .    .    .    .      .    .       layer iou    layer iou     layer iou     layer iou    layer loss    layer loss     layer loss     layer loss     .    .    .    .    .      a  polyg lstm     .    .    .    .    .    .    .    .            k    loss      layer iou    layer iou     layer iou     layer iou    layer loss    layer loss     layer loss     layer loss    loss  iou     .        k    k    k    k    k     b  polyg cbm     c  td rate comparison    figure  . training on cityscapes. dashed lines denote training loss  and the bold lines denote testing iou. left  polyg lstm networks.  deep models are difficult to train and suffer from high training loss. the convergence of    layer is not shown. middle  polyg cbm  networks adopting  .  td rate. deep models are easy to train. right  comparison between different td rates on    and    layer models.  table  . classification accuracy on ucf     with element wise  production and addition settings. for r  both of the two settings adopt relu  conv    . for t   production setting adopts  sigmoid conv     while addition adopts relu  conv    .    production  addition    recognition    .     .     anticipation    .     .     besides  results of polygon rnn task in tab.   also  prove that our cbm cell is more suitable for stacking deep   and comparisons between fig.   a  and fig.   b  further reveal that using convlstm to stack deep leads to higher  training loss and lower iou value.  analysis on overlap coherence training scheme all  the deep models above adopt our overlap coherence training scheme. from the results  we can see that it works well    deep models are trainable on commonly used gpus and  all the models learn effective temporal features. under this  scheme  though it may not transfer temporal information as  smoothly as the original training scheme  the overlaps and  the coherence loss guarantee the consistency of temporal  information among the clips to a certain degree  and finally  we do benefit from the increasing structural depth by making some compromise on the sequence length.  analysis on merge function   all above experiments are committed with element wise production merge  function.  here  we also evaluate another setting   relu  conv     for r and t   and element wise addition  for the merge function  which treats the two flows equally  without discrimination when merging the information. for  action recognition and anticipation tasks on ucf      the  comparison of these two settings is shown in tab.  . we find  that the production setting is marginally better than the addition one  possibly because the production setting extracts  better spatial representation features that are more useful for  video classification problems.  analysis on td rate to show the influence of td rate   we set the final td rate to  .    .    .    .  and  .   gradually decay as the above experiments  and results of action  recognition task on ucf     are shown in tab.  . we can    table  . action recognition accuracy on ucf     with different  td rates. we use vgg   as backbones     layer cbm as the  rnn part  and element wise production as merge function.  td rate   .    .     acc    .     .     td rate   .    .     acc    .     .     td rate   .     acc    .     see that  .  td rate achieves the best result. when the td  rate is set to  .   the temporal information can only flow  backward in its own layer  forbidding the temporal communication among different layers  thus leading to a relatively  non ideal performance. for polygon rnn task  the results  shown in fig.   c  reveal consistent conclusions.     . conclusion  in this paper  we proposed a deep rnn framework for  visual sequential applications. the first part of our deep  rnn framework is the cbm structure designed to balance  the temporal flow and representation flow. based on the  characteristics of these two flows  we proposed the temporal dropout to simplify the training process and enhance  the generalization ability. the second part is the overlap  coherence training scheme aiming at resolving the large  resource consuming of deep rnn models  which can significantly reduce the length of sequences loaded into the  model and guarantee the consistency of temporal information through overlaps simultaneously.  we conducted extensive experiments to evaluate our  deep rnn framework. compared with the conventional  shallow rnn  our deep rnn framework achieves remarkable improvements on action recognition  action anticipation  auxiliary annotation and video future prediction tasks.  comprehensive analysis is presented to further validate the  effectiveness and robustness of our specific designs.     . acknowledgements  this work is supported in part by the national key r d  program of china  no.     yfa         national natural science foundation of china under grants         .     