introduction    creative machine learning is an extremely relevant research area today. generative models form the basis of creative learning  and for this reason  a high level  of sophistication is required from these models. also  identifying features in the  subjective fields of art  literature and music is an arduous task  which is only  made more difficult when more elaborate learning is desired. deep architectures   therefore  present themselves as an ideal framework for generative models  as  they are inherently stochastic and support increasingly complex representations  with each added layer. recurrent neural networks  rnns  have also used with  great success as regards generative models  particularly handwriting generation   where they have been used to achieve the current state of the art results. the  internal feedback or memory state of these neural networks is what makes them  a suitable technique for sequence generation in tasks like polyphonic music composition.  there have been many attempts to generate polyphonic music in the past   and a variety of techniques have been applied  some of which will be discussed  here. matic et al.     used neural networks and cellular automata to generate          kratarth goel  raunaq vohra and j. k. sahoo    melodies. however  this is dependent on a feature set based on emotions and  further requires specialized knowledge of musical theory. a similar situation is  observed with the work done by maeda and kajihara      who used genetic algorithms for music generation. elman networks with chaotic inspiration were  used by     to compose music. while rnns are an excellent technique to model  sequences  they used chaotic inspiration as an external input instead of real  stochasticity to compose original music. a deep belief network with a sliding  window mechanism to create jazz melodies was proposed by    . however  due  to lack of temporal information  there were many instances of repeated notes  and pitches. most recently  boulanger lewandowski et al.     used rnn rbms  for polyphonic music generation and obtained promising results. we propose  a generic technique which is a combination of a rnn and a deep belief network for sequence generation  and apply it to automatic music composition. our  technique  the rnn dbn  effectively combines the superior sequence modeling  capabilities of a rnn with the high level data modeling that a dbn enables  to produce rich  complex melodies which do not feature significant repetition.  moreover  the composition of these melodies does not require any feature selection from the input data. this model is presesnted as a generic technique  i. e.  it does not make any assumptions about the nature of the data. we apply our  technique to a variety of datasets and have achieved excellent results which are  on par with the current state of the art.  the rest of this paper is organized as follows  section   discusses various deep  and neural network architectures that serve as the motivation for our technique   described in section  . we demonstrate the application of rnn dbns to the task  of polyphonic music generation in section   and present our results. section    discusses possible future work regarding our technique and concludes the paper.        .     preliminaries  restricted boltzmann machines    restricted boltzmann machines  rbms  are energy based models with their  energy function e v  h  defined as   e v  h     b  v   c  h   h  w v           where w represents the weights connecting hidden  h  and visible  v  units  and b  c are the biases of the visible and hidden layers respectively.  this translates directly to the following free energy formula   x  x  f  v     b  v    log  ehi  ci  wi v  .       i    hi    because of the specific structure of rbms  visible and hidden units are conditionally independent given one another. using this property  we can write   y  p hi  v        p h v     i     polyphonic music generation using a rnn dbn    p v h       y    p vj  h .                j    samples can be obtained from a rbm by performing block gibbs sampling   where visible units are sampled simultaneously given fixed values of the hidden  units. similarly  hidden units are sampled simultaneously given the visible unit  values. a single step in the markov chain is thus taken as follows   h n        w   v  n    c   v  n        w h n      b             where   represents the sigmoid function acting on the activations of the   n     th hidden and visible units. several algorithms have been devised for  rbms in order to efficiently sample from p v  h  during the learning process  the  most effective being the well known contrastive divergence  cd   k  algorithm     .  in the commonly studied case of using binary units  where vj and hi             we obtain  from eqn.      a probabilistic version of the activation function   p  hi     v      ci   wi v   p  vj     h      bj   wj  h            the free energy of an rbm with binary units thus further simplifies to   x  log     e ci  wi v   .       f  v     b  v    i    we obtain the following log likelihood gradients for an rbm with binary  units         log p v    i     ev  p hi  v    vj     vj     wi   v  i    ci     wij   log p v        ev  p hi  v       wi   v  i      ci   log p v    i      ev  p vj  h     vj      bj           where   x         e x     is the element wise logistic sigmoid function.   .     recurrent neural network    recurrent neural networks  rnns  are a particular family of neural networks  where the network contains one or more feedback connections  so that activation  of a cluster of neurons can flow in a loop. this property allows for the network to  effectively model time series data and learn sequences. an interesting property of  rnns is that they can be modeled as feedforward neural networks by unfolding          kratarth goel  raunaq vohra and j. k. sahoo    them over time. rnns can be trained using the backpropogation through time   bptt  technique. if a network training sequence starts at a time instant t   and ends at time t    the total cost function is simply the sum over the standard  error function esse ce at each time step   etotal      xt     t t     esse ce  t            and the gradient descent weight update contributions for each time step are  given by    wij          etotal  t    t    xt   esse ce  t      t t    wij   wij             e    sse ce  now have contributions  the partial derivatives of each component  w  ij  from multiple instances of each weight wij    wv t    h t      w ht    h t    and are  dependent on the inputs and hidden unit activations at previous time instants.  the errors now must be back propagated through time as well as through the  network.     .     deep belief network    rbms can be stacked and trained greedily to form deep belief networks  dbns .  dbns are graphical models which learn to extract a deep hierarchical representation of the training data    . they model the joint distribution between observed  vector x and the   hidden layers hk as follows             p  x  h   . . .   h             y    k      k    p  h  h    k             p  h      h               where x   h    p  hk    hk   is a conditional distribution for the visible units  conditioned on the hidden units of the rbm at level k  and p  h      h    is the  visible hidden joint distribution in the top level rbm.  the principle of greedy layer wise unsupervised training can be applied to  dbns with rbms as the building blocks for each layer    . we begin by training  the first layer as an rbm that models the raw input x   h    as its visible  layer. using that first layer  we obtain a representation of the input that will  be used as data for the second layer. two common solutions exist here  and  the representation can be chosen as the mean activations p h        h      or  samples of p h     h     . then we train the second layer as an rbm  taking the  transformed data  samples or mean activations  as training examples  for the  visible layer of that rbm . in the same vein  we can continue adding as many  hidden layers as required  while each time propagating upward either samples  or mean values.     polyphonic music generation using a rnn dbn     .          recurrent temporal restricted boltzmann machine    the recurrent temporal restricted boltzmann machine  rtrbm      is a sequence of conditional rbms  one at each time instant  whose parameters  btv   bth    w t   are time dependent and depend on the sequence history at time t  denoted  by a t     v        u          t   where u t  is the mean field value of h t   as seen  in    . the rtrbm is formally defined by its joint probability distribution   p  v  t    h t        t  y    p  v  t    h t   a t               t      where p  v  t    h t   a t    is the joint probability of the tth rbm whose parameters are defined below  from eqn.      and eqn.     . while all the parameters  of the rbms may usually depend on the previous time instants  we will consider  the case where only the biases depend on u t    .   t     bh   bh   wuh u t                t     b t   v   bv   wuv u                 which gives the rtrbm six parameters  w  bv   bh   wuv   wuh   u  . the more  general scenario is derived in similar fashion. while the hidden units h t  are binary during inference and sample generation  it is the mean field value u t  that  is transmitted to its successors  eqn.      . this important distinction makes  exact inference of the u t  easy and improves the efficiency of training   .   t     u t      wvh v  t    bh       wvh v  t    wuh u t      bh              observe that eqn.      is exactly the defining equation of a rnn  defined  in section    with hidden units u t  .         rnn dbn    the rtrbm can be thought of as a sequence of conditional rbms whose parameters are the output of a deterministic rnn      with the constraint that  the hidden units must describe the conditional distributions and convey temporal information for sequence generation. the use of a single rbm layer greatly  constricts the expressive power of the model as a whole. this constraint can be  lifted by combining a full rnn having distinct hidden units u t  with a rtrbm  graphical model  replacing the rbm structure with the much more powerful  model of a dbn. we call this model the rnn dbn.  in general  the parameters of the rnn dbn are made to depend only on  u t    given by eqn.      and eqn.      along with    t     bh    bh    wuh  u t               for the second hidden layer in an rnn dbn with   hidden layers. the joint  probability distribution of the rnn dbn is also given by eqn.       but with          kratarth goel  raunaq vohra and j. k. sahoo    u t  defined arbitrarily  as given by eqn.     . for simplicity  we consider the   t    t    t   rnn dbn parameters to be  wvh t    bv   bh    bh    for a   hidden layer rnndbn  shown in fig.     i.e. only the biases are variable  and a single layer rnn   whose hidden units u t  are only connected to their direct predecessor u t    and  to v  t  by the relation   u t      wvu v  t    wuu h t      bu  .            fig.    a rnn dbn with   hidden layers.  the dbn portion of the rnn dbn is otherwise exactly the same as any  general dbn. this gives the   hidden layer rnn dbn twelve parameters     wvh   wh  h    bv   bh    bh    wuv   wuh    wuh    u      wvu   wuu   bu  .  the training algorithm is based on the following general scheme    . propagate the current values of the hidden units u t  in the rnn portion of  the graph using eqn.     .   . calculate the dbn parameters that depend on the u t   eqn.            and        by greedily training layer by layer of the dbn  each layer as an rbm   train the first layer as an rbm that models the raw input as its visible  layer and use that first layer to obtain a representation of the input that will  be used as data for the second layer and so on .   . use cd k to estimate the log likelihood gradient  eq.      with respect to  w   bv and bh for each rbm composing the dbn.   . repeat steps   and   for each layer of the dbn.   t    t    t    . propagate the estimated gradient with respect to bv   bh and bh  backward  through time  bptt       to obtain the estimated gradient with respect to  the rnn  for the rnn dbn with   hidden layers.     polyphonic music generation using a rnn dbn              implementation and results    table    log likelihood  ll  for various musical models in the polyphonic music  generation task.  model    jsb chorales musedata   ll    ll   random     .       .    rbm    .      .    nade    .       .    note n gram     .      .    rnn rbm    .      .    rnn   hf     .      .    rtrbm     .      .    rnn rbm     .      .    rnn nade     .      .    rnn nade   hf    .      .    rnn dbn    .      .      nottingham   ll      .      .      .      .      .      .      .      .      .      .      .      pianomidi.de  ll      .       .       .      .      .      .      .      .      .      .      .      we demonstrate our technique by applying it to the task of polyphonic music  generation. we used a rnn dbn with   hidden dbn layers   each having      binary units   and     binary units in the rnn layer. the visible layer has     binary units  corresponding to the full range of the piano from a  to c . we  implemented our technique on four datasets   jsb chorales   musedata     nottingham   and piano midi.de. none of the preprocessing techniques mentioned  in     have been applied to the data  and only raw data has been given as input  to the rnn dbn. we evaluate our models qualitatively by generating sample  sequences and quantitatively by using the log likelihood  ll  as a performance  measure. results are presented in table    a more comprehensive list can be  found in     .  the results indicate that our technique is on par with the current state of theart. we believe that the difference in performance between our technique and the  current best can be attributed to lack of preprocessing. for instance  transposing  the sequences in a common tonality  e.g. c major minor  and normalizing the  tempo in beats  quarternotes  per minute as preprocessing can have the most  effect on the generative quality of the model. it also helps to have as pretraining  the initialization the wvh    wh  h    bv   bh    bh  parameters with independent  rbms with fully shuffled frames  i.e. wuh    wuh    wuv   wuu   wvu     .  initializing the wuv   wuu   wvu   bu parameters of the rnn with the auxiliary  cross entropy objective via either stochastic gradient descent  sgd  or  preferably  hessian free  hf  optimization and subsequently finetuning significantly               these marked results are obtained after various preprocessing  pretraining methods  and optimization techniques described in the last paragraph of this section.  http   www.musedata.org  ifdo.ca  seymour nottingham nottingham.html          kratarth goel  raunaq vohra and j. k. sahoo    helps the density estimation and prediction performance of rnns which would  otherwise perform worse than simpler multilayer perceptrons    . optimization  techniques like gradient clipping  nesterov momentum and the use of nade for  conditional density estimation also improve results.         conclusions and future work    we have proposed a generic technique called recurrent neural network deep  belief network  rnn dbn  for modeling sequences with generative models and  have demonstrated its successful application to polyphonic music generation. we  used four datasets for evalutaing our technique and have obtained results on par  with the current state of the art. we are currently working on improving the  results this paper  by exploring various pretraining and optimization techniques.  we are also looking at showcasing the versatility of our technique by applying  it to different problem statements.    