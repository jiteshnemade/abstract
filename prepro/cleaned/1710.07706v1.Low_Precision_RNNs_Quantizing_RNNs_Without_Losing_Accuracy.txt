introduction  recurrent neural networks  rnns  are a type of neural network that have a memory aspect  allowing  the network to draw on its previous knowledge. in this way  rnns have a loop of cyclical information  that can be used to interpret and assess the current information being processed. rnns are typically  used for translation and language modeling  and have become increasingly popular both in industry  and research due to their impressive performance           . owing to their state of the art accuracy   these networks have drawn considerable interest from the hardware community and various studies  have proposed methods to reduce the runtime complexity of rnns using quantization schemes      .  lowering the precision of data types used in these networks lowers the storage size  runtime memory  requirements  and makes rnns amenable to be deployed on low memory hardware platforms.    figure  . an rnn cell unrolled.  popular methods to quantize a network include reducing bit widths in order to shrink both the size  of the network by directly reducing the number of bits needed for weights  biases  and activations   while also decreasing the complexity of the computations. bit width reduction does however come  at the cost of network accuracy  as drastic loss in accuracy can be seen when networks are reduced  from their original   bit width.  this paper investigates the quantization of rnns without sacrificing network accuracy  with the  overall goal of efficient hardware implementation. our scheme will pair reduction in bit width with  increase in the model dimension  i.e. increasing the number of neurons in a layer. our method can  retain accuracy while still reducing the network size and runtime memory usage  thus achieving  baseline accuracy at a fraction of the memory cost of the baseline network.  we use a quantization method very similar to that proposed in    . we study the quantization  scheme along with increasing the number of neurons in a layer extensively on a language model on  the penn treebank  ptb  dataset    . we then apply our scheme of increasing the number of  neurons and lowering the bit width on baidu s deep speech model    . results show that the deep          speech model can be quantized down to     of its original size  while still maintain baseline  accuracy  thus proving the efficacy of our approach.     . neuron increase approach  the number of neurons in each layer of a neural network is representative of the computing  complexity of the network. typically  the number of neurons directly affects the accuracy of the  network. increasing the number of neurons in each layer will increase network accuracy  as it will  allow for more discriminative power within each layer  as well as from layer to layer.  our neuron increase approach will target certain layers in an rnn model in order to re gain  accuracy that was lost with bit width quantization. this will be done by first evaluating the network   layer by layer  in order to identify smaller layers that will benefit from an increase in neurons while  still maintaining the reduced model size. once the layers with neuron increase potential are identified   they will be altered by a factor of a set percentage in order to increase neurons. all other layers will  retain their original amount of neurons.  increasing or decreasing the neurons in a layer has a linear relationship in terms of the number  of compute operations with the layer size. for this reason  the layers targeted for neuron increase  are the never the network s larger layers. we find that increasing the neurons in certain layers  combined with bit width quantization effectively retains accuracy. model size is slightly increased  once neurons are added to certain layers  but values  such as weights  biases  and activations  are  still stored in limited bit widths. as a result  the network will retain the benefit of smaller storage size  and less complex computations. computations can be simpler if instead of floating point compute  one operates in integer mode.  the neuron increase approach was studied extensively through a language model on the ptb  dataset and then applied to the deep speech model  achieving the goal of a quantized network which  retains accuracy on both models.     . studies on a language model using ptb  in order to evaluate the rate in which accuracy is deteriorated in rnns when the bit width is reduced   a language model was studied. using the quantization methods discussed in      both the weights  and activations of a long short term memory  lstm  rnn model were reduced. the neuron  increase approach was then applied in order to see the effect it can have on re gaining the lost  accuracy in quantized rnns. the model used for testing consisted of one lstm layer containing      neurons  and was tested on the ptb dataset. this dataset  considered small in size  contains        unique words. this quantized rnn model  called bit rnn in      measures accuracy in  perplexity per word  ppw . table   summarizes the results of our quantized experiments on this  model.    table  . bit rnn model ppw with varying bit widths.  weight  bitwidth                                        .        .        .        .        .              .        .        .        .        .             .        .        .        .        .             .        .        .        .        .             .        .        .        .        .     activation bit width    table   shows that decreasing the bit width of the lstm model has a much more drastic  effect on the model accuracy  as accuracy only drops by approximately  .   when the  activation bits are halved  but drops by approximately  .   when the weight bits are halved.          using the quantized bit rnn models  the neuron increase approach was tested to see  the relationship between increasing neurons and network accuracy. because the bit rnn  model consists of only one layer  the neuron increase approach was applied without  needing to do layer by layer network evaluation. once the bit rnn model was quantized  and the neuron increase approach was applied  the model size reduction and runtime  memory usage was evaluated. the quantization methods used leads to a model size  reduction of       where k represents the specified weight bit  and reduces the runtime  memory usage by      as well  where k represents the specified activation bit. as stated  earlier  increasing the number of neurons will have a linear effect on model size. however   with model size increase  we also lower the bit width which lowers the overall memory  requirements.  table  . bit rnn model ppw with varying bit widths and neurons.  model  configuration   weight x  activation     number of neurons                         x        .        .        .      x        .        .        .      x        .        .        .       x        .        .        .     table   shows the effect of increasing the number of neurons in the network by  . x and    x. once the neurons are increased by  . x  full accuracy is regained with   bit weights  and   bit activations. accuracy is within    with   bit weights and   bit activations when  number of neurons are increased by  . x. we find the accuracy to plateau once neurons  are increased beyond  . x. additionally  the   bit weights and   bit activations model is only      in size of the baseline model size and runtime memory usage  and when the neurons  are increased by  . x of their original amount  the network can surpass baseline accuracy  while only having a    jump in model size and runtime memory usage.     . deep speech model studies  the deep speech model is a speech recognition system that can perform well on large  datasets. this model serves as a much larger arena to prove the effectiveness of bit width  reduction when paired with neuron increase. the deep speech model used in the following  experiments is constructed with six layers  five fully connected  fc  layers and one  recurrent bidirectional     lstm layer. three of the five fc layers consist of      neurons   the fc layer that feeds into the lstm layer contains double that  as does the entirety of the  lstm layer       neurons in the backward and in the forward cell   and the final layer of  the network contains the same number of neurons as characters in the language being  used.  the deep speech model was tested by first quantizing the network and then applying  the neuron increase approach. the models were trained using the librispeech training  dataset       which contains approximately      hours of speech  and the ted lium  training dataset      which contains approximately     hours of speech. the models were  tested on the ted lium testing dataset. accuracy is measured using the word error rate   wer .  the deep speech model was quantized in two different forms in order to see the effect  of quantization on the accuracy of the fc layers as compared to quantization in only the          recurrent lstm layer. this was done by first quantizing only the fc layers via their weights  and biases  and in a separate model only quantizing the recurrent lstm layer. due to  resource limitations on a gpu  the recurrent lstm layer was only quantized by its  activations  not weights. once quantized  the respective network models were evaluated  for neuron increase potential. the first quantized model had a neuron increase in the first   second  and fifth layers of the model  as these layers are relatively smaller layers in the  network  and thus will not greatly effect model size when their number of neurons are  increased. the second quantized model did not receive any neuron increases  as data from  the previous section s study suggests that when only activations are quantized  model  accuracy is maintained.  table  . deep speech model wer with reduced fc layers and increased neurons.  configuration    d wer      bit fc layer     .        bit fc layer       neuron  increase    bit fc layers       neuron  increase     .      .        bit fc layer     .        bit fc layer       neuron  increase     .      table   shows the difference in wer from the baseline model accuracy  and reveals  that the accuracy increases when neurons are added to the fc layers at a     increase  and a     increase. the results show that even the   bit fc layer model is able to retain  accuracy when the neurons in the first  second  and fifth  layer are increased by    . the  effect of the added neurons on model size can also be measured. in the   bit fc layer  model  once neurons are increased by     of their original size in layers one  two  and five   the model size is only increased by  .    and when the neurons are increased to     of  their original size  baseline accuracy is retained and the model size only jumps  .  .  table  . deep speech model wer with reduced recurrent layer.  configuration    d wer      bit recurrent layer activations     .        bit recurrent layer activations     .      table   shows that decreasing the activations in the recurrent layer has a minimal effect  on accuracy  even when reduced to   bits. additionally  runtime memory usage is reduced  to   .   of its original size when the lstm activation bits are quantized to   bits  and only    .   of its original size when the lstm activation bits are quantized to   bits.     . conclusions  results from a language model on the ptb dataset and the deep speech model study both  conclude that bit width quantization paired with neuron increases can effectively retain  accuracy. the language model on the ptb dataset study revealed that quantizing the  weights has a much larger effect on model accuracy when compared to quantizing  activations  and that the neuron increase approach can re build accuracy with only a              neuron increase. the deep speech model study proved that the model can retain baseline  accuracy when all fc layers are quantized to   bits and only three out of the six layers have  a neuron increase of    . additionally  model size is only increased by  .   in order for  accuracy to be retained in the   bit model.  our future work on this topic will include quantizing the weight bits of the recurrent layer  in combination with the activation bits and quantizing the recurrent layer and fc layers  simultaneously.     . 