introduction    recurrent neural networks  rnns  achieve state of the art  sota  accuracy for many applications  that use time series data. as a result  rnns can benefit important internet of things  iot  applications like wake word detection       human activity recognition          and predictive maintenance.  iot applications typically run on highly constrained devices. due to their energy  power  and cost  constraints  iot devices frequently use low bandwidth memory technologies and smaller caches  compared to desktop and server processors. for example  some iot devices have  kb of ram  and    kb of flash memory. the size of typical rnn layers can prohibit their deployment on iot  devices or reduce execution efficiency     . thus  there is a need for a compression technique that  can drastically compress rnn layers without sacrificing the task accuracy.  first  we study the efficacy of traditional compression techniques like pruning      and low rank  matrix factorization  lmf        . we set a compression target of     or more and observe that  neither pruning nor lmf can achieve the target compression without significant loss in accuracy. we  then investigate why traditional techniques fail  focusing on their influence on the rank and condition  number of the compressed rnn matrices. we observe that pruning and lmf tend to either decrease  matrix rank or lead to ill condition matrices and matrices with large singular values.  to remedy the drawbacks of existing compression methods  we propose to use kronecker products   kps  to compress rnn layers. we refer to the resulting models as kprnns. we are able to show  that our approach achieves sota compression on iot targeted benchmarks without sacrificing wall  clock inference time and accuracy.         related work    kps have been used in the deep learning community in the past        . for example       use kps to  compress fully connected  fc  layers in alexnet. we deviate from      by using kps to compress  rnns and  instead of learning the decomposition for fixed rnn layers  we learn the kp factors  directly. additionally       does not examine the impact of compression on inference run time. in       currently at amd research     th edition of workshop on energy efficient machine learning and cognitive computing at neurips               kps are used to stabilize rnn training through a unitary constraint. a detailed discussion of  how the present work differs from     can be found in section  .  the research in neural network  nn  compression can be roughly categorized into   topics  pruning        structured matrix based techniques      quantization        and tensor decomposition        .  compression using structured matrices translates into inference speed up  but only for matrices of  size             and larger      on cpus or when using specialized hardware    . as such  we  restrict our comparisons to pruning and tensor decomposition.        .     kronecker product recurrent neural networks  background    let a   rm n   b   rm   n  and c   rm   n  . then  the kp between b and c is given by  a b c  b      c    b      c  a    .  bm       c       b      c  b      c  .  b      c         ... b  n    c  ... b  n    c       .  .  ... bm   n    c       where  m   m    m    n   n    n    and   is the hadamard product. the variables b and c are  referred to as the kronecker factors of a. the number of such kronecker factors can be   or more.  if the number of factors is more than    we can use     recursively to calculate the resultant larger  matrix. for example  in the following equation w   w    w    w            w can be evaluated by first evaluating w     w   to a partial result  say r  and then evaluating  w   w     r.  expressing a large matrix a as a kp of two or more smaller kronecker factors can lead to significant  compression. for example  a   r        can be decomposed into kronecker factors b   r       and c   r     . the result is a     reduction in the number of parameters required to store a. of  course  compression can lead to accuracy degradation  which motivates the present work.   .     prior work on using kp to stabilize rnn training flow    jose et al.     used kp to stabilize the training of vanilla rnn. an rnn layer has two sets of weight  matrices   input hidden and hidden hidden  also known as recurrent . jose et al.     use kronecker  factors of size       to replace the hidden hidden matrices of every rnn layer. thus a traditional  rnn cell  represented by   ht   f   wx wh      xt   ht                ht   f   wx w    w  ...   wf         xt   ht                is replaced by     where wx  input hidden matrix    rm n   wh  hidden hidden or recurrent matrix    rm m    wi   r    for i               f       xt   rn     ht   rm     and f   log   m    log   n . thus a            sized matrix is expressed as a kp of   matrices of size      . for an rnn layer with input  and hidden vectors of size      this can potentially lead to      compression  as we only compress  the wh matrix . the aim of jose et al.     was to stabilize rnn training to avoid vanishing and  exploding gradients. they add a unitary constraint to these       matrices  stabilizing rnn training.  however  in order to regain baseline accuracy  they needed to increase the size of the rnn layers  significantly  leading to more parameters being accumulated in the wx matrix in    . thus  while  they achieve their objective of stabilizing vanilla rnn training  they achieve only minor compression        . in this paper  we show how to use kp to compress both the input hidden and hidden hidden  matrices of vanilla rnn  lstm and gru cells and achieve significant compression        . we  show how to choose the size and the number of kronecker factor matrices to ensure high compression  rates   minor impact on accuracy  and inference speed up over baseline on an embedded cpu.         .     kprnn layer    choosing the number of kronecker factors  a matrix expressed as a kp of multiple kronecker  factors can lead to significant compression. however  deciding the number of factors is not obvious.  we started by exploring the framework of    . we used       kronecker factor matrices for hiddenhidden recurrent matrices of lstm layers of the key word spotting network     . this resulted in an  approximately    reduction in the number of parameters. however  the accuracy dropped by     relative to the baseline. when we examined the       matrices  we observed that  during training  the  values of some of the matrices hardly changed after initialization. this behavior may be explained by  the fact that the gradient flowing back into the kronecker factors vanishes as it gets multiplied with  the chain of       matrices during back propagation. in general  our observations indicated that as  the number of kronecker factors increased  training became harder  leading to significant accuracy  loss when compared to baseline.  algorithm   implementation of matrix vector product  when matrix is expressed as a kp of two  matrices  input  matrices b of dimension m    n    c of dimension m    n  and x of dimension n    .  m   m    m    n   n    n   output  matrix y of dimension m         x   reshape x  n    n     reshapes the x vector to a matrix of dimension n    n        bt   b.transpose       y   c   x   bt     y   reshape y  m      reshapes the y vector to a matrix of dimension m       additionally  using a chain of       matrices leads to significant slow down during inference on a  cpu. for inference on iot devices  it is safe to assume that the batch size will be one. when the  batch size is one  the rnn cells compute matrix vector products during inference. to calculate the  matrix vector product  we need to multiply and expand all of the       to calculate the resultant  larger matrix  before executing the matrix vector multiplication. referring to      we need to multiply  w    ..  wf to create wh before executing the operation wh   ht   . the process of expanding the  kronecker factors to a larger matrix  followed by matrix vector products  leads to a slower inference  than the original uncompressed baseline. thus  inference for rnns represented using     is faster  than the compressed rnn represented using    . the same observation is applicable anytime the  number of kronecker factors is greater than  . the slowdown with respect to baseline increases with  the number of factors and can be anywhere between       .  however  if the number of kronecker factors is restricted to two  we can avoid expanding the  kronecker factors into the larger matrix and achieve speed up during inference. algorithm   shows  how to calculate the matrix vector product when the matrix is expressed as a kp of two kronecker  factors. the derivation of this algorithm can be found in     .  choosing the dimensions of kronecker factors  a matrix can be expressed as a kp of two  kronecker factors of varying sizes. the compression factor is a function of the size of the kronecker  factors. for example  a           matrix can be expressed as a kp of       and           matrices   algorithm   finding dimension of kronecker factors for a matrix of dimension m   n  input  list  is the sorted list of prime factors of m  list  is the sorted list of prime factors of n  output  lista   dimension of the first kronecker factor. listb   dimension of the second kronecker  factor     function reducelist  inputlist       temp    inputlist         inputlist.del      delete the element at position zero      inputlist      inputlist    temp       inputlist.sort  ascending        return inputlist     list   list    reducelist list    reducelist list  .sort  descending       lista  listb    list     list       listb    list     list               table    benchmarks evaluated in this paper. these benchmarks represent some of the key applications in the iot domain   image classification  key word spotting  bidirectional lstm. we cover a  wide variety of applications and rnn cell types.  mnistlstm    uspskwshar fastrnn  lstm  bilstm  reference paper                  cell type  lstm  fastrnn  lstm  bi lstm  dataset                         table    model accuracy and runtime for our benchmarks before and after compression. the baseline networks  are compared to networks with rnn layers in the baseline compressed using kps  magnitude pruning  lmf   or by scaling the network size  small baseline . each compressed network has fewer rnn parameters than  the baseline  size indicated . for each row  best results are indicated in bold. the kp based networks are  consistently the most accurate alternative while still having speed up over the baseline.  compression technique  magnitude  baseline small baseline  lmf  kp  pruning     model size  kb     .     .     .     .    .    accuracy        .      .      .      .      .    mnist lstm  compression factor              .      .      .    runtime  ms    .    .    .     .    .   model size  kb        .     .     .      .      .    accuracy        .      .      .      .      .    har  bilstm  compression factor         .      .      .      .    runtime  ms          .      .     .         model size  kb       .     .     .      .      .    accuracy        .     .      .      .      .   kws lstm  compression factor         .      .       .      .     runtime  ms     .    .     .    .     .   model size  kb     .     .     .     .     .    accuracy        .      .      .      .      .    usps fastrnn  compression factor        .     .              runtime  ms    .     .    .      .     .      model size is calculated assuming    bit weights. further opportunities exist to compress the network via  quantization and compressing the fully connected softmax layer.     we measure the amount of compression of the lstm fastrnn layer of the network  benchmark name    parameter measured    leading to a    reduction in the number of parameters used to store the matrix. however  if we use  kronecker factors of size        and         we achieve a compression factor of    . in this paper   we choose the dimensions of the factors to achieve maximum compression using algorithm  .  compressing lstms  grus and rnns using the kp  kprnn cells are rnn  lstm and gru  cells with all of the matrices compressed by replacing them with kps of two smaller matrices. for  example  the rnn cell depicted in     is replaced by   kp rn n cell   ht   f   w    w       xt   ht            where xt   rn     ht   rm     w    rm   n    w    rm   n    m    m    m and n     n     m   n . lstm  gru and fastrnn cells are compressed in a similar fashion. instead of  starting with a trained network and decomposing its matrices into kronecker factors  we replace the  rnn lstm gru cells in a nn with its kp equivalent and train the entire model from the beginning.         results    other compression techniques evaluated  we compare networks compressed using kprnn  with three techniques   pruning  lmf and small baseline.  training platform  infrastructure  and inference run time measurement  we use tensorflow   .   as the training platform and   nvidia rtx      gpus to train our benchmarks. to measure        the inference run time  we implement the baseline and the compressed cells in c   using the eigen  library and run them on the arm cortex a   core of a hikey     development board.  dataset and benchmarks  we evaluate the impact of compression using the techniques discussed  in section   on a wide variety of benchmarks.table   shows the benchmarks used in this work.   .     kprnn networks    table   shows the results of applying the kp compression technique across a wide variety of  applications and rnn cells. as mentioned in section    we target the point of maximum compression  using two matrix factors.   .     possible explanation for the accuracy difference between kprnn  pruning  and lmf    in general  the poor accuracy of lmf can be attributed to significant reduction in the rank of the  matrix  generally      . kps  on the other hand  will create a full rank matrix if the kronecker  factors are fully ranked     . we observe that  kronecker factors of all the compressed benchmarks  are fully ranked. a full rank matrix can also lead to poor accuracy if it is ill conditioned. however   the condition numbers of the matrices of the best performing kp compressed networks discussed  in this paper are in the range of  .  to  . . to prune a network to the same compression factor as  kp  networks need to be pruned to     sparsity or above. pruning fastrnn cells to the required  compression factor leads to an ill conditioned matrix. this may explain the poor accuracy of sparse  fastrnn networks. however  for other pruned networks  the resultant sparse matrices have a  condition number less than    and are fully ranked. thus  condition number does not explain the  loss in accuracy for these benchmarks. to further understand the loss in accuracy of pruned lstm  networks  we looked at the singular values of the resultant sparse matrices in the kws lstm  network. let y   ax. the largest singular value of a upper bounds kyk    i.e. the amplification  applied by a. thus  a matrix with larger singular value can lead to an output with larger norm      . since rnns execute a matrix vector product followed by a non linear sigmoid or tanh layer   the output will saturate if the value is large. the matrix in the lstm layer of the best performing  pruned kws lstm network has its largest singular value in the range of    to    while the baseline  kws lstm network learns a lstm layer matrix with largest singular value of    and the kronecker  product compressed kws lstm network learns lstm layers with singular values less than   . this  might explain the especially poor results achieved after pruning this benchmark. similar observations  can be made for the pruned har  network.         conclusion    we show how to compress rnn cells by     to     using kronecker products. we call the cells  compressed using kronecker products as kprnns. kprnns can act as a drop in replacement for  most rnn layers and provide the benefit of significant compression with marginal impact on accuracy.  none of the other compression techniques  pruning  lmf  match the accuracy of the kronecker  compressed networks. we show that this compression technique works across   benchmarks that  represent key applications in the iot domain.    