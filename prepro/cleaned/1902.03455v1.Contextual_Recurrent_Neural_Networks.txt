introduction    the initialization method of the hidden state of recurrent neural networks  rnn  is typically defaulted to a constant value  most commonly equal to zero. this is due to rnns having the characteristic of accumulating information over time  producing outputs as a function of the hidden state  and input at each time step. interestingly  by choosing an arbitrary constant value for the initialization of the hidden state the rnn will learn to correct for discrepancies in the output credited to  the initial value. in this paper  we show that the rate of the initial hidden state correction can be  improved by learning a value conditioned on contextual information related to the task.  regardless  rnns have been successfully applied to many hard generalization tasks such as  natural language processing     and reinforcement learning       . many successful works that use  rnns rely on the underlying dynamics  capacity  and generalization of the hidden state  such as the  use of long short term memory  lstm      and fast weights  fw      units. while many successes  have been achieved with constant zero initialization of the hidden state  further gains may be found  by learning and conditioning the initialization of the hidden state.      .     preliminaries     . .     recurrent neural networks    a recurrent neural network  rnn  is a neural network that consists of a hidden state h which  operates on a variable length sequence x    x    ...  xt  . at each time step t  the hidden state ht of  the rnn is updated by  ht   f  ht     xt              where f is a non linear activation function  such as a lstm unit . the rnns mentioned in this  paper are optimized by stochastic gradient descent.    code    for this paper is publicly available at https   github.com fomorians contextual rnn          the specification of the hidden state h  to a constant value is attributed to an rnns ability to  accumulate information over time. in other words  the rnn learns to eliminate the impact of a  misspecification in the initialization on the outputs.   . .     related works    one of the difficulties with finite unfolding in time is the proper initialization of the initial hidden  state h  . fixing the hidden state to zero is a common choice  this has been shown to force the rnn  to focus on the input sequence regardless of the initial choice of the hidden state  forcing the error  in the initialization to diminish over time and training    . zimmermann et al.     proposed a noise  term  proportional to the back propagated error  added to the initial hidden state to regularize and  stabilize the dynamics of the rnn. as an alternative to an adaptive noise term  we propose that  conditioning an initial hidden state distribution  using a reparameterization trick      on contextual  information related to the task can provide a better foundation for end to end handling of the  underlying instabilities.  this contextual information can be  but is not limited to  an encoded summary of an input  sequence         a coarse image with multiple objects      or the first element of the input sequence.  ba et al.     feed a down sampled input image through a contextual network to initialize the hidden  state of an rnn and potentially provide sensible hints on where attention should be directed.  building on the ideas from this work and other similar sources  contextual rnn provide a general  framework for learning the initial hidden state on a variety of forms of contextual information.  rnn encoder decoder     architectures use different initial hidden state initialization in the  encoder and decoder rnn. cho et al.     and bahdanau et al.     initialize the hidden state of the  encoder henc      and the decoder hdec    tanh whdec henc        t  . by using the final hidden state of the  encode    the decoder is conditioned on contextual information representing a summary of the  encoder ht  input sequence. these works provided the foundations for the successful seq seq framework using  rnn. in the spirit of the initialization of the decoder rnn  contextual rnn proposes a potential  alternative initialization for the encoder rnn as opposed to using a zero value.  more recent work uses attention mechanisms applied to the hidden state of rnn in various ways        . these methods attempt to increase both the long  and short term memory capacities and  handle very different time scales and alignments of sequential data. bahdanau et al.     use a nondec  dec  linear context function q applied to the hidden states over time  q  hdec      h    ...ht     to model  longer sequences and jointly learn alignments for language translation. contrary to the decoder  context used by this work  the fw formulation from ba et al.     forces the rnn to iteratively  attend to the recent past by a scalar product between the current hidden state ht and the previous  hidden states ht   . in light of the successes of the attention mechanism applied to the hidden states  formulated by the fw rnn  contextual rnn can be used to initialize the hidden state and improve  upon the convergence speed on the associative retrieval task introduced by    .     .     our contributions    in this paper  we propose a method of parameterizing the initial hidden state of an rnn. the  resulting architecture  referred to as a contextual rnn  can be trained end to end. a neural network  called the context network produces the initial hidden state of an rnn and is capable of improving  the performance on multiple tasks. initially  we qualitatively demonstrate that contextual rnn  improves convergence speed on an associative retrieval task compared to the vanilla fw. next  we  describe a novel method to generate sequences by sampling hidden states conditioned on a regression  quantity. finally  we compare and summarize the results and generated samples trained on   d linear  cosine decay.               methods    we explore the use of contextual information taken from the input sequence to improve on the  generalization of missing sequential key value pairs. furthermore  we propose a novel method of  generating sequential data given contextual information in the form of classification labels or regression quantities.     .     contextual recurrent neural networks    we formally define a contextual rnn as an rnn with a learned initial hidden state conditioned on  contextual information. using eq.    we outline various ways to initialize the initial hidden state     h    constant       a     h    variable       b     h    g context        c     where h  can hold a constant value  most commonly h        a variable  or a neural network g.  when the initialization   a  is chosen  the error of the specification is inherently ignored and  the rnn is expected to overcome this error by accumulating information over time and training.  alternatively  we can choose the initialization   b   a free parameter  to allow back propagation to  automatically correct for the error in the specification of the initial value over training. although   this method does not scale well beyond online gradient descent  in this context  this means the batch  size is   . therefore  to promote the feasibility of this method with modern training approaches  the  variable can be tiled  and noise can optionally be added to the copies of the variable  in order to  be compatible with batches of data. finally  the initialization   c  credits the specification error to  contextual information through a neural network g. when defining g as a contextual neural network   this method has several advantages and is the foundation of the contextual rnn.     .    . .     experiments  associative retrieval task    figure    the contextual rnn used in the associative retrieval task.  the associative retrieval task  art      tests the ability of an rnn to store and retrieve temporary memories. after generating a sequence of k character digit pairs from the alphabet  a  b  c  ...z   without replacement  one of the k different characters is selected at random as the query and the  network must predict the target digit. in the input sequence  the query is presented directly after       characters. for example  the network is presented the input sequence c k j f   k  a query        figure    comparison of the validation log likelihood  left  and accuracy  right  on the associative  retrieval task. the performance of the baseline zero is improved by using a learned context state.  character for the sequence could be k and the corresponding target digit would be  .  a training  set was used consisting of         samples  along with a validation set consisting of        samples.  we demonstrate the use of a single layer as the context network g with a shared character  embedding   x    that is conditioned on the first character in the input sequence  see figure   . the  models share the same architecture as in ba et al.      using a character embedding and a fw rnn  with    hidden units. all the models were trained using batches of size     and the adam optimizer       with a learning rate of  .    for     epochs.  from figure    we see that the contextual rnn form of the fw model  learned   c  significantly  outperforms the vanilla fw from ba et al.     that use initialization methods  zero   a  and  free    b   without any fine tuning.   . .     linear cosine decay task    figure    the contextual rnn used in the linear cosine decay task.  the linear cosine decay  lcd  task tests the ability of an rnn to interpolate and generalize to  missing regression quantities. lcd  is defined as                cos   t  t      tf   t  tf  x t    x                    tf       the    our    set of all other possibilities for this sequence are   c       j       f       implementation of lcd uses the tensorflow.train.linear cosine decay         .          figure    example of an lcd task where the  constants are       and      .     the period  t    .   the total decay time tf        and the  initial value x       . .    figure    comparison of the validation squared  error on the linear cosine decay task. the  learned distribution model is trained and evaluated using   random seeds.    where t is the current time  tf is the total decay time  t is the period  alpha and beta are constants   and x    is the initial value. an example of a generated lcd sequence is shown in figure  .  in our experiments  the initial value is sampled from a uniform distribution x      u               and      .   . training and validation sets are created using corresponding period sets   ttrain and tvalidation . for each period t in ttrain     .    .    .    .    .    we sample      lcd  sequences x of length tf      to create the the training set. we do the same for each period t  in tvalidation     .    .    .    .    sampling     lcd sequences x of length tf      to create the  validation set. the model is trained to produce the next input in the sequence by predicting the  change in input  xt   xt   xt   at each time step   xt is added to the previous input to produce  the next input xt    xt   xt   . all values are scaled to be in the range        .  we demonstrate the use of the context network g with a hidden layer  with    units  that is  conditioned on the initial value x  and period t  see figure   . the output of g is used as the  mean   of a re parameterized normal distribution     with a free variable for the scale    as in the  equation  h    n  g x    t    sof tplus               by conditioning a distribution on the initial value and period of the input sequence  we can implicitly  generate a distribution of possible trajectories given the learned uncertainty under the initial hidden  state given the data.  we chose to compare the zero initial hidden state  h       rnn and the contexual rnn. in  order to properly compare the zero initial hidden state with the contextual rnn  we append the  period to each time step in the input sequences    x    t     x    t    ... xt   t    . each model has a      unit lstm followed by a final layer with a single unit corresponding to the output  xt . all  the models were trained using batches of size     and the adam optimizer      with a learning rate  of  .     for    epochs. the results on the validation set of during training are shown in figure   . after each epoch  we sampled    initial hidden states for a single example of each period in the  validation set and show the results in figure  .          figure    generated samples of periods t    .   left  and t    .   right  from the contextual rnn  trained on the linear cosine decay task for    epochs.         conclusion    conventional methods initialize the hidden state of rnn to zero regardless of the task. other  methods introduce contextual information to both introduce new frameworks for solving variable  length sequence problems        and allowing rnn to summarize the input sequence    . this paper  provides a simple method of producing meaningful initial hidden states that are shown to improve  performance on a range of tasks and introduces a novel generative model for sequences. these  contributions explore the use of contextual information to improve generalization of rnn.         acknowledgments    we would like to thank dan saunders  andrew wang  mike qiu  eddie costantini and danielle  swank at fomoro for their constructive feedback on this report.    