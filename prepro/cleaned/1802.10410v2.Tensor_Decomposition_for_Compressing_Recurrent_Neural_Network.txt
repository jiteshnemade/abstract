introduction  in recent years  rnns have achieved many state of the arts  on sequential data modeling task and significantly improved  the performance on many tasks  such as speech recognition           and machine translation         . there are several reasons  behind the rnns impressive performance  the availability of  data in large quantities and the advance of modern computer  performances such as gpgpu. the recent hardware advance  allows us to train and infer rnn models with million of  parameters in a reasonable amount of time.  some devices such as mobile phones or embedded systems  only have limited computing and memory resources. therefore  deploying a model with a large number of parameters  in those kind of devices is a challenging task. therefore  we  need to represent our model with more efficient methods and  keep our model representational power at the same time.  some researchers have conducted important works to balance the trade off between the model efficiency and their  representational power. there are many different approaches to  tackle this issue. from the low level optimization perspective   courbariaux et al.     replace neural network weight parameters with binary numbers. hinton et al.     compress a larger  model into a smaller model by training the latter on soft target  instead of hard target. rnns are composed by multiple linear  transformations and followed by non linear transformation.  most of rnn parameters are used to represent the weight  matrix in those linear transformations and the total number  of parameters depends on the input and hidden unit size.  therefore  some researchers also tried to represent the dense  weight matrices with several alternative structures. denil et al.        employed low rank matrix to replace the original weight  matrix.  instead of using low rank matrix decomposition  novikov  et al.     used tt format to represent the weight matrices  in the fully connected layer inside a cnn model. tjandra  et al.     applied tt decomposition to compress the weight  matrices inside rnn models. besides tt decomposition  there  are several popular tensor decomposition methods such as cp  decomposition and tucker decomposition.  however  those methods have not been explored for compressing rnn weight matrices  thus we are interested to see  the extensive comparison between all tensor decomposition  method performances under the same number of parameters. in this paper  we utilized several tensor decomposition  methods including cp decomposition  tucker decomposition  and tt decomposition for compressing rnn parameters. we  represent gru rnn weight matrices with these tensor decomposition methods. we conduct extensive experiments on  sequence modeling with a polyphonic music dataset. we  compare the performances of uncompressed gru model and  three different tensor based compressed rnn models  cpgru  tucker gru and tt gru     on various number of  parameters. from our experiment results  we conclude that ttgru achieved the best result in various number of parameters  compared to other tensor decomposition method.  ii. recurrent neural network  rnns are a type of neural networks designed for modeling  sequential and temporal data. for each timestep  the rnn  calculate its hidden states by combining previous hidden states  and a current input feature. therefore  rnns are able to  capture all previous information from the beginning until  current timestep.  a. elman recurrent neural network  elman rnns are one of the earliest type of rnn models      . in some cases  elman rnn also called as simple rnn.  generally  we represent an input sequence as x    x    ...  xt     hidden vector sequence as h    h    ...  ht   and output vector  sequence as y    y    ...  yt  . as illustrated in fig.    a simple  rnn at t th time step is can be formulated as   ht         f  w xh xt   whh ht     bh             yt         g why ht   by  .            where w xh represents the weight parameters between the  input and hidden layer  whh represents the weight parameters  between the previous and current hidden layers  why represents  the weight parameters between the hidden and output layer   and bh and by represent bias vectors for the hidden and  output layers. functions f     and g    are nonlinear activation  functions  such as sigmoid or tanh.          it           w xi xt   whi ht     wci ct     bi             ft           w x f xt   wh f ht     wc f ct     b f             ct         ft    ot           w xo xt   who ht     wco ct   bo      ht         ot    ct     it    tanh w xc xt   whc ht     bc      tanh ct                       where      is sigmoid activation function and it   ft   ot and ct are  respectively the input gates  the forget gates  the output gates  and the memory cells. the input gates retain the candidate  memory cell values that are useful for the current memory  cell and the forget gates retain the previous memory cell values  that are useful for the current memory cell. the output gates  retain the memory cell values that are useful for the output  and the next time step hidden layer computation.  figure  . recurrent neural network    b. gated recurrent neural network  learning over long sequences is a hard problem for standard rnn because the gradient can easily vanish or explode            . one of the sources for that problem is because  rnn equations are using bounded activation function such  as tanh and sigmoid. therefore  training a simple rnn is  more complicated than training a feedforward neural network.  some researches addressed the difficulties of training simple  rnns. from the optimization perspective  martens et al.       utilized a second order hessian free  hf  optimization  rather than the first order method such as stochastic gradient  descent. however  to calculate the second order gradient or  their approximation requires some extra computational steps.  le et al.      changed the activation function that causes the  vanishing gradient problem with a rectifier linear  relu  function. they are able to train a simple rnn for learning longterm dependency with an unbounded activation function and  identity weight initialization. modifying the internal structure  from rnn by introducing gating mechanism also helps rnns  solve the vanishing gradient problems. the additional gating  layers control the information flow from the previous states  and the current input     . several versions of gated rnns  have been designed to overcome the weakness of simple rnns  by introducing gating units  such as long short term memory   lstm  rnn and gru rnn.     long short term memory rnn  an lstm      is a  gated rnn with memory cells and three gating layers. the gating layers purpose is to control the current memory states by  retaining the important information and removing the unused  information. the memory cells store the internal information  across time steps. as illustrated in fig.    the lstm hidden  layer values at time t are defined by the following equations    figure  . long short term memory unit.       gated recurrent unit rnn  a gru      is one variant  of gated rnn. it was proposed an alternative to lstm. there  are several key differences between gru and lstm. first  a  gru does not seperate the hidden states with the memory cells      . second  instead of three gating layers  it only has two   reset gates and update gates. as illustrated in fig.    the gru  hidden layer at time t is defined by the following equations         rt           w xr xt   whr ht     br             zt           w xz xt   whz ht     bz             h t         f  w xh xt   whh  rt    ht              zt      ht     zt    ht       bh    h t                  where      is a sigmoid activation function  f     is a tanh  activation function  rt   zt are the reset and update gates  h t is the  candidate hidden layer values  and ht is the hidden layer values  at time t th. the reset gates control the previous hidden layer  values that are useful for the current candidate hidden layer.  the update gates decide whether to keep the previous hidden  layer values or replace the current hidden layer values with  the candidate hidden layer values. gru can match lstm s  performance and its convergence speed sometimes surpasses  lstm  despite having one fewer gating layer     .     figure  . cp decomposition for  rd order tensor w    where  r     ..r   g  r   rm    g  r   rm    g  r   rm    r   z  is  the number of factors combinations  cp rank  and   denotes  kronecker product operation. elementwise  we can calculate  the result by     figure  . gated recurrent unit    w x  y  z     iii. tensor rnn    r  x    g  r  x  g  r  y  g  r  z             r      in this section  we explain our approaches to compress the  parameters in the rnn. first  we define the tensorization  process to transform the weight matrices inside the rnn  model into higher order tensors. then  we describe two tensor  decompositions method called as candecomp parafac   cp  decomposition and tucker decomposition. last  we explain about tensorization and rnn parameters compression  with the tensor decomposition methods.    in figure    we provide an illustration for eq.    in more  details.     tucker decomposition  tucker decomposition             factorizes a tensor into a core tensor multiplied by a matrix  along each mode. assume we have a  rd order tensor w    rm   m   m    we can approximate it with tucker decomposition     a. vector  matrix and tensor    where g    rr   r   r  is the core tensor  g    rm   r     g    rm   r    g    rm   r  are the factor matrices and  n is  the n th mode product operator. the mode product between  a tensor g    rn   n   n  and a matrix g    rm   n  is a tensor  rm   n   n  . by applying the mode products across all modes   we can recover the original w tensor. elementwise  we can  calculate the element from tensor w by     before we start to explain any further  we will define  different notations for vectors  matrices and tensors. vector  is an one dimensional array  matrix is a two dimensional  array and tensor is a higher order multidimensional array. in  this paper  bold lower case letters  e.g.  b  represent vectors   bold upper case letters  e.g.  w  represent matrices and bold  calligraphic upper case letters  e.g.  w  represent tensors. for  representing the element inside vectors  matrices and tensors   we explicitly write the index in every dimension without bold  font. for example  b i  is the i th element in vector b  w p  q   is the element on p th row and q th column from matrix w  and w i    ..  id   is the i    ..  id  th index from tensor w.  b. tensor decomposition method  tensor decomposition is a method for generalizing lowrank approximation from a multi dimensional array. there are  several popular tensor decomposition methods  such as canonical polyadic  cp  decomposition  tucker decomposition and  tensor train decomposition. the factorization format differs  across different decomposition methods. in this section  we  explain briefly about cp decomposition and tucker decomposition.     cp decomposition  canonical polyadic decomposition   candecomp parafac            or usually referred to  cp decomposition factorizes a tensor into the sum of outer  products of vectors. assume we have a  rd order tensor w    rm   m   m    we can approximate it with cp decomposition   w     r  x  r      g  r   g  r   g  r            w   g     g     g     g     w x  y  z       r   r  x  r  x  x            g   s    s    s       s     s     s        g   x  s    g   y  s    g   z  s               where x       ..  m     y       ..  m     z       ..  m   . figure   gives  an illustration for eq.       figure  . tucker decomposition for  rd order tensor w    c. tensor train decomposition  tensor train decomposition      factorizes a tensor into  a collection of lower order tensors called as tt cores. all  tt cores are connected through matrix multiplications across  all tensor order to calculate the element from original tensor.     assume we have a  rd order tensor w   rm   m   m    we can  approximate the element at index x  y  z by   w x  y  z       r  x  r   x    g   x  s   g   s    y  s   g   s    z             s     s        where x       ..  m     y       ..  m     z       ..  m    and g     rm   r    g    rr   m   r    g    rr   m  as the tt cores. figure    gives an illustration for eq.   .    we can reformulate the eq.    to calculate y p  elementwise  with   x  y fi  p      w  fi  p   j    ..  jd   x  j    ..  jd    j   ..  jd      b fi  p      by enumerating all columns q position with j    ..  jd and fi  p      i   p   ..  id  p  .  for cp decomposition  we represent our tensor w with  multiple factors gmk r   gnk r where  k     ..d   r      ..r    gmk r   rmk   gnk r   rnk  . from here  we replace  eq.    with     r d     x    x  y               y fi  p      gmk r  ik  p  gnk r   jk       x  j    ..  jd    j   ..  jd    r   k        b fi  p  .  figure  . tensor train decomposition for  rd order tensor w    d. rnn parameters tensorization  most of rnn equations are composed by multiplication  between the input vector and their corresponding weight  matrix   y   wx   b            where w   r m n is the weight matrix  b   r m is the  bias vector and x   rn is the input vector. thus  most of  rnn parameters are used to represent the weight matrices.  to reduce the number of parameters significantly  we need  to represent the weight matrices with the factorization of  higher order tensor. first  we apply tensorization on the weight  matrices. tensorization is the process to transform a lowerorder dimensional array into a higher order dimensional array.  in our case  we tensorize rnn weight matrices into tensors.  given a weight matrix w   r m n   we can represent them  q  as a tensor w   rm   m   .. md  n   n   .. nd where m   dk   mk  qd  and n   k   nk . for mapping each element in matrix w to  tensor w  we define one to one mapping between row column  and tensor index with bijective functions fi   z    zd  and  f j   z    zd  . function fi transforms each row p       ..  m   into fi  p     i   p   ..  id  p   and f j transforms each column  q       ..  n  into f j  q      j   q   ..  jd  q  . following this  we  can access the value from matrix w p  q  in the tensor w  with the index vectors generated by fi  p  and f j  q  with these  bijective functions.  after we determine the shape of the weight tensor   we choose one of the tensor decomposition methods   e.g.  cp decomposition  sec.iii b    tucker decomposition   sec.iii b   or tensor train       to represent and reduce the  number of parameters from the tensor w. in order to represent  matrix vector products inside rnn equations  we need to  reshape the input vector x   rn into a tensor x   rn   .. nd and  the bias vector b   r m into a tensor b   rm   .. md . therefore                     by using cp decomposition for representing the weight matrix  w  we reduce the number of parameters from m   n into  p  r     dk   mk   nk  .  for tucker decomposition  we represent out tensor w  with a tensor core g    rr   ... rd  rd    ... r d where  k      ..d   rk   mk and  k     ..d   rd k   nk and multiple factor  matrices gmk   gnk   where  k     ..d    gmk   rmk  rk   gnk    rnk  rd k  . generally  the tensor core ranks r    r    ..  rd are corresponding to the row in tensor index and rd     rd     ..  r d are  corresponding to the column in tensor index. from here  we  replace eq.    with       rd    .. r d  x    r   .. rdx       g   s    ..sd   sd     ..  s d    y fi  p         j   ..  jd s   ..sd  sd    .. s d     d  y       gmk  ik  p   sk  gnk   jk   sd k       x  j    ..  jd    k        b fi  p  .        by using tucker decomposition for representing the weight  matrix w  we reduce the number of parameters from m   n  p  q  into dk    mk   rk   nk   rd k        d  k   rk  .  for the tt decomposition  we refer to     on how to represent the tensor w and how to calculate the linear projection  to replace eq.   .  in this work  we focus on compressing gru rnn by  representing all weight matrices  input to hidden and hiddento hidden  with tensors and factorize the tensors with low rank  tensor decomposition methods. for compressing other rnn  architectures such as elman rnn  sec. ii a  or lstm rnn   sec. ii b    we can follow the same steps by replacing all  the weight matrices with factorized tensors representation.  e. tensor core and factors initialization trick  because of the large number of recursive matrix multiplications  followed by some nonlinearity  e.g  sigmoid  tanh   the  gradient from the hidden layer will diminish after several timestep     . consequently  training recurrent neural networks     is much harder compared to standard feedforward neural  networks.  even worse  we decompose the weight matrix into multiple  smaller tensors or matrices  thus the number of multiplications needed for each calculation increases multiple times.  therefore  we need a better initialization trick on the tensor  cores and factors to help our model convergences in the early  training stage.  in this work  we follow glorot et al.      by initializing  the weight matrix with a certain variance. we assume that  our original weight matrix w has a mean   and the variance    w . we utilize the basic properties from a sum and a product  variance between two independent random variables.  definition iii. . let x and y be independent random variables  with the mean    then the variance from the sum of x and y  is var x   y    var x    var y   definition iii. . let x and y be independent random variables  with the mean    then the variance from the product of x and  y is var x   y    var x    var y   after we decided the target variance   w for our original  weight matrix  now we need to derive the proper initialization  rules for the tensor core and factors. we calculate the variance  for tensor core and factors by observing the number of sum  and product operations and utilize the variance properties from  def. iii.  and iii. . for weight tensor w based on the cpdecomposition  we can calculate  g as the standard deviation  for all factors gmk r   gnk r with   r   g       d      w  r            and initialize gmk r   gnk r   n      g  .  for weight tensor w based on the tucker decomposition   we can calculate  g as the standard deviation for the core  tensor g  and the factor matrices gmk   gnk with   s    w    d      g          q d  k   rk  and initialize g    gmk   gnk   n      g  .  for weight tensor w based on the tensor train decomposition  we refer to     for initializing the tt cores gi .  iv. experiments  in this section  we describe our dataset and all model  configurations. we performed experiments with three different  tensor decompositions  cp decomposition  tucker decomposition and tt decomposition  to compress our gru and also  the baseline gru. in the end  we report our experiment results  and finish this section with some discussions and conclusions.  our codes are available at https   github.com androstj   tensor rnn.    a. dataset  we evaluated our models with sequential modeling tasks.  we used a polyphonic music dataset      which contains    different datasets    nottingham  musedata  pianomidi and  jsb chorales. for each active note in all time step  we set  the value as    otherwise  . each dataset consists of at least    hours of polyphonic music and the total is      hours.  b. models  we evaluate several models in this paper  gru rnn   no compression   cp gru  weight compression via cp decomposition   tucker gru  weight compression via tucker  decomposition   tt gru      compressed weight with ttdecomposition . for each timestep  the input and output targets  are vectors of    binary value. the input vector is projected by  a linear layer with     hidden units  followed by leakyrelu       activation function. for the rnn model configurations   we enumerate all the details in the following list      gru    input size  n          hidden size  m           tensor based gru    input size  n          tensor input shape  n ..                      hidden size  m          tensor hidden shape  m ..                    a  cp gru    cp rank  r                          b  tucker gru    core  g    shape                                                                                                                                                                                            c  tt gru    tt ranks                                                                                                                      in this task  the training criterion is to minimize the negative  log likelihood  nll . in evaluation  we measured two different  scores  nll and accuracy  acc . for calculating the accuracy   we follow bay et al.      formulation   pt  t   t p t         acc   pt  t    t p t    fp t    fn t    where t p t   fp t   fn t  is the true positive  false positive  and false negative at time t.    dataset    are     boulanni icml        downloaded    from     http   www etud.iro.umontreal.ca      for training models  we use adam      algorithm for  our optimizer. to stabilize our training process  we clip our  gradient when the norm    w      . for fair comparisons  we  performed a grid search over learning rates   e     e     e     and dropout probabilities   .    .    .    .  . the best model  based on loss in validation set will be used for the test set  evaluation.  c. result and discussion    figure  . nll comparison between tt gru  tucker gru  and cp gru on  pianomidi test set    figure  . nll comparison between tt gru  tucker gru  and cp gru on  nottingham test set    figure   . nll comparison between tt gru  tucker gru  and cp gru  on musedata test set    v. related work    figure  . nll comparison between tt gru  tucker gru  and cp gru on  jsb chorales test set    we report results of our experiments in table. i. for the  baseline model  we choose standard gru rnn without any  compression on the weight matrices. for the comparison  between compressed models  cp gru  tucker gru and ttgru   we run each model with   different configurations and  varied the number of parameters ranged from      up to       . in figure       we plot the negative log likelihood   nll  score corresponding to the number of parameters for  each model. from our results  we observe that tt gru  performed better than tucker gru in every experiments with  similar number of parameters. in some datasets  e.g.  pianomidi  musedata  nottingham   cp gru has better results  compared to tucker gru and achieves similar performance   albeit slightly worse  as tt gru when the number of parameters are greater than     .    compressing neural network has been studied intensively  in the recent years. some works have been proposed to  reduce the number of bits needed to represent neural network  weight values. instead of using full precision    bit floating  points  courbariaux et al.      and gupta et al.      half  precision floating points is sufficient to represent the neural  network weights. later  courbariaux et al.     represented the  floating point numbers in the weight matrices into the binary  values and replace most arithmetic operations with bit wise  operations.   distilling  the knowledge from a larger model into a  smaller model is popularized by hinton et al.    . there  are several steps for knowledge distillation     train a large  neural network model with hard labels as the output target      using a trained large neural network  generate the soft  label from each input by taking the last softmax output with  higher temperature     train a smaller neural network with  the soft target as the output target. tang et al.      adapt  knowledge distillation by using large dnn soft targets to  assist the rnn model training. kim et al.      proposed  sequence level knowledge distillation for compressing neural  machine translation models.     table i  comparison between all models and their configurations based on the number of parameters  negative log likelihood and accuracy of polyphonic test set  model    config    gru  in      out      cp gru  in           out             tucker gru  in           out             tt gru  in           out             rank                       cores                                               tt rank                                                              nottingham  nll  acc    dataset  jsb  pianomidi  nll  acc  nll  acc    musedata  nll  acc                .         .      .        .       .        .       .        .                                        .     .     .     .     .        .      .      .      .      .       .     .     .     .     .        .      .      .      .      .       .     .     .     .     .        .      .      .      .      .       .     .     .     .     .        .      .      .      .      .                                         .     .     .     .     .        .      .      .      .      .       .     .     .     .     .        .      .      .      .      .       .     .     .     .     .        .      .      .      .      .       .     .     .     .     .        .      .      .      .      .                                        .     .     .     .     .        .      .      .      .      .       .     .     .     .     .        .      .      .      .      .       .     .     .     .     .        .      .      .      .      .       .     .     .     .     .        .      .      .      .      .      param    low rank approximation for representing the weight parameters in neural network has been studied by                . the  benefits from low rank approximation are reducing the number  of parameters as well as the running time during the training  and inference stage. novikov et al.     replaced the weight  matrix in the convolutional neural network  cnn  final layer  with tensor train      tt  format. tjandra et al.     and yang  et al.      utilized the tt format to represent the rnn weight  matrices. based on the empirical results  tt format are able  to reduce the number of parameters significantly and retain  the model performance at the same time. recent work from       used block decompositions to represent the rnn weight  matrices.  besides the tensor train  there are several tensor decomposition methods that are also popular such as cp and tucker  decomposition. however  both the cp and the tucker decomposition have not yet been explored for compressing the  rnn model. in this paper  we utilized the cp and the tucker  decomposition to compress rnn weight matrices. we also  compared the performances between the cp  tucker and tt  format by varying the number of parameters at the same task.  vi. conclusion  in this work  we presented some alternatives for compressing rnn parameters with tensor decomposition methods.  specifically  we utilized cp decomposition and tucker decomposition to represent the weight matrices. for the experiment   we run our experiment on polyphonic music dataset with  uncompressed gru model and three tensor based rnn models  cp gru  tucker gru and tt gru . we compare the  performance of between all tensor based rnns under various  number of parameters. based on our experiment results  we  conclude that tt gru has better performances compared to  other methods under the same number of parameters.    acknowledgment  part of this work was supported by jsps kakenhi grant  numbers jp  h      and jp  k     . the authors would  like to thank shiori yamaguchi for helpful discussions and  comments.  