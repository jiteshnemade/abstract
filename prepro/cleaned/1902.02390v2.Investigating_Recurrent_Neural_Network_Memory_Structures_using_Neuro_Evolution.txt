introduction  this work is motivated by a major open question in the  field of artificial neural network  ann  research  what neural memory structures appear to be optimal for time series  prediction  conducting such a search over ann structures  entails manual  primarily human driven labor and activity. as  more advances are made in the field  the number of possible  architecture variations and modifications explodes combinatorially. the growing space of architecture structures combined  with the limited  often simple heuristic local search that can  be conducted by human experts means that efficiently finding  architectures that generalize well while still maintaining low  parameter complexity  given that regularization is important  for most data sample sizes  makes for a nearly impossible   largely intractable search problem.  in natural biological systems  the process of evolution  over  long time spans  endows organisms with various inductive  biases that allow them to adapt and learn their environment  quickly and readily. it is thought that these inductive biases  are what provide infants the ability to quickly learn complex  pattern recognition detection functions with limited data across  various sensory modalities           such as in visual and  speech sensory domains. while artificial forms of evolution   such as the classical genetic algorithm      are significantly  simplified from the actual evolutionary process that creates    useful inductive biases to drive development and survival of  organisms at large  these optimization procedures offer the  chance to develop non human centered ways of generating  useful and even potentially optimal neural architectures.  while neuro evolution  applying evolutionary processes to  the development of anns  been used in searching the space  of feed forward and even convolutional architectures for tasks  involving static inputs           less effort has been put  into exploring the evolution of recurrent memory structures  that operate with complex time based data sequences and  uncovering what forms and structures the neuro evolutionary  process finds. insights extracted from the evolved architectures  can serve as inductive biases for subsequent research in neural  network design and development.  the evolution of recurrent neural networks  rnns  poses  significant challenges above beyond the already challenging  task of evolving feed forward and convolutional neural networks. first  rnns are more challenging to train due to  issues with exploding and vanishing gradients which occur  when unrolling a rnn over a long time series with the  backpropagation through time  bptt  algorithm     . due to  this issue  development of recurrent memory cells which can  preserve memory and long term dependencies while alleviating  the exploding and vanishing gradient problem has been an area  of significant study          . further  as in the case of timevarying data  input samples are strictly ordered in time and  thus induce long term dependencies that any useful stateful  adaptive process model must extract in order to generalize. the  complexity of time series tasks varies  entailing the prediction  of a particular variable of interest over time or even constructing a full generative model of all the available variables   perhaps additionally involving other non trivial tasks such as  data imputation.  due to these issues  rnns for time series data prediction  can require the use of a variety of memory cell structures  in addition to recurrent connections spanning different time  spans. to optimize within this large search space  evolutionary  exploration of augmenting memory models  examm  integrates an extensible collection of different complex memory  cell types with simple neuronal building blocks and recurrent  connections of varying time spans. examm was used to  evolve rnns with various cell types to predict time series     data from two real world  large scale data sets.  in total       examm runs were done using repeated kfold cross valdation  generating a large set of evolved rnns  to analyze statistically. results are particularly interesting  in that while they show allowing selection from any of  the available memory cell structures can provide very well  performing networks in many test cases  it does come at the  cost of reliability  how well the evolved rnns perform in  an average case . further  even small modifications to the  neuro evolutionary process  such as allowing simple neurons   can generally improve predictions  but they can also have  unintended consequences   in some examples it meant the  difference between a memory cell structure performing the  best as opposed to the worst. the authors hope that these  results can help guide not only the further development of  neuro evolution algorithms  but also help refine and inform  the development of new memory cell structures and human  designed rnns.    ii. e volving r ecurrent n etworks  several methods for evolving nn topologies along with  weights have been searched and deployed  with neuroevolution of augmenting topologies  neat       perhaps being the most well known. examm differs from neat in  that it includes more advanced node level mutations  see  section iii a    and utilizes lamarckian weight initialization   see section iii b  along with backpropagation to evolve  the weights as opposed to a simpler less efficient genetic  strategy. additionally  it has been developed with large scale  concurrency in mind  and utilizes an asynchronous steady state  approach which has been shown to facilitate scalability to  potentially millions of compute nodes     .  other recent work by rawal and miikkulainen has investigated an information maximization objective      strategy  to evolve rnns. this strategy essentially utilizes neat with  long short term memory  lstm  neurons instead of regular  neurons. examm provides a more in depth study of the  performance of possible recurrent cell types as it examines  both simple neurons and four other cell structures beyond  lstms. rawal and miikulainen have also utilized tree based  encoding      to evolve recurrent cellular structures within  fixed architectures built of layers of the evolved cell types.  combining this evolution of cell structure along with the  overall rnn structure stands as interesting future work.  ant colony optimization  aco  has also been investigated  as a way to select which connections should be utilized in  rnns and lstm rnns by desell and elsaid          . in  particular  this aco approach was shown to reduce the number  of trainable connections in half while providing a significant  improvement in predictions of engine vibration     . however   this approach works within a fixed rnn architecture and  cannot evolve an overall rnn structure.    iii. e volutionary e x ploration of augmenting  m emory m odels  the examm algorithm presented in this work expands on  an earlier algorithm  exalt      which can evolve rnns  with either simple neurons or lstm cells. it further refines exalt s mutation operations to reduce hyperparameters  using statistical information from parental rnn genomes.  also  exalt only used a single steady state population  and  examm expands on this to use islands  which have been  shown by alba and tomassini to greatly improve performance  of distributed evolutionary algorithms  potentially providing  superlinear speedup     . a master process maintains each  population of islands  and generates new rnns form islands  in a round robin manner to be trained upon request by workers.  when a worker completes training a rnn  it is inserted  into the island it was generated from if its fitness  mean  squared error on the test data  is better than the worst in the  island  and then the worst rnn in the island is removed.  this asynchrony is particularly important as the generated  rnns will have different architectures  each taking a different  amount of time to train. by having a master process control  the population  workers can complete the training of the  generated rnns at whatever speed they can and the algorithm  is naturally load balanced. this allows examm to scale  to the number of available processors  having a population  size independent of processor availability  unlike synchronous  parallel evolutionary strategies. the examm codebase has  a multithreaded implementation for multicore cpus as well  as an mpi      implementation for use on high performance  computing resources.  a. mutation and recombination operations  rnns are evolved with edge level operations  as done in  neat  as well as with new high level node mutations as in  exalt and exact. whereas neat only requires innovation  numbers for new edges  examm requires innovation numbers  for both new nodes  new edges and new recurrent edges. the  master process keeps track of all node  edge and recurrent  edge innovations made  which are required to perform the  crossover operation in linear time without a graph matching  algorithm. figures   and   display a visual walkthrough of all  the mutation operations used by examm. nodes and edges  selected to be modified are highlighted  and then new elements  to the rnn are shown in green. edge innovation numbers are  not shown for clarity. enabled edges are in black  disabled  edges are in grey.  it should be noted that for the following operations  whenever an edge is added  unless otherwise specified  it is probabilistically selected to be a recurrent connection with the  re    where nre  following recurrent probability  p   nf fn n  re  is the number of enabled recurrent edges and nf f is the  number of enabled feed forward edges in the parent rnn. a  recurrent connection will go back a randomly selected number  of time steps with bound specified as search parameters  in this  work    to    time steps   allowing for recurrent connections  of varying time spans. any newly created node is selected      a  the edge between input   and output   is selected to be split. a new  node with innovation number  in    is created.     b  input   and node in   are selected to have an edge between them  added.     c  the edge between input   and output   is enabled.     d  a recurrent edge is added between output   and node in       e  the edge between input   and output   is disabled.    fig.    edge mutation operations.    uniformly at random as a simple neuron or from the memory  cell types specified by the examm run input parameters.     edge mutations    a  disable edge  this operation randomly selects an  enabled edge or recurrent edge in a rnn genome and disables  it so that it is not used. the edge remains in the genome.  as the disable edge operation can potentially make an output  node unreachable  after all mutation operations have been  performed to generate a child rnn genome  if any output  node is unreachable that rnn genome is discarded without  training.    b  enable edge  if there are any disabled edges or  recurrent edges in the rnn genome  this operation selects  a disabled edge or recurrent edge at random and enables it.  c  split edge  this operation selects an enabled edge at  random and disables it. it creates a new node and two new  edges  and connects the input node of the split edge to the  new node  and the new node to the output node of the split  edge. if the split edge was recurrent  the new edges will also  be recurrent  with the same time skip   otherwise they will be  feed forward.  d  add edge  this operation selects two nodes n  and  n  within the rnn genome at random  such that depthn     depthn  and such that there is not already an edge between  those nodes in this rnn genome. then it adds an edge from  n  to n  .  e  add recurrent edge  this operation selects two nodes  n  and n  within the rnn genome at random and then adds  a recurrent edge from n  to n    selecting a time span as  described before. the same two nodes can be connected with  multiple recurrent connections  each spanning different times   however it will not create a duplicate recurrent connection with  the same time span.     node mutations    a  disable node  this operation selects a random nonoutput node and disables it along with all of its incoming and  outgoing edges. note that this allows for input nodes to be  dropped out  which can be useful when it is not previously  known which input parameters are correlated to the output.  b  enable node  this operation selects a random disabled node and enables it along with all of its incoming and  outgoing edges.  c  add node  this operation selects a random depth  between   and    noninclusive. given that the input node is  always depth   and the output nodes are always depth    this  depth will split the rnn in two. a new node is created at  that depth  and the number of input and output edges and  recurrent edges are generated using normal distributions with  mean and variances equal to the mean and variances for the  of input output edges and recurrent edges of all nodes in the  parent rnn.  d  split node  this operation takes one non input  nonoutput node at random and splits it. this node is disabled   as in the disable node operation  and two new nodes are  created at the same depth as their parent. at least one input and  one output edge are assigned to each of the new nodes  of a  duplicate type from the parent  with the others being assigned  randomly  ensuring that the newly created nodes have both  inputs and outputs.  e  merge node  this operation takes two non input  nonoutput nodes at random and combines them. selected nodes  are disabled  as in the disable node operation  and a new node  is created at depth equal to the average of its parents. this  node is connected to the inputs and outputs of its parents with  a duplicate type from the parent  as input edges connected to  lower depth nodes and output edges connect to greater depth  nodes.      a  a node with in   is selected to be added at a depth between the inputs    node in  . edges are randomly added to input   and    and node in    and output  .     d  node in   is selected to be enabled  along with all its input and output  edges.     b  node in   is selected to be split. it is disabled with its input output  edges. it is split into nodes in   and    which get half the inputs. both  have an output edge to output   since there was only one output from  node in  .   e  node in   is selected to be disabled  along with all its input and  output edges.    fig.    node mutation operations  continued .    b. lamarckian weight initialization     c  node in   and   are selected for a merger  input output edges are  disabled . node in   is created with edges between all their inputs outputs.    fig.    node mutation operations.       other operations    a  crossover  creates a child rnn using all reachable  nodes and edges from two parents. a node or edge is reachable  if there is a path of enabled nodes and edges from an input  node to it as well as a path of enabled nodes and edges from  it to an output node  i.e.  a node or edge is reachable if it  actually affects the rnn. crossover can be done either within  an island  intra island  or between islands  inter island . interisland crossover selects a random parent in the target island   and the best rnn from the other islands.  b  clone  creates a copy of the parent genome  initialized  to the same weights. this allows a particular genome to  continue training in cases where further training may be more  beneficial than performing a mutation or crossover.    for rnns generated during population initialization  the  weights are initialized uniformly at random between   .   and  . . biases and weights for new nodes and edges are  initialized from a normal distribution based on the average   mu  and variance        of the parents  weights. however   rnns generated through mutation or crossover re use parental  weights  allowing the rnns to train from where the parents  are left off  i.e.   lamarckian  weight initialization.  during crossover  in the case where an edge or node exists in  both parents  the child weights are generated by recombining  the parents  weights. given a random number   .     r      .   a child s weight wc is set to wc   r wp    wp      wp     where wp  is the weight from the more fit parent  and wp   is the weight from the less fit parent. this allows the child  weights to be set along a gradient calculated from the weights  of the two parents.  this weight initialization strategy is particularly important  as newly generated rnns do not need to be completely  retrained from scratch. in fact  the rnns only need to be  trained for a few epochs to investigate the benefits of newly  added structures. in this work  the generated rnns are only  trained for    epochs  see section v b   where training a static  rnn structure from scratch may require hundreds or even  thousands of epochs.     c. a collection of memory cells  node types can range quite a bit in terms of complexity  and their design largely governs the form of the underlying memory structure. simple neurons can be evolved into  generalized versions of traditional elman and jordan neurons  as examm adds recurent connections. below describes how  simple neurons can evolve to generalized elman and jordan  neurons  as well as complex cells such as the delta rnn   the minimally gated unit  mgu        the update gated rnn   ugrnn        the gated recurrent unit  gru        and  long short term memory  lstm      .  elman  jordan and arbitrary recurrent connections  in  examm  simple neurons are represented as potential recurent  neurons with both recurrent and feed forward inputs. i is the  set of all nodes with a feed forward connection to simple  neuron j while r is the set of all nodes with a recurrent  connection to simple neuron j. at time step t  the output is  a weighted summation of all feed forward inputs  where wij  is the feed forward weight connecting node i to node j  plus  a weighted summation of recurrent inputs  where vrjk is the  recurrent weight from node r at time step t   k to the node  j  where k is the time span of the recurrent connection. thus  the state function  s for a computing a simple neuron is      x  x  sj  t     s  wij   si  t     vrjk   sr  t   k   i i    r r k    the overall state is a linear combination of the projected input  and an affine transformation of the vector summary of the past.  the post activation function   s      can be any differentiable  element wise function  however for the purposes of this work  it was limited to the hyperbolic tangent   v    tanh v      e  v        e  v      .  elman rnns      are the simplest of all rnns  where  recurrent nodes connections to themselves and potentailly all  other hidden nodes in the same layer. jordan rnns       have recurrent connections from output node s  to hidden  node s . examm can evolve elman connections when the  add recurrent edge mutation adds a recurrent edge from a  simple neuron back to itself  and jordan connections when it  adds a recurrent edge from an output to a simple neuron. it  generalizes these structures by allowing varying time spans   jordan and elmann network traditionally span only one time  step  and arbitrary recurrent connections between from any  pair of simple neurons or cells  i.e.  cross layer recurrent  connections.  the delta rnn    rnn  cell  for models more complex  than the simple rnn model  we looked to derive a vast set of  gated neural architectures unified under a recent framework  known as the differential state framework  dsf      . a  dsf neural model is essentially a composition of an inner  function used to compute state proposals and an outer  mixing  function used to decide how much of a state proposal  is incorporated into a slower moving state  i.e.  the longer  term memory. these models are better equipped to handle    the    bias is omitted for clarity and simplicity of presentation.    the vanishing gradient problem induced by the difficulty of  learning long term dependencies over sequences     . other  models that can be derived under the dsf include the lstm   the gru  and the mgu     . one of the simplest dsf models  is the   rnn  which has been shown to perform competitively  with more complex memory models in problems ranging from  language modeling            to image decoding     . with                bj   m  as learnable coefficient scalars  the   rnn  state is defined as   x  x  ew  wij   si  t     vrjk   sr  t   k   evj   m   sj  t       j    i i    r r k    d j       evj   ew  j      d j        evj        ew  j      sej  t     s  d j   d j       rj     ew  j   bj      sj  t     s       rj     sej  t    rj   sj  t         the long short term memory  lstm   the lstm       is one of the most commonly used gated neural model when  modeling sequential data. the original motivation behind the  lstm was to implement the  constant error carousal  in order  to mitigate the problem of vanishing gradients. this means  that long term memory can be explicitly represented with a  separate cell state ct .  the lstm state function  without extensions  such as   peephole  connections  is implemented as follows   x f  x f  fj       wij   si  t     vrjk   sr  t   k    i i    r r k    x  x  i  i  ij       wij    si  t     vrjk    sr  t   k    i i    e  cj   tanh     r r k    x    c  wij      si  t       i i    x    c  vijk    sr  t   k      r r k    x  x  o  o  oj       wij    si  t     vijk    sr  t   k    i i    r r k    cj  t    fj  t    cj  t        ij   e  cj      sj  t    oj    s  cj  t      where we depict the sharing of the transformation function s  output across the forget  ft    input  it    cell state proposal  e  ct     and output  ot   gates. the lstm is by far the most parameterhungry of the dsf models we explore.  the gated recurrent unit  gru   the gated recurrent  unit  gru        can be viewed as an early attempt to simplify  the lstm. among the changes made  the model fuses the  lstm input and forgets gates into a single gate  and merges  the cell state and hidden state back together. the state function  based on the gru is calculated using the following equations   x  x  z  z  zj       wij    si  t     vijk    sr  t   k         i i    r r k    x  x  r  r  rj       wij    si  t     vijk    sr  t   k    i i           r r k    x  x  s  s  sej  t     s    wij    si  t     vijk     rj   sr  t   k        i i    r r k    sj  t    zj   sej        zj     sj  t       noting that  s  v    tanh v .            the minimally gated unit  mgu   the mgu model is very  similar in structure to the gru  reducing number of required  parameters by merging its reset and update gates into a single  forget gate     . the state computation proceeds as follows   x f  x f  fj       wij   si  t     vijk   sr  t   k         i i    sej  t     s      r r k    x    s  wij    i i    x      si  t       s  vijk     fj   sr  t   k          r r k    sj  t    zj   sej        zj     sj  t     .           the update gated rnn  ugrnn   the ugrnn       updates are defined in the following manner   x  x  c  c  cj    s    wij    si  t     vijk    sr  t   k         i i    r r k    x g  x g  gj       wij   si  t     vijk   sr  t   k    i i           r r k    sj  t    gj   sj  t             gj     cj .            the ugrnn  though more expensive than the   rnn  is a  simple model  essentially working like an elman rnn with a  single update gate. this extra gate decides whether a hidden  state is carried over from the previous time step or if the state  should be updated.  iv. o pen data and r eproducibility  this work utilizes two data sets to benchmark the memory  cells and rnns evolved by exam. the first comes from a  selection of    flights worth of data from the national general  aviation flight information database  ngafid       and the  other comes from a coal fired power plant  which has requested to remain anonymous . both datasets are multivariate      and    parameters  respectively   non seasonal  and the  parameter recordings are not independent. furthermore  they  are very long   the aviation time series range from   to   hours  worth of per second data while the power plant data consists  of    days worth of per minute readings. these data sets are  provided openly through the examm github repository    in  part for reproducibility  but also to provide a valuable resource  to the field. to the authors  knowledge  real world time series  data sets of this size and at this scale are not freely available.  rpm  rotations per minute  and pitch were selected as  prediction parameters from the aviation data since rpm is  a product of engine activity  with other engine related parameters being correlated  and since pitch is directly influenced by  pilot controls  making it particularly challenging to predict. for  the coal plant data  main flame intensity and supplementary  fuel flow were selected as parameters of interest. similar to  the choices from the ngafid data  main flame intensityy is  mostly a product of conditions within the  coal  burner  while  supplementary fuel flow is more directly controlled by human  operators.    https   github.com travisdesell exact    a. aviation flight recorder data  with permission  data from    flights was extracted from  the ngafid. each of the    flight data files last over an hour   and consist of per second data recordings from    parameters      altitude above ground level  altagl      engine   cylinder head temperature    e  cht       engine   cylinder head temperature    e  cht       engine   cylinder head temperature    e  cht       engine   cylinder head temperature    e  cht       engine   exhaust gas temperature    e  egt       engine   exhaust gas temperature    e  egt       engine   exhaust gas temperature    e  egt       engine   exhaust gas temperature    e  egt        engine   oil pressure  e  oilp       engine   oil temperature  e  oilt       engine   rotations per minute  e  rpm       fuel quantity left  fqtyl       fuel quantity right  fqtyr       gndspd   ground speed  gndspd       indicated air speed  ias       lateral acceleration  latac       normal acceleration  normac       outside air temperature  oat       pitch      roll      true airspeed  tas       voltage    volt        voltage    volt        vertical speed  vspd       vertical speed gs  vspdg   these files had identifying information  fleet identifier  tail  number  date and time  and latitude longitude coordinates   which was removed to protect the identify of the pilots. the  data is provided unnormalized.  for this work  two parameters were selected as prediction  targets  rpm and pitch. these are interesting as the first   rpm  is a product of engine activity  with other engine related  parameters being correlated  while pitch is most directly  influenced by pilot controls  making it particularly challenging  to predict.  b. coal fired power plant data  this data set consists of    days of per minute data readings  extracted from    of the plant s burners. each of these    data  files has    parameters of time series data      conditioner inlet temp     conditioner outlet temp     coal feeder rate     primary air flow     primary air split     system secondary air flow total     secondary air flow     secondary air split     tertiary air split      total combined air flow         supplementary fuel flow      main flame intensity  in order to protect the confidentiality of the power plant  which provided the data  along with any sensitive data elements  all identifying data has been scrubbed from the data sets   such as dates  times  locations and facility names . further   the data has been pre normalized between   and   as a further  precaution. so while the data cannot be reverse engineered to  identify the originating power plant or actual parameter values    it still is an extremely valuable test data set for times series  data prediction as it consists of real world data from a highly  complex system with interdependent data streams.  for this data set  two of the parameters were of key interest  for time series data prediction  main flame intensity and  supplementary fuel flow. similar to the choices from the  ngafid data  main flame intensity is mostly a product of  conditions within the burner  while supplementary fuel flow  is more directly controlled by humans.  v. r esults  a. computing environment  results were gathered using university research computing  systems. compute nodes utilized ranged between    core  .   ghz intel r xeon r cpu e       v      core  .  ghz amd  opterontm processor      se and    core  .  ghz amd  opterontm processor      ses  which was unavoidable due  to cluster scheduling policies. all compute nodes ran redhat  enterprise linux  .  . this did result in some variation in  performance  however discrepancies in timing were overcome  by averaging over multiple runs in aggregate.  b. experimental design  to better understand how the different memory cells performed in time series data prediction  multiple examm runs  were conducted that allowed different types of memory cells.  the first set of runs     only any added cells of only a single   particular memory cell type  i.e.  either a   rnn  gru   lstm  mgu  or ugrnn. the next set of runs     was nearly  identical  except these allowed nodes to be simple neurons in  addition to each particular memory cell type  such runs are  appended with a  simple in the result tables . one final version  was run where all cell types and simple neurons were allowed   resulting in    different examm run types  such runs are  labeled as all in the result tables . these different types of  runs were done for each of the four prediction parameters   rpm  pitch  main flame intensity  and supplementary fuel  flow . k fold cross validation was done for each prediction  parameter  with a fold size of  . this resulted in   folds for  the ngafid data  as it had    flight data files   and   folds  for the coal plant data  as it has    burner data files . each  fold and examm setting run was repeated    times. in total   each of the    examm run types was done     times      times for the ngafid data k fold validation and    times for  the coal data k fold validation   for a total of        separate  runs.    all neural networks were trained with backpropagation and  stochastic gradient descent  sgd  using the same hyperparameters. sgd was run with a learning rate      .      utilizing nesterov momentum with mu    . . no dropout  regularization was used since it has been shown in other  work to reduce performance when training rnns for time  series prediction     . to prevent exploding gradients  gradient  clipping  as described by pascanu et al.       was used  when the norm of the gradient was above a threshold of   . . to improve performance for vanishing gradients  gradient  boosting  the opposite of clipping  was used when the norm  of the gradient was below a threshold of  .  . the forget gate  bias of the lstm cells had  .  added to it as this has been  shown to yield significant improvements in training time by  jozefowicz et al.       otherwise weights were initialized as  described in section iii b.  each examm run consisted of    islands  each with a  population size of    and new rnns were generated via intraisland crossover  at       mutation  at       and inter island  crossover at      . all mutation operations  described in section iii  except for split edge were utilized  as split edge can  be recreated with the add node and disable edge operations.  the    utilized mutation operations were performed each with  a uniform     chance. each examm run generated       rnns  with each rnn being trained for    epochs. these  runs were performed utilizing    processors in parallel  and  on average required approximately  .  compute hours. in  total  these results come from training over             rnns   requiring          cpu hours of compute time.  c. evolved rnn performance  table i shows aggregated results for each of the    or     examm runs done    or   folds  each with    repeats  that  allowed only one of each memory cell type  along with the  examm runs that allowed for added nodes to be of all node  types  i.e.  all . table ii further shows the aggregated results  for runs which allowed both simple neurons and one particular  cell type  i.e.   simple .  these tables present the minimum  average  and maximum  mean squared error  mse  on average across the runs. table i  also shows the minimum  average  and maximum number of  hidden nodes  which would be entirely of the one memory  cell type  or of any memory cell type or simple neurons in  the case of the all runs  as well as how the the number  of nodes correlate to the mse. similar statistics are shown  for the numbers of feed forward edges and the numbers  of recurrent edges. note that as a lower mse is better  a  negative correlation means that having more nodes or edges  was correlated to lower mse. table ii also divides hidden  nodes into counts for simple neurons and number of memory  cell nodes for a certain run type. top   best models  avg   scores are shown in bold.  the best found mse scores across the four prediction  parameters had a wide range  which could be expected due to  varying complexities. as such  to properly rank performance  of different run types a metric was needed. table iv orders     all    rnn  gru  lstm  mgu  ugrnn    flame intensity  edges  max  corr.    min    mse  avg    max    min    avg     .            .            .            .            .            .              .           .           .           .           .           .             .           .           .          .           .           .                                                               min    mse  avg    max    min    avg     .     e      .     e      .     e      .     e      .     e      .     e        .            .            .            .            .           .              .            .            .            .            .            .                                                                 min    mse  avg    max    min     .           .           .           .           .           .             .           .          .           .           .           .             .          .          .          .         .          .                                                                                         min    mse  avg    max    min    avg     .           .           .           .           .           .             .           .           .           .          .           .            .           .           .           .           .           .                                                                run type    run type  all    rnn  gru  lstm  mgu  ugrnn                                .       .       .       .       .       .                           fuel flow  edges  max  corr.    min                              rec. edges  avg  max    min     .       .        .       .        .      .                            corr.    min     .      .         .      .      .       .                           edges  max    corr.    min                                .      .        .        .      .        .                            corr.    min                                .       .       .       .       .       .                                 rec. edges  avg  max    corr.    min                               .      .       .       .       .       .                               rec. edges  avg  max    corr.    min      .       .       .       .       .       .                                 corr.    min      .       .       .       .       .       .                                  .    .    .    .    .    .      .    .    .    .    .    .     hidden nodes  avg  max                                                      hidden nodes  avg  max                            corr.    .       .      .       .       .       .                                 corr.   .       .       .      .        .      .       rpm  run type  all    rnn  gru  lstm  mgu  ugrnn    edges  avg  max     .    .    .    .    .    .                               hidden nodes  avg  max                                                      corr.   .        .         .       .     .       .       pitch  run type  all    rnn  gru  lstm  mgu  ugrnn    rec. edges  avg  max      .       .    .                                 hidden nodes  avg  max                                                      corr.    .      .       .       .      .        .        table i  statistics for rnns evolved with individual memory cells and rnns evolved with all memory types.      rnn simple  gru simple  lstm simple  mgu simple  ugrnn simple    flame intensity  rec. edges  min  avg  max    min    mse  avg    max    min    avg    edges  max    corr.     .           .           .           .           .             .          .          .          .          .            .          .          .          .         .                                                                               .      .      .      .      .                       min    mse  avg    max    min    avg    edges  max    corr.    min     .    e      .    e      .    e      .    e      .    e        .           .           .           .           .             .           .           .           .           .                                                                              .      .      .       .       .                       min    mse  avg    max    min    avg    edges  max    corr.    min     .          .          .          .          .            .          .          .          .          .            .         .         .        .         .                                                                              .      .       .     .       .                       min    mse  avg    max    min    avg    edges  max    corr.    min     .          .          .          .          .            .          .          .          .          .            .          .          .         .          .                                                                               .     .       .      .     .                      run type    corr.    min                            .       .      .       .       .                        rec. edges  avg  max    corr.    min                            .      .         .       .       .                        rec. edges  avg  max    corr.    min                            .       .       .     .       .                        rec. edges  avg  max    corr.    min      .      .        .       .       .                         .    .    .    .    .     memory cells  avg  max   .    .    .    .    .                      corr.    min      .       .       .      .         .                        corr.    min      .     .       .        .       .                         corr.    min     .        .        .       .        .                          corr.    min      .        .       .       .       .                        simple neurons  avg  max   .    .    .       .                      corr.    .       .      .        .       .       fuel flow  run type    rnn simple  gru simple  lstm simple  mgu simple  ugrnn simple     .       .    .    .     memory cells  avg  max   .    .    .    .    .                      simple neurons  avg  max   .    .    .    .    .                      corr.    .        .       .        .        .       rpm  run type    rnn simple  gru simple  lstm simple  mgu simple  ugrnn simple     .    .    .    .    .     memory cells  avg  max   .     .       .     .                      simple neurons  avg  max   .    .    .    .    .                      corr.    .      .       .       .       .        pitch  run type    rnn simple  gru simple  lstm simple  mgu simple  ugrnn simple     .    .    .    .    .                        memory cells  avg  max   .     .     .    .     .                       simple neurons  avg  max   .       .     .                          corr.    .       .         .      .      .       table ii  statistics for rnns evolved with simple neurons and memory cells.    run type  flame  fuel flow  pitch  rpm    min    avg    simple  max    corr    min    avg    lstm  max    corr    min                   .    .    .    .                    .      .        .       .                     .    .    .    .                     .     .      .      .                    ugrnn  avg  max   .    .    .    .                   corr    min      .      .      .       .                       rnn  avg  max   .    .    .    .                   corr    min    avg    mgu  max    corr    min    avg    gru  max    corr      .      .      .      .                      .    .    .    .                    .      .       .      .                     .    .    .    .                     .     .       .      .      table iii  hidden node counts and correlations to mean square error for examm runs using all cell types.     best case    rnn    rnn simple  all  ugrnn simple  lstm  lstm simple  gru  mgu simple  ugrnn  mgu  gru simple      .         .         .         .         .         .         .        .        .        .       .        flame intensity  avg. case    rnn simple    .      lstm simple    .      mgu simple    .         rnn    .        gru   .       ugrnn   .       gru simple   .       all   .       ugrnn simple   .       lstm   .      mgu   .        worst case  all    .      lstm simple    .      mgu simple    .      mgu    .       gru    .         rnn   .        lstm   .       delta simple   .       ugrnn   .      gru simple   .       ugrnn simple   .        best case  all  ugrnn simple  lstm  ugrnn    rnn  mgu simple    rnn simple  lstm simple  gru simple  gru  mgu      .         .        .         .         .         .         .         .       .        .       .        fuel flow  avg. case  lstm    .        rnn simple    .      mgu simple    .      gru simple    .       lstm simple    .       gru    .       all   .       mgu   .         rnn   .       ugrnn simple   .      ugrnn   .        worst case  lstm simple    .      lstm    .        rnn simple    .     mgu    .       mgu simple    .      gru simple    .       gru   .        all   .       ugrnn simple   .         rnn   .       ugrnn   .        best case  gru  mgu simple    rnn  lstm simple    rnn simple  ugrnn  lstm  mgu  ugrnn simple  gru simple  all      .       .        .        .         .         .         .        .        .       .      .        rpm  avg. case  lstm simple  mgu simple    rnn  gru    rnn simple  gru simple  ugrnn  all  mgu  lstm  ugrnn simple      .        .        .        .        .         .         .        .        .        .       .       worst case  gru    .      lstm simple    .        rnn simple    .       mgu simple    .       ugrnn    .       ugrnn simple    .       gru simple   .         rnn   .       all   .      lstm   .      mgu   .        best case  mgu simple  all  lstm simple  lstm  gru simple  ugrnn  gru  mgu    rnn    rnn simple  ugrnn simple      .        .        .        .        .         .        .        .       .       .       .        pitch  avg. case  ugrnn simple  gru simple    rnn simple  lstm simple  gru  mgu  delta  mgu simple  ugrnn  lstm  all      .        .         .         .         .        .          .         .        .        .        .        worst case  gru    .      ugrnn simple    .         rnn simple    .       lstm simple    .         rnn    .       lstm    .       ugrnn    .       mgu    .          all   .       mgu simple   .       gru simple   .        best case  mgu simple  lstm simple  lstm    rnn  all  ugrnn    rnn simple  gru  ugrnn simple  gru simple  mgu      .         .         .         .         .         .         .         .         .        .       .        overall combined  avg. case  lstm simple    .        rnn simple    .       mgu simple    .       gru    .      gru simple    .         rnn    .        ugrnn simple   .       lstm   .       mgu   .       ugrnn   .       all   .         worst case  lstm simple    .       gru    .        rnn simple    .       mgu simple    .       all   .        lstm   .       mgu   .       ugrnn simple   .         rnn   .       ugrnn   .       gru simple   .         table iv  examm run types prediction error ranked by  standard deviation from mean.  the    different run types by how many standard deviations  the results were from the mean for each prediction parameter.  it also provides combined rankings  averaging the deviation  from the mean across the four prediction parameters. each  of these tables are ordered from best to worst   a negative  deviation from the mean is that many standard deviations less  than the average mse  and lower mse is better.  the results of these experiments led to some interesting  findings which the authors feel can help inform further development of neuro evolution algorithms as well as rnn memory  cells. many of these findings can also serve as warnings to  those looking to train well performing rnns for time series  prediction. we summarize the main takeaways from these  results as follows   a  no memory structure was truly the best   in the  overall rankings  table iv   the   rnn  lstm  and mgu  cells seemed to have the highest rankings for best overall  performance as well as average and worst case performance   with gru cells being slightly behind. while ugrnn nodes    did not do so well in the total ranking  it should be noted that  when coupled with simple neurons  they did perform  nd best  for the best case for fuel flow  and were the best for the average  case in predicting pitch. this highlights the importance of  testing a wide selection of memory cell types when developing  rnns  as there is no free lunch in machine learning   each  memory cell type had its own strengths and weaknesses. it is  valuable to note that one of the simplest memory cell types   i.e.  the   rnn  performs consistently and competitively with  the more complicated  multi gate lstm  at the top of the  rankings   which is consistent with a growing body of results                  .  b  adding simple neurons generally helped   with some  notable exceptions   when looking at the overall rankings   when simple neurons were added as an option to the neuroevolution process  the networks performed better. the only  exception to this was gru cells  which tended to perform  worse when simple neurons were allowed. these results may  indicate that these memory cells lack the capability to track  some kinds of dependencies which the additional simple  neurons make up for  this means that there is potentially room  to improve these cell structures to capture whatever the simple  neurons were providing. further examination of why the gru  cells performed worse with simple neurons compared to the  other memory cells may help determine the cause of this and  makes for an interesting direction for future rnn work.  another very interesting finding was that utilizing simple  neurons with mgu cells resulted in a dramatic improvement   bringing them from some of the worst rankings to some of  the best rankings  e.g.  in the overall rankings for best found  networks  mgu cells alone performed the worst while mgu  and simple neurons performed the best . other cell types   lstm and   rnn  showed less of an improvement. this  finding may highlight that the mgu cells could stand to benefit  from further development. this should serve as a warning to  others developing neuro evolution algorithms  in that even the  rather simple change of allowing simple neurons can result  in significant changes in rnn predictive ability. selection  of node and cell types for neuro evolution should be done  carefully.  c  allowing all memory cells has risks and benefits   the  authors had hoped that allowing the neuro evolution process  to simply select from all of the memory cell types would  allow it to find and utilize whichever cells were most suited to  the prediction problem. unfortunately  this was not always the  case. while using all memory cell types generally performed  better than the mean on the best case  in the average and  worst cases  it performed worse than the mean. this was most  likely due to the fact that whenever a node was added to  the network it could have been from any of the   types  so  choosing from them uniformly at random ended up sometimes  selecting the nodes not best to the task  as they improved the  population  but not as well as another memory cell choice  could have done . these results are backed up by table iii   which shows the correlations between node types and mse   again  a negative correlation means more of that cell type     resulted in a lower better mse   as well as the min  average  and max memory cell counts among the networks found by  examm.  that being said  allowing all cell types did find the best  networks in the case of fuel flow   nd best in the case of  pitch  and  rd best in the case of flame intensity  which  is impressive given the larger search space for the neuroevolution process. we expect to further improve this result  by dynamically adapting the rates at which memory cells are  generated in part based on how well they have improved the  population in the past  with caveats described in the next point     this stands as future work which can make examm an even  stronger option for generating rnns  especially in the average  and worse cases where they did not fare as well.  d  larger networks tended to perform better  yet memory  cell count correlation to mse was not a great indicator  of which cells performed the best   this last point raises  some significant challenges for developing neuro evolution  algorithms. when looking at table iii and examining the  memory cells types most correlated to improved performance  against the memory cell types most frequently selected by  examm  meant that examm was not selecting cell types  that would produce the best performing rnns. this may due  to the fact that  in some cases  an rnn with a small number  of well trained memory cells was sufficient to yield good  predictions  and adding more cells to the network only served  to confuse the predictions.  the implications of this are two fold      running a neuroevolution strategy allowing all memory cell types and then  utilizing counts or correlations to select a single memory cell  type for future runs may not produce the best results  and      dynamically tuning which memory cells are selected by a  neuro evolution strategy is more challenging since the process  may not select the best cell types  e.g.  when the network  already has enough memory cells    so this would at least  need to be coupled with another strategy to determine when  the network is  big enough .  vi. c onclusions and f uture w ork  this work introduced a new neuro evolution algorithm   evolutionary exploration of augmenting memory models   examm   for evolving recurrent neural architectures by  directly incorporating powerful memory cells such as the    rnn  mgu  gru  lstm and ugrnn units into the  evolutionary process. examm was evaluated on the task of  predicting   different parameters from two large  real world  time series datasets. by using repeated k fold cross validation  and high performance computing  enough rnns were evolved  to be rigorously analyzed   a methodology the authors think  should be highlighted as novel. instead of utilizing them to outperform other algorithms on benchmarks  neuro evolutionary  processes can be used as selection methodologies  providing  deeper insights into what neural structures perform best on  certain tasks.  key findings from this work show that a neuro evolution  strategy that selects from a wide number of memory cell    structures can yield performant architectures. however  it does  so at the expense of reliability in the average and worst cases.  furthermore  a simple modification to the evolutionary process  i.e.  allowing simple neurons  can have dramatic effects  on network performance. in general  while this largely benefits  most memory cells  outlier cases showed wide swings from  worst to best and best to worst performance. the authors hope  that these results will guide future memory cell development   as the addition of simple neurons dramatically improved mgu  performance  but also decreased gru performance. understanding cases like that of the gru could yield improvements  in cell design. results showed that cell selection does not  necessarily correlate well to the best cell types for a particular  problem  partly due to the fact that good cells may not  necessarily require a large network. these results should serve  as cautionary information for future development of neuroevolution algorithms.  this paper opens up many avenues of future work. this  includes extending examm s search process to allow cellular  elements of the underlying neural model to be evolved  as  in rawal and miikulainen        evolving over a large set  of post activation functions  allowing for stochastic operations  e.g.  bernoulli sampling  and incorporating operators  such as convolution  to handle video sequences time series   or its simple approximation  the perturbative operator     .  additional future work will involve various hyperparameter  optimization strategies to dynamically determine rnn training  metaparameters as well as what probabilities examm uses to  choose memory cell structures and what probabilities it uses  for the mutation and recombination operators. lastly  implementing even larger scale mutation operations  such as multinode layer mutations could potentially speed up examm s  neuro evolution process even further.     