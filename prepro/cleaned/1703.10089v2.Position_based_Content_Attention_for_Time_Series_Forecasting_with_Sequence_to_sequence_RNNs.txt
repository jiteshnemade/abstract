introduction    predicting future values of temporal variables is termed as time series forecasting and  has applications in a variety of fields  as finance  economics  meteorology  or customer  support center operations. time series often display pseudo periods  i.e. time intervals  at which there is a strong correlation  positive or negative  between the values of the  times series. in a forecasting scenario  the pseudo periods correspond to the difference  between the positions of the output being predicted and specific inputs. pseudo periods  may be due to seasonality or to the patterns underlying the activities measured.  a considerable number of stochastic     and machine learning based     approaches  have been proposed for this problem. a particular class of approaches that has recently  received much attention for modelling sequences is based on sequence to sequence recurrent neural networks  rnns       hereafter referred to as seq rnns. in order to  capture pseudo periods in seq rnns  one needs a memory of the input sequence  i.e. a  mechanism to reuse specific  representations of  input values to predict output values.  as the input sequence is usually longer than the pseudo periods underlying the time  series  longer term memories that store information pertaining to past input sequences   as described in e.g.            are not required. a particular model of interest here is the  content attention model proposed in     and described in section  . this model allows  one to reuse the content of the input sequence to predict the output values. however   this model was designed for text translation and does not directly capture positionbased pseudo periods in time series. it has nevertheless been specialized in      under  the name pointer network  so as to select the best input to be reused as the output. this  model would be perfect for noise free  truly periodic times series. in practice  however   times series are noisy and if the output is highly correlated to the input corresponding          y. cinar  h. mirisaee  p. goswami  e. gaussier  a. a  t bachir  v. strijov    to the pseudo period  it is not an exact copy of it. we propose in this paper extensions  of the attention model that capture pseudo periods and lead to state of the art methods  for time series forecasting.  the remainder of the paper is organised as follows  section   discusses the related  work. section   presents the position based content attention models for both univariate  and multivariate time series. experiments illustrating the behaviour of the proposed  models are described in section  . lastly  section   concludes the paper.         related work    various stochastic models have been developed for time series modeling and forecasting. notable among these are autoregressive  ar      and moving averages  ma        models  that were combined in a more general and effective framework  known as autoregressive moving average  arma   or autoregressive integrated moving average   arima  when the differencing is included in the model     . vector arima  or  varima       is the multivariate extension of the univariate arima model. more  recently  based on the development of statistical machine learning  time series prediction has been formulated as a regression problem typically solved with support vector  machines  svm       and  even more recently  with random forests  rf          . rf  have been in particular used for prediction in the field of finance      and bioinformatics       and have been shown to outperform arima in different cases     .  in this study  rnns are used for modeling time series as they incorporate contextual  information from past inputs and are thus an attractive choice for predicting sequence  data  including time series    . early work      has shown that rnns  a  are a type of  nonlinear autoregressive moving average  narma  model and  b  outperform feedforward networks and various types of linear statistical models on time series. subsequently  various rnn based models were developed for different time series  as noisy  foreign exchange rate prediction       chaotic time series prediction in communication  engineering      or stock price prediction     . a detailed review can be found in       for different time series prediction tasks.  rnns based on lstms       that we consider here  alleviate the vanishing gradient  problem of the traditional rnns. they have furthermore been shown to outperform  traditional rnns on various temporal tasks         . recently  they have been used for  predicting the next frame in a video and for interpolating intermediate frames       for  forecasting the future rainfall intensity in a region       or for modeling clinical data of  multivariate time series     . the attention model in seq rnns        has been studied  very recently for time series prediction      and classification     . in particular  the  study in      uses the attention to determine the importance of a factor for prediction.  none of the previous studies  to the best of our knowledge  investigated the possibility to capture pseudo periods in time series via the attention model. this is precisely the  focus of the present study  that introduces generalizations of the content based attention  model to capture pseudo periods and improve forecasting in time series.         theoretical framework    we first focus on univariate time series. as mentioned before  time series forecasting  consists in predicting future values from past  observed values. the time span of the  past values  denoted by t   is termed as history  whereas the time span of the future  values to be predicted  denoted by t     is termed as forecast horizon  in multi step ahead     position based content attention for time series forecasting         prediction  which we consider here  t       . the prediction problem can be formulated  as a regression like problem where the goal is to learn the relation y   r x  where y     yt      . . .   yt  i   . . .   yt  t     is the output sequence and x    x    . . .   xj   . . .   xt   is  the input sequence. both input and output sequences are ordered and indexed by time  instants. for clarity s sake  and without loss of generality  for the input sequence x     x    . . .   xj   . . .   xt    the output sequence y is rewritten as y    y    . . .   yi   . . .   yt    .   .     background    seq rnns with memories rely on three parts  one dedicated to encoding the input   and referred to as encoder  one dedicated to generating the output  and referred to  as decoder  and one dedicated to the memory model  the role of which being to provide information from the input to generate each output element. the encoder rep            resents each input xj       j   t as a hidden state  hj   f  xj   h j      with        hj   rn and where the function f is non linear transformation that takes different forms depending on the rnn considered. we use here lstms with peephole  connections as described in     . the function f is further refined  in bidirectional  rnns       by reading the input both forward and backward  leading to two vectors                          h j   f  xj   h j     and h j   f  xj   h j    . the final hidden state for any input xj is  constructed simply by concatenating the corresponding forward and backward hidden             states  i.e. hj     h j   h j  t   where now hj   r n .  the decoder parallels the encoder by associating each output yi       i   t   to a  hidden state vector si that is directly used to predict the output   yi   wout si   bout   si   g yi     si     ci    with si   rn . ci is usually referred to as a context and corresponds to the output of the  memory model. in this study  the function g corresponds to an lstm with peephole  connections integrating a context    .  the memory model builds  from the sequence of input hidden states hj       j   t    the context vector c   q  h    . . .   hj   . . .   ht    that provides a summary of the input  sequences to be used for predicting the output. in its most simple form  the function  q just selects the last hidden state      q  h    . . .   hj   . . .   ht      ht . more recently   in      a content attention model is used to construct different context vectors  also  called attention vectors  ci for different outputs yi      i   t     as a weighted sum of  the hidden states of the encoder representing the input history   eij      vat    tanh wa si     ua hj     ij   softmax eij    ci      t  x     ij hj           j      where softmax normalizes the vector ei of length t to be the attention mask over the input. the weights  ij   referred to as the attention weights  correspond to the importance  of the input at time j to predict the output at time i. they allow the model to concentrate  or put attention  on certain parts of the input history to predict each output. lastly   wa   ua and va are trained in conjunction with the entire encoder decoder framework.  we present below two extensions for univariate time series to integrate pseudoperiods.           .     y. cinar  h. mirisaee  p. goswami  e. gaussier  a. a  t bachir  v. strijov    position based content attention mechanism    we assume here that the pseudo periods of a time series lie in the set     ...  t   where  t is the history size of the time series  . one can then explicitly model all possible  pseudo periods as a real vector  which we will refer to as         of dimension t   whose  coordinate j encodes the importance of the input at position j in the input sequence  to predict output at position i. from this  one can modify the weight of the original  attention mechanism relating input j to output i as follows      vat tanh wa si                 i j    ua hj   if  i   t   j    t  eij       otherwise  where   .  .   denotes the scalar product and   i j    rt is a binary vector that  is   on dimension  i   t   j  and   elsewhere.   i j  thus selects the coordinate of        corresponding to the difference in positions between input j and output i. this  coordinate is then used to increase or decrease the importance of the hidden state hj in  eij . note that  as the history is limited to t   there is no need to consider dependencies  between an input j and an output i that are distant by more than t time steps  hence the  test  i   t   j   t  .  y   yi  yt  yi         s     ...    si    si      ...    st          i            . . .        h         h     x      i t     i j    ...    ...      ...          hj        hj    xj    ...          ht    ...          ht    xt    fig.  . the illustration of the proposed position based content attention mechanism.    the vector ei can then be normalized using the softmax operator again  and a context be built by taking the expectation of the hidden states over the normalized weights.  for practical purposes  however  one can simplify the above formulation by extending  the vectors       and   i j  with t   dimensions that are set to   in   i j    and by considering a vector   of dimension  t   t     that has   on its first t coordinates and    on the last t   ones. the resulting position based attention mechanism then amounts to      e   vat tanh wa si                 i j    ua hj   i t  j          ij  t  x  rnn                        softmax e      c      ij hj     ij  ij  i     j             as one can note   i t  j will either decrease or increase the hidden state vector hj  for output i. since       is learned along with the other parameters of the seq rnn        this assumption is easy to satisfy by increasing the size of the history if the pseudo periods  are known or by resorting to a validation set to tune t .     position based content attention for time series forecasting                we expect that  i t  j will be high for those values of i   t   j that correspond to  pseudo periods of the time series. we will refer to this model as rnn       . lastly  note  that the original attention mechanism can be recovered by setting       to    a vector  consisting of   on each coordinate .  in the above formulation  the position information is used to modify the importance  of each hidden state in the input side. it may be  however  that some elements in hj are  less important than others to predict output i. it is possible to capture this by considering  that  instead of having a scalar at each position relating the input to the output  one has  a vector in r n that can now reweigh each coordinate of hj independently. this leads  to      t       i j      hj    i t  j       eij   va tanh wa si     ua           t       x  rnn               ij hj        ij   softmax eij    ci    j      where  denotes the hadamard product  element wise multiplication  and       is a     matrix in r n  t  t   .   i j  and   are defined as before. we will refer to this model       as rnn   .  figure   illustrates the overall network in which   is a vector for rnn       and a  matrix for rnn       .   .     multivariate extensions    as each variable in a k multivariate time series can have its own pseudo periods  a  direct extension of the above approaches to multivariate time series is to consider that  each variable k      k   k  of the time series has its own encoder and attention  mechanism. the context vector for the ith output of the k th variable is then defined by  pt   k    k   k    k   ci   j    ij hj   where hj is the input hidden state at time stamp j for the k th   k     variable and  ij are the weights given by the attention mechanism of the k th variable.  to predict the output while taking into account potential dependencies between different  variables  one can simply concatenate the context vectors from the different variables  into a single context vector ci that is used as input to the decoder  the rest of the decoder  architecture being unchanged      t    ci    ci   k      k t t          ci         as each ci is of dimension  n  that is the dimension of the input hidden states    ci is of dimension  kn. this strategy can readily be applied to the original attention  mechanism as well as the ones based on       and       .  it is nevertheless possible to rely on a single attention model for all variables while  having separate representations for them in order to select  for each output  specific  hidden states from the different variables. to do so  one can simply concatenate the     t   k t t  hidden states of each variable into a single hidden state  hj    hj       hj      and  deploy the previous attention model on top of them. this leads to the multivariate model  which we refer to as rnn       and is based on the same ingredients and equations as     rnn         the only difference being that rnn       is now a matrix in r kn  t  t   .  we now turn to the experimental validation of the proposed models.               y. cinar  h. mirisaee  p. goswami  e. gaussier  a. a  t bachir  v. strijov    experiments    we retained six widely used and publicly available      datasets  described in table     to assess the models we proposed. the values for the history size were set so as they encompass the known periods of the datasets. they can also be tuned by cross validation  if one does not want to identify the potential periods by checking the autocorrelation  curves. in general  the forecast horizon should reflect the nature of the data and the  application one has in mind  with of course a trade off between long forecast horizon  and prediction quality. for this purpose  the forecast horizons of these sets along the  sampling rates are chosen as illustrated in table  . all datasets were split by retaining  the first     of each dataset for training validation and the last     for testing. for  rnn based methods  the training validation sets were further divided by retaining the  first     for training    .    of the data  and the last     for validation    .    of  the data . for the baseline methods  we used   fold cross validation on the trainingvalidation sets to tune the hyperparameters. lastly  linear interpolation was used whenever there are missing values in the time series  .  table  . datasets.  name  polish electricity  pse   polish weather  pw   numenta benchmark  nab   air quality  aq   appliances energy pred.  aep   ozone level detection  old     usage  univariate  univariate  univariate  univ. multiv.  univ. multiv.  univ. multiv.     instances history forecast horizon sampling rate                                                                                               hours    days    minutes    hour     minutes    day    we compared the methods introduced before  namely rnn             with the original attention model  rnn a  and several baseline methods  namely arima  an ensemble learning method  rf  and the standard support vector regression methods. among  these baselines  we retained arima and rf as these were the two best performing  methods in our datasets. these methods  discussed in section    have also been shown to  provide state of the art results on various forecasting problems  e.g.      . for arima   we relied on the seasonal variant     . to implement the rnn models  we used theano   and lasagne   on a linux system with    gb of memory and    core intel xeon    .  ghz. all parameters are regularized and learned through stochastic backpropagation  the mini batch size was set to     with an adaptive learning rate for each parameter       the objective function being the mean square error  mse  on the output.  for tuning the hyperparameters  we used a grid search over the learning rate  the regularization type and its coefficient  and the number of units in the lstm and attention  models. the values finally obtained are      for the initial learning rate and      for  the coefficient of the regularization  the type of regularization selected being l  . the               we compared several methods for missing values  namely linear  non linear spline and kernel based fourier transform interpolation as well as padding for the rnn based models. the  best reconstruction was obtained with linear interpolation  hence its choice here.  http   deeplearning.net software theano   https   lasagne.readthedocs.io     position based content attention for time series forecasting         table  . overall results for univariate case with mse  left value  and smape  right value .  dataset  aq  old  aep  nab  pw  pse    rnn a   .       .       .       .       .       .       .       .       .       .      .       .        rnn         .     .      .     .      .       .       .     .      .     .      .     .        rnn         .    .      .     .       .       .       .      .       .       .       .       .       arima   .       .       .       .       .     .      .       .      .       .      .       .        rf  selected     .       .                    .      .                .     .                   .      .             .     .                   .      .              number of units vary among the set            for lstms and            for the attention models respectively. we report hereafter the results with the minimum mse on  the test set. for evaluation  we use mse and the symmetric mean absolute percentage  error  smape . mse corresponds to the objective function used to learn the model.  smape presents the advantage of being bounded and represents a scaled l  error.  overall results on univariate time series  for univariate experiments using multivariate time series  we chose the following  variables from the datasets  for pw  we selected the max temperature series from the  warsaw metropolitan area that covers only one weather recording station  for aq we  selected c h  gt   for aep we selected the outside humidity  rh    for nab we  selected the amazon web services cpu usage and for old we selected t . table    displays the results obtained with the mse  left value  and the smape  right value   as evaluation measures. once again  one should note that mse was the metric being  optimized. for each time series  the best performance among all methods is shown  in bold and other methods are marked with an asterisk if they are significantly worse  than the best method according to a paired t test with    significance level. lastly  the  last column of the table  selected    indicates which method  among rnn           was  selected as the best method on the validation set using mse.  as one can note  except for aep where the baselines are better than rnn based  methods  for all other datasets  the best results are obtained with rnn       and rnn        these results being furthermore significantly better than the ones obtained with  rnn a and baseline methods  for both mse and smape. the mse improvement  varies from one dataset to another  between     pw  and      nab  w.r.t rnn a.  compared to arima  one can achieve an improvement ranging from            in  old   to            in pse   w.r.t mse  smape . in addition  the selected rnn    method  column selected    is the best performing method on three out of six datasets   aq  pw  pse  and the best performing rnn   method on aep. it is furthermore equivalent to the best performing method on old  the only dataset on which the selection  fails being nab  a failure means here that the selection does not select the best rnn    method . however  on this dataset  the selected method is still better than the original  attention model and the baselines. overall  these results show that rnn         significantly improves forecasting in the univariate time series we considered  and that one  can automatically select the best rnn   method.  lastly  to illustrate the ability of rnn   to capture pseudo periods  we display in  figure    left  the autocorrelation plot for pse  and in figure    right  the average attention weights for the same time series obtained with rnn a and rnn        averaged     pse attention weights    y. cinar  h. mirisaee  p. goswami  e. gaussier  a. a  t bachir  v. strijov  pse autocorrelation           week           day     .     rnn a attention weights   .   rnn       attention weights     .                                              days      days   days   days   days   days      day    lags in hours  history  fig.  . autocorrelation of pse  left . rnn a and rnn       attention weights  right .    over all test examples and forecast horizon points . as one can see from the autocorrelation plot  pse has two main weekly and daily pseudo periods. this two pseudo periods  are clearly visible in the attention weights of rnn       that gives higher weights to the  four points located at positions minus   days and minus   day  these four points correspond to the four points of the forecast horizon . the attention weights of rnn         not shown here for space reasons  are very similar. in contrast  the attention weights of  the original attention model  rnn a  follow a general increasing behaviour with more  weights on the more recent time stamps. this model thus misses the pseudo periods.  results on multivariate time series  as mentioned in table    we furthermore conducted multivariate experiments on  aq  aep and old using the multivariate extensions described in section  . for aq   we selected the four variables associated to real sensors  namely c h  gt   no  gt    co gt  and nox gt  and predicted the same one as the univariate case  c h  gt  .  for aep  we selected two temperature time series  namely t  and t   and two humidity  time series  rh  and rh   and we predict rh  as in the univariate case. for old  we  trained the model using t  to t  and predicted t   as we did on the univariate case. as  rf outperformed arima on five out of six univariate datasets and was equivalent on  the sixth one  we retained only rf and rnn a for comparison with rnn           .  table   shows the results of our experiments on multivariate sets with mse  smape   not displayed here for readability reasons  has a similar behaviour . as before  for each  time series  the best result is in bold and an asterisk indicates that the method is significantly worse than the best method  again according to a paired t test with    significance level . similarly to the univariate case  the best results  that are always significantly better than the other results  are obtained with the rnn   methods  for aq and  old datasets  rnn   can respectively bring     and     of significant improvement  over rnn a. similarly  for aep  the improvement is significant over rnn a       with rnn        . compared to rf  one can obtain between      aep  and      aq   of improvement. as one can note  the selected method is always rnn       . the selection is this time not as good as for the univariate case as the best method  sometimes  significantly better than the one selected  is missed. that said  rnn       remains better  than the state of the art baselines retained  rnn a and rf.         conclusion    we studied in this paper the use of seq rnns  in particular the state of the art bidirectional lstms encoder decoder with a content attention model  for modelling and     position based content attention for time series forecasting         table  . overall results for multivariate case with mse.  dataset  aq  old  aep    rnn a rnn       rnn       rnn       rf selected     .      .       .      .     .                         .      .      .      .     .                         .      .      .      .      .              forecasting time series. if content attention models are crucial for this task  they were  not designed for time series and currently are deficient as they do not capture pseudoperiods. we thus proposed three extensions of the content attention model making use  of the  relative  positions in the input and output sequences  hence the term positionbased content attention . the experiments we conducted over several univariate and  multivariate time series demonstrate the effectiveness of these extensions  on time series with either clear pseudo periods  as pse  or less clear ones  as aep. indeed  these  extensions perform significantly better than the original attention model as well as stateof the art baseline methods based on arima and random forests.  in the future  we plan on studying formal criteria to select the best extension for both  univariate and multivariate time series. this would allow one to avoid using a validation  set that may be not large enough to properly select the best method. we conjecture that  this is what happening on the multivariate time series we have retained.    