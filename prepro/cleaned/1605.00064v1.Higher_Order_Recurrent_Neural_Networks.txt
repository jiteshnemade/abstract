introduction  in the recent resurgence of neural networks in deep learning  deep neural networks have achieved huge successes  in various real world applications  such as speech recognition  computer vision and natural language processing.  deep neural networks  dnns  with a deep architecture  of multiple nonlinear layers are an extremely expressive  model that can learn complex features and patterns in data.  each layer of dnns learns some concepts and transfers  them to the next layer and the next layer may continue to  extract more complicated features  and finally the last layer  generates the desirable output. from some early theoretical work  cybenko        hornik         it is well known    that neural networks may be used as the so called universal  approximators to map from any fixed size input to another  fixed size output. recently  more and more empirical results have demonstrated that the deep structure in dnns is  not just powerful in theory but also can be reliably learned  in practice from a large amount of training data.  sequential modeling is a challenging problem in machine  learning  which has been extensively studied in the past.  recently  many deep neural network based models have  been very successful in this area  as shown in various tasks  such as language modeling  mikolov         sequence generation  graves        sutskever et al.         machine  translation  sutskever et al.        and speech recognition   graves et al.       . among various neural network models  recurrent neural networks  rnns  are appealing for  modeling sequential data because they can capture long  term dependency in sequential data using a simple mechanism of recurrent feedback  elman       . rnns can  learn to model sequential data over an extended period of  time  then carry out rather complicated transformations on  the sequential data. rnns have been theoretically proved  to be a turing complete machine  siegelmann   sontag        . rnns in principle can learn to map from one  variable length sequence to another. when unfolded in  time  rnns are equivalent to very deep neural networks  that share model parameters and receive the input at each  time step. the recursion in the hidden layer of rnns can  act as an excellent memory mechanism for the networks.  in each time step  the learned recursion weights may decide what information to discard and what information to  keep in order to relay onwards along time.  while rnns are theoretically powerful  the learning of  rnns needs to use the so called back propagation through  time  bptt  method  werbos        due to the internal recurrent cycles. unfortunately  in practice  it turns out to be  rather difficult to train rnns to capture long term dependency due to the fact that the gradients in bptt tend to either vanish or explode  bengio et al.       . many heuristic methods have been proposed to solve these problems.     higher order recurrent neural networks    for example  a simple method  called gradient clipping  is  used to avoid gradient explosion  mikolov       . however  rnns still suffer from the vanishing gradient problem since the gradients decay gradually as they are backpropagated through time. as a result  some new recurrent  structures are proposed  such as long short term memory   lstm   hochreiter   schmidhuber        and gated recurrent unit  gru   cho et al.       . these models use  some learnable gates to implement rather complicated feedback structures  which ensure that some feedback paths can  allow the gradients to flow back in time effectively. these  models have given promising results in many practical applications  such as sequence modeling  graves         language modeling  sundermeyer et al.         hand written  character recognition  liwicki et al.         machine translation  cho et al.         speech recognition  graves et al.        .  in this paper  we explore an alternative method to learn recurrent neural networks  rnns  to model long term dependency in sequential data. we propose to use more memory  units to keep track of more preceding rnn states  which  are all recurrently fed to the hidden layers as feedback  through different weighted paths. analogous to digital filters in signal processing  we call these new recurrent structures as higher order recurrent neural networks  hornns .  at each time step  the proposed hornns directly combine multiple preceding hidden states from various history  time steps  weighted by different matrices  to generate the  feedback signal to each hidden layer. by aggregating more  history information of the rnn states  hornns are provided with better short term memory mechanism than the  regular rnns. moreover  those direct connections to more  previous rnn states allow the gradients to flow back more  smoothly in the bptt learning stage. all of these ensure  that hornns can be more effectively learned to capture  long term dependency. similar to rnns and lstms  the  proposed hornns are general enough for a variety of sequential modeling tasks. in this work  we have evaluated  hornns for the language modeling task on two popular  data sets  namely the penn treebank  ptb  and english  text  sets. experimental results have shown that hornns  yield the state of the art performance on both data sets  significantly outperforming the regular rnns as well as the  popular lstms.  the remainder of this paper is organized as follows. we  will briefly review some related work in the literature in  section  . in section    we first present the key idea  of higher order rnns  hornns  in detail  and then introduce several variant hornn structures using different  pooling functions to generate the feedback signals. in section    we report and discuss the experimental results on  two language modeling tasks. finally  we conclude the paper with our findings in section  .     . related work  hierarchical recurrent neural network proposed in  hihi    bengio        is one of the earliest papers that attempt to  improve rnns to capture long term dependency in a better way. it proposes to add linear time delayed connections to rnns to improve the gradient descent learning  algorithm to find a better solution  eventually solving the  gradient vanishing problem. however  in this early work   the idea of multi resolution recurrent architectures has only  been preliminarily examined for some simple small scale  tasks. this work is somehow relevant to our work in this  paper but the higher order rnns proposed here differs in  several aspects. firstly  we propose to use weighted connections in the structure  instead of simple multi resolution  short cut paths. this makes our models fall into the category of higer order models. secondly  we have proposed  to use various pooling functions in generating the feedback  signals  which is critical in normalizing the dynamic ranges  of gradients flowing from various paths. our experiments  have shown that the success of our models is largely attributed to this technique.  the most successful approach to deal with vanishing gradients so far is to use long short term memory  lstm   model  hochreiter   schmidhuber       . lstm relies  on a fairly sophisticated structure made of gates to control  flow of information to the hidden neurons. the drawback  of the lstm is that it is complicated and slow to learn.  the complexity of this model makes the learning very time  consuming  and hard to scale for larger tasks. another  approach to address this issue is to add a hidden layer to  rnns  mikolov et al.       . this layer is responsible for  capturing longer term dependencies in input data by making its weight matrix close to identity. recently  clockwork rnns  koutnik et al.        are proposed to address  this problem as well  which splits each hidden layer into  several modules running at different clocks. each module  receives signals from input and computes its output at a  predefined clock rate. gated feedback recurrent neural networks  chung et al.        attempt to implement a generalized version using the gated feedback connection between  layers of stacked rnns  allowing the model to adaptively  adjust the connection between consecutive hidden layers.  more recently  some short cut skipping connections have  been found useful in learning very deep feed forward neural networks as well  such as  szegedy et al.        lee  et al.        he et al.       . these skipping connections between various layers of neural networks can improve the flow of information in both forward and backward passes. among them  highway networks  srivastava  et al.        introduce rather sophisticated skipping connections between layers  controlled by some gated functions.     higher order recurrent neural networks     . higher order recurrent neural networks  a recurrent neural network  rnn  is a type of neural network suitable for modeling a sequence of arbitrary length.  at each time step t  an rnn receives an input xt   the state  of the rnn is updated recursively as follows  as shown in  the left part of figure      ht   f  win xt   wh ht               where f     is an nonlinear activation function  such as sigmoid or rectified linear  relu   and win is the weight matrix in the input layer and wh is the state to state recurrent  weight matrix. due to the recursion  this hidden layer may  act as a short term memory of all previous input data.  given the state of the rnn  i.e.  the current activation signals in the hidden layer ht   the rnn generates the output  according to the following equation   yt   g wout ht             where g    denotes the softmax function and wout is the  weight matrix in the output layer. in principle  this model  can be trained using the back propagation through time   bptt  algorithm  werbos       . this model has been  used widely in sequence modeling tasks like language  modeling  mikolov       .    shown in the right part of figure    instead of using only  the previous rnn state as the feedback signal  we propose  to employ multiple memory units to generate the feedback  signal at each time step by directly combining multiple preceding rnn states in the past  where these time delayed  rnn states go through separate feedback paths with different weight matrices. analogous to the filter structures  used in signal processing  we call this new recurrent structure as higher order rnns  hornns in short. the order  of hornns depends on the number of memory units used  for feedback. for example  the model used in the right of  figure   is a  rd order hornn. on the other hand  regular  rnns may be viewed as  st order hornns.  in hornns  the feedback signal is generated by combining multiple preceding rnn states. therefore  the state of  an n  th order hornn is recursively updated as follows      n  x  whn ht n       ht   f win xt    n      where  whn   n            n   denotes the weight matrices  used for various feedback paths.    figure  . unfolding a  rd order hornn    rnns are very deep in time and the hidden layer at each  time step represents the entire input history  which acts as  a short term memory mechanism. however  due to the gradient vanishing problem in back propagation  it turns out  to be very difficult to learn rnns to model long term dependency in sequential data.    similar to rnns  hornns can also be unfolded in time to  get rid of the recurrent cycles. as shown in figure    we  unfold a  rd order hornn in time  which clearly shows  that each hornn state is explicitly decided by the current  input xt and all previous   states in the past. this structure  looks similar to the skipping short cut paths in deep neural networks but each path in hornns maintains a learnable weight matrix. the new structure in hornns can  significantly improve the model capacity to capture longterm dependency in sequential data. at each time step   by explicitly aggregating multiple preceding hidden activities  hornns may derive a good representation of the history information in sequences  leading to a significantly enhanced short term memory mechanism.    in this paper  we extend the standard rnn structure to better model long term dependency in sequential data. as    during the backprop learning procedure  these skipping  paths directly connected to more previous hidden states of    figure  . comparison of model structures between an rnn   st  order  and a higher order rnn   rd order . the symbol z    denotes a time delay unit  equivalent to a memory unit .     . . higher order rnns  hornns      higher order recurrent neural networks    figure  . illustration of all back propagation paths in bptt for a   rd order hornn.    quential data. on the other hand  they may also complicate  the learning in a different way. due to different numbers  of hidden layers along various paths  the signals flowing  from different paths may vary dramatically in the dynamic  range. for example  in the forward pass in figure    three  different feedback signals from different time scales  e.g.  ht     ht   and ht     flow into the hidden layer to compute the new hidden state ht . the dynamic range of these  signals may vary dramatically from case to case. the situation may get even worse in the backward pass during the  bptt learning. for example  in a  rd order hornn in  figure    the node ht   may directly receive an error signal from the node ht . in some cases  it may get so strong  as to overshadow other error signals coming from closer  neighbours of ht   and ht   . this may impede the learning of hornns  yielding slow convergence or even poor  performance.    hornns may allow the gradients to flow more easily back  in time  which eventually leads to a more effective learning  of models to capture long term dependency in sequences.  therefore  this structure may help to largely alleviate the  notorious problem of vanishing gradients in the rnn learning.  obviously  hornns can be learned using the same bptt  algorithm as regular rnns  except that the error signals  at each time step need to be back propagated to multiple  feedback paths in the network. as shown in figure    for  a  rd order hornn  at each time step t  the error signal  from the hidden layer ht will have to be back propagated  into four different paths  i  the first one back to the input  layer  xt   ii  three more feedback paths leading to three  different histories in time scales  namely ht     ht   and  ht   .  interestingly enough  if we use a fully unfolded implementation for hornns as in figure    the overall computation complexity is comparable with regular rnns. given a  whole sequence  we may first simultaneously compute all  hidden activities  from xt to ht for all t . secondly  we  recursively update ht for all t using eq.   . finally  we  use gpus to compute all outputs together from the updated  hidden states  from ht to yt for all t  based on eq.   . the  backward pass in learning can also be implemented in the  same three step procedure. except the recursive updates  in the second step  this issue remains the same in regular  rnns   all remaining computation steps can be formulated  as large matrix multiplications. as a result  the computation of hornns can be implemented fairly efficiently  using gpus.    figure  . a pooling function is used to calibrate various feedback  paths in hornns.    here  we have proposed to use some pooling functions to  calibrate the signals from different feedback paths before  they are used to recursively generate a new hidden state   as shown in figure  . in the following  we will investigate  three different choices for the pooling function in figure     including max based pooling  fofe based pooling and  gated pooling.   . . . m ax   based p ooling     . . pooling functions for hornns    max based pooling is a simple strategy that chooses the  most responsive unit  exhibiting the largest activation  value  among various paths to transfer to the hidden layer  to generate the new hidden state. many biological experiments have shown that biological neuron networks tend to  use a similar strategy in learning and firing.    as discussed above  the shortcut paths in hornns may  help the models to capture long term dependency in se     in this case  instead of using eq.     we use the following  formula to update the hidden state of hornns      higher order recurrent neural networks          ht   f win xt   maxn  n    whn ht n             steps of the hidden layer. this allows the network to potentially remember information for a longer period of time.    where maximization is performed element wisely to  choose the maximum value in each dimension to feed to  the hidden layer to generate the new hidden state. the aim  here is to capture the most relevant feature and map it to a  fixed predefined size.  the max pooling function is simple and biologically inspired. however  the max pooling strategy also has some  serious disadvantages. for example  it has no forgetting  mechanism and the signals may get stronger and stronger.  furthermore  it loses the order information of the preceding histories since it only choose the maximum values but  it does not know where the maximum comes from.   . . . fofe  based p ooling  the so called fixed size ordinally forgetting encoding   fofe  method was proposed in  zhang et al.        to  encode any variable length sequence of data into a fixedsize representation. in fofe  a single forgetting factor                is used to encode the position information  in sequences based on the idea of exponential forgetting to  derive invertible fixed size representations. in this work   we borrow this simple idea of exponential forgetting to calibrate all preceding histories using a pre selected forgetting  factor as follows     ht   f    win xt      n  x        n   whn ht n           n      where the forgetting factor   is manually pre selected between          . the above constant coefficients related to   play an important role in calibrating signals from  different paths in both forward and backward passes of  hornns since they slightly underweight the older history  over the recent one in an explicit way.   . . . g ated hornn s  in the section  we follow the ideas of the learnable gates  in lstms  hochreiter   schmidhuber        and grus   cho et al.        as well as the recent soft attention in   bahdanau et al.       . instead of using constant coefficients derived from a forgetting factor  we may let the  network automatically determine the combination weights  based on the current state and input. in this case  we may  use sigmoid gates to compute combination weights to regulate the information flowing from various feedback paths.  the sigmoid gates take the current data and previous hidden state as input to decide how to weight all of the precede  hidden states. the gate function weights how the current  hidden state is generated based on all the previous time     figure  . gated hornns use learnable gates to combine various  feedback signals.    in a gated hornn  the hidden state is recursively computed as follows      n        x  rn  whn ht n       ht   f win xt    n      where  denotes element wise multiplication of two  equally sized vectors  and the gate signal rn is calculated  as  g  g  rn      w n  xt   w n  ht n         g  g  where      is the sigmoid function  and w n  and w n  denote two weight matrices introduced for each gate.    note that the computation complexity of gated hornns is  comparable with lstms and grus  significantly exceeding the other hornn structures because of the overhead  from the gate functions in eq.    .     . experiments  in this section  we evaluate the proposed higher order  rnns  hornns  on several language modeling tasks. a  statistical language model  lm  is a probability distribution  over sequences of words in natural languages. recently   neural networks have been successfully applied to language  modeling  bengio et al.        mikolov et al.         yielding the state of the art performance. in language modeling tasks  it is quite important to take advantage of the  long term dependency of natural languages. therefore  it is  widely reported that rnn based lms can outperform feedforward neural networks in language modeling tasks. we  have chosen two popular lm data sets  namely the penn  treebank  ptb  and english text  sets  to compare our  proposed hornns with traditional n gram lms  rnnbased lms and the state of the art performance obtained     higher order recurrent neural networks  table  . the sizes of the ptb and english text  corpora are given  in number of words.    corpus  ptb  text     train     k    . m    valid    k       test    k   .  m    table  . perplexities on the ptb test set for various hornns are  shown as a function of order          . note the perplexity of a  regular rnn   st order  is      as reported in  mikolov et al.        .    models  by lstms  graves        mikolov et al.         fofe  based feedforward nns  zhang et al.        and memory  networks  sukhbaatar et al.       . details of the two data  sets can be found in table  .  in our experiments  we use the mini batch stochastic gradient decent  sgd  algorithm to train all neural networks.  the number of back propagation trough time  bptt  steps  is set to    for all recurrent models. each model update  is conducted using a mini batch of    subsequences  each  of which is of    in length. all model parameters  weight  matrices in all layers  are randomly initialized based on a  gaussian distribution with zero mean and standard deviation of  . . a hard clipping is set to  .  to avoid gradient  explosion during the bptt learning. the initial learning  rate is set to  .  and we halve the learning rate at the end of  each epoch if the cross entropy function on the validation  set does not decrease. we have used the weight decay  momentum and column normalization  pachitariu   sahani         in our experiments to improve model generalization.  in the fofe based pooling function for hornns  we set  the forgetting factor     to  . . we have used     nodes in  each hidden layer for the ptb data set and     nodes per  hidden layer for the english text  set. in our experiments   we do not use the dropout regularization  zaremba et al.         in all experiments since it significantly slows down  the training speed  not applicable to any larger corpora.     . . language modeling on ptb  the standard penn treebank  ptb  corpus consists of  about  m words. the vocabulary size is limited to   k.  the preprocessing method and the way to split data into  training validation test sets are the same as  mikolov et al.        . the size of ptb is summarized in table  . ptb  is a relatively small text corpus. we first investigate various model configurations for the hornns based on ptb  and then compare the best performance with other results  reported on this task.   . . . e ffect of o rders in hornn s  in the first experiment  we first investigate how the used orders in hornns may affect the performance of language  models  as measured by perplexity . we have examined  all different higher order model structures proposed in this     we will soon release the code for readers to reproduce all  results reported in this paper.    hornn  max hornn  fofe hornn  gated hornn     nd order     rd order     th order                                                                      paper  including hornns and various pooling functions  in hornns. the orders of these examined models varies  among      and  . we have listed the performance of different models on ptb in table  . as we may see  we are able  to achieve a significant improvement in perplexity when  using higher order rnns for language models on ptb   roughly       reduction in ppl over regular rnns. we  can see that performance may improve slightly when the  order is increased from   to   but no significant gain is observed when the order is further increased to  . as a result   we choose the  rd order hornn structure for the following experiments. among all different hornn structures   we can see that fofe based pooling and gated structures  yield the best performance on ptb.  in language modeling  both input and output layers account  for the major portion of model parameters. therefore  we  do not significantly increase model size when we go to  higher order structures. for example  in table    a regular  rnn contains about  .  millions of weights while a  rdorder hornn  the same for max or fofe pooling structures  has about  .  millions of weights. in comparison  an  lstm model has about  .  millions of weights and a  rdorder gated hornn has about  .  millions of weights.  as for the training speed  most hornn models are only  slightly slower than regular rnns. for example  one epoch  of training on ptb running in one nvidia s titan x  gpu takes about    seconds for an rnn  about     seconds for a  rd order hornn  the same for max or fofe  pooling structures . similarly  training of gated hornns  is also slightly slower than lstms. for example  one  epoch on ptb takes about     seconds for an lstm  and  about     seconds for a  rd order gates hornn.   . . . e ffect of forgetting factor in fofe  hornn  in this experiment  we study the effect of the forgetting factor     on the performance of fofe based pooling hornns. we have trained a number of  rd order  fofe based pooling hornns by using the same hyperparameters except the forgetting factor     varies between     higher order recurrent neural networks    ppl     .  and  . . the performance of these models in perplexity  is shown as a function of   in figure  . the results are consistent with the finding in  zhang et al.        that fofe  works the best when   lies between  .  and  . . therefore   in our experiments  we always choose      . .                                              .      .      .      .      .      .      .      .     alpha    figure  . perplexities of  rd order fofe hornns are shown as  a function of forgetting factor  .     . . . m odel c omparison on p enn t ree bank  at last  we report the best performance of various  hornns on the ptb test set in table  . we compare our  rd order hornns with all other models reported on this task  including rnn  mikolov et al.          stack rnn  pascanu et al.         deep rnn  pascanu  et al.         fofe fnn  zhang et al.        and lstm   graves       .    from the results in table    we can see that our proposed  higher order rnn architectures significantly outperform all  other baseline models reported on this task. both fofebased pooling and gated hornns have achieved the stateof the art performance  i.e.      in perplexity on this task.  to the best of our knowledge  this is the best reported performance on ptb under the same training condition.   . . language modeling on english text   in this experiment  we will evaluate our proposed  hornns on a much larger text corpus  namely the english  text  data set. the text  data set contains a preprocessed  version of the first     million characters downloaded from    table  . perplexities on the ptb test set for various examined  models.    models  kn   gram  mikolov et al.         rnn  mikolov et al.         lstm  graves         stack rnn  pascanu et al.         deep rnn  pascanu et al.         fofe fnn  zhang et al.         hornn   rd order   max hornn   rd order   fofe hornn   rd order   gated hornn   rd order     test ppl                                                      the wikipedia website. we have used the same preprocessing method as  mikolov et al.        to process the data set  to generate the training and test sets. we have limited the  vocabulary size to about   k by replacing all words occurring less than    times in the training set with an  unk   token. as shown in table    the text  set is about    times  larger than ptb in corpus size. the model training on text   takes much longer to finish. we have not tuned hyperparameters in this data set. we simply follow the best setting used in ptb to train all hornns for the text  data  set. meanwhile  we also follow the same learning schedule  used in  mikolov et al.         we first initialize the learning rate to  .  and run   epochs using this learning rate   after that  the learning rate is halved at the end of every  epoch.  because the training is very time consuming  we have only  evaluated  rd order hornns on the text  data set. the  perplexities of various hornns are summarized in table   . we have compared our hornns with all other baseline models reported on this task  including rnn  mikolov  et al.         lstm  mikolov et al.         scrnn   mikolov et al.        and end to end memory networks   sukhbaatar et al.       . results have shown that all  hornn models work pretty well in this data set except  the normal hornn significantly underperforms the other  three models. among them  the gated hornn model  has achieved the best performance  i.e.      in perplexity  on this task  which is slightly better than the recent result  obtained by end to end memory networks  using a rather  complicated structure . to the best of our knowledge  this  is the best performance reported on this task.         all models in table   do not use the dropout regularization   which is somehow equivalent to data augmentation. in  zaremba  et al.        kim et al.         the proposed lstm lms  word  level or character level  achieve lower perplexity but they both use  the dropout regularization and much bigger models and it takes  days to train the models  which is not applicable to other larger  tasks.     . conclusions  in this paper  we have proposed some new structures for  recurrent neural networks  called as higher order rnns   hornns . in these structures  we use more memory units     higher order recurrent neural networks  table  . perplexities on the text  test set for various models.    models  rnn  mikolov et al.         lstm  mikolov et al.         scrnn  mikolov et al.         e e mem net  sukhbaatar et al.         hornn   rd order   max hornn   rd order   fofe hornn   rd order   gated hornn   rd order     test ppl                                            to keep track of more preceding rnn states  which are  all fed along various feedback paths to the hidden layer to  generate the feedback signals. in this way  we may enhance the model to capture long term dependency in sequential data. moreover  we have proposed to use several types of pooling functions to calibrate multiple feedback paths. experiments have shown that the pooling technique plays a critical role in learning higher order rnns  effectively. in this work  we have examined hornns for  the language modeling task using two popular data sets   namely the penn treebank  ptb  and text  sets. experimental results have shown that the proposed higher order  rnns yield the state of the art performance on both data  sets  significantly outperforming the regular rnns as well  as the popular lstms. as the future work  we are going to continue to explore hornns for other sequential  modeling tasks  such as speech recognition  sequence tosequence modelling and so on.    