introduction  with the increasing availability of high frequency sensor  data  recent trends in time series forecasting have explored the use of deep neural networks to make predictions from real time data streams. successful applications have also spanned a multitude of fields   including  real time human activity recognition based on wearable  sensors in healthcare  nweke et al.         local rainfall  prediction in weather forecasting  chao et al.         and  high frequency market microstructure prediction in finance   zhang et al.       .  recurrent neural networks  rnns   in particular  have several properties that make them attractive for real time predictions from a methodological standpoint. firstly  rnns     department of engineering science  university of oxford  oxford  united kingdom   oxford man institute of quantitative finance  university of oxford  oxford  united kingdom. correspondence to  bryan lim  blim robots.ox.ac.uk    stefan zohren  zohren robots.ox.ac.uk   stephen roberts   sjrob robots.ox.ac.uk .    learn complex cross sectional and temporal relationships  in a purely data driven manner  which is useful for complex datasets where the underlying data generation process  is not well understood. in addition  rnns also naturally  retain information over time through the recursive update  of an internal memory state. this helpful in cases where  the exact length of relevant history is unknown  and architectures that rely on a fixed look back window   such as  convolutional neural networks  cnns    might not be fully  capture all relevant information.  however  long term dependency learning with rnns remains difficult in practice  mainly due to inherent problems with backpropagation through time  bptt  with  stochastic gradient descent  sgd    such as exploding vanishing gradients seen in standard elman rnn architectures  hochreiter et al.       . challenges still persist even with modern architectures which stabilise gradient flow   such as long short term memory  lstm    hochreiter   schmidhuber          with multiple lines  of active research looking at both memory enhancements  and training improvements to help rnns learn longterm dependencies  neil et al.        zhang et al.         trinh et al.        kanuparthi et al.       . furthermore   standard minibatch sgd  where long trajectories are truncated into shorter sequences for minibatches  also runs the  risk of excluding relevant information during training   as  neural networks are unable to establish links between observations and historical drivers which lie outside the truncation window. better performance for long term dependency modelling could hence be achieved by exploring  training methods that do not rely on gradient based bptt.  gradient free evolutionary computation techniques have  previously been used to train deep neural networks in reinforcement learning  with methods such as deep neuroevolution  such et al.        exhibiting comparable results to standard gradient based approaches. inspired by  this success  we investigate the use of population based  optimisation  pbo  algorithms   i.e. evolution strategies  salimans et al.        and particle swarm optimisation  kennedy   eberhart          in rnn training  specifically to overcomes issues in learning long term dependencies with gradient based methods. focusing on appli      population based global optimisation methods for learning long term dependencies with rnns    cations in time series forecasting  we evaluate the use of  pbo methods to train a variety of modern rnn architectures  demonstrating the performance improvements over  standard gradient based stochastic backpropagation while  maintaining a comparable computational budget   as measured by the number of feed forward passes through the  network during training.    ing. as unbounded terms can dominate gradient backpropagation   and inadvertently hamper long term dependency  learning   they propose what they term the h detach trick to  suppress this term by stochastically dropping it during training. while effective  we note that this approach is solely  restricted to the lstm model  and pbo methods can be  easily applied to any rnn architecture.     . related works    evolutionary algorithms in reinforcement learning  recent works in deep reinforcement learning have explored  evolutionary algorithms as scalable alternatives to training  deep neural networks  such et al.        salimans et al.        . using simple random gaussian perturbations to mutate network weights at each training step  these methods  utilise large populations of individuals to efficiently converge on the optimum coefficients         offspring in  such et al.           all of which can be efficiently distributed on parallel workers. to maintain the speed of communication between workers for big networks with many  weight parameters  they propose a simple compact representations of weights in each offspring of the population   saving down a single random seed which can be used to  generate the full weight perturbation vector. while wellstudied in renouncement learning  little work has been done  to evaluate the efficacy of evolutionary algorithms in capturing long term dependencies with rnns. to the best of our  knowledge  this paper is the first to examine the use pbo  methods in the context of long term dependency modelling    along with its implications on time series forecasting.    architectural innovations the bulk of research in longterm dependency learning has focused on architectural improvements   especially pertaining to the internal memory  state of the rnn. the inclusion of the forget gate in longshort term memory  lstm   hochreiter   schmidhuber          for instance  reduces vanishing exploding gradient  issues by introducing linear temporal paths which facilitate gradient flow  kanuparthi et al.       . more recently   fourier recurrent units  frus   zhang et al.        have  been proposed  improving gradient flow via fourier basis  functions in its internal memory state. in other works   neil et al.        also introduced the phased lstm  plstm  to address situations where sparse  asynchronous  sensor updates infrequently contribute to predictions   using an additional time gate to control how often observations contribute to the lstm s internal memory state.  this helps to improve predictions for long event based  sequences  particularly where irregularly sampled data is  present. while issues with gradient based methods have  been addressed in part  long term dependency learning fundamentally remains an area of active research for rnns   with performance improvements still being gained by enhancing standard training methods even for existing architectures  see below .  modifications to standard rnn training an alternative class of methods investigates the enhancement of standard training methods  goodfellow et al.         namely  the augmentation of loss functions or gradient flows during training  trinh et al.        kanuparthi et al.       . in  trinh et al.         a combination of truncated bptt and  an auxiliary loss function is adopted   generated by selecting a random anchor point during training  feeding internal  states from that point into a separate prediction decoder   and backpropagating through the original rnn to a predetermined truncation point. in contrast to pbo   which  performs a full feed forward pass through the network to  compute losses   this approach stills applies backpropagation to truncated sequences  making it difficult to effectively learn dependencies beyond a specified window. alternatively  kanuparthi et al.        explicitly decompose  the lstm recursion equations into a bounded linear and  an unbounded polynomial gradient component  with the  former being responsible for long term dependency learn      . population based global optimisation  techniques  population based optimisation  pbo  methods are traditionally divided into two categories  wu et al.              evolutionary algorithms that mimic biological evolution   and    swarm intelligence approaches which simulate social behaviour of large groups of animals. for simplicity  and ease of comparison  we interchangeably refer to population members in both evolutionary algorithms and swarm  intelligence as individuals in this paper.  pbo methods in general comprise the following steps    . initialisation   create a default initial population of  individuals and optimisation parameters  e.g. randomly distributing them over weight space or setting  to  .   . population update   at each training iteration  the  weights for each individual are updated based on their  respective meta heuristics   e.g. by mutation or particle movement.   . score computation   loss functions are then evalu      population based global optimisation methods for learning long term dependencies with rnns    ated for each individual before control parameters are  updated   i.e. the generation of offspring for es and  global local optimum weights for pso   . repeating steps   and   until convergence.  we next proceed to describe our specific implementations  based on the general framework above.   . . evolution strategies  given the comparable performance between both deep  neuroevolution and evolution strategies  we adopt the simple es implementation explored in the reinforcement learning application of salimans et al.          an outline of  which is presented in algorithm   for reference.  algorithm   evolution strategies  input  training data x  learning rate    noise standard  deviation    initial weights     initialise global optimal weights   g           for k     to max iteration k do  for i     to n do  population update   sample  i   n     i   update individuals   i  k     g  k      i  score computation   compute reward r i     l  x    i  k    end for  set global weights    g  k         g  k     end for        n    pn    i      r i   i    here we define   i  k    rc to be the vector of c rnn parameters for individual i at training iteration k  and l  x      to be the loss function used for training given the input data  and network parameters  . we note that the loss function  is computed here by conducting a full feed forward pass  across the network  avoiding any truncation of the data or  minibatching beforehand.   . . neuroparticle swarm optimisation  given the relative simplicity of the mutation function used  in es  we also explore the use of more sophisticated population update rules through pso   which we refer to as  neuroparticle swarm optimisation  npso  in the context  rnn training.  adopting the formulation of shi   eberhart         a hy     perparameter w is defined for inertial weights  and set the  velocity v  i  k  and position   i  k  as below for each  training iteration.    i  k      i  k        v  i  k             v  i  k    w v  i  k         c  u   i  k    l  i  k          i  k          c  u   i  k    g  k          i  k                 where c    c      are fixed constants  u   i  k  and  u   i  k  are samples from standard uniform distributions  u         l  i  k      is the best position observed locally  by each particle  and  g  k      is the best global position  across all particles. a full description can be found in algorithm   for additional clarity  noting that  g  k  is used  to generate forecasts at run time.  algorithm   neuroparticle swarm optimisation  input  training data x  inertial weight w   initial weight variance      initialise  i       g        l  i       v  i            i       n        i  llmin  i       lgmin      for k     to max iteration k do  population update   v  i  k    update v  i  k        using equation        i  k      i  k        v  i  k   score computation   if l  x    i  k     lmin  i  then   l  i  k      i  k   lmin  i    l  x    i  k    if llmin  i    lgmin then   g  k     l  i  k   lgmin   llmin  i   end if  end if  end for     . experiments with intraday volatility  forecasting  to evaluate the effectiveness of the pbo in learning longterm dependencies  we apply our methods for training  rnns to the problem of volatility forecasting   a key  area of interest in finance. given the presence of volatility clustering at a daily time scales  cont        and  the evidence of intraday periodicity of returns volatility     population based global optimisation methods for learning long term dependencies with rnns     andersen   bollerslev         volatility datasets present  rnns with a mixture of long term and short term relationships to be learnt   making them particularly relevant for  our evaluation.   . . description of dataset  we consider the application of rnns to forecasting   min intraday realised variances  andersen et al.        for  ftse     index returns. this was derived using   min index returns sub sampled from thomson reuters tick history level    trth l   quote data from   january      to    july     .     . . results and discussion  network performance was evaluated using the meansquared error  mse  of one step ahead volatility forecasts   with results presented in table   normalised by the mse of  the lstm trained using sgd.    lstm  p lstm  fru    sgd    es    npso     .       .      .        .       .      .         .      .         .       table  . normalised mses for volatility forecasts     . . rnn benchmarks  tests are performed on a variety of modern rnn benchmarks as specified below     standard lstm  hochreiter   schmidhuber           phased lstm  p lstm   neil et al.           fourier recurrent unit  fru   zhang et al.         as described in section    both the p lstm and fru are  specifically designed for long term dependency modelling    allowing us to determine if these relationships can be  learnt using better architectures alone.   . . training methods  in addition  the following optimisation methods were tested  in experiments     stochastic gradient descent  sgd  with the adam  optimiser kingma   ba           evolution strategies  es   salimans et al.           neuroparticle swarm optimisation  npso   for the sgd approach      iterations of random search  are performed for hyperparameter optimisation with backpropagation performed up to a maximum of     epochs or  convergence   making up a maximum of   k feedforward  passes through network during training. to explicitly consider the effects of short truncation windows  rnns were  only unrolled back    time steps for bptt.  using this to set the overall computational budget  evolutionary computation methods utilised a population of     particles over    training iterations   limiting to    iterations of random search for hyperparameter optimisation.    from the mses reported  we can see that training rnns  using population based approaches methods lead to significant improvements in predictive performance   with es  reducing mses by more than     on average. performance improvements are also observed for architectures  designed specifically with long term dependencies in mind   overcoming the limitations with sgd.  while both es and npso do lead to better rnn performance in general  apart from the npso trained fru which  leads to large propagated errors  the simpler population  update rules in es appears to lead to more consistent results in general   with npso exhibiting a higher variance  across the architectures. this could be attributed to the hyperparameter ranges selected for our initial population and  inertial weights  and improved results can potentially be  achieved through better hyperparameter search and varying  the c  and c  parameters which are currently fixed.  focusing on the sgd trained models alone  we note  that more sophisticated architectures underperformed compared to the standard lstm for this specific volatility forecasting application. one possible reason is our use of very  short truncated segments for bptt   with rnns unrolled  for only    time steps   making it difficult for complex networks to learn the temporal relationships and resulting in  overfitting.     . conclusions and future work  in this paper  we investigate the use of population based  global optimisation techniques for learning long term dependencies with rnns in time series datasets. testing this  on an application in volatility forecasting  we observe that  these gradient free approaches help circumvent the issues  observed with standard sgd optimisation  leading to better  predictive performance across a variety of network architectures. while pbo does improve performance in general   simple evolution strategies appear to lead to more stable  results in our specific application.     population based global optimisation methods for learning long term dependencies with rnns    while our tests were performed on single workstations to  ensure a comparable computational load to sgd  we note  that es is typically used with large distributed computing  environments. as such  future extensions could achieve  even better results by using es with larger populations distributed over many parallel workers   unlocking the full  potential of pbo for time series prediction tasks.    