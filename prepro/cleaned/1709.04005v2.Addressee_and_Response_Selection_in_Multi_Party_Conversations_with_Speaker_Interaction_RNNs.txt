introduction    real world conversations often involve more than two  speakers. in the ubuntu internet relay chat channel  irc    for example  one user can initiate a discussion about an  ubuntu related technical issue  and many other users can  work together to solve the problem. dialogs can have complex speaker interactions  at each turn  users play one of  three roles  sender  addressee  observer   and those roles  vary across turns.  in this paper  we study the problem of addressee and response selection in multi party conversations  given a responding speaker and a dialog context  the task is to select  an addressee and a response from a set of candidates for the  responding speaker. the task requires modeling multi party  conversations and can be directly used to build retrievalbased dialog systems  lu and li       hu et al.        ji  lu  and li       wang et al.      .  the previous state of the art dynamic  rnn model  from ouchi and tsuboi        maintains speaker embeddings to track each speaker status  which dynamically  changes across time steps. it then produces the context embedding from the speaker embeddings and selects the adcopyright         association for the advancement of artificial  intelligence  www.aaai.org . all rights reserved.    dressee and response based on embedding similarity. however  this model updates only the sender embedding  not the  embeddings of the addressee or observers  with the corresponding utterance  and it selects the addressee and response  separately. in this way  it only models who says what and  fails to capture addressee information. experimental results  show that the separate selection process often produces inconsistent addressee response pairs.  to solve these issues  we introduce the speaker interaction recurrent neural network  si rnn . si rnn redesigns the dialog encoder by updating speaker embeddings in a role sensitive way. speaker embeddings are updated in different gru based units depending on their roles   sender  addressee  observer . furthermore  we note that the  addressee and response are mutually dependent and view  the task as a joint prediction problem. therefore  si rnn  models the conditional probability  of addressee given the  response and vice versa  and selects the addressee and response pair by maximizing the joint probability.  on a public standard benchmark data set  si rnn significantly improves the addressee and response selection  accuracy  particularly in complex conversations with many  speakers and responses to distant messages many turns in  the past. our code and data set are available online.          related work    we follow a data driven approach to dialog systems. singh  et al.         henderson  lemon  and georgila         and  young et al.        optimize the dialog policy using reinforcement learning or the partially observable markov decision process framework. in addition  henderson  thomson  and williams        propose to use a predefined ontology as a logical representation for the information exchanged in the conversation. the dialog system can be divided into different modules  such as natural language  understanding  yao et al.       mesnil et al.        dialog state tracking  henderson  thomson  and young        williams  raux  and henderson        and natural language generation  wen et al.      . furthermore  wen et  al.        and bordes and weston        propose end toend trainable goal oriented dialog systems.       the released code  https   github.com ryanzhumich sirnn     recently  short text conversation has been popular. the  system receives a short dialog context and generates a response using statistical machine translation or sequence tosequence networks  ritter  cherry  and dolan       vinyals  and le       shang  lu  and li       serban et al.        li et al.       mei  bansal  and walter      . in contrast to response generation  the retrieval based approach  uses a ranking model to select the highest scoring response from candidates  lu and li       hu et al.        ji  lu  and li       wang et al.      . however  these  models are single turn responding machines and thus still  are limited to short contexts with only two speakers. as  for larger context  lowe et al.        propose the next utterance classification  nuc  task for multi turn two party  dialogs. ouchi and tsuboi        extend nuc to multiparty conversations by integrating the addressee detection  problem. since the data is text based  they use only textual information to predict addressees as opposed to relying on acoustic signals or gaze information in multimodal dialog systems  jovanovic   akker  and nijholt        op den akker and traum      .  furthermore  several other papers are recently presented  focusing on modeling role specific information given the dialogue contexts  meng  mou  and jin       chi et al.        chen et al.      . for example  meng  mou  and jin         combine content and temporal information to predict the utterance speaker. by contrast  our sirnn explicitly utilizes  the speaker interaction to maintain speaker embeddings and  predicts the addressee and response by joint selection.        .     preliminaries     t      t     c     asender   aaddressee   u t    tt     t      t     where asender says u t  to aaddressee at time step t  and t  is the total number of time steps before the response and  addressee selection. the set of speakers appearing in c is  denoted a c . as for the output  the addressee is selected  from a c   and the response is selected from a set of candidates r. here  r contains the ground truth response and  one or more false responses. we provide some examples in  table    section   .    dynamic  rnn model    in this section  we briefly review the state of the art  dynamic  rnn model  ouchi and tsuboi        which  our proposed model is based on. dynamic  rnn solves the  task in two phases     the dialog encoder maintains a set of  speaker embeddings to track each speaker status  which dynamically changes with time step t     then dynamic  rnn  produces the context embedding from the speaker embeddings and selects the addressee and response based on embedding similarity among context  speaker  and utterance.    notation  ares  c  r  a   a c   r r   t   asender   t   aaddressee  u t   u t    t   ai    table    notations for the task and model.  dialog encoder. figure    left  illustrates the dialog encoder in dynamic  rnn on an example context. in this example  a  says u    to a    then a  says u    to a    and finally  a  says u    to a  . the context c will be   c     a    a    u        a    a    u        a    a    u                 with the set of speakers a c     a    a    a   .   t   for a speaker ai   the bold letter ai   rds denotes its  embedding at time step t. speaker embeddings are initialized as zero vectors and updated recurrently as hidden states  of grus  cho et al.       chung et al.      . specifically   for each time step t with the sender asdr and the utterance  u t    the sender embedding asdr is updated recurrently from  the utterance    t      t       asdr   gru asdr   u t        addressee and response selection    ouchi and tsuboi        propose the addressee and response selection task for multi party conversation. given a  responding speaker ares and a dialog context c  the task is  to select a response and an addressee. c is a list ordered by  time step      .     data  responding speaker  input  context  candidate responses  addressee  output  response  sender id at time t  addressee id at time t  utterance at time t  utterance embedding at time t  speaker embedding of ai at time t    where u t    rdu is the embedding for utterance u t  . other  speaker embeddings are updated from u t     . the speaker  embeddings are updated until time step t .  selection model. to summarize the whole dialog context  c  the model applies element wise max pooling over all the  speaker embeddings to get the context embedding hc    hc      max    ai  a   ... a a c       t      ai      rds           the probability of an addressee and a response being the  ground truth is calculated based on embedding similarity. to  be specific  for addressee selection  the model compares the  candidate speaker ap   the dialog context c  and the responding speaker ares    p ap  c       ares   hc    wa ap             where ares is the final speaker embedding for the responding  speaker ares   ap is the final speaker embedding for the candidate addressee ap     is the logistic sigmoid function         is the row wise concatenation operator  and wa   r ds  ds  is a learnable parameter. similarly  for response selection   p rq  c       ares   hc    wr rq    du           where rq   r is the embedding for the candidate response  rq   and wr   r ds  du is a learnable parameter.     figure    dialog encoders in dynamic  rnn  left  and si rnn  right  for an example context at the top. speaker embeddings are initialized as zero vectors and updated recurrently as hidden states along the time step. in si rnn  the same speaker  embedding is updated in different units depending on the role  igrus for sender  igrua for addressee  gruo for observer .  the model is trained end to end to minimize a joint crossentropy loss for the addressee selection and the response selection with equal weights. at test time  the addressee and  the response are separately selected to maximize the probability in eq   and eq  .         speaker interaction rnn    while dynamic  rnn can track the speaker status by capturing who says what in multi party conversation  there are  still some issues. first  at each time step  only the sender  embedding is updated from the utterance. therefore  other  speakers are blind to what is being said  and the model  fails to capture addressee information. second  while the addressee and response are mutually dependent  dynamic rnn selects them independently. consider a case where  the responding speaker is talking to two other speakers  in separate conversation threads. the choice of addressee  is likely to be either of the two speakers  but the choice  is much less ambiguous if the correct response is given   and vice versa. dynamic  rnn often produces inconsistent addressee response pairs due to the separate selection.  see table   for examples.  in contrast to dynamic  rnn  the dialog encoder in sirnn updates embeddings for all the speakers besides the  sender at each time step. speaker embeddings are updated  depending on their roles  the update of the sender is different  from the addressee  which is different from the observers.  furthermore  the update of a speaker embedding is not only  from the utterance  but also from other speakers. these are  achieved by designing variations of grus for different roles.  finally  si rnn selects the addressee and response jointly  by maximizing the joint probability.     .     utterance encoder    to encode an utterance u    w    w    ...  wn   of n words   we use a rnn with gated recurrent units  cho et al.        chung et al.         hj   gru hj     xj      where xj is the word embedding for wj   and hj is the gru  hidden state. h  is initialized as a zero vector  and the utterance embedding is the last hidden state  i.e. u   hn .     .     dialog encoder    figure    right  shows how si rnn encodes the example in  eq  . unlike dynamic  rnn  si rnn updates all speaker  embeddings in a role sensitive manner. for example  at the  first time step when a  says u    to a    dynamic  rnn  only updates a  using u      while other speakers are updated  using  . in contrast  si rnn updates each speaker status  with different units  igrus updates the sender embedding  a  from the utterance embedding u    and the addressee embedding a    igrua updates the addressee embedding a   from u    and a    gruo updates the observer embedding  a  from u    .  algorithm   gives a formal definition of the dialog encoder in si rnn. the dialog encoder is a function that takes  as input a dialog context c  lines      and returns speaker  embeddings at the final time step  lines       . speaker  embeddings are initialized as ds  dimensional zero vectors   lines     . speaker embeddings are updated by iterating  over each line in the context  lines       .     .     role sensitive update    in this subsection  we explain in detail how  igrus  igrua  gruo update speaker embeddings  according to their roles at each time step  algorithm   lines        .  as shown in figure    igrus  igrua  gruo are all  gru based units. igrus updates the sender embedding   t     from the previous sender embedding asdr   the previous   t     addressee embedding aadr   and the utterance embedding  u t      t    t      t     asdr   igrus  asdr   aadr   u t        algorithm   dialog encoder in si rnn     input     dialog context c with speakers a c     t      t      t      t        c     asender   aaddressee   u t    t  t       a c     a    a    . . .   a a c         where asender   aaddressee   a c         initialize speaker embeddings     for ai   a    a    . . .   a a c   do             a i       rd s     end for        update speaker embeddings      for t         . . .   t do          update sender  addressee  observers                                                                                               t     asdr   asender   t   aadr   aaddressee  o   a c     asdr   aadr       compute utterance embedding  u t    utteranceencoder u t      t     u t    concatenate asdr   u t        update sender embedding   t    t      t     asdr   igrus  asdr   aadr   u t        update addressee embedding   t    t      t     aadr   igrua  aadr   asdr   u t        update observer embeddings  for aobr in o do   t    t     aobr   gruo  aobr   u t     end for  end for     return final speaker embeddings  output   t    return ai for ai   a    a    . . .   a a c      figure    illustration of igrus  upper  blue   igrua   middle  green   and gruo  lower  yellow . filled circles  are speaker embeddings  which are recurrently updated. unfilled circles are gates. filled squares are speaker embedding  proposals.    the update  as illustrated in the upper part of figure    is   t   controlled by three gates. the rs gate controls the previ t      t   ous sender embedding asdr   and ps controls the previous   t     addressee embedding aadr . those two gated interactions   t   esdr . fitogether produce the sender embedding proposal a   t    t   esdr and  nally  the update gate zs combines the proposal a   t     the previous sender embedding asdr to update the sender   t   embedding asdr . the computations in igrus  including   t    t    t    t   esdr   and the  gates rs   ps   zs   the proposal embedding a   t   final updated embedding asdr   are formulated as    t      t         vsr aadr       t      t         vsp aadr       t      t         vsz aadr      rs    wsr u t    urs asdr  ps    wsp u t    ups asdr  zs    wsz u t    uzs asdr   t      t        t        t        t      t       esdr   tanh ws u t    us  rs  a   t       vs  ps   t      t     asdr   zs     t       asdr    asdr       t       aadr      t            zs       t     esdr  a    where  wsr   wsp   wsz   urs   ups   uzs   vsr   vsp   vsz   ws    us   vs   are learnable parameters. igrua uses the same  formulation with a different set of parameters  as illustrated  in the middle of figure  . in addition  we update the observer  embeddings from the utterance. gruo is implemented  as the traditional gru unit in the lower part of figure  .  note that the parameters in igrus  igrua  gruo are not  shared. this allows si rnn to learn role dependent features  to control speaker embedding updates. the formulations of  igrua and gruo are similar.     .     joint selection    the dialog encoder takes the dialog context c as input and   t    returns speaker embeddings at the final time step  ai . recall from section  .  that dynamic  rnn produces the  context embedding hc using eq   and then selects the addressee and response separately using eq   and eq  .  in contrast  si rnn performs addressee and response  selection jointly  the response is dependent on the addressee and vice versa. therefore  we view the task as a sequence prediction process  given the context and responding  speaker  we first predict the addressee  and then predict the  response given the addressee.  we also use the reversed prediction order as in eq  .   in addition to eq   and eq    si rnn is also trained  to model the conditional probability as follows. to predict  the addressee  we calculate the probability of the candidate  speaker ap to be the ground truth given the ground truth response r  available during training time    p ap  c  r       ares   hc   r   war ap              the key difference from eq   is that eq   is conditioned on  the correct response r with embedding r. similarly  for response selection  we calculate the probability of a candidate  response rq given the ground truth addressee aadr    p rq  c  aadr        ares   hc   aadr    wra rq             at test time  si rnn selects the addressee response  pair from a c    r to maximize the joint probability  p rq   ap  c    a   r       arg max    p rq   ap  c     ap  rq  a c  r         arg max    p rq  c    p ap  c  rq             ap  rq  a c  r      p ap  c    p rq  c  ap    in eq    we decompose the joint probability into two terms   the first term selects the response given the context  and then  selects the addressee given the context and the selected response  the second term selects the addressee and response  in the reversed order.          experimental setup    data set. we use the ubuntu multiparty conversation  corpus  ouchi and tsuboi       and summarize the data  statistics in table  . the whole data set  including the  train dev test split and the false response candidates  is  publicly available.  the data set is built from the ubuntu  irc chat room where a number of users discuss ubunturelated technical issues. the log is organized as one file per  day corresponding to a document d. each document consists of  time  senderid  utterance  lines. if users explicitly  mention addressees at the beginning of the utterance  the addresseeid is extracted. then a sample  namely a unit of input   the dialog context and the current sender  and output  the  addressee and response prediction  for the task  is created  to predict the ground truth addressee and response of this  line. note that samples are created only when the addressee  is explicitly mentioned for clear  unambiguous ground truth  labels. false response candidates are randomly chosen from  all other utterances within the same document. therefore   distractors are likely from the same sub conversation or even  from the same sender but at different time steps. this makes  it harder than lowe et al.        where distractors are randomly chosen from all documents. if no addressee is explicitly mentioned  the addressee is left blank and the line is  marked as a part of the context.  baselines. apart from dynamic  rnn  we also include  several other baselines. r ecent  tf idf always selects  the most recent speaker  except the responding speaker  ares   as the addressee and chooses the response to maximize the tf idf cosine similarity with the context. we improve it by using a slightly different addressee selection       detail  we also considered an alternative decomposition of  the joint probability as log p rq   ap  c        log p rq  c     log p ap  c  rq     log p ap  c    log p rq  c  ap     but the performance was similar to eq  .     https   github.com hiroki   response ranking tree master   data input    heuristic  d irect r ecent  tf idf   select the most recent speaker that directly talks to ares by an explicit addressee mention. we select from the previous    utterances   which is the longest context among all the experiments.  this works much better when there are multiple concurrent sub conversations  and ares responds to a distant message in the context. we also include another gru based  model s tatic  rnn from ouchi and tsuboi       . unlike  dynamic  rnn  speaker embeddings in s tatic  rnn are  based on the order of speakers and are fixed. furthermore   inspired by zhou et al.        and serban et al.          we implement s tatic  h ier  rnn  a hierarchical version  of s tatic  rnn. it first builds utterance embeddings from  words and then uses high level rnns to process utterance  embeddings.  implementation details for a fair comparison  we follow the hyperparameters from ouchi and tsuboi          which are chosen based on the validation data set. we  take a maximum of    words for each utterance. we use      dimensional glove word vectors    which are fixed during training. si rnn uses    dimensional vectors for both  speaker embeddings and hidden states. model parameters  are initialized with a uniform distribution between   .   and   .  . we set the mini batch size to    . the joint crossentropy loss function with  .    l  weight decay is minimized by adam  kingma and ba      . the training is  stopped early if the validation accuracy is not improved for    consecutive epochs. all experiments are performed on a  single gtx titan x gpu. the maximum number of epochs  is     and most models converge within    epochs.         results and discussion    for fair and meaningful quantitative comparisons  we follow  ouchi and tsuboi        s evaluation protocols. si rnn  improves the overall accuracy on the addressee and response  selection task. two ablation experiments further analyze the  contribution of role sensitive units and joint selection respectively. we then confirm the robustness of si rnn with  the number of speakers and distant responses. finally  in a  case study we discuss how si rnn handles complex conversations by either engaging in a new sub conversation or  responding to a distant message.  overall result. as shown in table    si rnn significantly  improves upon the previous state of the art. in particular   addressee selection  adr  benefits most  with different number of candidate responses  denoted as res cand   around      in res cand     and more than     in res cand      . response selection  res  is also improved  suggesting role sensitive grus and joint selection are helpful for  response selection as well. the improvement is more obvious with more candidate responses     in res cand      and    in res cand      . these together result in significantly better accuracy on the adr res metric as well.  ablation study. we show an ablation study in the last  rows of table  . first  we share the parameters of  igrus  igrua  gruo . the accuracy decreases significantly  indicating that it is crucial to learn role sensitive units       http   nlp.stanford.edu projects glove      chance  recent tf idf  direct recent tf idf  static rnn   ouchi and tsuboi        static hier rnn   zhou et al.         serban et al.        dynamic rnn   ouchi and tsuboi        si rnn  ours   si rnn w  shared igrus  si rnn w o joint selection    t                                                                dev  adr res   .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      res cand      test  adr res adr   .     .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      res    .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      dev  adr res   .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      res cand       test  adr res adr   .     .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      res    .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      .      table    addressee and response selection results on the ubuntu multiparty conversation corpus. metrics include accuracy  of addressee selection  adr   response selection  res   and pair selection  adr res . res cand  the number of candidate  responses. t   the context length.      docs    utters    samples  adr mention freq    speakers   doc    utters   doc    words   utter    total          . m    .      .     .     train          . m     . k   .      .      .     .     dev          . k    . k   .      .      .     .     test          . k    . k   .      .      .     .     table    data statistics.  adr mention freq  is the frequency of explicit addressee mention.    to update speaker embeddings. second  to examine our joint  selection  we fall back to selecting the addressee and response separately  as in dynamic  rnn. we find that joint  selection improves adr and res individually  and it is particularly helpful for pair selection adr res.  number of speakers. numerous speakers create complex dialogs and increased candidate addressee  thus the  task becomes more challenging. in figure    upper   we  investigate how adr accuracy changes with the number  of speakers in the context of length     corresponding  to the rows with t    in table  . r ecent  tf idf always chooses the most recent speaker and the accuracy  drops dramatically as the number of speakers increases.  d irect r ecent  tf idf shows better performance  and  dynamic  rnnis marginally better. si rnn is much more  robust and remains above     accuracy across all bins. the  advantage is more obvious for bins with more speakers.  addressing distance. addressing distance is the time difference from the responding speaker to the ground truth addressee. as the histogram in figure    lower  shows  while  the majority of responses target the most recent speaker   many responses go back five or more time steps. it is im     figure    effect of the number of speakers in the context   upper  and the addressee distance  lower . left axis  the  histogram shows the number of test examples. right axis   the curves show adr accuracy on the test set.    portant to note that for those distant responses  dynamic rnn sees a clear performance decrease  even worse than  d irect r ecent  tf idf. in contrast  si rnn handles  distant responses much more accurately.                sender  codepython  wafflejock  wafflejock    addressee  wafflejock  codepython  theoletom         codepython                                      guest       theoletom  guest       theoletom  theoletom  releaf  releaf  releaf  codepython    wafflejock                  jordan u  umeaboy  wafflejock  model prediction  direct recent tf idf  dynamic rnn  si rnn    codepython  releaf  addressee  theoletom  codepython    releaf    utterance  thanks  yup np  you can use sudo apt get install packagename   reinstall  to have apt get install reinstall some  package metapackage and redo the configuration for the program as well  i installed ubuntu on a separate external drive. now when i boot into mac  the external drive does  not show up as bootable. the blue light is on. any ideas   hello there. wondering to anyone who knows  where an ubuntu backup can be retrieved from.  it s not a program. it s a desktop environment.  did some searching on my system and googling  but couldn t find an answer  be a trace of it left yet there still is.  i think i might just need a fresh install of ubuntu. if there isn t a way to revert to default settings  what s your opinion on a      laptop that will be a dedicated ubuntu machine   are any of the pre loaded ones good deals   if not  are there any laptops that are known for being oem heavy or otherwise ubuntu friendly   my usb stick shows up as bootable  efi  when i boot my mac. but not my external hard drive on  which i just installed ubuntu. how do i make it bootable from mac hardware   did you install ubuntu to this external drive from a different machine   what country you from   response  ubuntu install fresh  no prime is the replacement    there are a few ubuntu dedicated laptop providers like umeaboy is asking depends on where  you are     a  si rnn chooses to engage in a new sub conversation by suggesting a solution to  releaf  about ubuntu dedicated laptops.  sender  addressee  utterance     verybewitching  nicomachus  anything i should be concerned about before i do it   nicomachus  verybewitching  always back up before partitioning.        verybewitching  nicomachus  i would have assumed that  i was wondering more if this is something that tends to be  touch and go  want to know if i should put coffee on         techmonger  it was hybernating. i can ping it now  techmonger  why does my router pick up disconnected devices when i reset my device list  or how        ionic  because the dhcp refresh interval hasn t passed yet   techmonger  so dhcp refresh is different than device list refresh      d  p  techmonger  what an enlightenment  techmonger            buzzardbuzz  dhcp refresh for all clients is needed when you change your subnet ip  buzzardbuzz  if you want them to work together     ionic  buzzardbuzz  uhm  no.        chingao  techmonger  nicomachus asked this way at the beginning  is the machine that you  re trying to ping  turned on      nicomachus  model prediction  addressee  response  direct recent tf idf   verybewitching i have tried with this program y ppa manager  yet still doesn t work.  dynamic rnn  chingao  install the package  linux generic   that will install the kernel and the headers if they are  not installed  si rnn    verybewitching   if it s the last partition on the disk  it won t take long. if gparted has to copy data to  move another partition too  it can take a couple hours.   b  si rnn remembers the distant sub conversation   and responds to  verybewitching  with a detailed answer.    table    case study.   denotes the ground truth. sub conversations are coded with different numbers for the purpose of analysis   sub conversation labels are not available during training or testing .  case study. examples in table   show how si rnn can  handle complex multi party conversations by selecting from     candidate responses. in both examples  the responding speakers participate in two or more concurrent subconversations with other speakers.  example  a  demonstrates the ability of si rnn to engage in a new sub conversation. the responding speaker   wafflejock  is originally involved in two sub conversations     the sub conversation   with  codepython   and the ubuntu  installation issue with  theoletom . while it is reasonable to  address  codepython  and  theoletom   the responses from  other baselines are not helpful to solve corresponding issues.  tf idf prefers the response with the  install  key word   yet the response is repetitive and not helpful. dynamic rnn selects an irrelevant response to  codepython . sirnn chooses to engage in a new sub conversation by sug      gesting a solution to  releaf  about ubuntu dedicated laptops.  example  b  shows the advantage of si rnn in responding to a distant message. the responding speaker  nicomachus  is actively engaged with  verybewitching  in  the sub conversation   and is also loosely involved in the  sub conversation     chingao  mentions  nicomachus  in  the most recent utterance. si rnn remembers the distant  sub conversation   and responds to  verybewitching  with  a detailed answer. d irect r ecent  tf idf selects the  ground truth addressee because  verybewitching  talks to   nicomachus   but the response is not helpful. dynamic rnn is biased to the recent speaker  chingao   yet the response is not relevant.         conclusion    si rnn jointly models who says what to whom by updating speaker embeddings in a role sensitive way. it provides  state of the art addressee and response selection  which can  instantly help retrieval based dialog systems. in the future   we also consider using si rnn to extract sub conversations  in the unlabeled conversation corpus and provide a largescale disentangled multi party conversation data set.         acknowledgements    we thank the members of the umichigan ibm sapphire  project and all the reviewers for their helpful feedback. this  material is based in part upon work supported by ibm under  contract           . any opinions  findings  conclusions  or recommendations expressed above are those of the authors and do not necessarily reflect the views of ibm.    