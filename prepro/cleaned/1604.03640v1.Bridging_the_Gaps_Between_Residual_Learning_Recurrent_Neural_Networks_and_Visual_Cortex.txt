introduction    residual learning      a novel deep learning scheme characterized by ultra deep architectures has recently achieved  state of the art performance on several popular vision benchmarks. the most recent incarnation of this idea       with hundreds of layers demonstrate consistent performance improvement over shallower networks. the  .    top    error achieved by residual networks on the imagenet test set arguably rivals human performance.  because of recent claims      that networks of the alexnet     type successfully predict properties of neurons in  visual cortex  one natural question arises  how similar is an ultra deep residual network to the primate cortex  a  notable difference is the depth. while a residual network has as many as      layers     biological systems seem  to have two orders of magnitude less  if we make the customary assumption that a layer in the nn architecture  corresponds to a cortical area. in fact  there are about half a dozen areas in the ventral stream of visual cortex from  the retina to the inferior temporal cortex. notice that it takes in the order of   ms for neural activity to propagate  from one area to another one  remember that spiking activity of cortical neurons is usually well below     hz .  the evolutionary advantage of having fewer layers is apparent  it supports rapid     msec from image onset to  meaningful information in it neural population  visual recognition  which is a key ability of human and non human  primates         .  it is intriguingly possible to account for this discrepancy by taking into account recurrent connections within each  visual area. areas in visual cortex comprise six different layers with lateral and feedback connections       which are  believed to mediate some attentional effects                     and even learning  such as backpropagation      .   unrolling  in time the recurrent computations carried out by the visual cortex provides an equivalent  ultra deep   feedforward network  which might represent a more appropriate comparison with the state of the art computer  vision models.  in addition  we conjecture that the effectiveness of recent  ultra deep  neural networks primarily come from the  fact they can efficiently model the recurrent computations that are required by the recognition task. we show  compelling evidences for this conjecture by demonstrating that  . a deep residual network is formally equivalent to  a shallow rnn   . such a rnn with weight sharing  thus with orders of magnitude less parameters  depending on  the unrolling depth   can retain most of the performance of the corresponding deep residual network.  furthermore  we generalize such a rnn into a class of models that are more biologically plausible models of cortex  and show their effectiveness on cifar   .        .     equivalence of resnet and rnn  intuition    we discuss here a very simple observation  a residual network  resnet  approximates a specific  standard recurrent  neural network  rnn  implementing the discrete dynamical system described by    ht   k    ht       ht             where ht is the activity of the neural layer at time t and k is a nonlinear operator. such a dynamical systems  corresponds to the feedback system of figure    b . figure    a  shows that unrolling in  discrete  time the feedback  system gives a deep residual network with the same  that is  shared  weights among the layers. the number of  layers in the unrolled network corresponds to the discrete time iterations of the dynamical system. the identity  shortcut mapping that characterizes residual learning appears in the figure.  thus  resnets with shared weights can be reformulated into the form of a recurrent system. in section  .   we show  experimentally that a resnet with shared weights retains most of its performance  on cifar    .  a comparison of a plain rnn and resnet with shared weights is in the appendix figure   .             h    ...    h         i    k    h     unfold       i    i    k    fold    h        i    h     k i         k    x    x      a  resnet with shared weights    xt x  t     b  resnet in recurrent form    figure    a formal equivalence of a resnet  a  with weight sharing and a rnn  b . i is the identity operator. k is  an operator denoting the nonlinear transformation called f in the main text. xt is the value of the input at time t.   t is a kronecker delta function.     .     formulation in terms of dynamical systems  and feedback     we frame recurrent and residual neural networks in the language of dynamical systems. we consider here dynamical  systems in discrete time  though most of the definitions carry over to continuous time. a neural network  that we  assume for simplicity to have a single layer with n neurons  can be a dynamical system with a dynamics defined as    ht     f  ht   wt     xt           where ht   rn is the activity of the n neurons in the layer at time t and f   rn   rn is a continuous  bounded  function parametrized by the vector of weights wt . in a typical neural network  f is synthesized by the following  relation between the activity yt of a single neuron and its inputs xt        yt     hw  xt   i   b             where   is a nonlinear function such as the linear rectifier               .  a standard classification of dynamical systems defines the system as     . homogeneous if xt       t      alternatively the equation reads as ht     f  ht   wt   with the inital condition  h    x      . time invariant if wt   w.    residual networks with weight sharing thus correspond to homogeneous  time invariant systems which in turn  correspond to a feedback system  see figure    with an input which is non zero only at time t      xt     x    xt       t      and with f  z     k   i    z           hn   f  ht   wt      k   i n   x             normal  residual networks correspond to homogeneous  time variant systems. an analysis of the corresponding  inhomogeneous  time invariant system is provided in the appendix.         a generalized rnn for multi stage fully recurrent processing    as shown in the previous section  the recurrent form of a resnet is actually shallow  if we ignore the possible depth  of the operator k . in this section  we generalize it into a moderately deep rnn that reflects the multi stage  processing in the primate visual cortex.     .     multi state graph    we propose a general formulation that can capture the computations performed by a multi stage processing hierarchy  with full recurrent connections. such a hierarchy can be characterized by a directed  cyclic  graph g with vertices  v and edges e.    g    v  e            where vertices v is a set contains all the processing stages  i.e.  we also call them states . take the ventral stream  of visual cortex for example  v    lgn  v    v    v    it  . note that retina is not listed since there is no known  feedback from primate cortex to the retina. the edges e are a set that contains all the connections  i.e.  transition  functions  between all vertices states  e.g.  v  v   v  v   v  it  etc. one example of such a graph is in figure     a .     .     pre net and post net    the multi state fully recurrent system does not have to receive raw inputs. rather  a  deep  neural network can  serve as a preprocesser. we call the preprocesser a  pre net  as shown in figure    b . on the other hand  one also  needs a  post net  as a postprocessor and provide supervisory signals to the recurrent system and the pre net. the  pre net  recurrent system and post net are trained in an end to end fashion with backpropagation.  for most models in this paper  unless stated otherwise  the pre net is a simple  x  convolutional layer and the  post net is a pipeline of a batch normalization  a relu  a global average pooling and a fully connected layer  or a   x  convolution  we use these terms interchangeably .  take primate visual system for instance  the retina is a part of the  pre net . it does not receive any feedback from  the cortex and thus can be separated from the recurrent system for simplicity. in section  . .   we also tried   layers  of  x  convolutions as an pre net  which might be more similar to a retina  and observed slightly better performance.     .     transition matrix    the set of edges e can be represented as a   d matrix where each element  i j  represents the transition function  from state i to state j.  one can also extend the representation of e to a   d matrix  where the third dimension is time and each element   i j t  represents the transition function from state i to state j at time t. in this formulation  the transition functions        pre net    post net  ...    retina    v     v     v     it    v     v     v     loss    it    ...  input     a  multi state  fully  recurrent neural network     b  full model    ...    t t    v     v     v     it    pool  loss    loss    h    ...    conv    ...  k i    t      v     v     v     it    h  k i    t      v     v     v     it    h    ...    input    input  optional     c  simulating our model in time by unrolling    conv     d  an example resnet  for comparison    figure    modeling the ventral stream of visual cortex using a multi state fully recurrent neural network  can vary over time  e.g.  being blocked from time t  to time t    etc. . the increased expressive power of this  formulation allows us to design a system where multiple locally recurrent systems are connected sequentially  a  downstream recurrent system only receives inputs when its upstream recurrent system finishes  similar to recurrent  convolutional neural networks  e.g.       . this system with non shared weights can also represent exactly the  state of the art resnet  see figure   .  nevertheless  time dependent dynamical systems  that is recurrent networks of real neurons and synapses  offer  interesting questions about the flexibility in controlling their time dependent parameters.  example transition matrices used in this paper are shown in figure  .  when there are multiple transition functions to a state  their outputs are summed together.     .     shared vs. non shared weights    weight sharing is described at the level of an unrolled network. thus  it is possible to have unshared weights with a   d transition matrix   even if the transitions are stable over time  their weights could be time variant.  given an unrolled network  a weight sharing configuration can be described as a set s  whose element is a set of tied        pool  conv    pool  e.g.  size    x  x      ...    e.g.  size   x x      loss    h    ...    ...    e.g.  size    x  x      k i    ...    e.g.  size    x  x    k i    input    h    k  i    h     h   k  i    input     a  resnet without changing spacial feature sizes    h    h   k  i    h     conv    pre net    loss    h     h     h    e.g.  size    x  x      conv    connection available only  at a specified time t    conv     b  resnet with changes of spacial feature sizes  he et. al.     post net  pre net     c  recurrent form of a    h     h   subsample   increase  features    h     post net    subsample   increase  features     d  recurrent form of b    figure    we show two types of resnet and corresponding rnns.  a  and  c   single state resnet. the spacial  and featural sizes are fixed.  b  and  d    state resnet. the spacial size is reduced by a factor of   and the featural  size is doubled at time n and  n  where n is a meta parameter. this type corresponds to the ones proposed by he  et. al.    .  weights s    wi   j   t    ...  wim  jm  tm    where wim  jm  tm denotes the weight of the transition functions from state im  to jm at time tm . this requires   . all weights wim  jm  tm   s have the same initial values.  . the actual gradients  used for updating each element of s is the sum of the gradients of all elements in s    w   s       x  e   e   used        original   w   w                w  s    where e is the training objective.  for rnns  weights are usually shared across time  but one could unshare the weights  share across states or perform  more complicated sharing using this framework.     .     notations  unrolling depth vs. readout time    the meaning of  unrolling depth  may vary in different rnn models since  unrolling  a cyclic graph is not well  defined. in this paper  we adopt a biologically plausible definition  we simulate the time after the onset of the visual  stimuli assuming each transition function takes constant time  . we use the term  readout time  to refer to the  time the post net reads the data from the last state.  this definition in principle allows one to have quantitive comparisons with biological systems. e.g.  for a model with  readout time t in this paper  the wall clock time can be estimated to be   t to   t ms  considering the latency of a  single layer of biological neurons.  regarding the initial values  at t     all states are empty except that the first state has some data received from the        bn relu conv x    brcx      brcx  i    brcx     brcx     brcx  i    v     v     v     brcx     it  h     brcx  i    bn relu deconv x    brdx    t  ...inf.    v     v     v     v     brcx  i    brcx     brcx     brcx     v     brdx     brcx  i    brcx     brcx     v     brdx     h     brdx     brdx     h   brcx  i  brdx     h   brcx     brcx  i    brdx     brcx     h     h     connection available only  at a specified time t    brcx  i    h   t  ...inf.    h     h     h     brcx  i    brcx     h     brdx     brcx  i    brcx   brcx     brdx  brcx  i    t    ...inf.    h     h     h     brcx  i    conv   only at t n     h   h      c  an example   d  transition matrix  for a   state fully  recurrent nn     b  an example   d transition  matrix for a   state fully  recurrent nn    brcx  i    h     brcx  i     a  an example   d transition matrix  for a   state fully recurrent nn for  modeling the visual cortex    brcx  i    h     brcx     brcx  i    brcx     h   brdx     h     it    h     it    h     brcx     brcx  i    t  ...inf.    brdx  brcx  i    h   brcx     brcx     h     brdx     brcx  i    brcx  i    h   t    ...inf.  conv   only at t  n     h     h  brcx  i    brcx  i     d    d transition matrix of  a   state resnet     e  transition  matrix of a    state resnet    figure    the transition matrices used in the paper.  bn  denotes batch normalization and  conv  denotes  convolution. deconvolution layer  denoted by  deconv   is      used as a transition function from a spacially small  state to a spacially large one. brcx  brdx  denotes a bn relu conv deconv bn relu conv deconv pipeline   similar to a residual module      . there is always a  x  subsampling upsampling between nearby states  e.g.   v  h     x    v  h     x    v  h   x   it  x  . stride    convolution  or upsampling    deconvolution  is used  in transition functions to match the spacial sizes of input and output states. the intermediate feature sizes of  transition function brcx  brdx  or brcx  brdx  are chosen to be the average feature size of input and output  states.   i  denotes a identity shortcut mapping. the design of transition functions could be an interesting topic  for future research.  pre net. we only start simulate a transition function when its input state is populated.     .     sequential vs. static inputs outputs    as an rnn  our model supports sequential data processing and in principle all other tasks supported by traditional  rnns. see figure   for illustrations. however  if there is a batch normalization in the model  we have to use   time specific normalization  described in section  .   which might not be feasible for some tasks.     .     batch normalizations for rnns    as an additional observation  we found that it generally hurts performance when the normalization statistics  e.g.   average  standard deviation  learnable scaling and shifting parameters  in batch normalization are shared across  time. this may be consistent with the observations from     .  however  good performance is restored if we apply a procedure we call a  time specific normalization   mean and  standard deviation are calculated independently for every t  using training set . the learnable scaling and shifting  parameters should be time specific. but in most models we do not use the learnable parameters of bn since they  tend not to affect the performance much.  we expect this procedure to benefit other rnns trained with batch normalization. however  to use this procedure   one needs to have a initial t     and enumerate all possible ts. this is feasible for visual processing but needs  modifications for other tasks.          input    recurrence    output  ...    t t    v     v     ...    v     yt    it    ...    xt    t      v     v     v     y     it    ...  x     v     v     v     it    ...  x     v     v     it    v     v     v     it    v     v     v     it    v     v     v     it    one to one    ...    t      v     v     v     v     it    v     v     v     it    v     v     v     it    v     v     v     it    one to many    v     v     v     it    v     v     v     it    v     v     v     it    v     v     v     it    t t  .  .  .  t    t      many to one  v     v     v     it    v     v     v     it    v     v     v     it    v     v     v     it    t t  .  .  .  t    t      many to many    an example of sequential data processing    figure    our model supports sequential inputs outputs  which includes one to one  many to one  one to many and  many to many input output mappings.         related work    deep recurrent neural networks  our final model is deep and similar to a stacked rnn            with several  main differences   . our model has feedback transitions between hidden layers and self transition from each hidden  layer to itself.  . our model has identity shortcut mappings inspired by residual learning.  . our transition functions  are deep and convolutional.  as suggested by       the term depth in rnn could also refer to input to hidden  hidden to hidden or hidden to output  connections. our model is deep in all of these senses. see section  . .  recursive neural networks and convolutional recurrent neural networks  when unfolding rnn  into a feedforward network  the weights of many layers are tied. this is reminiscent of recursive neural networks   recursive nn   first proposed by     . recursive nn are characterized by applying same operations recursively on  a structure. the convolutional version was first studied by    . subsequent related work includes      and     . one  characteristic distinguishes our model and residual learning from recursive nn and convolutional recurrent nn is  whether there are identity shortcut mappings. this discrepancy seems to account for the superior performance of  residual learning and of our model over the latters.  a recent report     we became aware of after we finished this work discusses the idea of imitating cortical feedback  by introducing loops into neural networks.  a highway network      is a feedforward network inspired by long short term memory      featuring more general  shortcut mappings  instead of hardwired identity mappings used by resnet .        .     experiments  dataset and training details    we test all models on the standard cifar         dataset. all images are   x   pixels with color. data augmentation  is performed in the same way as    .          momentum was used with hyperparameter  . . experiments were run for    epochs with batchsize    unless stated  otherwise. the learning rates are  .   for the first    epochs   .    for epoch    to    and  .     for the last     epochs. all experiments used the cross entropy loss function and softmax for classification. batch normalization   bn       is used for all experiments. but the learnable scaling and shifting parameters are not used  except for the  last bn layer in the post net . network weights were initialized with the method described in    . although we do  not expect the initialization to matter as long as batch normalization is used. the implementations are based on  matconvnet    .     .    . .     experiment a  resnet with shared weights  sharing weights across time    we conjecture that the effectiveness of resnet mainly comes from the fact that it efficiently models the recurrent  computations required by the recognition task. if this is the case  one should be able to reinterpret resnet as a  rnn with weight sharing and achieve comparable performance to the original version. we demonstrate various  incarnations of this idea and show it is indeed the case.  we tested the   state and   state resnets described in figure   and  . the results are shown in figure  .     .      .           .      .      .                  epoch                .      .      .      .                  epoch                .     non shared   param         shared   param           .      .      .      .      .                  epoch                 state  resnet     .      .      .      .      .                  epoch                 state  fully recurrent     .     non shared   param         shared   param           non shared   param          shared   param            .     validation error on cifar        .      .      .     non shared   param          shared   param            .       state  resnet    validation error on cifar        .       state  fully recurrent     .     non shared   param         shared   param            .     training error on cifar        .     training error on cifar        .     non shared   param         shared   param          training error on cifar        .       state  resnet    validation error on cifar         state  resnet     .      .      .      .                  epoch                            epoch          figure    all models are robust to sharing weights across time. this supports our conjecture that deep networks  can be well approximated by shallow moderately deep rnns. the transition matrices of all models are shown in  figure  .   param  denotes the number of parameters. the   state resnet has a single state of size   x  x     height   width    features . it was trained and tested with readout time t   . the   state resnet has   states  of size   x  x      x  x   and  x x     there is a transition  via a simple convolution  at time   and     each  state has a self transition unrolled   times. the   state fully recurrent nn has   states of the same size    x  x  .  it was trained and tested with readout time t     same as   state resnet . it is a generalization of and directly  comparable with   state resnet  showing the benefit of having more states. the   state fully recurrent nn with  shared weights and fewer parameters outperforms   state and   state resnet with non shared weights.     . .     sharing weights across all convolutional layers  less biologically plausible     out of pure engineering interests  one could further push the limit of weight sharing by not only sharing across time  but also across states. here we show two   state resnets that use a single set of convolutional weights across all  convolutional layers and achieve reasonable performance with very few parameters  figure   .          cifar    with very few parameters  feat.      param.       feat.      param           .   validation error on cifar       training error on cifar        .      .      .      .      .          feat.      param.       feat.      param           .      .      .      .                              epoch                                  epoch    figure    a single set of convolutional weights is shared across all convolutional layers in a   state resnet. the  transition  at time t    and    between nearby states is a  x  max pooling with stride  . this means that each  state has a self transition unrolled   times.  feat.  denotes the number of feature maps  which is the same across all    states. the learning rates were the same as other experiments except that more epochs are used  i.e.          and     .     .    . .     experiment b  multi state fully densely recurrent neural networks  shared vs. non shared weights    although an rnn is usually implemented with shared weights across time  it is however possible to unshare the  weights and use an independent set of weights at every time t. for practical applications  whenever one can have  a initial t     and enumerate all possible ts  an rnn with non shared weights should be feasible  similar to the  time specific batch normalization described in  . . the results of   state fully recurrent neural networks with shared  and non shared weights are shown in figure  .     . .     the effect of readout time    in visual cortex  useful information increases as time proceeds from the onset of the visual stimuli. this suggests  that recurrent system might have better representational power as more time is allowed. we tried training and  testing the   state fully recurrent network with various readout time  i.e.  unrolling depth  see section  .   and  observe similar effects. see figure  .     . .     larger models with more states    we have shown the effectiveness of   state fully recurrent network above by comparing it with   state resnet. now  we discuss several observations regarding   state and   state networks.  first    state models seem to generally outperform   state ones. this is expected since more parameters are  introduced. with a   state models with minimum engineering  we were able to get  .    validation error on  cifar   .             state fully recurrent nn with different readout time t  t     param.        t     param.         t     param.         t      param.            .      .   validation error on cifar       training error on cifar        .      .      .      .          t     param.        t     param.         t     param.         t      param.            .      .      .      .                                 epoch                           epoch    figure    a   state fully recurrent network with readout time t         or     see section  .  for the definition of  readout time . there is consistent performance improvement as t increases. the number of parameters changes  since at some t  some recurrent connections have not been contributing to the output and thus their number of  parameters are subtracted from the total.  next  for computational efficiency  we tried only allowing each state to have transitions to adjacent states and to  itself by disabling bypass connections  e.g.  v  v   v  it  etc. . in this case  the number of transitions scales linearly  as the number of states increases  instead of quadratically. this setting performs well with   state networks and  slightly less well with   state networks  perhaps as a result of small feature parameter sizes . with only adjacent  connections  the models are no longer fully recurrent.  finally  for   state fully recurrent networks  the models tend to become overly computationally heavy if we train  it with large t or large number of feature maps. with small t and feature maps  we have not achieved better  performance than   state networks. reducing the computational cost of training multi state densely recurrent  networks would be an important future work.  for experiments in this subsection  we choose a moderately deep pre net of three  x  convolutional layers to model  the layers between retina and v   conv bn relu conv bn relu conv. this is not essential but outperforms  shallow pre net slightly  within    validation error .  the results are shown in figure  .     . .     generalization across readout time    as an rnn  our model supports training and testing with different readout time. based on our theoretical analyses  in section  .   the representation is usually not guaranteed to converge when running a model with time t    .  nevertheless  the model exhibits good generalization over time. results are shown in figure   . as a minor detail   the model in this experiment has only adjacent connections and does not have any self transition  but we do not  expect this to affect the conclusion.             state fully densely recurrent neural networks    it    full    v     v     v     it     .    .     .    .     .     adjacent     .       .    .     .    .     .                epoch                 .       .      .     training error on cifar       v     full   param         adjacent   param            .    validation error on cifar       v     training error on cifar       v     full   param         adjacent   param            .       .       .                epoch          full   param          adjacent   param             .      .     validation error on cifar         state fully densely recurrent neural networks  small model    .      .      .      .     .      .        .      .      .     .      .        .        .        .        .                          epoch          full   param          adjacent   param             .              epoch          figure    the performance of   state and   state models. the state sizes of the   state model are    x  x     x  x      x x     x x  . the state sizes of the   state model are    x  x      x  x      x x   .   state models are small  since they are computationally heavy. the readout time is t   for both models. all models are time invariant  systems  i.e.  weights are shared across time .         discussion    the dark secret of deep networks  trying to imitate recurrent shallow networks   a radical conjecture would be  the effectiveness of most of the deep feedforward neural networks  including but not  limited to resnet  can be attributed to their ability to approximate recurrent computations that are prevalent in  most tasks with larger t than shallow feedforward networks. this may offer a new perspective on the theoretical  pursuit of the long standing question  why is deep better than shallow          .  equivalence between recurrent networks and turing machines  dynamical systems  in particular discrete time systems  that is difference equations  are turing universal  the game   life  is a cellular automata that has been demonstrated to be turing universal . thus dynamical systems such as  the feedback systems we discussed can be equivalent to turing machine. this offers the possibility of representing  a computation more complex than a single  for instance boolean  function with the same number of learnable  parameters. consider for instance the powercase of learning a mapping f between an input vector x and an output  vector y   f  x  that belong to the same n dimensional space. the output can be thought as the asymptotic states  of the discrete dynamical system obtained iterating some map f . we expect that in many cases the dynamical  system that asymptotically performs the mapping may have a much simpler structure than the direct mapping f .  in other words  we expect that the mapping f such that f  n   x    f  x  for appropriate  possibly very large n can  be much simpler than the mapping f  here f  n  means the n th iterate of the map f  .  empirical finding  recurrent network or residual networks with weight sharing work well  our key finding is that multi state time invariant recurrent networks seem to perform as well as very deep residual  networks  each state corresponds to a cortical area  without shared weights. on one hand this is surprising because  the number of parameters is much reduced. on the other hand a recurrent network with fixed parameters can be  equivalent to a turing machine and maximally powerful.  conjecture about cortex and recurrent computations in cortical areas  most of the models of cortex that led to the deep convolutional architectures and followed them   such as the  neocognitron      hmax      and more recent models       have neglected the layering in each cortical area and the  feedforward and recurrent connections within each area and between them. they also neglected the time evolution  of selectivity and invariance in each of the areas. the conjecture we propose in this paper changes this picture quite  drastically and makes several interesting predictions. each area corresponds to a recurrent network and thus to a           the model is trained with readout time      .   error on training set  error on test set     .     .      error on cifar        .     .     .    .     .     .    train     .                                         test readout time t                      figure     training and testing with different readout time. a   state recurrent network with adjacent connections  is trained with readout time t    and test with t                  and   .  system with a temporal dynamics even for flashed inputs  with increasing time one expects asymptotically better  performance  masking with a mask an input image flashed briefly should disrupt recurrent computations in each  area  performance should increase with time even without a mask for briefly flashed images. finally  we remark that  our proposal  unlike relatively shallow feedforward models  implies that cortex  and in fact its component areas are  computationally as powerful a universal turing machines.    acknowledgments  this work was supported by the center for brains  minds and machines  cbmm   funded by nsf stc award ccf           .    references  