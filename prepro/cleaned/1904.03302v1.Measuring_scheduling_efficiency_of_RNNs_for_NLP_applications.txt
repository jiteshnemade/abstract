introduction  i. r ecurrent n eural n etworks  rnn s    recurrent neural networks are a type of deep neural network   dnn  that make use of sequential information. they are  used in tasks where the ordering of the input sequence is  important  e.g.  time based data  natural language processing .  the fundamental component of a rnn is a cell. the cells  have weights and an internal state. the state is updated by  applying the same computation and weights to every element  of a sequence  in sequence order  over multiple time steps.  this state is called the hidden vector and acts as a  memory .  multiple cells can be stacked on top of each other to form a  multilayer recurrent neural network. most popular rnn cell  types are long short term memories  lstms      and gated  recurrent units  grus     .  figure   represents a   layer rnn network followed by a  fully connected softmax layer. the input to the network is  the query   who are you   spread over three time steps. the  weights of rnn cell   and   are denoted by  cell weights     and  cell weights   . these cell weights do not change across  each time step. figure   shows the equations executed by a  lstm layer. here xt denotes the input at time step t  ht    denotes the memory element from previous time step and ht  denotes the memory element after the end of the computation  during the current time step. the cell weights in figure   are  the concatenation of w and u matrices in figure  .  ii. c urrent rnn s cheduling  to efficiently execute the operations in figure    these  weight matrices are concatenated together to create a larger    t     cell   weights  g  matrix    t     t     fc    fc    fc    rnn  cell      rnn  cell      rnn  cell      rnn  cell      rnn  cell      rnn  cell      who    are    you     cell   weights  g  matrix    input query    who are you    fig.  .   layer rnn network    ft    g  wf   xt   uf   ht      it    g  wi   xt   ui   ht      ot    g  wo   xt   uo   ht      ct   ft   ct     it    c  wf   xt   uf   ht      ht   ot    ct  fig.  . equations executed by a lstm layer    matrix  referred to as g in this paper  which is then multiplied  by the concatenation of the input vector and the hidden vector.        wf wi wo wc  g   uf ui uo uc       xt  concatenatedinputv ector i     ht    g is a   row concatenation of the w and u matrices  where  each row has   w or u matrices. thus  if a single w or u  matrix is of size  n n   then the concatenated matrix  g  has  the size    n   n .  following the equations in figure    g is multiplied by i.  the resultant output vector is divided into   vectors to execute  the rest of the operations.     the current schedule for inference of a multilayered lstm  network is      an input i is broken down into multiple vectors  x    x     x    . . .   xt .     the computations are scheduled as described in algorithm       step   is repeated for each layer in the multilayered  rnn.  the above schedule is not restricted to lstms only and is  applicable to any rnn cell network.  iii. i ssues with current rnn scheduling          benchmarks with higher dre    algorithm   scheduling algorithm for non streaming applications  each input x  t is concatenated with the hidden  vector from the previous step  h and multiplied with g.  the result passes through a non linearity layer  after which  it is split into   sub vectors. these sub vectors are linearly  combined to generate the hidden vector for the current time  step.     input   g  t  n  x  t     r         c         h         i         for r   t do      i   concat xr   h       y  g i      y     sig y            n            f   y        n        i   y    n       n        o   y        n       n        c    tanh y      n       n            c   f   c   i   c        h o c       r  r        end for      return h                                                                             dre values    fig.  . distribution of dre values for all benchmarks with small vocabulary      words .    for nlp applications  embeddings are present in the first and  the last layers of the network.  a. data reuse efficiency  dre   this paper introduces a new metric to measure the efficiency  of scheduling rnn operations in a modern deep learning  framework. the metric measures how much more data than  the working set of the rnn application was used during the  execution of the rnn application. thus  efficiency in this  context is an indication of the usage of memory bandwidth.  this metric requires measuring two quantities   bandwidth  usage of the application and application working set size.  the bandwidth usage of an application can be measured by  using the performance counters in the memory controller of  the platform. when measuring the bandwidth usage  each  benchmark is run     times. the average amount of data read  from and written to a memory controller  avgrw   across  all runs is measured. next  the working set  weights   word  embeddings   intermediate values  of the rnn network is  calculated from the benchmark configuration parameters. the  inefficiency in scheduling is measured by calculating the ratio     algorithms    to identify the issue with the current scheduling scheme   we run experiments on a desktop intel haswell platform with  a    mb last level cache  llc . we use benchmarks written  in tensorflow  .  compiled with the intel mkl library. an  optimized dataflow graph for deployment is created using the  freeze graph tool in tensorflow and the resultant graph is  executed to process the input queries.  table i lists the configuration of the lstm benchmarks  evaluated. all the parameters in table i are combined to create  a total of     benchmarks. the basic architecture for each  benchmark is similar to that shown in figure     multiple  rnn layers followed by a softmax layer. the benchmarks  cover small and large vocabularies. vocabulary is the number  of words tokens used in a nlp application. each word token  is represented via a vector and the matrix of vectors of all the  words tokens used by the application is called an embedding.    datareuseef f iciency dre    avgrw w orkingset       the lower the value of dre  the better a scheduling  algorithm. an ideal system will have a dre of   i.e.  it is  able to cache all the weights until they are no longer required.  b. results  lstm  with small vocabulary   for small vocabulary   the total size of output embedding does not exceed  .  mb.  figure   shows the   of benchmarks exceeding a certain  dre value. the benchmarks that have a dre value       have a working set size close to or greater than the size of  llc.     of the benchmark s working set size is      mb.  accounting for overheads  tensorflow  python  os  perf  and  shell scripts used to run experiments and collect results   the  benchmarks whose working set size is close to    mb will     config. name  rnn size  number of layers  rnn cell type  length of input  vocabulary size  batch size    description  size of the hidden vector  number of stacked rnn cells on top of each other  type of rnn cell  number of time steps over which input is fed to the rnn  network  number of classes in the dictionary  number of inferences simultaneously processed    value                                        lstm  gru               small       large                                                       benchmarks with higher dre      benchmarks with higher dre    table i  b enchmark parameters  t otal of     benchmarks                                                                                                                  dre values                                                  dre values    input length       input length         input length          fig.  . distribution of dre values for all benchmarks with small vocabulary      words . the distribution divided based on input length.    also not fit in the cache. these benchmarks will compete  with overheads for space in the cache. if we assume that the  overheads take up   mb of space in the cache  a working set  size of    mb      of the benchmarks  or more will not fit  in the cache completely. thus a total of     of benchmarks  have a dre value    . other benchmarks  along with the  overheads  completely fit in the cache and do not see any read  write traffic at the memory controller. since dre captures  the avgrw traffic across     runs  the value of avgrw  will be less than the working set of these benchmark.  there is also a strong correlation between the length of  input and the dre value. figure   breaks down the values  in figure   based on input length. it shows a bar chart of    of benchmarks with input of length    to     and satisfying  a dre criterion. the first blue bar of figure   implies that    .   of benchmarks that have an input length of     have a  dre value     while     of benchmarks that have an input  length of      have a dre value     .  impact of a larger vocabulary  there is also a correlation  between dre value and the vocabulary size. when the  vocabulary size is         words  the word embedding could  be anywhere between  .   mb for hidden unit vector of size     and    mb for hidden unit vector of size     . figure    shows that with a larger vocabulary  the number of benchmarks  that have a dre value     and dre value     increases  to    .  results for gru  the results for gru cell based networks  are very similar. for brevity  we do not discuss them in the    fig.  . distribution of dre values for all benchmarks with large vocabulary          words .    paper.  c. key observations               the current scheduling algorithm does not focus on data  reuse in rnns during inference   for some benchmarks     x more data is read than the working set of the  benchmark.  for smaller caches  as seen in mobile devices   the  problem of high dre should be even more prominent  the dre value is proportional to the number of layers   size of the hidden vector  vocabulary size and the length  of the input. it increases as each of these values increases.    iv. i mproving the bandwidth efficiency of rnn  c ells  breaking the g matrix  reuse in rnns is not obvious  when we schedule them as discussed in section ii. however   across all the time steps  the matrix g does not change. matrix  g is composed of   sets of matrices. wf  i o c gets multiplied  with inputs x    x    x    . . .   xt while uf  i o c gets multiplied  with the hidden vectors generated at each time step.  generally for nlp applications  the inputs are available  before hand. even if the input is streaming  buffering of inputs  can create a scenario where inputs across some of the time  steps are available beforehand. this provides an opportunity to  implement a bandwidth efficient scheduling scheme by making  three changes to the algorithm discussed in section ii           g should be broken down into two sets of matrices   g   and g .        g    w f w i w o w c        g    u f u i u o u c         next  the inputs should be concatenated across all time  steps into a single matrix.     i     x          x     x     .    .    .    xt         finally  the computations should be executed as described  in algorithm  .    algorithm   bandwidth efficient lstm computation  the  input elements x  t are all concatenated into a single matrix  i   . this matrix gets multiplied by g  and the result is stored  in x   . for each time step  g  is multiplied with the hidden  vector from the previous time step and the result is added  with the column representing the output of g  i  for that time  step. this vector passes through a non linearity layer and gets  split into multiple sub vectors which are linearly combined to  generate the hidden vector for the current time step.     input   g   g   t  i     n     r         c         h         x     g    i       for r   t do      y     g    h      y     x       r    y        y     sig y            n            f   y        n        i   y    n       n        o   y        n       n        c    tanh y      n       n            c   f   c   i   c        h o c       r  r        end for      return h  ideally  a scheduler should first schedule computation in  line  . now  instead of recalculating wf   xt   wi   xt   wo    xt and wc   xt at every time step  the scheduling algorithm  could read the tth column of x   and use that as the input to  subsequent computations  line   . by doing the computation  this way  we read g  once. next  the computations for line     are scheduled. these computations need to be scheduled for  each vector in the input sequence. thus  g  still needs to be  read at every time step because of the sequential dependency  on the hidden vector  line   .  assuming a input vector and hidden vector size of     each   the total size of the g matrix is   mb    w and   u matrices   each of size   mb . if the size of the cache is smaller than    mb  g matrix will not fit in the cache. for a system using the  scheduling scheme described in section ii  the g matrix will    be reread from the memory at every time step. if the input  length is      the total amount of data read from the memory  is     mb. by using the new schedule  we can reduce the  amount of data read from the memory to     mb instead of      mb  thus improving the dre value.  a similar scheduling scheme is discussed in    . however   this work differs from     in two ways. firstly  they discuss this  scheduling scheme from the perspective of a gpu. secondly   to enable enough parallelism  they do two computations of  input time steps simultaneously instead of all time steps in  the above implementation  line    algorithm   . lastly  they  do not look at the impact of this scheduling scheme on the  memory bandwidth consumption.  v. e valuating the impact of the optimized  scheduler    we developed an analytical model to gauge the impact of  the optimized scheduler on the memory system. it takes in  the network configuration to create a data flow graph based  on a scheduling algorithm. the data flow graph generates the  memory requests which are fed to a least recently used  lru   cache of size    mb. the cache size is chosen to mimic a  modern day mobile soc. cache misses  along with writeback   are used to measure the read and write traffic to and from the  memory. the matrix vector multiplication operations generate  requests assuming cache blocking techniques to maximize  reuse in the cache.  a. benchmarks evaluated  the   applications modelled are language translation   google s neural machine translation  gnmt        speech  recognition  deepspeech        language modelling  lm       and named entity recognition  bytener     . gnmt has    encoder decoder rnn layers along with an attention module  and a vocabulary of size       . deepspeech  has   rnn  layers and uses a vocabulary size of    words. lm consists of    lstm layers and uses a vocabulary of size       . bytener  has   rnn layers and uses a vocabulary of size  .  the benchmarks are evaluated using the following scheduling algorithms     schedule a   this schedule is described in section ii of  this paper and is the one used in tensorflow compiled  with mkl. computations are scheduled one layer at a  time. each layer processes inputs across all the time steps  before moving to the next layer.    schedule a    this is the schedule discussed in section iv where the g matrix is broken into two matrices  g  and g .  b. results  impact on bandwidth  figure   shows the impact of the  optimized schedule  measuring the memory traffic to and from  the memory. the values             and     indicate the  number of time steps across which the input is fed to the  network. the values have been normalized to schedule a s  traffic for each benchmark.     r eferences        .     normalized dram traffic     .    .      .    .    .    .    .    .                        gnmt                                    language modeling   rescoring   schedule a                      deepspeech                                     bytener    schedule a     fig.  . bandwidth savings after breaking the g matrix for different scheduling  algorithms and input lengths    for lm  traffic to the memory reduces by  .    for smaller  input lengths and  .    for longer input lengths. lm uses  cells of size       for one layer and       for the other. the  g  and g  matrices are equally sized for the first layer. this  gets  x improvement for those layers. however  for the second  layer  g  is larger than g  as each hidden hidden weight  matrix is of size     x     while the input weight matrix is  of size     x    . the optimization discussed in section iv  reduces the cost of reading g  multiple times. since g  is    times smaller than g   a  is not as effective for layer   as  for layer  . thus  the overall benefit of a  does not reach the  expected factor of  . additionally  the g matrix is    mb for  the first layer and     mb for the second layer. these values  will not fit in the cache. thus  schedule a will not see any  reuse.  gnmt is composed of encoder and decoder rnn layers   attention layers and word embedding layers. the g matrix  splitting does not apply to attention layers. the decoder layers  need the output of the previous time step as an input to the  next time step. as a result  the g matrix splitting optimization  cannot be applied to the decoder layers. thus  a  is only  applied to the encoder layers  resulting in an improvement  over schedule a by a factor of  .   to  .  .  bytener is a smaller network of size  .  mb. as a result   the network will fit in the cache and there will be significant  reuse of data structures within the cache across multiple time  steps for all the schedules. thus  both schedules perform  equally well.  vi. c onclusion  this paper introduces dre  a new metric to measure the  efficiency in scheduling rnn applications on cpus. using  this metric  we have uncovered that typical rnn applications  communicate with memory        their working set size  due  to inefficient data organization and scheduling. to counter this   this paper also introduced a new optimization to significantly  improve the dre value and  consequently  improve the  memory utilization efficiency.    