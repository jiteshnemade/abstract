introduction    recurrent neural networks  rnns  are today among  the most effective solutions for modeling time series   speech  text  audio  video  etc.  schmidhuber       .  an rnn is a special type of nn using its internal  state  memory  to process sequences of inputs. this  internal memory makes the rnn able to remember  the relevant information about the previous samples   in order to model their dynamics.  in contrast to feed forward nn  ffnn   which  does not explicitly consider the notion of sequence  in  the rnn the input information cycles through a loop.  this structure allows the simultaneous processing of  both the current and the recent samples. in the rnn   the deep learning algorithm tweaks its weights  through gradient descent and backpropagation  through time  bptt  mazumdar et al.       . in  essence  bptt is backpropagation  bp  applied to an  equivalent unfolded ffnn. specifically  figure  a  shows a basic rnn  made by an mlp layer and a  cyclic connection from the output to the input neuron.  an rnn with finite response to finite length settles  to zero in finite time  and can be modelled as a  directed acyclic graph. this rnn can be unfolded  and transformed into an ffnn  i.e.  an equivalent    static mlps chain  with each mlp working at an  instant of time of the finite response  i.e.  working  without memory  figure  b . thus  within bbtt the  error is back propagated from the last to the first time  step. the weights are updated by calculating the error  for each time step. since the unfolded nn is static  it  can be trained by bp. however  in case of high  number of time steps  the unfolded nn is much  larger  and contains a large number of weights  which  makes bbtt computationally expensive.  a major issue with bp on large nn chains is  related to the gradient descend. in essence  bp goes  backwards through the nn to find the partial  derivatives of the error with respect to the weights  in  order to subtract the error from the weights. such  derivatives are used by the gradient descent  algorithm  which iteratively minimizes a given  objective function. for better efficiency  the unfolded  nn can be transformed into a computational graph of  derivatives before training  goodfellow et al.       .  a problem when training computational graph is to  manage the order of magnitude of gradients  throughout a large graph  pascanu et al       . the  exploding gradients problem occurs when error  gradients accumulate during an update. as a result   very large gradients are produced and  in turn  large  updates to the network weights of long term     components. this may cause network instability and  weights overflow. the problem can be easily solved  by clipping gradients when their norm exceeds a  given threshold  goodfellow et al.         by weight  regularization  i.e.  applying a penalty to the networks  loss function for large weight values  pascanu et al        .  on the other side  the vanishing gradient problem  occurs when the values of a gradient are too small. as  a consequence  the model slows down or stops  learning. thus  the range of contextual information  that standard rnns can access is in practice quite  limited.     a      b     figure     a  an rnn  b  the equivalent nn unfolded in  time.    long short term memory  lstm  graves et al.         is an rnn specifically designed to address the  exploding and vanishing gradient problems. an  lstm hidden layer consists of recurrently connected  subnets  called memory blocks. each block contains  a set of internal units  or cells  whose activation is  controlled by three multiplicative gates  the input  gate  the forget gate  and the output gate. an lstm  network can remember of arbitrary time intervals.  the cell decides whether to store  by the input gate    to delete  by the forget gate   or to provide  output  gate  information  based on the importance assigned.  the assignment of importance happens through  weights  which are learned by the algorithm. since  the gates in an lstm are analog  in the form of  sigmoid  the network is differentiable  and trained by  bp.  in recent years  lstm networks have become the  state of the art models for many machine learning  problems  greff et al.       . this has attracted the  interest of researchers on the computational  components of lstm variants.  this paper focuses on a novel concept of  computational memory in rnns  based on stigmergy.  stigmergy is defined as an emergent mechanism for  self coordinating actions within complex systems  in  which the trace left by a unit s action on some  medium stimulates the performance of a subsequent  unit s action  heylighen       . to our knowledge   this is the first study that proposes and lays down a    basic design for the derivation of stigmergic memory  rnn  sm rnn . in the literature  stigmergy it is a  well known mechanism for swarm intelligence and  multi agent systems. although its high potential   demonstrated by the use of stigmergy in biological  systems at diverse scales  the use of stigmergy for  pattern recognition and data classification is still  poorly investigated  heylighen       . as an  example  in  cimino et al.       a stigmergic  architecture has been proposed to perform adaptive  context aware aggregation. in  alfeo et al.        a  multi layer architectures of stigmergic receptive  fields for pattern recognition have been experimented  for human behavioral analysis. in  galatolo et al.          the temporal dynamics of stigmergy is applied  to weights  bias and activation threshold of a classical  neural perceptron  to derive a non recurrent  architecture called stigmergic nn  s nn . however   due to the large nn produced by the unfolding  process  the s nn scalability is limited by the  vanishing gradient problem. in contrast  the sm rnn  proposed in this paper employs ff nn as store and  forget cells operating on a multi mono dimensional  sm  in order to reduce the network complexity.  to appreciate the computational power achieved  by sm rnn  in this paper a conventional ff nn  an  s nn  galatolo et al.         an rnn and an lstmnn have been trained to solve the mnist digits  recognition benchmark  lecun et al.       .  specifically  two mnist variants have been  considered  spatial  i.e.  as sequences of bitmap rows   and temporal  i.e.  as sequences of pen strokes  de  jong  e. d.       .  the remainder of the paper is organized as  follows. section   discusses the architectural design  of sm nns. experiments are covered in section  .  finally  section   summarizes conclusions and future  work.         architectural design    let us consider  in neuroscience  the phenomenon of  selective forgetting that characterizes memory in the  brain  information pieces that are no longer reinforced  will gradually be lost with respect to recently  reinforced ones. this behavior can be modeled by  using stigmergy. figure   shows the ontology of an  sm  made by four concepts  stimulus  deposit   removal  and mark. in essence  the stimulus is the  input of a stigmergic memory. the past dynamics of  the stimulus are indirectly propagated and stored in  the mark. this propagation is mediated by deposit  and removal  stimulus affects deposit and removal     which  respectively  reinforces and weakens mark.  mark can be reinforced weakened up to a  saturation finishing level. on the other side  mark  itself affects deposit and removal. this behavior can  be characterized as recurrent.  figure   shows an example of dynamics of a  mono dimensional sm  i.e.  a real valued mark  variable  generically called    . specifically  the       and for           it  mark starts from  undergoes a weakening by                    respectively  up to the finishing level . for              the mark variable undergoes a  reinforcement by                      respectively  up to the saturation level .    according to this concept  figure   shows the  structure of an sm rnn based classification unit.  here  the deposit  removal  and classification mlps  are realized by spatial ff mlps. the sm is based on  an array of m mono dimensional mark variables   where m is also equal to the number of outputs of the  deposit and removal mlps  as well as to the number  of inputs of the classification mlp.    figure    ontology of a stigmergic memory  figure    structure of an sm rnn based classification  unit    figure    example of dynamics of a mono dimensional  stigmergic memory.    let us consider a mono dimensional stimulus   i.e.  a real valued variable generically called    . for  each         and       are determined by  deposit and removal  respectively  on the basis of     . thus      is a sort of aggregated memory of  the     dynamics. the relationship between      and     is not prefixed. by using     to feed a  subsequent classification or regression unit  this  relationship can be trained via supervised learning.    specifically  the linear layer at the input of  deposit and removal mlp is a single layer of linear  neurons  i.e.  neurons with linear activation function.  it performs a linear projection of the sm data.  the three mlps have the same structure   represented in figure     i  an input linear layer   ii   a prelu  parametric rectified linear unit   activation function  which solves the vanishing  gradient problem   iii  an output linear layer   iv  a  relu  rectified linear unit  activation function  for  the deposit and removal mlps  or a prelu  activation function  for the classification mlp   respectively.         experimental studies    the architecture of an sm nn has been  developed with the pytorch framework  pytorch         and made publicly available on github   github       .     to appreciate the computational power of an smrnn  different nns have been trained to solve the  mnist digits recognition benchmark  lecun et al.          an sm rnn  an ff nn  an s nn  an rnn  and an lstm nn.    extraction of        images  the remaining         images makes the testing set.     a   figure    structure of a deposit  removal  or  classification mlp    the purpose is twofold  to measure the  computational power of sm rnn with respect to ffnn  and to compare the performances of the other  existing temporal nn. for this purpose  the following  two variants of the mnist benchmark have been  used.  in the spatial mnist dataset  lecun et al.          the input image is made by             pixels  and  the output is made by    classes corresponding to  decimal digits. in the case of ff nn  the handwritten  character is supplied in the form of full static bitmap.  for the other nns  the handwritten character is  supplied row by row  in terms of    inputs  over     subsequent instants of time. in this case  once  provided the last chunk  the nn provides the  corresponding output class.  in the temporal mnist dataset  de jong  e. d.          the handwritten character is supplied as a  sequence of pen strokes. in this case  at each instant  of time   the next input is provided as a movement in  the horizontal and vertical directions            .  once provided the last pen stroke  the nn provides  the corresponding output class. as an example   figure   shows the representation of a handwritten  digit in the spatial mnist  a  and temporal mnist   b .  to adequately compare the different nns  the  following methodology has been used. first  the ffnn and the sm rnn have been dimensioned to  achieve their best classification performance.  secondly  the s nn  rnn and lstm nn have been  dimensioned to have a similar number of parameters  with respect to the sm rnn.  overall  the data set is made of        images. at  each run  the training set is generated by random     b     figure    representation of a handwritten digit in the  spatial mnist  a  and temporal mnist  b     table   shows the overall complexity of each nn.  the complexity values correspond to the total number  of parameters. specifically  the sm is made by m       mark variables. thus  the deposit and removal  mlps topology is made by     temporal        linear  layer  inputs  i.e.    inputs. the linear layer before  the deposit and removal mlps contains        weights      biases       parameters. the  deposit removal mlps contains the input linear  layer        weights      biases       parameters .  the prelu contains    parameters. the output  linear layer contains       weights      biases        parameters. the classification mlp contains the  input linear layer        weights      biases        parameters . the prelu contains    parameters. the  output linear layer contains       weights       biases      . thus  the total number of parameter is                                                .  for a detailed calculation of the complexity of the  other nns  the interested reader is referred to   galatolo et al.      .  table    performance and complexity of different nns  solving the spatial mnist digits recognition benchmark.  neural network  sm rnn  ff nn  lstm rnn  s nn  rnn    complexity                                         classification rate  .       .     .       .      .       .     .       .     .       .       in addition  table   shows the performance  evaluations  which are based on the     confidence  interval of the classification rate  i.e.  the ratio of  correctly classified inputs to the total number of  inputs   calculated over    runs.     the adaptive moment estimation  adam  method   kingma et al.        has been used to compute  adaptive learning rates for each parameter of the  gradient descent optimization algorithms  carried out  with batch method.  it is apparent from the table that the sm rnn  outperforms the other nns  in terms of both  complexity and classification rate. specifically  the  ff nn employs a very large number of parameters   about two order of magnitude larger with respect to  the sm rnn. to assess the quality of the training  process  figure   and figure   show a scenario of  classification rate and the function  on the training set   against the number of iterations  respectively. the  loss function is calculated as the negative loglikelihood  nll  using the softmax activation  function at the output layer of the nn  which is  commonly used in multi class learning problems.    figure    scenario of classification rate on training set  against number of iterations  for the spatial mnist data  set    figure    scenario of loss function on training set against  number of iterations  for the spatial mnist data set    with regard to the temporal mnist data set  three  kinds of temporal nns have been used  i.e.  smrnn  lstm rnn  and rnn.  table   shows the overall complexity of each nn.  the complexity values correspond to the total number  of parameters. specifically  the sm is made by m       mark variables. thus  the deposit and removal  mlps topology is made by   temporal inputs  i.e.  the  horizontal and vertical directions  the stroke and digit  ends        linear layer  inputs  i.e.    inputs. the  linear layer before the deposit and removal mlps  contains       weights      biases       parameters.  the deposit removal mlps contains the input  linear layer        weights      biases        parameters . the prelu contains    parameters. the  output linear layer contains       weights       biases       parameters. the classification mlp  contains the input linear layer        weights       biases       parameters . the prelu contains     parameters. the output linear layer contains        weights      biases      . the activation relu  contains    neurons. thus  the total number of  parameters is                                                   .  the recurrent nn is made by the following layers   an input linear layer        weights      biases          parameters   a prelu     parameters   an  output linear layer        weights      biases          parameters   and an activation prelu      parameters . thus  the total number of parameters is                                 . in the recurrent  nn  each output neuron has a backward connection  to the input and to the classification mlp which  in  turn  is made by the following layers  the input linear  layer        weights      biases          parameters   the prelu     parameters   the output  linear layer        weights      biases         and  the activation prelu     neurons . thus  the total  number of parameters of the recurrent and the  classification nns is                               .  the lstm rnn fed by   inputs. for each lstm  layer  the number of parameters is calculated  according to the well known formula   o  i o      where o and i is the number of outputs and inputs   respectively. the topology is made by a      lstm  layer  a       lstm layer  and a       output  linear layer. thus  the overall number of parameters  is                                                     .  in addition  table   shows the performance  evaluations  based on the same criteria detailed for  table  . it is apparent from the table that the sm      rnn and the lstm rnn are equivalent in terms of  classification rate. in contrast  the rnn is not able to  gain a sufficient stability and accuracy. to assess the  quality of the training process  figure   and figure     show a scenario of classification rate and loss  function  on the training set  against the number of  iterations  respectively. the loss function of figure     is calculated as in figure  .    overall  the proposed sm rnn shows a very good  convergence with respect to the other nns. in  consideration of the relative scientific maturity of the  other comparative nns  the experimental results with  the novel sm rnn looks very promising  and  encourage further investigation activities for future  work    table    performance and complexity of different nns  solving the temporal mnist data set.         neural network  sm rnn  lstm rnn  rnn    in this paper  the concept of computational stigmergy  is used as a basis for developing a stigmergic  memory for recurrent neural networks. some  important issues in the research field  related to the  gradient descend  are first discussed. the novel  architectural design of the sm rnn is then detailed.  finally  the effectiveness of the approach is shown via  experimental studies  carried out on the spatial and  temporal mnist data benchmarks. the sm rnn  can be appreciated for their impressive computational  power with respect to the other types of rnns.  experimental results are promising  and show that the  sm rnn outperforms the ff nn  the s nn  the  rnn  and is equivalent to lstm nn  in terms of  both complexity and classification rate. future work  will focus on further experimentation and  investigation.    complexity                         classification rate  .        .      .        .      .        .        conclusions    acknowledgements  figure    scenario of classification rate on training set  against number of iterations  for the temporal mnist  data set    this work was partially carried out in the framework  of the sciadro project  co funded by the tuscany  region  italy  under the regional implementation  programme for underutilized areas fund  par fas             and the research facilitation fund   far  of the ministry of education  university and  research  miur .  this research was supported in part by the pra          project entitled  wearable sensor systems   personalized analysis and data security in healthcare   funded by the university of pisa    