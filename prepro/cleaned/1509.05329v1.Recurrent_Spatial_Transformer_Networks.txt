introduction     . related work    attention mechanisms have been used for machine  translation  bahdanau et al.         speech recognition   chorowski et al.        and image recognition  ba et al.         gregor et al.        xu et al.       .  the recently proposed spatial transformer network  spn    jaderberg   simonyan        is a new method for incorporating spatial attention in neural networks. spn uses a  learned affine transformation of the input and bilinear interpolation to produce its output. this allows the spn network to zoom  rotate and skew the input. a spn layer  can be used as any other layer in a feed forward convolutional network  . the feed forward spn  ffn spn  is illustrated in figure   panel a  where a spn combined with a    gregor et al.      introduced a differentiable attention  mechanism based on an array of gaussians and combined  it with a rnn for both generative and discriminative  tasks. ba et al.      and sermanet et al. combined a nondifferentiable attention mechanism with an rnn and used  it for classification. their attention mechanism was trained  using reinforcement learning. other related work include   xu et al.        who applies visual attention in an encoderdecoder structure.         see e.g. https   goo.gl  ho m      . spatial transformer network  the spn network is implemented similarly to   jaderberg   simonyan       . a spn network takes an     transformer network    figure  . spn networks for predicting the sequence     from an image. a  a ffn spn network attend to the entire sequence  blue box .  the digits are classified by three separate softmax layers at the top of the network. because the network cannot zoom in on individual  digits in the sequence all digits are classified from the same image crop. b  a rnn spn where the transformation is predicted with an  rnn. this allows the model to create a separate crop for each digit. each crop  indicated with blue numbers  is then passed through the  same classification network. the structure enables the model to zoom in on each individual digit.    image or a feature map from a convolutional network as  input. an affine transformation and bilinear interpolation  is then applied to the input to produce the output of the  spn. the affine transformation allows zoom  rotation and  skew of the input. the parameters of the transformation  are predicted using a localization network floc          floc  i    a                                              where i is the input to the spn with shape  h   w   c    height  width  channels  and matrix a  specifies the affine  transformation. the affine transformation is applied on a  mesh grid g   rh w    g    y    x      y    x     ... y    x      y    x      ... yw   xh       yh   xw   .           where w and h does not need to be equal to h and w . g  is illustrated in figure   panel a . the grid is laid out such  that  y    x               and  yh   xw              with  the points in between spaced equally. the affine transformation is applied to g to produce an image s which tells  how to select points from i and map them back onto g        yi  sij   a   xj                where we have augmented each point with a  . since the  mapped points in s does not correspond exactly to one  pixel in i bilinear interpolation is used to interpolate each  point in s. the sub gradients for the bilinear interpolation are defined and we can use standard backpropagation  through the transformation to learn the parameters in floc .  the sampling process is illustrated in figure   where panel  a  illustrates g and panel b  illustrates s after we have applied the transformation.     . . down sampling  we can vary the number of sampled points by varying h and  w. having h and w less than h and w will downsample  the input to the spn. we specify the down sampling with  d. d larger than   will downsample the input. the number  of sampled points form i is            h  w  w  h        .       npoints    d  d  d   for  d images the sampled points decrease quadratically  with d.   . . rnn spn  in the original ffn spn the localization network is a feedforward convolutional neural network. we modify this  model by letting an rnn predict the transformation matrices such that  c   fconv  i   rnn  ht   floc   c  ht      a    g ht  .                     where fconv is a convolutional network taking i as input and  rnn  creating a feature map c  floc  is an rnn  and g is a ffn.  here an affine transformation is produced at each time step  from the hidden state of the rnn. importantly the affine  transformations are conditioned on the previous transformations through the time dependency of the rnn.     . experiments  we test the model on a dataset of sequences of mnist digits cluttered with noise. the dataset was created by placing    random mnist digits on a canvas of size           pixels. the first digits was placed by randomly sampling an     transformer network    figure  . a  the sampling grid g of equally spaced sampling points. we set y  and x  to    and yh   xw to   . b  a recurrent spn  is able to zoom in on each element in the sequence. c  bilinear transformation will interpolate the red cross by calculating a weighted  average of the four nearest pixels. the operation is differentiable.    y position on the canvas. the x positions were randomly  sampled subject to the entire sequence must fit the canvas  and the digits are non overlapping. subsequent digits are  placed by following a slope sampled from     . finally  the images are cluttered by randomly placing   patches of  size       pixels sampled from the original mnist digits.  for the test  validation and training sets we sample from  the corresponding set in the original mnist dataset. we  create       examples for training        for validation   and       for testing  . figure   shows examples of the  generated sequences.  as a baseline model we trained a fnn spn with the spn  layer following immediately after the input. the classification network had   layers of conv maxpool dropout layers  followed by a fully connected layer with     units and finally a separate softmax layer for each position in the sequence. the convolutional layers had    filters with size        and rectified linear units were used for nonlinearity  in both the convolutional and fully connected layers. for  comparison we further train a purely convolutional network  similar to the classification network used in the ffn spn.  the rnn spn use a gated recurrent unit  gru    chung et al.        with     units. the gru is run for    time steps. at each time step the gru unit use c as input.  we apply a linear layer to convert ht into at  . the rnnspn is followed by a classification convolutional network  similar to the network used in the ffn spn model  except  that the convolutional layers only have    filters.  in all experiments  the localization networks had   layers of  max pooling convolutional layers. all convolutional layers  had    filters with size        . all models were trained  with rmsprop  tieleman   hinton        down sampling  factors and dropout rates optimized on the validation set.  a complete description of the models can be found in the     the script for generating the dataset is available along with  the rest of the code.    table  . per digit error rates on mnist sequence dataset  d is the  down sampling factor.    cluttered mnist sequences  model  err.      rnn spn d    .   rnn spn d    .   rnn spn d    .   rnn spn d    .   ffn spn d    .   ffn spn d    .   ffn spn d    .   ffn spn d    .   conv. net.   .     appendix.  the models were implemented using theano   bastien et al.        and lasagne  dieleman et al.        . the spn has been merged into the lasagne  library  . code for models and dataset is released at  https   goo.gl rspkzy.     . results  table   reports the per digit error rates for the tested models.  the rnn spn models perform better than both convolutional networks   .    and ffn spn networks   .   . in  figure   we show where the model attend on three sample  sequences from the test set. the last three columns show  image crops after the affine transformation using a downsampling factor of three. we found that increasing the  down sampling factor above one encouraged the model to  zoom. when the down sampling factor is greater than one  we introduce an information bottleneck forcing the model  to zoom in on each digit. the poor performance of the ffn     available here  http   goo.gl kgsk t     transformer network    figure  . the left column shows three examples of the generated cluttered mnist sequences. the next column shows where the model  attend when classifying each digit. in the last   column we show the image crops that the rnn spn uses to classify each digit. the  input sequences are           pixels. each image crop is         pixels because the model uses a down sample factor of  .    spn convolutional net for high down sampling values is  explained by the effective decrease in resolution since the  model needs to fit all three digits in the image crop.    chorowski  jan  bahdanau  dzmitry  cho  kyunghyun  and  bengio  yoshua. end to end continuous speech recognition using attention based recurrent nn  first results. december     .     . conclusion    chung  junyoung  gulcehre  caglar  cho  kyunghyun   and bengio  yoshua. empirical evaluation of gated recurrent neural networks on sequence modeling. arxiv  preprint arxiv     .      december     .    we have shown that the spn can be combined    