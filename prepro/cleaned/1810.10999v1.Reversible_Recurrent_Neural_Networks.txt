introduction    recurrent neural networks  rnns  have attained state of the art performance on a variety of tasks   including speech recognition      language modeling         and machine translation       . however   rnns are memory intensive to train. the standard training algorithm is truncated backpropagation  through time  tbptt        . in this algorithm  the input sequence is divided into subsequences of  smaller length  say t . each of these subsequences is processed and the gradient is backpropagated.  if h is the size of our model s hidden state  the memory required for tbptt is o t h .  decreasing the memory requirements of the tbptt algorithm would allow us to increase the length  t of our truncated sequences  capturing dependencies over longer time scales. alternatively  we could  increase the size h of our hidden state or use deeper input to hidden  hidden to hidden  or hidden tooutput transitions  granting our model greater expressivity. increasing the depth of these transitions  has been shown to increase performance in polyphonic music prediction  language modeling  and  neural machine translation  nmt            .  reversible recurrent network architectures present an enticing way to reduce the memory requirements  of tbptt. reversible architectures enable the reconstruction of the hidden state at the current timestep  given the next hidden state and the current input  which would enable us to perform tbptt without  storing the hidden states at each timestep. in exchange  we pay an increased computational cost to  reconstruct the hidden states during backpropagation.  we first present reversible analogues of the widely used gated recurrent unit  gru       and long  short term memory  lstm       architectures. we then show that any perfectly reversible rnn  requiring no storage of hidden activations will fail on a simple one step prediction task. this task is  trivial to solve even for vanilla rnns  but perfectly reversible models fail since they need to memorize  the input sequence in order to solve the task. in light of this finding  we extend the memory efficient    nd conference on neural information processing systems  nips        montr al  canada.     reversal method of maclaurin et al.       storing a handful of bits per unit in order to allow perfect  reversal for architectures which forget information.  we evaluate the performance of these models on language modeling and neural machine translation  benchmarks. depending on the task  dataset  and chosen architecture  reversible models  without  attention  achieve       fold memory savings over traditional models. reversible models achieve  approximately equivalent performance to traditional lstm and gru models on word level language  modeling on the penn treebank dataset      and lag     perplexity points behind traditional models  on the wikitext   dataset     .  achieving comparable memory savings with attention based recurrent sequence to sequence models  is difficult  since the encoder hidden states must be kept simultaneously in memory in order to  perform attention. we address this challenge by performing attention over a small subset of the  hidden state  concatenated with the word embedding. with this technique  our reversible models  succeed on neural machine translation tasks  outperforming baseline gru and lstm models on the  multi  k dataset      and achieving competitive performance on the iwslt           benchmark.  applying our technique reduces memory cost by a factor of       in the decoder  and a factor of       in the encoder.          background    we begin by describing techniques to construct reversible neural network architectures  which we  then adapt to rnns. reversible networks were first motivated by the need for flexible probability  distributions with tractable likelihoods             . each of these architectures defines a mapping  between probability distributions  one of which has a simple  known density. because this mapping is  reversible with an easily computable jacobian determinant  maximum likelihood training is efficient.  a recent paper  closely related to our work  showed that reversible network architectures can be  adapted to image classification tasks     . their architecture  called the reversible residual network  or revnet  is composed of a series of reversible blocks. each block takes an input x and produces  an output y of the same dimensionality. the input x is separated into two groups  x    x    x     and  outputs are produced according to the following coupling rule   y    x    f  x     y    x    g y          where f and g are residual functions analogous to those in standard residual networks     . the  output y is formed by concatenating y  and y    y    y    y   . each layer s activations can be  reconstructed from the next layer s activations as follows   x    y    g y     x    y    f  x          because of this property  activations from the forward pass need not be stored for use in the backwards pass. instead  starting from the last layer  activations of previous layers are reconstructed  during backpropagation  . because reversible backprop requires an additional computation of the  residual functions to reconstruct activations  it requires     more arithmetic operations than ordinary  backprop and is about     more expensive in practice. full details of how to efficiently combine  reversibility with backpropagation may be found in gomez et al.     .         reversible recurrent architectures    the techniques used to construct revnets can be combined with traditional rnn models to produce  reversible rnns. in this section  we propose reversible analogues of the gru and the lstm.   .  reversible gru  we start by recalling the gru equations used to compute the next hidden state h t    given the  current hidden state h t  and the current input x t   omitting biases     z  t    r t        w  x t    h t          g  t    tanh u  x t    r t     h t               h t    z  t  h t           z  t    g  t   here  denotes elementwise multiplication. to make this update reversible  we separate the hidden  state h into two groups  h    h    h   . these groups are updated using the following rules        code will be made available at https   github.com matthewjmackay reversible rnn  the activations prior to a pooling step must still be saved  since this involves projection to a lower  dimensional space  and hence loss of information.              t      t      t        z    r        w   x t    h    t      t      t      t       g    tanh u   x t    r     h      t   h      t   z             t   z      t     h      t                t      t      z    r        w   x t    h               t                 t   g      t      t     g    tanh u   x t    r     h         t   h      t   z             t   z      t        t     h                     t   g      t     note that h  and not h   is used to compute the update for h  . we term this model the reversible   t   gated recurrent unit  or revgru. note that zi      for i        as it is the output of a sigmoid   which maps to the open interval       . this means the revgru updates are reversible in exact   t    t    t    t    t    t   arithmetic  given h t     h    h     we can use h  and x t  to find z    r    and g  by redoing   t     part of our forwards computation. then we can find h   using    t       h    t       h      .      t      t      t        h         z        t     g         z            is reconstructed similarly. we address numerical issues which arise in practice in section  . .    reversible lstm    we next construct a reversible lstm. the lstm separates the hidden state into an output state h  and a cell state c. the update equations are    f  t    i t    o t        w  x t    h t                 c t    f  t            c t      i t     g  t     g  t    tanh u  x t    h t        h t    o t            tanh c t                t     we cannot straightforwardly apply our reversible techniques  as the update for h is not a non zero  linear transformation of h t    . despite this  reversibility can be achieved using the equations    t      t      t      t      t        f    i    o    p        w   x t    h    t      t     c    f      t       c      t       i            t     g      t      t       g    tanh u   x t    h            t      t      t       h    p             h      t       o                  t     tanh c           t      t     we calculate the updates for c    h  in an identical fashion to the above equations  using c  and h  .  we call this model the reversible lstm  or revlstm.   .     reversibility in finite precision arithmetic    we have defined rnns which are reversible in exact arithmetic. in practice  the hidden states cannot  be perfectly reconstructed due to finite numerical precision. consider the revgru equations   and   . if the hidden state h is stored in fixed point  multiplication of h by z  whose entries are less than     destroys information  preventing perfect reconstruction. multiplying a hidden unit by      for  example  corresponds to discarding its least significant bit  whose value cannot be recovered in the  reverse computation. these errors from information loss accumulate exponentially over timesteps   causing the initial hidden state obtained by reversal to be far from the true initial state. the same  issue also affects the reconstruction of the revlstm hidden states. hence  we find that forgetting is  the main roadblock to constructing perfectly reversible recurrent architectures.  there are two possible avenues to address this limitation. the first is to remove the forgetting step.   t   t    t    t   for the revgru  this means we compute zi   ri   and gi as before  and update hi using    t      t       hi   hi     t            zi       t     gi            we term this model the no forgetting revgru or nf revgru. because the updates of the nfrevgru do not discard information  we need only store one hidden state in memory at a given time  during training. similar steps can be taken to define a nf revlstm.  the second avenue is to accept some memory usage and store the information forgotten from the  hidden state in the forward pass. we can then achieve perfect reconstruction by restoring this  information to our hidden state in the reverse computation. we discuss how to do so efficiently in  section  .        h     a    a    b    c    c    b    a    h       h       h       h       h       h       b    c    c    b    h     a    figure    unrolling the reverse computation of an exactly reversible model on the repeat task yields a sequenceto sequence computation. left  the repeat task itself  where the model repeats each input token. right   unrolling the reversal. the model effectively uses the final hidden state to reconstruct all input tokens  implying  that the entire input sequence must be stored in the final hidden state.         impossibility of no forgetting    we have shown reversible rnns in finite precision can be constructed by ensuring that no information  is discarded. we were unable to find such an architecture that achieved acceptable performance on  tasks such as language modeling  . this is consistent with prior work which found forgetting to be  crucial to lstm performance         . in this section  we argue that this results from a fundamental  limitation of no forgetting reversible models  if none of the hidden state can be forgotten  then the  hidden state at any given timestep must contain enough information to reconstruct all previous hidden  states. thus  any information stored in the hidden state at one timestep must remain present at all  future timesteps to ensure exact reconstruction  overwhelming the storage capacity of the model.  we make this intuition concrete by considering an elementary sequence learning task  the repeat task.  in this task  an rnn is given a sequence of discrete tokens and must simply repeat each token at the  subsequent timestep. this task is trivially solvable by ordinary rnn models with only a handful of  hidden units  since it doesn t require modeling long distance dependencies. but consider how an  exactly reversible model would perform the repeat task. unrolling the reverse computation  as shown  in figure    reveals a sequence to sequence computation in which the encoder and decoder weights  are tied. the encoder takes in the tokens and produces a final hidden state. the decoder uses this  final hidden state to produce the input sequence in reverse sequential order.  notice the relationship to another sequence learning task  the memorization task  used as part of  a curriculum learning strategy by zaremba and sutskever     . after an rnn observes an entire  sequence of input tokens  it is required to output the input sequence in reverse order. as shown in  figure    the memorization task for an ordinary rnn reduces to the repeat task for an nf revrnn.  hence  if the memorization task requires a hidden representation size that grows with the sequence  length  then so does the repeat task for nf revrnns.  we confirmed experimentally that nf revgru and nf revlsm networks with limited capacity  were unable to solve the repeat task  . interestingly  the nf revgru was able to memorize input  sequences using considerably fewer hidden units than the ordinary gru or lstm  suggesting it may  be a useful architecture for tasks requiring memorization. consistent with the results on the repeat  task  the nf revgru and nf revlstm were unable to match the performance of even vanilla  rnns on word level language modeling on the penn treebank dataset     .         reversibility with forgetting    the impossibility of zero forgetting leads us to explore the second possibility to achieve reversibility   storing information lost from the hidden state during the forward computation  then restoring it in the  reverse computation. initially  we investigated discrete forgetting  in which only an integral number  of bits are allowed to be forgotten. this leads to a simple implementation  if n bits are forgotten in  the forwards pass  we can store these n bits on a stack  to be popped off and restored to the hidden  state during reconstruction. however  restricting our model to forget only an integral number of  bits led to a substantial drop in performance compared to baseline models  . for the remainder of       we discuss our failed attempts in appendix a.  we include full results and details in appendix b. the argument presented applies to idealized rnns able  to implement any hidden to hidden transition and whose hidden units can store    bits each. we chose to use  the lstm and the nf revgru as approximations to these idealized models since they performed best at their  respective tasks.     algorithmic details and experimental results for discrete forgetting are given in appendix d.             algorithm   exactly reversible multiplication  maclaurin et al.                                          input  buffer integer b  hidden state h     rh h    forget value z     rz z   with     z      rz  b   b    rz  make room for new information on buffer   b   b    h  mod  rz    store lost information in buffer   h    h     rz  divide by denominator of z   h    h    z    multiply by numerator of z   h    h     b mod z      add information to hidden state   b   b   z    shorten information buffer   return updated buffer b  updated value h     rh h     this paper  we turn to fractional forgetting  in which a fractional number of bits are allowed to be  forgotten.  to allow forgetting of a fractional number of bits  we use a technique introduced by maclaurin et al.       to store lost information. to avoid cumbersome notation  we do away with super  and subscripts  and consider a single hidden unit h and its forget value z. we represent h and z as fixed point  numbers  integers with an implied radix point . for clarity  we write h     rh h  and z     rz z   .  hence  h  is the number stored on the computer and multiplication by   rh supplies the implied  radix point. in general  rh and rz are distinct. our goal is to multiply h by z  storing as few bits as  necessary to make this operation reversible.  the full process of reversible multiplication is shown in detail in algorithm  . the algorithm  maintains an integer information buffer which stores h  mod  rz at each timestep  so integer  division of h  by  rz is reversible. however  this requires enlarging the buffer by rz bits at each  timestep. maclaurin et al.      reduced this storage requirement by shifting information from the  buffer back onto the hidden state. reversibility is preserved if the shifted information is small enough  so that it does not affect the reverse operation  integer division of h  by z    . we include a full review  of the algorithm of maclaurin et al.      in appendix c. .  however  this trick introduces a new complication not discussed by maclaurin et al.       the  information shifted from the buffer could introduce significant noise into the hidden state. shifting  information requires adding a positive value less than z   to h  . because z          rz    z is the output  of a sigmoid function and z     rz z      h     rh h  may be altered by as much   rz       rh .  if rz   rh   this can alter the hidden state h by   or more  . this is substantial  as in practice we  observe  h      . indeed  we observed severe performance drops for rh and rz close to equal.  the solution is to limit the amount of information moved from the buffer to the hidden state by setting  rz smaller than rh . we found rh      and rz      to work well. the amount of noise added  onto the hidden state is bounded by  rz  rh   so with these values  the hidden state is altered by at  most      . while the precision of our forgetting value z is limited to    bits  previous work has  found that neural networks can be trained with precision as low as       bits and reach the same  performance as high precision networks         . we find our situation to be similar.  memory savings to analyze the savings that are theoretically possible using the procedure above   consider an idealized memory buffer which maintains dynamically resizing storage integers bhi for  each hidden unit h in groups i        of the revgru model. using the above procedure  at each  timestep the number of bits stored in each bhi grows by            rz   log   zi h      log   rz  zi h    log     zi h          if the entries of zi h are not close to zero  this compares favorably with the na ve cost of    bits  per timestep. the total storage cost of tbptt for a revgru model with hidden state size h on a  sequence of length t will be        t h     xx   t    t      log   z  h     log   z  h          t t h      thus  in the idealized case  the number of bits stored equals the number of bits forgotten.          we illustrate this phenomenon with a concrete example in appendix c. .   t    t   for the revlstm  we would sum over pi and fi terms.          attention       ...  ...    ...    ...   sos     ...    encoder    decoder    figure    attention mechanism for nmt. the word embeddings  encoder hidden states  and decoder hidden  states are color coded orange  blue  and green  respectively  the striped regions of the encoder hidden states  represent the slices that are stored in memory for attention. the final vectors used to compute the context vector  are concatenations of the word embeddings and encoder hidden state slices.     .  gpu considerations  for our method to be used as part of a practical training procedure  we must run it on a parallel  architecture such as a gpu. this introduces additional considerations which require modifications to  algorithm        we implement it with ordinary finite bit integers  hence dealing with overflow  and      for gpu efficiency  we ensure uniform memory access patterns across all hidden units.  overflow. consider the storage required for a single hidden unit. algorithm   assumes unboundedly  large integers  and hence would need to be implemented using dynamically resizing integer types   as was done by maclaurin et al.     . but such data structures would require non uniform memory  access patterns  limiting their efficiency on gpu architectures. therefore  we modify the algorithm  to use ordinary finite integers. in particular  instead of a single integer  the buffer is represented  with a sequence of    bit integers  b    . . .   bd  . whenever the last integer in our buffer is about to  overflow upon multiplication by  rz   as required by step   of algorithm    we append a new integer  bd   to the sequence. overflow will occur if bd       rz .  after appending a new integer bd     we apply algorithm   unmodified  using bd   in place of b.  it is possible that up to rz     bits of bd will not be used  incurring an additional penalty on storage  cost. we experimented with several ways of alleviating this penalty but found that none improved  significantly over the storage cost of the initial method.  vectorization. vectorization imposes an additional penalty on storage. for efficient computation   we cannot maintain different size lists as buffers for each hidden unit in a minibatch. rather  we must  store the buffer as a three dimensional tensor  with dimensions corresponding to the minibatch size   the hidden state size  and the length of the buffer list. this means each list of integers being used as a  buffer for a given hidden unit must be the same size. whenever a buffer being used for any hidden  unit in the minibatch overflows  an extra integer must be added to the buffer list for every hidden unit  in the minibatch. otherwise  the steps outlined above can still be followed.  we give the complete  revised algorithm in appendix c. . the compromises to address overflow and  vectorization entail additional overhead. we measure the size of this overhead in section  .   .  memory savings with attention  most modern architectures for neural machine translation make use of attention mechanisms          in this section  we describe the modifications that must be made to obtain memory savings when  using attention. we denote the source tokens by x      x      . . .   x t     and the corresponding word  embeddings by e      e      . . .   e t   . we also use the following notation to denote vector slices  given  a vector v   rd   we let v   k    rk denote the vector consisting of the first k dimensions of v.  standard attention based models for nmt perform attention over the encoder hidden states  this is  problematic from the standpoint of memory savings  because we must retain the hidden states in  memory to use them when computing attention. to remedy this  we explore several alternatives to  storing the full hidden state in memory. in particular  we consider performing attention over     the  embeddings e t    which capture the semantics of individual words     slices of the encoder hidden        table    validation perplexities  memory savings  on penn treebank word level language modeling. results  shown when forgetting is restricted to       and   bits per hidden unit per timestep and when there is no restriction.  reversible model      bit      bits      bits    no limit    usual model    no limit      layer revgru    layer revgru      .     .      .     .        .     .      .     .        .    .      .    .        .    .      .    .        layer gru    layer gru      .     .       layer revlstm    layer revlstm      .     .      .     .        .     .      .     .        .    .      .    .        .    .      .    .        layer lstm    layer lstm      .     .      t     states  henc    k   where we consider k      or       and    the concatenation of embeddings and   t   hidden state slices   e t    henc    k  . since the embeddings are computed directly from the input  tokens  they don t need to be stored. when we slice the hidden state  only the slices that are attended  to must be stored. we apply our memory saving buffer technique to the remaining d   k dimensions.  in our nmt models  we make use of the global attention mechanism introduced by luong et   t   al.       where each decoder hidden state hdec is modified by incorporating context from the source  annotations  a context vector c t  is computed as a weighted sum of source annotations  with weights  g   t    t    t    j    hdec and c t  are used to produce an attentional decoder hidden state hdec . figure   illustrates  this attention mechanism  where attention is performed over the concatenated embeddings and hidden  state slices. additional details on attention are provided in appendix f.   .  additional considerations   t   restricting forgetting. in order to guarantee memory savings  we may restrict the entries of zi  to lie in  a     rather than         for some a    . setting a    .   for example  forces our model to  forget at most one bit from each hidden unit per timestep. this restriction may be accomplished by   t   applying the linear transformation x         a x   a to zi after its initial computation  .  limitations. the main flaw of our method is the increased computational cost. we must reconstruct  hidden states during the backwards pass and manipulate the buffer at each timestep. we find that  each step of reversible backprop takes about     times as much computation as regular backprop. we  believe this overhead could be reduced through careful engineering. we did not observe a slowdown  in convergence in terms of number of iterations  so we only pay an increased per iteration cost.         experiments    we evaluated the performance of reversible models on two standard rnn tasks  language modeling  and machine translation. we wished to determine how much memory we could save using the  techniques we have developed  how these savings compare with those possible using an idealized  buffer  and whether these memory savings come at a cost in performance. we also evaluated our  proposed attention mechanism on machine translation tasks.   .  language modeling experiments  we evaluated our one  and two layer reversible models on word level language modeling on the penn  treebank      and wikitext        corpora. in the interest of a fair comparison  we kept architectural  and regularization hyperparameters the same between all models and datasets. we regularized the  hidden to hidden  hidden to output  and input to hidden connections  as well as the embedding  matrix  using various forms of dropout  . we used the hyperparameters from merity et al.    . details  are provided in appendix g. . we include training validation curves for all models in appendix i.   . .  penn treebank experiments  we conducted experiments on penn treebank to understand the performance of our reversible models   how much restrictions on forgetting affect performance  and what memory savings are achievable.  performance. with no restriction on the amount forgotten  one  and two layer revgru and  revlstm models obtained roughly equivalent validation performance   compared to their non t           t     for the revlstm  we would apply this transformation to pi and fi .  we discuss why dropout does not require additional storage in appendix e.      test perplexities exhibit similar patterns but are     perplexity points lower.             table    validation perplexities on wikitext   word level language modeling. results shown when forgetting is  restricted to       and   bits per hidden unit per timestep and when there is no restriction.  reversible model      bits      bits      bits    no limit    usual model    no limit      layer revgru    layer revgru      .     .       .     .       .     .       .     .       layer gru    layer gru      .     .       layer revlstm    layer revlstm      .     .       .     .       .     .       .     .       layer lstm    layer lstm      .     .     reversible counterparts  as shown in table  . to determine how little could be forgotten without  affecting performance  we also experimented with restricting forgetting to at most       or   bits per  hidden unit per timestep using the method of section  . . restricting the amount of forgetting to        or   bits from each hidden unit did not significantly impact performance.  performance suffered once forgetting was restricted to at most   bit. this caused a     increase in  perplexity for the revgru. it also made the revlstm unstable for this task since its hidden state   unlike the revgru s  can grow unboundedly if not enough is forgotten. hence  we do not include  these results.  memory savings. we tracked the size of the information buffer throughout training and used this  to compare the memory required when using reversibility vs. storing all activations. as shown in  appendix h  the buffer size remains roughly constant throughout training. therefore  we show  the average ratio of memory requirements during training in table  . overall  we can achieve a        fold reduction in memory when forgetting at most     bits  while maintaining comparable  performance to standard models. using equation     we also compared the actual memory savings  to the idealized memory savings possible with a perfect buffer. in general  we use about twice the  amount of memory as theoretically possible. plots of memory savings for all models  both idealized  and actual  are given in appendix h.   . .  wikitext   experiments  we conducted experiments on the wikitext   dataset  wt   to see how reversible models fare on a  larger  more challenging dataset. we investigated various restrictions  as well as no restriction  on  forgetting and contrasted with baseline models as shown in table  . the revgru model closely  matched the performance of the baseline gru model  even with forgetting restricted to   bits. the  revlstm lagged behind the baseline lstm by about   perplexity points for one  and two layer  models.   .     neural machine translation experiments    we further evaluated our models on english to german neural machine translation  nmt . we used  a unidirectional encoder decoder model and our novel attention mechanism described in section   . . we experimented on two datasets  multi  k       a dataset of         sentence pairs derived  from flickr image captions  and iwslt            a larger dataset of          pairs. experimental  details are provided in appendix g.   training and validation curves are shown in appendix i.    multi  k  and i.   iwslt   plots of memory savings during training are shown in appendix h. .  for multi  k  we used single layer rnns with     dimensional hidden states and     dimensional  word embeddings for both the encoder and decoder. our baseline gru and lstm models achieved  test bleu scores of   .   and   .    respectively. the test bleu scores and encoder memory  savings achieved by our reversible models are shown in table    for several variants of attention  and restrictions on forgetting. for attention  we use emb to denote word embeddings  xh for a  x dimensional slice of the hidden state     h denotes the whole hidden state   and emb xh to  denote the concatenation of the two. overall  while emb attention achieved the best memory savings   emb   h achieved the best balance between performance and memory savings. the revgru with  emb   h attention and forgetting at most   bits achieved a test bleu score of   .    outperforming  the standard gru  while reducing activation memory requirements by  .   and   .   in the encoder  and decoder  respectively. the revlstm with emb   h attention and forgetting at most   bits  achieved a test bleu score of   .    outperforming the standard lstm  while reducing activation  memory requirements by  .   and   .   in the encoder and decoder respectively.        table    performance on the multi  k dataset with different restrictions on forgetting. p denotes the test bleu  scores  m denotes the average memory savings of the encoder during training.  model    attention      bit      bit      bit      bit    no limit    p    m    p    m    p    m    p    m    p    m    revlstm      h     h     h  emb  emb   h      .      .      .      .      .        .    .    .     .     .       .      .      .      .      .       .    .    .     .    .       .      .      .      .      .       .    .    .     .    .       .      .      .      .      .       .    .    .     .    .       .      .      .      .      .       .    .    .     .    .     revgru      h     h     h  emb  emb   h      .      .      .      .      .       .    .    .     .    .       .      .      .      .      .       .    .    .     .    .       .      .      .      .      .       .    .    .     .    .       .      .      .      .      .       .    .    .     .    .       .      .      .      .      .       .    .    .     .    .     for iwslt       we used   layer rnns with     dimensional hidden states and     dimensional  word embeddings for the encoder and decoder. we evaluated reversible models in which the decoder  used emb   h attention. the baseline gru and lstm models achieved test bleu scores of   .    and   .    respectively. the revgru achieved a test bleu score of   .    outperforming the gru   while saving  .    and   .    in the encoder and decoder  respectively. the revlstm achieved a  score of   .    competitive with the lstm  while saving  .    and  .    memory in the encoder  and decoder  respectively. both reversible models were restricted to forget at most   bits.         related work    several approaches have been taken to reduce the memory requirements of rnns. frameworks  that use static computational graphs          aim to allocate memory efficiently in the training  algorithms themselves. checkpointing              is a frequently used method. in this strategy   certain activations are stored as checkpoints throughout training and the remaining activations are  recomputed as needed in the backwards pass. checkpointing has previously been used    to train  recurrent neural networks on sequences of length t by storing the activations every d t e layers      . gruslys et al.      further developed this strategy by using dynamic programming to determine  which activations to store in order to minimize computation for a given storage budget.  decoupled neural interfaces          use auxilliary neural networks trained to produce the gradient of  a layer s weight matrix given the layer s activations as input  then use these predictions to train  rather  than the true gradient. this strategy depends on the quality of the gradient approximation produced  by the auxilliary network. hidden activations must still be stored as in the usual backpropagation  algorithm to train the auxilliary networks  unlike our method.  unitary recurrent neural networks              refine vanilla rnns by parametrizing their transition  matrix to be unitary. these networks are reversible in exact arithmetic       the conjugate transpose  of the transition matrix is its inverse  so the hidden to hidden transition is reversible. in practice  this  method would run into numerical precision issues as floating point errors accumulate over timesteps.  our method  through storage of lost information  avoids these issues.         conclusion    we have introduced reversible recurrent neural networks as a method to reduce the memory requirements of truncated backpropagation through time. we demonstrated the flaws of exactly reversible  rnns  and developed methods to efficiently store information lost during the hidden to hidden  transition  allowing us to reverse the transition during backpropagation. reversible models can  achieve roughly equivalent performance to standard models while reducing the memory requirements  by a factor of      during training. we believe reversible models offer a compelling path towards  constructing more flexible and expressive recurrent neural networks.  acknowledgments  we thank kyunghyun cho for experimental advice and discussion. we also thank aidan gomez   mengye ren  gennady pekhimenko  and david duvenaud for helpful discussion. mm is supported  by an nserc cgs m award  and pv is supported by an nserc pgs d award.        