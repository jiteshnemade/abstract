introduction    recently the sequence to sequence model   seq seq  in neural machine translation  nmt   has achieved certain success over the state ofthe art of statistical machine translation  smt        work done at huawei noah s ark lab  hongkong.    np     vv    np     input   output  tokoyo stock exchange approves new listing bank  reference  tokyo exchange approves shinsei bank  s application for listing   a . an example of discontinuous translation  np  input          output  they came from six families with two girls and two girls .  reference  they came from six families and two girls are without parents .   b . an example of over translation    figure    examples of nmt translation that fail to  respect source syntax.  on various language pairs  bahdanau et al.         jean et al.        luong et al.        luong and  manning       . however  shi et al.         show that the seq seq model still fails to capture  a lot of deep structural details  even though it is  capable of learning certain implicit source syntax  from sentence aligned parallel corpus. moreover   it requires an additional parsing task specific  training mechanism to recover the hidden syntax  in nmt. as a result  in the absence of explicit  linguistic knowledge  the seq seq model in nmt  tends to produce translations that fail to well  respect syntax. in this paper  we show that syntax  can be well exploited in nmt explicitly by taking  advantage of source side syntax to improve the  translation accuracy.  in principle  syntax is a promising avenue for  translation modeling. this has been verified  by tremendous encouraging studies on syntaxbased smt that substantially improves translation  by integrating various kinds of syntactic knowledge  liu et al.        marton and resnik            shen et al.        li et al.       . while it is yet to  be seen how syntax can benefit nmt effectively   we find that translations of nmt sometimes fail  to well respect source syntax. figure    a  shows a  chinese to english translation example of nmt.  in this example  the nmt seq seq model incorrectly translates the chinese noun phrase  i.e.       xinsheng     yinhang  into a discontinuous  phrase in english  i.e.  new ... bank  due to the  failure of capturing the internal syntactic structure  in the input chinese sentence. statistics on our development set show that one forth of chinese noun  phrases are translated into discontinuous phrases  in english  indicating the substantial disrespect of  syntax in nmt translation.  figure    b  shows  another example with over translation  where the  noun phrase   liang   ge    nvhai is translated twice in english. similar to discontinuous  translation  over translation usually happens along  with the disrespect of syntax which results in the  repeated translation of the same source words in  multiple positions of the target sentence.  in this paper we are not aiming at solving any  particular issue  either the discontinuous translation or the over translation. alternatively  we address how to incorporate explicitly the source syntax to improve the nmt translation accuracy with  the expectation of alleviating the issues above in  general. specifically  rather than directly assigning each source word with manually designed syntactic labels  as sennrich and haddow        do   we linearize a phrase parse tree into a structural  label sequence and let the model automatically  learn useful syntactic information. on the basis   we systematically propose and compare several  different approaches to incorporating the label sequence into the seq seq nmt model. experimentation on chinese to english translation demonstrates that all proposed approaches are able to improve the translation accuracy.         attention based nmt    as a background and a baseline  in this section   we briefly describe the nmt model with an attention mechanism by bahdanau et al.         which  mainly consists of an encoder and a decoder  as  shown in figure  .  encoder the encoding of a source sentence is for     manually examining     random such discontinuously  translated noun phrases  we find that     of them should be  continuously translated according to the reference translation.    h  h     h     yi  hm    h    hm    si      h     h     x     x   .. xm     a  encoder    atten    ci    mlp  si  rnn  yi       b  decoder    figure    attention based nmt model.  mulated using a pair of neural networks  i.e.  two  recurrent neural networks  denoted bi rnn   one  reads an input sequence x    x    ...  xm   from left  to right and outputs a forward sequence of hid          den states  h    ...  hm    while the other operates  from right to left and outputs a backward sequence             h    ...  hm  . each source word xj is represented  as hj  also referred to as word annotation vector                the concatenation of hidden states hj and hj . such  bi rnn encodes not only the word itself but also  its left and right context  which can provide important evidence for its translation.  decoder the decoder is also an rnn that predicts a target sequence y    y    ...  yn  . each target word yi is predicted via a multi layer perceptron  mlp  component which is based on a recurrent hidden state si   the previous predicted word  yi     and a source side context vector ci . here   ci is calculated as a weighted sum over source annotation vectors  h    ...  hm  . the weight vector   i   rm over source annotation vectors is obtained by an attention model  which captures the  correspondences between the source and the target  languages. the attention weight  ij is computed  based on the previous recurrent hidden state si    and source annotation vector hj .         nmt with source syntax    the conventional nmt models treat a sentence as  a sequence of words and ignore external knowledge  failing to effectively capture various kinds  of inherent structure of the sentence. to leverage external knowledge  specifically the syntax in  the source side  we focus on the parse tree of a  sentence and propose three different nmt models that explicitly consider the syntactic structure  into encoding. our purpose is to inform the nmt  model the structural context of each word in its  corresponding parse tree with the goal that the  learned annotation vectors  h    ...  hm   encode not     i    love    w     w     dogs  w      a  word sequence  s  np    vp    prn    vbp    np    i    love    nns    hw     hw     hw     hw     hw     hw     hl     hl     hl     hl     hl     hl                    hw  hw  hw     hl     hw  hw  hw     hl   hl   s np prn vp vbp np nns    i    love dogs  word rnn                                               hl          structural label rnn   a  parallel rnn encoder    dogs   b  phrase parse tree  s np prn vp vbp np nns    l  l     l     l     l     l     l     word rnn    hw     hw     hw     hw     hw     hw     ew     ew     ew                     c  structural label sequence  i    figure    an example of an input sentence  a   its  parse tree  b   and the parse tree s sequential form   c .  only the information of words and their surroundings  but also structural context in the parse tree. in  the rest of this section  we use english sentences  as examples to explain our methods.   .     syntax representation    to obtain the structural context of a word in its  parse tree  ideally the model should not only capture and remember the whole parse tree structure   but also discriminate the contexts of any two different words. however  considering the lack of  efficient way to directly model structural information  an alternative way is to linearize the phrase  parse tree into a sequence of structural labels and  learn the structural context through the sequence.  for example  figure   c  shows the structural label sequence of figure   b  in a simple way following a depth first traversal order. note that linearizing a parse tree in a depth first traversal order into a sequence of structural labels has also  been widely adopted in recent advances in neural  syntactic parsing  vinyals et al.        choe and  charniak         suggesting that the linearized sequence can be viewed as an alternative to its tree  structure.      we have also tried to include the ending brackets in the  structural label sequence  as what  vinyals et al.        choe    iove    dogs                hl     hl   structural label                 hl   hl   rnn  s np prn vp vbp np nns     b  hierarchical rnn encoder    figure    the graphical illustration of the parallel  rnn encoder  a  and the hierarchical rnn en          coder  b . here  hwj and hwj are the forward and              backward hidden states for word wj   hli and hli  are for structural label l  li   ewj is the word embedding for word wj   and  is for concatenation operator.    there is no doubt that the structural label sequence is much longer than its word sequence.  in order to obtain the structural label annotation  vector for wi in word sequence  we simply look  for wi  s part of speech  pos  tag in the label sequence and view the tag s annotation vector as  wi  s label annotation vector. this is because wi  s  pos tag location can also represent wi  s location  in the parse tree. for example  in figure    word  w  in  a  maps to l  in  c  since l  is the pos tag  of w  . likewise  w  maps to l  and w  to l  . that  is to say  we use l   s learned annotation vector as  w   s label annotation vector.  and charniak        do. however  the performance gap is  very small by adding the ending brackets or not.      .     rnn encoders with source syntax    in the next  we first propose two different encoders  to augment word annotation vector with its corresponding label annotation vector  each of which  consists of two rnns     in one encoder  the two  rnns work independently  i.e.  parallel rnn encoder  while in another encoder the two rnns  work in a hierarchical way  i.e.  hierarchical rnn  encoder . the difference between the two encoders lies in how the two rnns interact. then   we propose the third encoder with a single rnn   which learns word and label annotation vectors  stitchingly  i.e.  mixed rnn encoder . since any  of the above three approaches focuses only on the  encoder as to generate source annotation vectors  along with structural information  we keep the rest  part of the nmt models unchanged.  parallel rnn encoder figure    a  illustrates  our parallel rnn encoder  which includes two  parallel rnns  i.e.  a word rnn and a structural  label rnn. on the one hand  the word rnn  as in  conventional nmt models  takes a word sequence  as input and output a word annotation vector  for each word. on the other hand  the structural  label rnn takes the structural label sequence of  the word sequence as input and obtains a label  annotation vector for each label. besides  we  concatenate each word s word annotation vector  and its pos tag s label annotation vector as the  final annotation vector for the word. for example   the final annotation vector for word love in                 figure    a  is  hw    hw    hl    hl     where the first           two subitems  hw    hw    are the word annotation         vector and the rest two subitems  hl    hl    are its  pos tag vbp s label annotation vector.  hierarchical rnn encoder partially inspired  by the model architecture of gnmt  wu et al.         which consists of multiple layers of lstm  rnns  we propose a two layer model architecture in which the lower layer is the structural label  rnn while the upper layer is the word rnn  as  shown in figure    b . we put the word rnn in  the upper layer because each item in the word sequence can map into an item in the structural label  sequence  while this does not hold if the order of  the two rnns is reversed. as shown in figure     b   for example  the pos tag vbp s label anno       tation vector  hl    hl    is concatenated with word       hereafter  we simplify bi rnn as rnn.    h     h     h     h     h     h     h     h     h     h      h     h     h     h     h     h     h     h     h     h      s    np prn    i    vp vbp love np nns dogs    figure    the graphical illustration of the mixed              rnn encoder. here  hj and hj are the forward and  backward hidden annotation vectors for j th item   which can be either a word or a structural label.  love s word embedding ew  to feed as the input to  the word rnn.  mixed rnn encoder figure   presents our  mixed rnn encoder. similarly  the sequence of  input is the linearization of its parse tree  as in  figure    b   following a depth first traversal order  but being mixed with both words and structural labels in a stitching way. it shows that the  rnn learns annotation vectors for both the words  and the structural labels  though only the annotation vectors of words are further fed to decoding                              e.g.    h    h      h    h      h     h      . even though  the annotation vectors of structural labels are not  directly fed forward for decoding  the error signal  is back propagated along the word sequence and  allows the annotation vectors of structural labels  being updated accordingly.   .     comparison of rnn encoders with  source syntax    though all the three encoders model both word  sequence and structural label sequence  the differences lie in their respective model architecture  with respect to the degree of coupling the two sequences     in the parallel rnn encoder  the word rnn  and structural label rnn work in a parallel  way. that is to say  the error signal back  propagated from the word sequence would  not affect the structural label rnn  and vice  versa. in contrast  in the hierarchical rnn  encoder  the error signal back propagated  from the word sequence has a direct impact  on the structural label annotation vectors  and  thus on the structural label embeddings. finally  the mixed rnn encoder ties the structural label sequence and word sequence together in the closest way. therefore  the  degrees of coupling the word and structural     label sequences in these three encoders are  like this  mixed rnn encoder   hierarchical rnn encoder   parallel rnn encoder.    figure   and figure   suggest that the mixed  rnn encoder is the simplest. moreover   comparing to conventional nmt encoders   the difference lies only in the length of the input sequence. statistics on our training data  reveal that the mixed rnn encoder approximately triples the input sequence length compared to conventional nmt encoders.         experimentation    we have presented our approaches to incorporating the source syntax into nmt encoders. in  this section  we evaluate their effectiveness on  chinese to english translation.   .     experimental settings    our training data for the translation task consists  of  .  m sentence pairs extracted from ldc corpora  with   . m chinese words and   . m english words respectively.  we choose nist mt     dataset       sentence pairs  as our development set  and nist mt             and    datasets                  and      sentence pairs  respectively  as our test sets.  to get the source syntax for sentences on the source side  we parse the  chinese sentences with berkeley parser    petrov  and klein        trained on chinese treebank   .   xue et al.       . we use the case insensitive    gram nist bleu score  papineni et al.         for the translation task.  for efficient training of neural networks  we  limit the maximum sentence length on both source  and target sides to   . we also limit both the  source and target vocabularies to the most frequent    k words in chinese and english  covering approximately   .   and   .   of the two corpora  respectively. all the out of vocabulary words are  mapped to a special token unk. besides  the word  embedding dimension is     and the size of a hidden layer is     . all the other settings are the  same as in bahdanau et al.      .     the corpora include ldc    e    ldc    e     ldc    e    hansards portion of ldc    t     ldc    t   and ldc    t  .     http   www.itl.nist.gov iad mig   tests mt      https   github.com slavpetrov   berkeleyparser    the inventory of structural labels includes     phrase labels and    pos tags. in both our parallel rnn encoder and hierarchical rnn encoder   we set the embedding dimension of these labels as      and the size of a hidden layer as    . besides   the maximum structural label sequence length is  set to    . in our mixed rnn encoder  since we  only have one input sequence  we equally treat the  structural labels and words  i.e.  a structural label  is also initialized with     dimension embedding .  compared to the baseline nmt model  the only  different setting is that we increase the maximum  sentence length on source side from    to    .  we compare our method with two state of theart models of smt and nmt     cdec  dyer et al.         an open source hierarchical phrase based smt system  chiang        with default configuration and a    gram language model trained on the target  portion of the training data.     rnnsearch  a re implementation of the attentional nmt system  bahdanau et al.         with slight changes taken from dl mt  tutorial.  for the activation function f of an  rnn  rnnsearch uses the gated recurrent  unit  gru  recently proposed by  cho et al.       b . it incorporates dropout  hinton  et al.        on the output layer and improves  the attention model by feeding the lastly generated word. we use adadelta  zeiler         to optimize model parameters in training with  the mini batch size of   . for translation  a  beam search with size    is employed.   .     experiment results    table   shows the translation performances measured in bleu score. clearly  all the proposed  nmt models with source syntax improve the  translation accuracy over all test sets  although  there exist considerable differences among different variants.  parameters the three proposed models introduce  new parameters in different ways. as a baseline  model  rnnsearch has   . m parameters. due to  the infrastructure similarity  the parallel rnn system and the hierarchical rnn system introduce       https   github.com redpony cdec  https   github.com nyu dl   dl mt tutorial                            system  cdec  rnnsearch  parallel rnn  hierarchical rnn  mixed rnn     params    . m    . m    . m        time     m    m    m     m    mt      .     .     .      .      .      mt      .     .     .      .     .      mt      .     .     .     .      .      mt      .     .     .      .      .      mt      .     .     .     .      .      all    .     .     .      .      .      table    evaluation of the translation performance.   and    significant over rnnsearch at  .    .     tested by bootstrap resampling  koehn       .     is the additional number of parameters or training  time on the top of the baseline system rnnsearch. column time indicates the training time in minutes  per epoch for different nmt models  the similar size of additional parameters  resulting from the rnn model for structural label sequences  about  . m parameters  and catering either the augmented annotation vectors  as shown  in figure    a   or the augmented word embeddings  as shown in figure    b    the remain parameters . it is not surprising that the mixed rnn  system does not require any additional parameters  since though the input sequence becomes longer   we keep the vocabulary size unchanged  resulting  in no additional parameters.  speed introducing the source syntax slightly  slows down the training speed. when running on  a single gpu geforce gtx       the baseline  model speeds     minutes per epoch with   k  updates while the proposed structural label rnns  in both parallel rnn and hierarchical rnn systems only increases the training time by about      thanks to the small size of structural label embeddings and annotation vectors   and the mixed rnn  system spends     more training time to cater the  triple sized input sequence.  comparison with the baseline nmt model   rnnsearch  while all the three proposed nmt  models outperform rnnsearch  the parallel rnn  system and the hierarchical rnn system achieve  similar accuracy  e.g.    .  v.s.   .  . besides   the mixed rnn system achieves the best accuracy overall test sets with the only exception of  nist mt   . over all test sets  it outperforms  rnnsearch by  .  bleu points and outperforms  the other two improved nmt models by  .   .   bleu points  suggesting the benefits of high degree of coupling the word sequence and the structural label sequence. this is very encouraging  since the mixed rnn encoder is the simplest   without introducing new parameters and with only  slight additional training time.    figure    performance of the generated translations with respect to the lengths of the input sentences.  comparison with the smt model  cdec  table   also shows that all nmt systems outperform the smt system. this is very consistent  with other studies on chinese to english translation  mi et al.        tu et al.      b  wang et al.        .         analysis    as the proposed mixed rnn system achieves  the best performance  we further look at the  rnnsearch system and the mixed rnn system to  explore more on how syntactic information helps  in translation.   .     effects on long sentences    following bahdanau et al.         we group sentences of similar lengths together and compute  bleu scores. figure   presents the bleu scores  over different lengths of input sentences. it shows  that mixed rnn system outperforms rnnsearch  over sentences with all different lengths. it also  shows that the performance drops substantially     system  rnnsearch  mixed rnn    aer    .     .     system    rnnsearch  table    evaluation of alignment quality. the  lower the score  the better the alignment quality.  when the length of input sentences increases. this  performance trend over the length is consistent  with the findings in  cho et al.      a  tu et al.             a . we also observe that the nmt systems perform surprisingly bad on sentences over     in length  especially compared to the performance of smt system  i.e.  cdec . we think that  the bad behavior of nmt systems towards long  sentences  e.g.  length of     is due to the following two reasons      the maximum source sentence length limit is set as    in training    making  the learned models not ready to translate sentences  over the maximum length limit      nmt systems  tend to stop early for long input sentences.   .     analysis on word alignment    due to the capability of carrying syntactic information in source annotation vectors  we conjecture that our model with source syntax is also  beneficial for alignment. to test this hypothesis  we carry out experiments of the word alignment task on the evaluation dataset from liu and  sun         which contains     manually aligned  chinese english sentence pairs. we force the decoder to output reference translations  as to get automatic alignments between input sentences and  their reference translations. to evaluate alignment  performance  we report the alignment error rate   aer   och and ney        in table  .  table   shows that source syntax information  improves the attention model as expected by maintaining an annotation vector summarizing structural information on each source word.    mixed rnn    xp  pp  np  cp  qp  all  pp  np  cp  qp  all    cont.    .     .     .     .     .     .     .     .     .     .     analysis on phrase alignment    the above subsection examines the alignment performance at the word level. in this subsection  we  turn to phrase alignment analysis by moving from  word unit to phrase unit. given a source phrase  xp  we use word alignments to examine if the  phrase is translated continuously  cont.   or dis     though the maximum source length limit in mixed rnn  system is set to      it approximately contains    words in  maximum.    un.   .     .    .     .     .    .     .    .     .     .     table    percentages     of syntactic phrases in  our test sets being translated continuously  discontinuously  or not being translated. here pp is for  prepositional phrase  np for noun phrase  cp for  clause headed by a complementizer  qp for quainter phrase.  continuously  dis.   or if it is not translated at all   un. .  there are some phrases  such as noun phrases   nps   prepositional phrases  pps  that we usually expect to have a continuous translation. with  respect to several such types of phrases  table    shows how these phrases are translated. from  the table  we see that translations of rnnsearch  system do not respect source syntax very well.  for example  in rnnsearch translations    .       .    and  .   of pps are translated continuously  discontinuously  and untranslated  respectively. fortunately  our mixed rnn system is  able to have more continuous translation for those  phrases. table   also suggests that there is still  much room for nmt to show more respect to syntax.   .     analysis on over translation    to estimate the over translation generated by  nmt  we propose ratio of over translation  rot    p     .     dis.    .     .     .     .     .     .     .     .     .     .     rot      wi    t wi       w            where  w  is the number of words in consideration  t wi   is the times of over translation for  word wi . given a word w and its translation  e   e  e  . . . en   we have   t w     e     uniq e             where  e  is the number of words in w s translation e  while  uniq e   is the number of unique  words in e. for example  if a source word       system    rnnsearch    mixed rnn    pos  nr  cd  dt  nn  all  nr  cd  dt  nn  all    rot        .    .    .    .    .     .    .    .    .    .     system    rnnsearch    mixed rnn    pos  nn  nr  vv  cd  all  nn  nr  vv  cd  all    non unk    .     .     .     .     .     .     .     .    .     .     unk    .     .     .     .     .     .     .     .     .     .     un.    .     .     .     .     .     .     .     .     .     .     table    ratio of over translation  rot  on test  sets. here nr is for proper noun  cd for cardinal number  dt for determiner  and nn for nouns  except proper nouns and temporal nouns.    table    percentages     of rare words in our test  sets being translated into a non unk word  nonunk   unk  unk   or if it is not translated at all   un. .      xiangkang is translated as hong kong hong  kong  we say it being over translated   times.  table   presents rot grouped by some typical  pos tags. it is not surprising that rnnsearch system has high rot with respect to pos tags of nr   proper noun  and cd  cardinal number   this is  due to the fact that the two pos tags include high  percentage of unknown words which tend to be  translated multiple times in translation. words of  dt  determiner  are another source of over translation since they are usually translated to multiple  the in english. it also shows that by introducing  source syntax  mixed rnn system alleviates the  over translation issue by      rot drops from   .   to  .  .    word approach  sennrich et al.        as an example  for a word on the source side which is divided  into several subword units  we can synthesize subpos nodes that cover these units. for example  if  misunderstand vb is divided into units of mis and  understand  we construct substructure  vb  vb f  mis   vb i understand  .     .     analysis on rare word translation    we analyze the translation of source side rare  words that are mapped to a special token unk.  given a rare word w  we examine if it is translated  into a non unk word  non unk   unk  unk    or if it is not translated at all  un. .  table   shows how source side rare words are  translated. the four pos tags listed in the table  account for about     of all rare words in the test  sets. it shows that in mixed rnn system is more  likely to translate source side rare words into unk  on the target side. this is reasonable since the  source side rare words tends to be translated into  rare words in the target side. moreover  it is hard  to obtain its correct non unk translation when a  source side rare word is replaced as unk.  note that our approach is compatible with with  approaches of open vocabulary. taking the sub          related work    while there has been substantial work on linguistically motivated smt  approaches that leverage syntax for nmt start to shed light very recently. generally speaking  nmt can provide a  flexible mechanism for adding linguistic knowledge  thanks to its strong capability of automatically learning feature representations.  eriguchi et al.        propose a tree tosequence model that learns annotation vectors not  only for terminal words  but also for non terminal  nodes. they also allow the attention model to  align target words to non terminal nodes. our approach is similar to theirs by using source side  phrase parse tree. however  our mixed rnn system  for example  incorporates syntax information by learning annotation vectors of syntactic labels and words stitchingly  but is still a sequenceto sequence model  with no extra parameters and  with less increased training time.  sennrich and haddow        define a few linguistically motivated features that are attached to  each individual words. their features include lemmas  subword tags  pos tags  dependency labels   etc. they concatenate feature embeddings with  word embeddings and feed the concatenated em      beddings into the nmt encoder. on the contrast   we do not specify any feature  but let the model  implicitly learn useful information from the structural label sequence.  shi et al.        design a few experiments to investigate if the nmt system without external linguistic input is capable of learning syntactic information on the source side as a by product of training. however  their work is not focusing on improving nmt with linguistic input. moreover  we  analyze what syntax is disrespected in translation  from several new perspectives.  garc  a mart  nez et al.        generalize nmt  outputs as lemmas and morphological factors in  order to alleviate the issues of large vocabulary  and out of vocabulary word translation. the lemmas and corresponding factors are then used to  generate final words in target language. though  they use linguistic input on the target side  they are  limited to the word level features. phrase level  or  even sentence level linguistic features are harder  to obtain for a generation task such as machine  translation  since this would require incremental  parsing of the hypotheses at test time.         conclusion    in this paper  we have investigated whether and  how source syntax can explicitly help nmt to improve its translation accuracy.  to obtain syntactic knowledge  we linearize a  parse tree into a structural label sequence and  let the model automatically learn useful information through it. specifically  we have described three different models to capture the syntax knowledge  i.e.  parallel rnn  hierarchical rnn  and mixed rnn. experimentation on  chinese to english translation shows that all proposed models yield improvements over a state ofthe art baseline nmt system. it is also interesting  to note that the simplest model  i.e.  mixed rnn   achieves the best performance  resulting in obtaining significant improvements of  .  bleu points  on nist mt    to   .  in this paper  we have also analyzed the translation behavior of our improved system against the  state of the art nmt baseline system from several  perspectives. our analysis shows that there is still  much room for nmt translation to be consistent  with source syntax. in our future work  we expect  several developments that will shed more light on  utilizing source syntax  e.g.  designing novel syn     tactic features  e.g.  features showing the syntactic  role that a word is playing  for nmt  and employing the source syntax to constrain and guild the  attention models.    acknowledgments  the authors would like to thank three anonymous reviewers for providing helpful comments   and also acknowledge xing wang  xiangyu duan   zhengxian gong for useful discussions. this work  was supported by national natural science foundation of china  grant no.                               .    