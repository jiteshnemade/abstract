introduction    recommender systems  rs  have long been developed to predict  user s favorites  evolving from traditional collaborative filtering  permission to make digital or hard copies of part or all of this work for personal or  classroom use is granted without fee provided that copies are not made or distributed  for profit or commercial advantage and that copies bear this notice and the full citation  on the first page. copyrights for third party components of this work must be honored.  for all other uses  contact the owner author s .  conference     july       washington  dc  usa         copyright held by the owner author s .  acm isbn                      .  https   doi.org   .              cf  methods                  to recently grown popular deep learning approaches           . as online services like e commerce  amazon   social media  facebook   movie  youtube  and music  spotify   grow in an increasing speed of rate  how to improve recommendation quality from such expanding and wide ranging items is  prominent for both user experience and business profit.     .     personalized recommender systems    ever more importantly  personalized recommendation is one of the  most challenging issues in rs. user s intent is more than difficult  to predict  which can be influenced by many factors  both internal or external  from past or current. much research efforts have  focused on context aware rs      exploring contextual data like  temporal information  spatial location           user profiles          or even inter domain features         . among them  time aware  rs     is particularly studied in depth  since temporal information  is easy to obtain and indicative of user s information need. typically  temporal dynamics was added in cf methods to discover  temporal evolving features      and many other sophisticated nn  models were proposed  like time gates  point process  multi task               etc. what s more  the ordering of interactions is another new dimension of information that is valuable to be further  explored. recent works              show that rnn based recommender system can outperform other popular alternatives in certain  session based recommendation tasks.  sequential interactions between users and items are crucial data  sources for recommender systems. however  literatures above fail  to quantify the effectiveness of using such sequential data from the  past to present sessions. users  intents are constantly evolving and  same as the item popularities. as a result  they cannot be effectively  modeled based solely on short term or long term profiles.  example in an e commerce recommender system  a user alice  may come with a certain intent for some kitchen hand soap that  she wants to buy at present. then in this current session  alice is  more likely to click some similar items or accessories from the same  kitchenware category  like kitchen caddy  drying tower  trash bags   etc. this means such short term intra session data sequence should  mostly play a dominant role for the next basket recommendation.  at the same time  the dwell time she spent on each items could  indicate the probability of her interest of making a purchase. on  the other hand  her long term profile of tastes or preferences would  not change much over recent sessions  like her favorite brands  preferred color  fashion style  etc. moreover  the items she viewed or  bought in the past sessions may still give hints for the next session  recommendation. for instance  fast moving consumer goods  makeups and napkins have periodic purchase needs. so maintaining both     figure    an empirical data analysis      short term predominates  the mean percentage of user interactions hanging in the  top    categories items during the same session      long term counts  the mean percentage value of user clicking the repeated  categories and items that he had clicked before in the previous    sessions      dwell time helps  the normalized histogram  of click gap follows gamma distribution.  user s short term intra session context profile and long term intersession preference profile can lead to significant recommendation  performance increase.  the goal of this work is to make effective use of both intra session  and inter session profiles and to construct a better personalized  session aware recommender system. this raises several challenging  issues. first  traditional rnn cannot train with too long sequence  length  which will result in extreme training latency and large  memory cost. second  the interaction data is very noisy  some clicks  are meaningful  while some may even be clicked by mistake. last  but not least  data from the past sessions should play as different  roles as present session  but there is no specific rule for integrating  the session based short term profiles and session aware long term  profiles. therefore  a more sensitive approach to distinguish and  integrate data from different time scales with different significance  is required.     .     motivated by empirical data analysis    the motivation of our model design is inspired by real data observations and analysis  which comes from online tianchi       e commerce navigation log data having around    m interactions    m users and  m items from   k categories. we come up with the  following three observations       short term profile predominates  jannach et al.      have  shown that the short term intentions should be predominant  in the selection of the recommendations. we can see in the first  figure of figure.   the blue bar represents the mean percentage  of user interactions hanging in the top    categories during  the same session  and the orange one represents for that of  items. this indicates that within one session  nearly half of  interactions are in the main shopping target category  and     percent of clicks are for the target item.  overall both of them are subject to exponential decrease  which  proves that user s short term shopping goal plays a predominant role for the intra session interaction choices. notice that  hidasi et al.      propose gru rec  one early work on sessionaware rnn based recommender system  which avoids the cold  start problem and significantly outperforms conventional baseline methods. from this point of view  we view rnn as one of  the most advanced methods for short term recommendation.  and choose it as the basis of our model design.        long term profile counts  longer term behavioral patterns  and user preferences can also be important. in the middle figure  of figure.   we plot the mean percentage value of a user clicking  some repeated categories and items that he she had clicked  before in the previous    sessions. we can see that it tends to  grow logarithmically and almost    percent of categories and     percent of items are repeated clicked inter previous   sessions.  from this point of view  inter session information contributes  to    to    percent of information for next basket category  prediction and   to    percent of knowledge about repeated  items.  several existing works have tried to use a simple static weighting strategy or hierarchical rnn                          to combine the short term and long term models  but how to combine  them in a seamless way still remains an open research problem.  in order to enable long term profiling  we propose an intersession temporal dynamic model with anonymous session rnn  model.      time duration feature helps  one more common but not  fully exploited feature is the click gap time  which is also the  view dwell time of an item. this perfectly bridges the gap of  discrete interaction sequence data with potential weights. generally speaking  the longer time a user spends on the item  the  more interest he has in it. according to the normalized histogram showed in figure.   the click gap of user interactions  follows gamma distribution. most users spend around    seconds between each click and normally the time duration is not  longer than   minutes for each item.  such click gap time or item view duration helps us in connecting short term and long term profiles along time axis. so we  design a novel triangle parallel attention network to incorporate  temporal context in the rnn and perform efficient combination  for short term session sequence information.  motivated by the above observations  in this paper  we want  to quantify  exploit and integrate the effectiveness of user s intrasession and inter session profiles with temporal dynamics. first  of all  since short term profile plays a predominant role in user intent estimation  the very last actions in the present session should  represent an important piece of context information to be taken  into account when we make the recommendation. hidasi et al.       propose gru rec  one early work on session aware rnn based  recommender system  which takes these very last actions in users      session duration         session  gap time    action  gap time    action  time               t     t    mini batches    t    time    session    ...    user      i           i     i     i        ...    session         i     i     ...    i        session         i     i     ...             i     i  i  i              session   i     i  i  i          i        input    i        session                session   i     i  i  i  i     ...    user    .  .  .       i     i     i           i     i     ......    session    ......    i        i        ...            session   i     i           i     i  i     output    session      ...    ......                   i     i  i  i  i  i            i     i  i  i     input layer    figure    data flow of user and item interactions over time.    intra session sequence data. gru rec avoids the cold start problem and significantly outperforms conventional baseline methods.  from this point of view  we view rnn as one of the most advanced  methods for session based short term recommendation. in order  to maintain its short term privilege  we choose session based rnn  recommender system as the basis of our model design. however   as our exploratory data analysis shows above  long term profiles  are important for recommender system  while current state of art  session based approaches fail to model them effectively. several  existing works have tried to use a simple static weighting strategy  or hierarchical rnn              to combine the short term and  long term models  but how to combine them in a seamless way still  remains an open research problem. in order to enable long term  profiling  we propose an inter session temporal dynamic model  with anonymous session rnn model. we choose to use an efficient  embedding layer to automatically train and activate short and long  term profiles from user embedding  short term interest  user taste  evolution and user survival time. for personalized recommendation   we add local negative sampling method  selecting negative samples  in proportion to the item popularity within mini batch sequences  and ruling out the items appeared in his her history. finally  we  design a novel triangle parallel attention network to incorporate  temporal context in the rnn and perform efficient combination  for short term session sequence information. scott time binning  method and extendable attention layer fully exert the role of temporal information. in this way  user s item selection behavior can be  predicted by mixed decision of short term and long term efficiently.     .                       i     i  i  i  i  i  i               i     i  i  i  i           session   i     i  i     contributions    the contributions of our asars framework can be described as  follows       integrate long term by inter session temporal dynamics  model  we include long term user profiles for personalized  session based rs to learn the inter session pattern by temporal dynamics model in a seamless way. we integrate the time  changes in session rnn and add user embedding  short term interest  user taste evolution  user survival time and local negative  sampling.      exploit short time by triangle parallel attention network   we offer an novel attention model to exploit more intra session  information so as to enhance session based rs in time dimension. we design a triangle parallel attention method for single  sequence predicting and add a lower trigonometric transformation process to modulate the hidden states with multiplication  efficiently.    figure    session based rnn parallel mini batch creation.      extensive empirical results  we compete with four strong  baseline models including bpr mf  cf   youtube  dnn   wavenet   cnn  and gru rec  rnn  models and also compare five variants of our asars model. we conduct extensive experiments  on four real datasets from different domains and demonstrate  the effectiveness of asars for personalized recommendation.         asars framework    in this section  we describe our proposed personalized attention  session aware recommender system  asars  framework. first   we introduce the session based rnn framework in subsection  . .  next  we explain how asars model combine short time and longterm by inter session temporal dynamics model in subsection  . .  then  asars model enhances the short term profiles by using a a  triangle parallel attention network layer  .  to sustain and exploit  the inter session patterns. the overall structure of asars is shown  as figure.      .     session based rnn framework    to show the concepts and notations clearly  we present an simplified  data flow example in figure. . we define a user  session  as a set  of continuous navigation activities without interruption in the log  sequence. in our settings  we separate each session by at least onehour inactivity  which is commonly used in previous works     . we  denote a sequence of m activity sessions as s    s j  j      . . .   m    where each session s j represents a user interaction event sequence  n  s j    i  j   i  j   . . .   i j j  . given a sequence of activity sessions  our goal  is to predict what is the next item that the user mostly likely to  interact with. we formulate this as a ranking problem and solve  it by first finding a scoring function f     that outputs the score  of each item in the given the item list i   and then returning topk  ranked items based on their scores.  r k   f  i n  i      ... k       k   i .           first of all  in order to maintain the short term predominant effect  our model is built on the session based rnn model introduced  in     . session based rnn model is based on lstm gru layers  and the hidden gates model the interaction order and relationship  of user activities within a session.  when processing a sequence  session based rnn first input the  sequence into the input layer  as shown at the left part of figure. .  to deal with the various session length problem  it uses a sessionparallel mini batch approach to capture how a session evolves over  the order of interacting activities     . if any of the sessions end  the  next session is put behind of that sequence. note that this operation     q    bu    concat    f  in    i  ... n   u  t     x  x  ...    a     a     a     ...    graph legands    x    fully  connected  layer    an    x  x    trigonometric  transformation    p    x                     x    x         softmax    x    x    ...    hd     hd     ...  ...    hd     hdn    hq     embedding layer and dropout layer    i     i     hq     hqm    ...    t     t     time sequence  lstm hidden unit    hqm    lstm hidden unit    bu    user embedding    q    matching vector    f    functional component    tn    t     item sequence    hdk    ak    scott binning and scaled embedding    in    i     hq     in  tn    attention vector  fully connected layer  sigmoid unit    figure    overall structure of asars model.  assumes all sessions are independent to each other. formally  we  denote the ne mini batched output sequences as  n    e    e j      e j    . . .   e j j    j      . . .   ne             where e j n is the one hot representation vector of the item. next   the one hot mini batch vector is fed into a gru layer  and the  hidden states are reset when switching sessions. after that  the  output of rnn can be treated as session representations   hsession   gru  e j   hsession      j      . . .   ne .           finally  the last output of rnn gives the next step in this session   and the likelihood of being this item is calculated through a nonlinear activation layer.  r k     ek   hk    k   i .           normally  we use softmax  tanh or relu for the loss functions. there  are some typical loss functions for recommender systems  like crossentropy  bpr  and top  loss proposed by the gru rec model.  overall  session based rnn is one of the state of art dynamic  recommendation models which effectively exploits intra session  sequence order information.  however  sessions are not absolutely independent to each other   especially for task of personalized recommendation. in the next  subsection  we introduce how and why we design our model to  effectively exploit inter session patterns as well as temporal information.     .     inter session temporal dynamics model    in traditional matrix factorization  mf  based approaches  the temporal dynamics model like      is commonly used for modeling  time changes in data mining. hereby  we model and learn the time  changes by session rnn and user preferences as model drifting.  starting from the anatomy of a factor model with time changing  factor   rui   eit   eu   bu i         where ei and eu are the one hot representation vector of the items  and user profiles  and bu i represents the baseline predictor   bu i       bu   bi .           here bu   bi are the corresponding observed bias  and the overall  average is denoted by   .  time changing. an illustrative data flow example is shown in  figure. . we can see that there are four kinds of time information   action timestamp t  action gap time    t   session duration time   and  session gap time       . action timestamp can be used for periodical  purchasing feature training directly as contextual information  and  session gap time is helpful for survival analysis to predict user  return time     . among all these temporal features  action gap  time    t   also representing item dwell time  is the most valuable  feature that haven t been fully exploited in previous session based  models. therefore  adding time changing factor to the equation      and then it becomes   bui  t        bu  t    bi  t .       then  we want to improve the session aware recommender system by exploiting such item dwell time information. formally  for  each session j  we create a dwell time sequence with the same din  mension of item sequence as t j    t j    t j    . . .   t j j  . the item dwell  time follows gamma distribution as shown in figure. . we can take  bins of such time to reduce the dimensionality and then accelerate  the training process. we use scott binning method      for time  feature such that the bin width is proportional to the standard deviation of the data and inversely proportional to cube root of original  data size.  s               .       tbin      n  so the time model becomes  bi  t    bi   bi tbin .           next  we use an embedding method to represent dwell time  importance within sessions.  n    t  j     e t     et  j      et   j   . . .   e t  j               where t       . . .   n j    j       . . .   n j    n j is the number of users  grouped by mini batch size  and eu  j is the embedding vector of  time. after training with a lstm layer  instead of concatenating  the hidden outputs directly  we explore to use attention scheme  to integrate the timing effect to item sequence. intuitively  such      .     figure    triangle parallel attention net with user profile.    attention vectors are perfectly used to modulate the outputs of  hidden states representing session orders  and it s reported as a  very useful tool to extract the importance of sequence vector. the  triangle parallel attention network will be explained in section  . .  model evolving. in addition  asars model also uses the longterm profiles by adding user embedding to learn the cross session  pattern and user favorite evolution as  eu  t    eu    u   hu  t    bu  t    bu    u   devu  t                    in which eu is user embedding  bu is the user bias  hu  t  shows  the user short term interest   u learns the user taste evolution and  devu  t  gives us the user survival time.  formally in asars  for user u  we denote the sessions grouped  by users as s u     s j  j      . . .   mu    where s j is the number jth  session of user s total mu sessions. next  we use an embedding  method to represent user s all behavioral patterns across sessions.  now we come to the sequence data preprocessing stage. we take  the similar idea from parallel mini batch method and change it to  user parallel mini batch mechanism. instead of complementing the  dead end session row by the next session from all session lists  all  mini batches are selected and complemented within user s session  groups. so now we get the user parallel mini batched embedding  sequence in input layer   n    u  j     e u     eu  j      eu   j   . . .   eu  j               where u       . . .   mu    j       . . .   nbu    nbu is the number of  user grouped mini batch size  and eu  j is the embedding vector of  item. finally  we code the predictor as following      t    eit    eu    u   gru  hsessioni      devi  t     rui        bu    u   devu  t    bi   bi tbin .            such user representation aims to track user behavior patterns  across sessions. there are many ways to combine new feature  embeddings in neural network  such as concatenating features in  embedding input layer directly  stacking two rnn layers for each  feature respectively  co training hierarchical rnn layers      and  some more complex model structures like attention models        cross layers      memory networks      and meta learning        etc. among them  we first tried to implement a simple model like  concatenating or hierarchical rnns. although the user embeddings  in such simple model may not fully represent user behavior patterns   these methods do make some improvements since more information  have been included in training network and it can be trained faster.    triangle parallel attention network    notice that it s not straightforward to adapt sequence attention  network directly to session based rnn model. on one hand  traditional attention layer usually works with sequence in sequence out  rnns in nlp tasks  but here we only predict one output sample in  our recommender setting. on the other hand  in order to enable  data augmentation      to get more training samples  all subsequences need to be forward to the attention network  such that  the training time for forwarding process in attention network will  be exponentially increased and make the model more difficult to  train. therefore  we design triangle parallel attention method for  single sequence predicting and add a lower trigonometric transformation process to modulate the hidden states with multiplication  efficiently.  as shown in the top right of figure    we introduce the time  embedding t j in an attention network to reward items that play the  most important role within session. the global attention mechanism  yields the following formulas   pi   tanh ws ht ime   bs     pit h s e s s ion    e   i     t     p h s e s s ion  ie i     qt      i hsession                          i    where ws and bs are parameters for training  and hsession is the  hidden output of item lstm.  as mentioned above  the sequence weighting softmax and summation calculation cannot be adapted to the data augmentation  training and will cause exponential training time cost. to accelerate  this training process  we take lower trigonometric transformation  to the vector pti bu and forward it through the softmax function  as a whole. in such a way  the training process can be hundreds  of times faster. formally  for each hidden output pi   i       . . .   n    we create an n   n lower trigonometric matrix p with the sequence  row pi as   pi    pt    bu   . . .   pti   bu      . . .     .        after propagating such matrix p through functions      and        we get the self attention vector   and representation qt .  secondly  we also tried to use the attention network to combine  the user embeddings with rnn outputs  as shown in figure   .  similar to time attention scheme  we introduce user embedding eu  in an attention network to reward session representations that are  most favorite for the user. the self attention mechanism yields the  following formulas   t    e p i eu   i     t .        p eu  ie i  after propagating through the attention layer or just embedding  layer  we get the self attention vector   and representation bu .  finally  the user attention vector weighted session representation  q concatenate with user representation u and then goes to the  following fully connected layer.  r j k     qt   ek   b j   bk               where   is a non linear function for normalization  like softmax   tanh  relu  and etc.     table    dataset details.  dataset    movielens    recsys      tianchi    ours    events  users  items  sessions                                                                                                                                           session support  item support  user support                                                        .     improving extensions    loss functions  we tried several common used loss functions in  recommender systems  bpr       top      and hinge losses.  ns       log      r j   r k      ns j              ns           r j   r k         r j      ns j              bpr loss  l        top  loss  l      hinge loss  l   max    r j   r k          .            local negative sampling  previous study has shown that negative sampling plays an important role in performance     . instead  of random negative sampling  we also need to consider item popularity and user history issues. specifically  we select negative samples  in proportion to the item popularity within mini batch sequences.  furthermore  for each user  we need to rule out the items appeared  in his her history. this way  the local negative sampling method  not only improves performance but also reduces the computational  time as well.  data augmentation  note that some users only have a few  session histories  which may be insufficient for training the model  with long term user profiles. so we need to make full use of all  sequence samples and also their subsequences. first  we train each  sequence with all hidden outputs and make the predictions  which  fully explores the subsequences information. second  we leverage  the dropout layer for the sequences such that it makes regularization  as well as diversifies the input sequence data.         evaluation    in this section  we will demonstrate the effectiveness and efficiency  of our model for session aware recommendation. first  we test and  compare our model on multiple real world datasets  from open  source datasets to our own parsed real world e commerce dataset   covering both video and e commerce domain. second  we choose  very strong baseline models to compete with  including traditional  mf method and dnn  cnn  rnn based approaches. we describe  our setup details and show the benchmarks and overall evaluation  results as following.     .     datasets    totally we use four datasets in our experiments. the first is movielens       which is commonly used in recommender related works.  the movielens  m dataset contains the ratings of       movies    from       users from      to     . all users selected had rated  at least    movies. this movie rating data characterizes the user  profiles in a extreme long term for   years. so we use this datasets  mainly for concept proving of user long term effects. the second is  recsys challenge      dataset     that consists of            interaction events with        items in           sessions for   months.  this dataset only has the session info without user identities which  session based approaches commonly used  so we mainly test our  time short term effects on this dataset. in order to test our model for  both long and short term profiles  we choose tianchi dataset       containing    m interaction events of         users with            items from       categories for   days. however  the duration is  only   days which is not that long for user profiling. finally  we  also test on our newly clawed dataset from real world e commerce  website containing           events for         users interacting  with         items in         for two months.  most of the datasets have no session id info  we manually split  the raw data into sessions based on   hour inactivity. we add action  gap time between each interaction timestamp within the same  session and delete the last term. the most important preprocessing  step is filtering the attributes with different support number. since  we add user long term and time short term to session based model   we need to guarantee the user and item attributes have enough  support training samples. from our settings  we filter the items with  at least    events  filter the sessions with length longer or equals  to    and filter the users having    more sessions. to explore this  supporting number influence  each dataset is split into sparse and  dense two kinds of subsets for testing. all the dataset are partitioned  to training and testing parts by cross validation based on both time  and user. the test dataset contains sessions whose last timestamp  is larger than a time boundary. we filter out the items and users  that in the testing data but not in the training data. the details of  datasets are shown in table. .     .     comparing baselines and asars versions    we compare asars with several strong baseline models. all of  them are implicit ranking models. first  we choose bpr mf model       representing cf based approaches. second  gru rec      is  the common baseline model for session based rnn recommenders.  what s more  since deep neural network is very popular and powerful  we also compare with the youtube recommender model      representing dnn approaches and wavenet pixelrnn model       representing recurrent cnn models. notably that most related  session based works didn t compare with dnn and cnn models  previously and they only choose more cf based methods as baselines. especially for cnn models  from our knowledge there are  seldom papers using recurrent cnn model for sequential recommendation task. in some experimental settings  these methods are  really competitive and show their advantages. we will briefly introduce each baseline model and show the comparing results in the  following sections.    bpr mf model       matrix factorization techniques apply svd factoring the user item rating matrix  which are  dominated in collaborative filtering recommenders.    youtube dnn model      youtube model includes two  stages  candidate generation and ranking.     table    experimental comparison results   shown are the mrr top    and recall top    scores of four baseline models and  five asars variants on four datasets. we highlight some focal improvements in bold and underline the best results.  movielens  mrr    recall       recsys    mrr    recall       tianchi  mrr    recall       mrr       bpr mf cf  youtube dnn  wavenet cnn  gru rec rnn     .         .         .         .           .         .         .         .              .         .         .              .         .        .           .         .         .         .           .         .         .         .           .         .         .         .           .         .         .         .          asars user att  asars user cat  asars time att  asars time cat  asars time user     .         .         .         .         .           .         .         .         .         .                 .         .                    .         .              .         .         .         .         .           .         .         .         .         .           .         .         .         .         .           .         .         .         .         .          models    ours  recall       figure    results of mrr     mrr     mrr    mrr   and mrr all for movielens  recsys    tianchi and our datasets.    wavenet cnn model       pixelrnn aims to generate raw  audio waveforms or phoneme recognition at first. inner multiplicative relationships can be better exploited by its stacked  causal atrous convolutions.    gru rec rnn model       we adapt gru rec model  introduced in section. . .  as for asars  we adapt user profile and dwell time in five different ways  two for user profile test  two for time feature test  and one  for integrated version. the specifics of each model are as follows     asars user att model  adding user profile embedding  by self attention network. based on session rnn  we add  attention layer as equation      and propagate the mutual  score by lower trigonometric transformation as equation      .    asars user cat model  adding the user profile by directly  concatenating the hidden outputs and the user embeddings   followed by a fully connected layer.    asars time att model  adding time profile embedding  by global attention network as described in section.  .    asars time cat model  adding the user profile by directly  concatenating the time gap embeddings and the item embeddings  and feeding into rnn layer as input sequences.    asars time user model  integrating both time and user  profiles as final asars model as figure. . comparing the  design versions above  we choose to use attention net for  dwelling time and concatenate user profiles.     .     implementation and parameter tuning    we implement our model based on spotlight       an open pytorch  recommender framework. in this spotlight model zoo  all ids need  to be regenerated mapping to continuous numerical ids. the model    is trained end to end by back propagation. in our model  we use  single layer lstm for item and time training. during the training process  we first grid search all the possible hyper parameters  optimized by adagrad      or adam     . we also add early stop  scheme when the evaluation loss does not decrease in the following     epochs. we evaluate the top k ranking results using mrr k   mean reciprocal rank  and recall k metrics. all metrics are the  average of all item lists in testing dataset. the reciprocal rank is set  to   if the rank is above k.  in our settings  all comparing baseline models and our model  variants are trained by grid search and select the best result. the best  hyper parameter set for asars time user model for movielens  dataset is optimizing the hinge loss using adagrad. the mini batch  size is   . in the session information embedding  the maximum  session sequence length is      the embedding size of item is      embedding size of time gap is    and embedding size of item is   .  the hidden dimension of lstm layers are    . the learning rate  is set to  . . we used dropout regularization      before the rnn  layers with  . . for evaluation  we mainly focus on top    ranking  results.     .     comparing results    all evaluation results are reported in table. . we mainly list the  mrr top    and recall top    scores of the four baseline models  and five asars variants on the four datasets. we highlight some  focal improvements in bold and underline the best results over  all models. the detailed mrr     mrr     mrr    mrr   and  mrr all results for each datasets are shown in figure.  we finer  analyze the comparing models by illustrating the user and time  effects separately first  and then compare the overall performance.     performance with user long term profile  let s first study  the long term effects of user models  i.e. asars user att model  and asars user cat model. compared to the major baseline model  gru rec  we can see that the concatenating method always outperforms the baseline for around    improvement on movielens      on tianchi  and even     improvement on our parsed dataset.  this simple user model can exploit the long term profile efficiently.  however  our carefully designed user attention model does not  perform well and some results even got worse to baseline model.  our motivation of designing such attention network for user profile  is to learn the importance from the session sequence so that it can  select the most influential items from previous item sequences for  predicting. however  this scheme still cannot give better results  after trying all kinds of model and hyperparameter tuning. this  may because users  favorites and behavior patterns vary a lot and  hard to learn. what s more  although we guaranteed that all users  have at least    session in training data  it still far from enough to  train the attention network to work well. so user long term profile  is better to be used simply by concatenation model.  performance with time short term profile  we further investigate how the time short term profile can be better exploited.  comparing the results of asars time att model and asars time cat  model  we can see that asars time att works the best  which  gives around     improvement on recsys    tianchi and our own  datasets  except movielens data. this is expectable since movielens   m data totally last for   years and the rating gap time cannot represent the info of the dwell time in e commerce navigation sessions.  we can see that with such useful addition info  asars time cat  model can also beat the baseline model for about     improvement.  obviously  the global attention model for dwelling time helps more  in session based rnn model.  overall performance with both user and time  the last  asars time user model integrates the user concatenation model  and time attention model  and it shows that with both long and  short term info  our asars model can improve mrr    value  about     for tianchi and      for our dataset. notably that dnn  and cnn models performs better than our major baseline rnn  model on tianchi and our dataset. this shows that dnn and cnn  models are more robust and general than rnn model for different  recommender system settings. with the improving from our model  design  asars can give the best performance and beat all other  models.  memory and time cost  except for effectiveness  we also need  to compare the memory and training time cost. we did experiments  on nvidia tesla p   gpus  and the training speed and memory  cost are shown in figure.  and figure. . as expected  mf method is  fastest and cnn method takes the most memory. our model is half  slower than baseline rnn model and takes similar memory cost   which is acceptable for training process.         related work    much research efforts have been done to improve recommendation performance  like developing context aware recommendations      time aware recommenders     and sequence aware  recommender systems       exploiting contextual information   time dimension features and sequential order of the events. at the    figure    train speed  iter s .    figure    memory cost  mib .    beginning  we list and compare some related research in different methodology categories exploiting various domain features in  table. .  cf based rs. raised by the netflix prize      factorization based  methods have been popularized and they framed the item to item  recommender system  so called collaborative filtering method.  nowadays  knn  svd   and bpr mf              are still popular  baseline methods for today s recommender research. with the motivation to profile temporal evolution of user and item favorites   timesvd        is one of the major works to add temporal dynamics to cf rs  modeling the factor model with time changing  feature t. in addition to time  sequence prediction approach is further explored as well. fpmc      is proposed to combine user item  matrix with markov chains and it is still considered as one of the  state of art sequential cf based recommendation. although cfbased methods have been theoretically well developed and are less  expensive for computational cost  their practicalness and scalability  yield to nn based approaches.  nn based rs. as deep learning has been becoming in prominence in this decade  so as recommender system researchers began  to apply deep neural network on recommendation. one famous  model is the youtube dnn recommender    . it splits the recommendation task into two stages  a deep candidate generation model  and a separate deep ranking model  and gives dramatic performance improvements. speaking of time or sequence modeling in  nn  everyone would come up with recurrent neural network  typically lstms and grus        . several recent works have been  proposed to add temporal historical features for rnn recommender  systems. tims lstm      equips lstm with time gates to model  time intervals. rrn      endows both users and items with a lstm     table    related works compared by different methodology categories exploiting various domain features.  general  item impression  qi    multiplicative  interaction  bui    evolution  user favorite item trend  bu  t   qi  t     methods    approaches    user taste  pu    cf based    bpr mf  timesvd    fpmc                                     x     x    nn based    dnn  gru rec  asars       x                             x  x       autoregressive model that captures dynamics with a low rank factorization. nsr      uses survival analysis for return time prediction and exponential families for future activity analysis so as  to solve the problem of just in time recommendation. original  and detailed survival analysis comes from temporal point process                which can recover both meaningful clusters and temporal dynamics. except for modifications based on lstm  cross layer  scheme is another new proposed way to discover contextual features more expressively        . while these approaches did not  adapt to session based scheme  which could play a predominant  actor for recommendation as shown in section. . .  session based rs. session based rnn recommender is first proposed by hidasi et. al named gru rec     . at first they mainly  focus on anonymous cases and cold start problem in e commerce  recommender system  and they introduced session parallel minibatches rrn approach to fasten the training process. there are  many follow up papers based on that work  improving by data augmentation via sequence preprocessing       exploiting dwell time by  concatenating an additional dwell time rnn layer before item rnn        adding different types of interactions and list wise ranking  framework       and personalizing it with hierarchical rnn        etc. these works made incremental improvements for gru rec   but they do not make significant modification and haven t consider  long term intra session info and user action gap time feature  which  can make great gain according to section. .  most recently  the  most related work is stamp       but it has no use of dwelling  time  no rnn  different attention scheme with not very impressing  improvements.  long and short term based. there have been plenty of works  focusing on leveraging short term features or long term profiles  in the past. generally speaking  conventional matrix factorization  based methods are more able to capture users  long term general  tastes     . and it can be extended to detect their evolution trend  with temporal dynamics     . star model      learned the longterm profile by monte carlo markov chain and used latent dirichlet allocation for short term profiling. coupled tensor factorization  proposed by      shows the repeat pattern in previous purchasing   but rnns show their privilege in short term sequential pattern  mining than other item based or markov chain based approaches.  most recent rs for long and short term sequential recommendation like          also use rnns  but they neglected the temporal  info or not based on the session. it s impressing that google just    time  drift  t    sequence  info  seq    x     x    x     x    x  x       x  x       x  x       x          proposed a mixed model      almost integrate all my model variants  but my model is much lighter than that. to facilitate rnn  with long term profiling  the goal of this paper is to make effective  use of both long term and short profiles and construct a better  personalized session aware rnn recommender system.         conclusion    in this paper  we quantify  qualify and exploit the long term user  profile and short term temporal dynamics for session based rnn  recommender systems. in particular  we propose an attentional  session aware recommender system framework  called  asars    to integrate intra session and inter session profiles for both users  and items with two novel models. we introduce inter session temporal dynamic model to capture long term user profiles for sessionbased rs to learn the inter session pattern and user favorite evolution in a seamless way. we design a triangle parallel attention  network to leverage temporal dynamics scheme exploiting more  intra session time information so as to enhance session based rs in  time dimension.such triangle parallel attention network is newly  designed for sequence in single out rnn structure and data augmentation needs  and also accelerate the training speed as well.  we demonstrate the improvement by our model design on four  real world datasets and beat comparable baseline models.    