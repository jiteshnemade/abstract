introduction  modeling temporal and sequential data  which is crucial  in machine learning  can be applied in many areas  such as  speech and natural language processing. deep neural networks   dnns  have garnered interest from many researchers after being successfully applied in image classification     and speech  recognition    . another type of neural network  called a  recurrent neural network  rnn  is also widely used for speech  recognition      machine translation          and language  modeling         . rnns have achieved many state of the art  results. compared to dnns  they have extra parameters for  modeling the relationships of previous or future hidden states  with current input where the rnn parameters are shared for  each input time step.  generally  rnns can be separated by a simple rnn without  gating units  such as the elman rnn      the jordan rnn       and such advanced rnns with gating units as the long short  term memory  lstm  rnn      and the gated recurrent  unit  gru  rnn    . a simple rnn usually adequate to    model some dataset and a task with short term dependencies like slot filling for spoken language understanding     .  however  for more difficult tasks like language modeling  and machine translation where most predictions need longer  information and a historical context from each sentence  gating  units are needed to achieve good performance. with gating  units for blocking and passing information from previous or  future hidden layer  we can learn long term information and recursively backpropagate the error from our prediction without  suffering from vanishing or exploding gradient problems     .  in spite of this situation  the concept of gating mechanism does  not provide an rnn with a more powerful way to model the  relation between the current input and previous hidden layer  representations.  most interactions inside rnns between current input and  previous  or future  hidden states are represented using linear  projection and addition and are transformed by the nonlinear  activation function. the transition is shallow because no intermediate hidden layers exist for projecting the hidden states      . to get a more powerful representation on the hidden  layer  pascanu et al.      modified rnns with an additional  nonlinear layer from input to the hidden layer transition   hidden to hidden layer transition and also hidden to output  layer transition. socher et al.            proposed another  approach using a tensor product for calculating output vectors  given two input vectors. they modified a recursive neural  network  recnn  to overcome those limitations using more  direct interaction between two input layers. this architecture  is called a recursive neural tensor network  recntn    which uses a tensor product between child input vectors to  represent the parent vector representation. by adding the tensor  product operation to calculate their parent vector  recntn  significantly improves the performance of sentiment analysis  and reasoning on entity relations tasks compared to standard  recnn architecture. however  those models struggle to learn  long term dependencies because the do not utilize the concept  of gating mechanism.  in this paper  we proposed a new rnn architecture that  combine the gating mechanism and tensor product concepts  to incorporate both advantages in a single architecture. using  the concept of such gating mechanisms as lstmrnn and  grurnn  our proposed architecture can learn temporal and  sequential data with longer dependencies between each input  time step than simple rnns without gating units and combine  the gating units with tensor products to represent the hidden     layer with more powerful operation and direct interaction.  hidden states are generated by the interaction between current  input and previous  or future  hidden states using a tensor  product and a non linear activation function allows more  expressive model representation. we describe two different  models based on lstmrnn and grurnn. lstmrntn is  our proposed model for the combination between a lstm unit  with a tensor product inside its cell equation and grurntn  is our name for a gru unit with a tensor product inside its  candidate hidden layer equation.  in section ii  we provide some background information  related to our research. in section iii  we describe our proposed rnn architecture in detail. we evaluate our proposed  rnn architecture on word level and character level language  modeling tasks and reported the result in section iv. we  present related works in section vi. section vii summarizes  our paper and provides some possible future improvements.  ii. background  a. recurrent neural network  a recurrent neural network  rnn  is one kind of neural network architecture for modeling sequential and temporal dependencies    . typically  we have input sequence  x    x    ...  xt   and calculate hidden vector sequence h     h    ...  ht   and output vector sequence y    y    ...  yt   with  rnns. a standard rnn at time t th is usually formulated as   ht         f  xt w xh   ht   whh   bh      yt      g ht why   by  .                where w xh represents the input layer to the hidden layer weight  matrix  whh represents hidden to hidden layer weight matrix   why represents the hidden to the output weight matrix  bh and  by represent bias vectors for the hidden and output layers. f      and g    are nonlinear activation functions such as sigmoid or  tanh.    using a second order hessian free optimization     . another  approach  which addressed the vanishing and exploding gradient problem  modified the rnn architecture with additional  parameters to control the information flow from previous  hidden layers using the gating mechanism concept     . a  gated rnn is a special recurrent neural network architecture  that overcomes this weakness of a simple rnn by introducing  gating units. there are variants from rnn with gating units   such as long short term memory  lstm  rnn and gated  recurrent unit  gru  rnn. in the following sections  we  explain both lstmrnn and grurnn in more detail.     long short term memory rnn  a long short term  memory  lstm       is a gated rnn with three gating  layers and memory cells. the gating layers are used by the  lstm to control the existing memory by retaining the useful  information and forgetting the unrelated information. memory  cells are used for storing the information across time. the  lstm hidden layer at time t is defined by the following  equations        it           xt w xi   ht   whi   ct   wci   bi             ft           xt w x f   ht   wh f   ct   wc f   b f             ct         ft    ot           xt w xo   ht   who   ct wco   bo      ct     it    tanh xt w xc   ht   whc   bc      ht         ot    tanh ct                       where      is sigmoid activation function and it   ft   ot and ct are  respectively the input gates  the forget gates  the output gates  and the memory cells at time step t. the input gates keep the  candidate memory cell values that are useful for memory cell  computation  and the forget gates keep the previous memory  cell values that are useful for calculating the current memory  cell. the output gates filter which the memory cell values that  are useful for the output or next hidden layer input.    fig.  . recurrent neural network    b. gated recurrent neural network  simple rnns are hard to train to capture long term dependencies from long sequential datasets because the gradient can easily explode or vanish           . because the  gradient  usually  vanishes after several steps  optimizing  a simple rnn is more complicated than standard neural  networks. to overcome the disadvantages of simple rnns   several researches have been done. instead of using a firstorder optimization method  one approach optimized the rnn    fig.  . long short term memory unit.       gated recurrent unit rnn  a gated recurrent unit   gru      is a gated rnn with similar properties to a lstm.  however  there are several differences  a gru does not have  separated memory cells       and instead of three gating layers   it only has two gating layers  reset gates and update gates.     the gru hidden layer at time t is defined by the following  equations       rt           xt w xr   ht   whr   br      zt           xt w xz   ht   whz   br      h t         f  xt w xh    rt    ht              zt      ht    whh   bh      ht     zt    h t                            where      is a sigmoid activation function  rt   zt are reset and  update gates  h t is the candidate hidden layer values and ht is  the hidden layer values at time t. the reset gates determine  which previous hidden layer value is useful for generating the  current candidate hidden layer. the update gates keeps the  previous hidden layer values or replaced by new candidate  hidden layer values. in spite of having one fewer gating layer   the gru can match lstm s performance and its convergence  speed convergence sometimes outperformed lstm     .  fig.  . computation for parent values p  and p  was done in a bottom up  fashion. visible node leaves x    x    and x  are processed based on the given  binary tree structure.    fig.  . gated recurrent unit    c. recursive neural tensor network  a recursive neural tensor network  recntn  is a variant of a recursive neural network  recnn  for modeling  input data with variable length properties and tree structure  dependencies between input features     . to compute the  input representation with recnn  the input must be parsed  into a binary tree where each leaf node represents input  data. then  the parent vectors are computed in a bottom up  fashion  following the above computed tree structure whose  information can be built using external computation tools   i.e.  syntactic parser  or some heuristic from our dataset  observations. given fig.    p    p  and y was defined by    h  i     p    f x  x  w   b         h  i     p    f p  x  w   b              y   g p   wy   b y        where f     is nonlinear activation function  such as sigmoid  or tanh  g    depends on our task  w   r d d is the weight  parameter for projecting child input vectors x    x    x    rd  into the parent vector  wy is a weight parameter for computing output vector  and b  by are biases. if we want to  train recnn for classification tasks  g    can be defined as    fig.  . calculating vector p  from left input x  and right input x  based on  eq.       a softmax function. however  standard recnns have several  limitations  where two vectors only implicitly interact with  addition before applying a nonlinear activation function on  them      and standard recnns are not able to model very  long term dependency on tree structures. zhu et al.       proposed the gating mechanism into standard recnn model to  solve the latter problem. for the former limitation  the recnn  performance can be improved by adding more interaction  between the two input vectors. therefore  a new architecture  called a recursive neural tensor network  recntn  tried to  overcome the previous problem by adding interaction between  two vectors using a tensor product  which is connected by  tensor weight parameters. each slice of the tensor weight can  be used to capture the specific pattern between the left and  right child vectors. for recntn  value p  from eq.    and        is defined by   p          f    h    x     p          f    h    p        i    d    x    h  i  x  wtsr    x  x  w   b       x      i    d    p    h  i  x  wtsr    p  x  w   b       x        d   where wtsr    r d  d d is the tensor weight to map the tensor   i   product between two children vectors. each slice wtsr  is a   d  d  matrix r  . for more details  we visualize the calculation  for p  in fig.  .    iii. proposed architecture  a. gated recurrent unit recurrent neural tensor network   grurntn   previously in sections ii b and ii c  we discussed that  the gating mechanism concept can helps rnns learn longterm dependencies from sequential input data and that adding  more powerful interaction between the input and hidden layers  simultaneously with the tensor product operation in a bilinear  form improves neural network performance and expressiveness. by using tensor product  we increase our model expressiveness by using second degree polynomial interactions   compared to first degree polynomial interactions on standard  dot product followed by addition in common rnns architecture. therefore  in this paper we proposed a gated recurrent  neural tensor network  grurntn  to combine these two  advantages into an rnn architecture. in this architecture  the  tensor product operation is applied between the current input  and previous hidden layer multiplied by the reset gates for  calculating the current candidate hidden layer values. the  calculation is parameterized by tensor weight. to construct  a grurntn  we defined the formulation as   rt           xt w xr   ht   whr   br      zt         h t           xt w xz   ht   whz   bz       h  i    d     xt  f xt  r ht     wtsr   r ht       xt w xh    rt ht    whh   bh      ht              zt      ht     zt            fig.  . calculating candidate hidden layer h t from current input xt and  previous hidden layer multiplied by reset gate r   ht  based on eq.       b. lstm recurrent neural tensor network  lstmrntn   as with grurntn  we also applied the tensor product operation for the lstm unit to improve its performance. in this  architecture  the tensor product operation is applied between  the current input and the previous hidden layers to calculate  the current memory cell. the calculation is parameterized by  the tensor weight. we call this architecture a long short term  memory recurrent neural tensor network  lstmrntn . to  construct an lstmrntn  we defined its formulation   it           xt w xi   ht   whi   ct   wci   bi      ft         c t           xt w x f   ht   wh f   ct   wc f   b f     h i    d  h  i  ht    tanh xt wtsr   xt w xc   ht   whc   bc      ct         ft    ot           xt w xo   ht   who   ct wco   bo      ct     it    ht         ot    c t                  tanh ct         d   where wtsr    ri d d is a tensor weight to map the tensor  product between current input xt and previous hidden layer   i   ht   into our candidate cell c t . each slice wtsr  is a matrix  i d  r . fig.   visualizes the c t calculation in more detail.    h t       d   where wtsr    r i d   i d  d is a tensor weight for mapping the  tensor product between the input hidden layer  i is the input  layer size  and d is the hidden layer size. alternatively  in this  paper we use a simpler bilinear form for calculating h t     h i    d  h  i    rt ht      h t   f xt wtsr     xt w xh    rt    ht    whh   bh               i d    i   where wtsr    ri d d is a tensor weight. each slice wtsr  is  i d  a matrix r . the advantage of this asymmetric version is  that we can still maintain the interaction between the input  and hidden layers through a bilinear form. we reduce the  number of parameters from the original neural tensor network  formulation by using this asymmetric version. fig.   visualizes  the h t calculation in more detail.    fig.  . calculating candidate cell layer c t from current input xt and previous  hidden layer ht   based on eq.        c. optimizing tensor weight using backpropagation through  time  in this section  we explain how to train the tensor weight  for our proposed architecture. generally  we use backpropagation to train most neural network models     . for training  an rnn  researchers tend to use backpropagation through  time  bptt  where the recurrent operation is unfolded as a  feedforward neural network along with the time step when  we backpropagate the error           . sometimes we face  a performance issue when we unfold our rnn on such very  long sequences. to handle that issue  we can use the truncated  bptt     to limit the number of time steps when we unfold  our rnn during backpropagation.  assume we want to do segment classification      with an  rnn trained as function f   x   y  where x    x    ...  xt    as an input sequence and y    y    ...  yt   is an output label  sequence. in this case  probability output label sequence y   given input sequence x  is defined as   p y x       t  y    log p yi  x    ..  xi              i      and our objective is to minimize the negative log likelihood     d   w.r.t all weight parameters  . to optimize wtsr  weight     d   parameters  we need to find derivative e    w.r.t wtsr      e        d    wtsr         t  x   ei      i         d    wtsr    for applying backpropagation through time  we need to unfold  our grurntn and backpropagate the error from ei     to    ..d   all candidate hidden layer h j to accumulate wtsr  gradient  where j     ..i . if we want to use the truncated bptt to  ignore the history past over k time steps  we can limit j     max    i k ..i . we define the standard bptt on grurntn    ..d   to calculate  ei      wtsr      ei         d    wtsr         i  x   ei      h j   h j  w    d   tsr    j           i  x   ei      h j  j            a j     d    h j  a j  wtsr    i  x   ei      j       h j    h i  h  f    a j   x j  r j    h j        where  aj          h i    d  h  x j wtsr  r j   x j w xh    r j     ei         d    wtsr         i  x   ei      j            c j     d    c j  wtsr    i  x   ei      j            c j    i  x   ei      j      aj    i           j    for lstmrntn  we also need to unfold our lstmrnn  and backpropagate the error from ei     to all cell layers c j    ..d   to accumulate wtsr  gradients where j     ..i . we define the    ..d   standard bptt on lstmrntn to calculate  ei      wtsr               usually  we transform likelihood p y x  into a negative loglikelihood        t         y     p yi  x    ..  xi             e        log p y x      log          j     c j     c j    tanh a j    a j     d     tanh a j     a j   wtsr  h i  h  i  i j      tanh   a j    x j h j           where    p yi  x    ..  xi      i      t  x    and f       is a function derivative from our activation function              if f     is tanh function        f  a j           f  a j             f  a       f  a     if f     is sigmoid function    i   h j      h j    whh   bh         i                 i      h i    d  h  x j wtsr h j     x j w xc   h j   whc   bc         . in both proposed models  we can see partial derivative     d    ei      wtsr  in eqs.    and     the derivative from the  tensor product w.r.t the tensor weight parameters depends  on the values of our input and hidden layers. then all the  slices of tensor weight derivative are multiplied by the error  from their corresponding pre activated hidden unit values.  from these derivations  we are able to see where each slice  of tensor weight is learned more directly from their input  and hidden layer values compared by using standard addition  operations. after we accumulated every parameter s gradients  from all the previous time steps  we use a stochastic gradient  optimization method such as adagrad      to optimize our  model parameters.  iv. experiment settings  next we evaluate our proposed grurntn and lstmrntn models against baselines grurnn and lstmrnn  with two different tasks and datasets.  a. datasets and tasks  we used a penntreebank  ptb  corpus    which is a standard benchmark corpus for statistical language modeling. a  ptb corpus is a subset of the wsj corpus. in this experiment   we followed the standard preprocessing step that was done by  previous research     . the ptb dataset is divided as follows   a training set from sections      with total    .    words   a validation set from sections       with total   .    words   and a test set from sections       with total   .    words.  the vocabulary is limited to the   .    most common words   and all words outside are mapped into a   unk   token. we  used the preprocessed ptb corpus from the rnnlm toolkit  website  .    https   www.cis.upenn.edu  treebank     http   www.rnnlm.org      we did two different language modeling tasks. first  we  experimented on a word level language model where our rnn  predicts the next word probability given the previous words  and current word. we used perplexity  ppl  to measure our  rnn performance for word level language modeling. the  formula for calculating the ppl of word sequence x is defined  by        ppl      n    pn  i      log  p xi  x ..i                second  we experimented on a character level language model  where our rnn predicts the next character probability given  the previous characters and current character. we used the  average number of bits per character  bpc  to measure our  rnn performance for character level language modeling. the  formula for calculating the bpc of character sequence x is  defined by        n          x        bpc          log  p xi  x ..i          n i      table i  penntreebank test set bpc  model  nnlm             bptt rnn       hf mrnn             srnn       dot s  rnn       lstmrnn  w  adapt. noise  w o dyn. eval            lstmrnn  w  adapt. noise  w  dyn. eval            grurnn  our baseline   lstmrnn  our baseline   grurntn  proposed   lstmrntn  proposed     test bpc   .     .     .     .     .     .     .     .     .     .     .      v. results and analysis  a. character level language modeling    b. experiment models  in this experiment  we compared the performance from our  baseline models grurnn and lstmrnn with our proposed  grurntn and lstmrntn models. we used the same  dimensions for the embedding matrix to represent the words  and characters as the vectors of real numbers.  for the word level language modeling task  we used      hidden units for grurntn and lstmrntn      for  grurnn  and     for lstmrnn. all of these models  use     dimensions for word embedding. we used dropout  regularization with p    .  dropout probability for grurntn  and lstmrntn and p    .  for our baseline model. the  total number of free parameters for grurnn and grurntn  were about    million and about    million for lstmrnn  and lstmrntn.  for the character level language modeling task  we used      hidden units for grurntn and lstmrntn      for  grurnn  and     for lstmrntn. all of these models  used    dimensions for character embedding. we used dropout  regularization with p    .   dropout probability. the total  number of free parameters for grurnn and grurntn was  about  .  million and about  .  million for lstmrnn and  lstmrntn.  we constrained our baseline grurnn to have a similar  number of parameters as the grurntn model for a fair  comparison. we also applied such constraints on our baseline  lstmrnn to lstmrntn model.  for all the experiment scenarios  we used adagrad for  our stochastic gradient optimization method with mini batch  training and a batch size of    sentences. we multiplied  our learning rate with a decay factor of  .  when the cost  from the development set for current epoch is greater than  previous epoch. we also used a rescaling trick on the gradient       when the norm was larger than   to avoid the issue of  exploding gradients. for initializing the parameters  we used  the orthogonal weight initialization trick      on every model.    fig.  . comparison among grurnn  grurntn  lstmrnn  and lstmrntn bits per character  bpc  per epoch on ptb validation set. note   a  lower bpc score is better    in this section  we report our experiment results on ptb  character level language modeling using our baseline models  grurnn and lstmrnn as well as our proposed models  grurntn and lstmrntn. fig.   shows performance comparisons from every model based on the validation set s bpc  per epoch. in this experiment  grurnn made faster progress  than lstmrnn  but eventually lstmrnn converged into  a better bpc based on the development set. our proposed  model grurntn made faster and quicker progress than  lstmrntn and converged into a similar bpc in the last  epoch. both proposed models produced lower bpc than our  baseline models from the first epoch to the last epoch.  table i shows ptb test set bpc among our baseline models  our proposed models and several published results. our  proposed model grurntn and lstmrntn outperformed  both baseline models. grurntn reduced the bpc from  .      adaptive noise regularization is where the noise variance is learned and  applied with the weight    dynamic evaluation approach updates the model parameter during processing on the test data  only updated once per test dataset . our baseline and  proposed model experiment did not use dynamic evaluation.     table ii  penntreebank test set ppl  model  n gram       rnnlm  w o dyn. eval          rnnlm  w  dyn. eval          scrnn       srnn       dot s  rnn       grurnn  our baseline   lstmrnn  our baseline   grurntn  proposed   lstmrntn  proposed     test ppl          .      .           .      .     .       .      .      .      to  .     .   absolute    .    relative bpc  from the baseline  grurnn  and lstmrntn reduced the bpc from  .   to   .     .   absolute    .    relative bpc  from the baseline lstmrnn. overall  grurntn slightly outperformed  lstmrntn  and both proposed models outperformed all of  the baseline models on the character level language modeling  task.  b. word level language modeling    fig.  . comparison among grurnn  grurntn  lstmrnn and lstmrntn perplexity  ppl  per epoch on the ptb validation set. note   a lower  ppl value is better    in this section  we report our experiment results on  ptb word level language modeling using our baseline models grurnn and lstmrnn and our proposed models  grurntn and lstmrntn. fig.   compares the performance from every models based on the validation set s ppl  per epoch. in this experiment  grurnn made faster progress  than lstmrnn. our proposed grurntn s progress was  also better than lstmrntn. the best model in this task was  grurntn  which had a consistently lower ppl than the other  models.  table i shows the ptb test set ppl among our baseline  models  proposed models  and several published results. both  our proposed models outperformed their baseline models.  grurntn reduced the perplexity from   .   to   .      .   absolute     .    relative ppl  over the baseline grurnn    and lstmrntn reduced the perplexity from    .   to   .       .   absolute     .    relative ppl  over the baseline  lstmrnn. overall  lstmrntn improved the lstmrnn  model and its performance closely resembles the baseline  grurnn. however  grurntn outperformed all the baseline models as well as the other models by a large margin.  vi. related work  representing hidden states with deeper operations was  introduced just a few years ago     . in these works  pascanu  et al.      use additional nonlinear layers for representing  the transition from input to hidden layers  hidden to hidden  layers  and hidden to output layers. they also improved the  rnn architecture by a adding shortcut connection in the deep  transition by skipping the intermediate layers. another work  from      proposed a new rnn design for a stacked rnn  model called gated feedback rnn  gfrnn   which adds  more connections from all the previous time step stacked  hidden layers into the current hidden layer computations.  despite adding additional transition layers and connection  weight from previous hidden layers  all of these models still  represent the input and hidden layer relationships by using  linear projection  addition and nonlinearity transformation.  on the tensor based models  irsoy et al.      proposed a  simple rnn with a tensor product between the input and  hidden layers. such architecture resembles recntn  given  a parse tree with a completely unbalanced tree on one side.  another work from      also use tensor products for representing hidden layers on dnn. by splitting the weight matrix  into two parallel weight matrices  they calculated two parallel  hidden layers and combined the pair of hidden layers using a  tensor product. however  since not all of those models use a  gating mechanism  the tensor parameters and tensor product  operation can not be fully utilized because of the vanishing   or exploding  gradient problem.  on the recurrent neural network based model  sutskever et  al.      proposed multiplicative rnn  mrnn  for characterlevel language modeling using tensor as the weight parameters.  they proposed two different models. the first selected a  slice of tensor weight based on the current character input   and the second improved the first model with factorization  for constructing a hidden to hidden layer weight. however   those models fail to fully utilize the tensor weight with the  tensor product. after they selected the weight matrix based  on the current input information  they continue to use linear  projection  addition  and nonlinearity for interacting between  the input and hidden layers.  to the best of our knowledge  none of these works combined  the gating mechanism and tensor product concepts into a single  neural network architecture. in this paper  we built a new rnn  by combining gating units and tensor products into a single  rnn architecture. we expect that our proposed grurntn  and lstmrntn architecture will improve the rnn performance for modeling temporal and sequential datasets.     vii. conclusion  we presented a new rnn architecture by combining the  gating mechanism and tensor product concepts. our proposed  architecture can learn long term dependencies from temporal and sequential data using gating units as well as more  powerful interaction between the current input and previous  hidden layers by introducing tensor product operations. from  our experiment on the penntreebank corpus  our proposed  models outperformed the baseline models with a similar  number of parameters in character level language modeling  and word level language modeling tasks. in a character level  language modeling task  grurntn obtained  .   absolute    .    relative  bpc reduction over grurnn  and lstmrntn obtained  .   absolute   .    relative  bpc reduction  over lstmrnn. in a word level language modeling task   grurntn obtained   .  absolute    .    relative  ppl  reduction over grurnn  and lstmrntn obtained   .    absolute    .    relative ppl  reduction over lstmrnn. in  the future  we will investigate the possibility of combining our  model with other stacked rnns architecture  such as gated  feedback rnn  gfrnn . we would also like to explore  other possible tensor operations and integrate them with our  rnn architecture. by applying these ideas together  we expect  to gain further performance improvement. last  for further  investigation we will apply our proposed models to other  temporal and sequential tasks  such as speech recognition and  video recognition.  viii. acknowledgements  part of this research was supported by jsps kakenhi  grant number         .  