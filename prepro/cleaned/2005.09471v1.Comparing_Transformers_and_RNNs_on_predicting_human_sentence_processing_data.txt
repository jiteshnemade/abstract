introduction    recurrent neural networks  rnns  are widely used in both psycholinguistics and natural language  processing  nlp . psycholinguists have looked to rnns as an architecture for modelling human sentence processing  for a recent review  see frank et al.         . rnns have been used to account for  human reading times  monsalve et al.        goodkind and bicknell        and n    amplitudes in the  eeg signal during reading  frank et al.        rabovsky et al.        brouwer et al.        fitz and  chang       . since the introduction of the simple recurrent network  srn   elman         different  rnn architectures have been proposed that address the issue that srns had with capturing long term  patterns by adding gating mechanisms that control the flow of information over time  allowing the networks to weigh old and new inputs and memorise or forget information when appropriate. the most well  known of these are the long short term memory  lstm   hochreiter and schmidhuber        and the  gated recurrent unit  gru   cho et al.       .  these gated rnns have successfully been applied to nlp tasks such as translation  caption generation  and speech recognition  bahdanau et al.        xu et al.        zeyer et al.        michel and neubig        . however  a recent study which compared srn  gru and lstm models  ability to predict reading  times and n    amplitudes found no significant differences between the three recurrent architectures   aurnhammer and frank       .  in essence  all these rnn types process sequential information by recurrence. previous input is represented as the hidden state of the recurrent computation and each new input is processed and combined  with the hidden state. recently  a new neural network architecture called the transformer has been introduced  vaswani et al.       . importantly  the transformer is not simply an improved type of rnn as  the lstm and gru were. the transformer is a fundamentally different type of architecture that does  not use recurrence at all. a transformer cell as originally proposed by vaswani et al.         consists of  self attention layers  luong et al.        followed by a linear feed forward layer. in contrast to recurrent  processing  self attention layers are allowed to  attend  to parts of previous input directly.  since its introduction  the transformer has received substantial attention in the nlp community and  achieved state of the art results on several nlp tasks  devlin et al.        hayashi et al.        karita et     al.       . pre trained transformer based models such as bert and gpt   make it possible to employ  the power of networks trained on huge amounts of data. many studies have fine tuned such models and  broken benchmark nlp scores. not much is known however  about how the transformer fares as a  model of human sentence processing. the success of rnns in explaining human behavioural and neurophysiological data suggests that something akin to recursive processing might be involved in human  sentence processing and as such rnns seem more cognitively plausible than the transformer. especially the direct access that attention operations have to past input regardless of temporal distance is not  biologically plausible. even though the transformer seems less biologically plausible it has not yet been  confirmed that the transformer is a worse model of sentence comprehension.  we will compare the transformer with the gru and investigate how well they perform as models of  human sentence processing. we model human sentence processing using transformer and gru based  language models  lms . we compare how well their word by word surprisal predicts human processing  effort as measured by self paced reading  eye tracking and electroencephalography  n    response  using the same human reading data as aurnhammer and frank        used to compare rnn architectures.  we think the introduction of the transformer merits a similar comparison as the differences between  transformers and rnns are more fundamental than the differences between rnn types. looking ahead  to our results  we surprisingly find that the transformer outperforms our rnn models.        .     background  models of human sentence processing    an important question in human sentence processing research is why some words are more difficult to  process than others. it has long been known that more predictable words are generally read faster than  less predictable words and are more likely to be skipped  ehrlich and rayner       . predictability has  been formalised as surprisal  a measure which can be derived from lms. to generate surprisal values   lms are trained to estimate the probability of the next word in a sentence given the preceding context.  in this sense  a lm has an expectation of a word w at time t given the preceding words w    ...  wt   . we  formally measure this as the surprisal of a word given by  surprisal wt       log p  wt  w    ...  wt    .  in surprisal theory  surprisal acts as a  causal bottleneck  between computational models and behavioural  observations  levy         meaning that for instance the model architecture  transformer or rnn  only  affects predictions about human processing difficulty through the generated word probabilities.  surprisal has long been related to human word processing effort in sentence comprehension. the  central idea of such expectation based theories of sentence comprehension is that less expected words  lead to more processing effort. for instance  in psycholinguistics it is common to take reading times as  a measure of word processing difficulty and the positive correlation between reading time and surprisal  has firmly been established  hale        levy        mitchell and keller        monsalve et al.         smith and levy        hahn and keller        with goodkind and bicknell        recently showing  that the predictive power of surprisal values increases linearly with the quality of the language model.  high surprisal has been shown to correlate with greater neural activity in fmri and meg studies   ettinger et al.        henderson et al.       . the n     a brain potential peaking around     ms after  stimulus onset and associated with semantic incongruity  kutas and hillyard         has been shown to  correlate with word surprisal in both eeg and meg studies  wehbe et al.        frank et al.       .  most recent models of processing difficulty used rnn based lms. in this paper  we will compare  rnn and transformer based lms on their ability to predict human reading data. the work most closely  related to the current study is that by aurnhammer and frank       . they compared srns  lstms and  grus  three rnn types differing in how they integrate their  memory  of past input with the next input  in the sequence  on human reading data from three psycholinguistic experiments. despite the gru and  lstm generally outperforming the srn on nlp tasks  aurnhammer and frank found no difference in  how well the models  surprisal predicted human processing effort in the self paced reading  eye tracking  and eeg experiments. the transformer has to the best of our knowledge not yet been evaluated as a  model of human sentence processing.     figure    comparison of how sequential information flows through the transformer and rnn. in the  transformer  every time step has access to all previous time steps. the rnn encodes incoming information and adds it to a single hidden state that is passed on to the next layer and time step.   .     comparing rnn and transformer architectures    for a complete overview of the transformer and our implementation we refer to vaswani et al.        and  our code https   github.com dannymerkx next word prediction. here we briefly  highlight the difference between rnns and the transformer in how the models process sequential information. the way activation flows through the network is represented in figure    which shows an  example with a five word sentence. we only consider uni directional versions of both the transformer  and rnns here  since language modelling is trivial for a bi directional network. note that figure   only  shows how activation is propagated through time and across layers  not how specific architectures compute the hidden states  see  elman        hochreiter and schmidhuber        cho et al.        vaswani  et al.        for specifics on the srn  lstm  gru  and transformer respectively .  in an rnn  incoming information is immediately processed and represented as a hidden state. the  next token in the sequence is again immediately processed and combined with the previous hidden state  to form a new hidden state. across layers  each time step also only gets to see its corresponding hidden  state from the previous layer and the hidden state of the previous time step. so  processing is immediate  and incremental. information from previous time steps is encoded in the hidden state  but this state is  limited in how much it can encode so decay of previous time steps is implicit.  the transformer s attention layer allows each input to look at all previous time steps. hidden states  are a weighted combination of all time steps seen so far. this basically unlimited memory is a big  conceptual difference with rnns  where long distance dependencies can only be propagated through  the hidden states. processing is not incremental over time as in a single layer  the processing of word  wt is not dependent on the results of processing words w  through wt   . while the rnn is inherently  sequential  the transformer can only use order information if it is explicitly added to the input or if the  network is multi layered. consider h    in the first layer which is based on w    w  and w  . hidden state  h    does not depend on the order of the previous inputs  any order will result in the same hidden state .  however  h    depends on h      h    and h    . if the order of the inputs w    w    w  is different  h     will be the same hidden state but h    and h    will not  resulting in a different hidden state at h    .   rnns handling of sequential inputs make them seemingly more plausible as a cognitive model. christiansen and chater        argue for a  now or never  bottleneck in language processing  incoming inputs  need to be rapidly recoded and passed on for further processing to prevent being interfered with by the  rapidly incoming stream of new material. in line with this theory  futrell et al.        proposed a model  of lossy context surprisal based on a lossy representation of memory. recurrent processing  where input  is forgotten as soon as it is processed and only available for subsequent processing through a bounded  size hidden state  is more compatible with these theories than the transformer s attention operation.     note that this is only true for uni directional transformers. bi directional transformers are not sensitive to order unless  explicit order information is given.          methods    we train lms with transformer and gru architectures and compare how well their surprisal explains  human behavioural and neural data. it has been shown that the predictive power of surprisal is a linear  function of language model quality  goodkind and bicknell       . so  to separate the effects of lm  quality from the effects of the architectural differences  we compare the architectures at equal language  modelling capability. a state of the art pre trained model such as gpt    radford et al.        can likely  achieve a lower perplexity  but we opt for training our own lms to have control over the training material  and hyperparameters such that a fair comparison between the two architectures can be made.   .     language model architectures    in this work we test only the gated recurrent unit  gru . aurnhammer and frank        extensively  compared grus  lstms and srns on the same behavioural and electrophysiological data that we use  here and found no significant differences. we use the gru because of the three it was most recently  introduced and it has fewer weights than the lstm  cho et al.       . we train single layer and twolayer lms for both the gru and the transformer  to also investigate the effect of network depth.  first we trained a gru model where we used the same architecture as used in aurnhammer and frank          an embedding layer with     dimensions per word  a     unit gru layer followed by a    unit linear layer with a tanh activation function and finally an output layer with log softmax activation  function. all lms used in this experiment use randomly initialised embedding layers  that is  no pretrained word embeddings were used.  to minimise the differences between the lms we picked parameters for the transformer such that the  total number of weights is as close as possible to the gru model. we also make sure the embedding  layers for the models share the same initial weights. the transformer model has an embedding layer  with     dimensions per word followed by a single transformer layer with   attention heads and a fully  connected layer with      units and finally an output layer with log softmax activation function. the  transformer was described as an encoder decoder model vaswani et al.       . we use the encoder  side of the model  that is  the transformer with one instead of two attention operations.  we implement  the transformer in pytorch following  vaswani et al.        and make our implementation including  all training and analysis scripts available on https   github.com dannymerkx next word   prediction. the total number of parameters for our single layer gru and transformer models are            and           respectively.  we also train two layer gru and transformer models. it is known that neural networks tend to increase in expressiveness with depth  learning more complex representations  e.g.  giulianelli et al.         abnar et al.        . furthermore  an additional layer allows the transformer to make use of implicit order information. the analysis  see section  .   showed that the two layer transformer outperformed the  single layer transformer in explaining the human reading data so we decided to train a four layer transformer as well too see if this trend continues. we did not see a performance increase in the two layer  gru over the the single layer gru however and did not increase the layer depth further. as the analysis showed that the transformer outperformed the gru we trained a gru model with a transformer  self attention operation in between the gru layer and the linear layer to see whether the transformer s  advantage was solely due to the unlimited access to past states.   .     training materials    we train our lms on section   of the english corpora from the web  encow        schfer          consisting of random sentences taken from the web. we first exclude tokens containing numerical values  or punctuation other than hyphens and apostrophes. we treat common contractions such as  don t  as a  single token instead of two. following aurnhammer and frank        we then select the        most  frequent word types from encow.     word types from the test data  see section  .   that were not  covered by these most frequent words were added for a final vocabulary of        words. we selected the     the decoder has a self attention operation and a second attention operation attending to a context vector given by the  encoder. in the current setting the encoder with only self attention is more appropriate  since there is no context vector.     data  spr  et  eeg    participants                sentences                   sentence length                      mean sentence length    .    .    .     tokens                      datapoints                             table    overview of the test data  the number of participants and different sentences  for spr each  participant only saw a subset of the     sentences   the range of sentence lengths and mean sentence  length  the total number of word tokens and the final number of datapoints used in the analysis after  exclusion criteria are applied.  sentences from encow that consisted only of words in the vocabulary and limit the sentence length to     tokens  reflecting the longest sentence in the test data. our training data contains           sentences  with a total of            tokens.   .     language model training    we use a standard next word prediction task with cross entropy loss to train the lms. because the  attention operation in the transformer inherently allows each position in the sentence to attend to all  other words in the sentence  including future words  we apply a mask to the upper diagonal of the  attention matrix such that each position can only attend to itself and previous positions.  to account for random effects of weight initialisation and data presentation order we train eight lms  of each type described in section  .  and share the random seeds between the lm types so each random  presentation order and embedding layer initialisation is present in each of the lm types. the lms were  trained for two epochs using stochastic gradient descent with a momentum of  . . the initial learning  rates were chosen such that the models still improved near the end of the second epoch while keeping the  language modelling performance of the gru and transformer models as similar as possible. the initial  learning rate for the gru models was  .   and for the transformer models  .   . the learning rate was  halved after         and all of the sentences during the first epoch and then kept constant over the second  epoch. the lms were trained on minibatches of ten sentences.   .     human reading data    we use the lms to calculate surprisal values for sentences used in human sentence processing experiments and evaluate our models on how well the surprisal values predict human processing effort. we  use the self paced reading  spr  and eye tracking  et  data from frank et al.        and the electroencephalography  eeg  data from frank et al.       . in these experiments  participants read english  sentences from unpublished novels. in the spr and eeg experiments  the participants were presented  sentences one word at a time. in the spr experiment the reading was self paced  i.e.  participants proceed to the next word with a key press  while in the eeg experiment words had a fixed presentation time.  in the et experiments  participants were shown full sentences while an eye tracking device monitored  which word was fixated. the spr stimuli consists of     sentences  with the eeg and et stimuli being  a subset of the spr stimuli. the experimental measures representing processing effort of a word are  reading time for the spr data  time between key presses   gaze duration for the et data  time a word  is fixated before the first fixation on a different word  and n    amplitude for the eeg data  average  amplitude at the centroparietal electrodes between     and     ms after word onset  frank et al.        .  for our analysis we exclude the word at the start and end of each sentence  and words attached to a  comma. for the spr and et data we also exclude the word following a comma. for the eeg data we  exclude datapoints that were marked by frank et al.        as containing artifacts. for the spr and et  data we excluded words with a reading time under    ms or over      ms and for the et data we exclude  words that were not fixated. table   gives an overview of the test data.   .     analysis procedure    we save each lm s parameters at    different points during training   k   k    k    k     k     k    m   m sentences and after the first and second epoch . we use each of these parameter states to generate     model  spr    dependent  log rt     et  eeg    log gaze dur   n       fixed effects  surp   prev surp   log freq    char   word pos   prev log freq    prev char    prev log rt   surp   prev surp   log freq    char   word pos   prev log freq    prev char  surp   log freq    char   word pos   baseline    table    summary of the dependent variables and fixed effects for the mixed linear models. the baseline  models are fitted excluding the surprisal data  indicated in boldface . a   indicates variables for which  we included interactions.  surprisal values for the     test sentences for a total of     sets of surprisal values      parameter states        repetitions       lms  .   . .     linear mixed effects regression    following aurnhammer and frank         we analyse how well each set of surprisal values predicts the  human sentence processing data using linear mixed effects regression  lmer  models with the mixedmodels package in julia  bates et al.       . for each of the human behavioural datasets  spr  et  and eeg  we fit a baseline lmer model which takes into account several factors known to influence  processing effort. the dependent variables of the spr and et models are reading time  ms  and gaze  duration  ms  respectively  both log transformed . the dependent variable of the eeg model is the size  of the n    response. all lmer models include log transformed training corpus word frequency  word  length  characters  and the word s position in the sentence as fixed effects. spill over is known to affect  reading time and can occur when processing of a word is not yet fully done by the time the next word  is read  e.g.   rayner        mitchell        ferreira and henderson        . in order to account for  spill over in the spr and et data we add in the previous word s frequency and length. for the spr data  we add the previous word s reading time to account for the high correlation between consecutive word s  reading times in spr paradigms. for the eeg data  we include the baseline activity which is the average amplitude in the     ms before word onset. all lmer models have by subject and by item  word  token  random intercepts and by subject random slopes for all fixed effects. we included interaction  effects between all fixed effects and all fixed effects were normalised for mean and standard deviation.  after fitting the baseline models  we fit lmer models where we add in the surprisal values and for  the spr and et data also the previous word s surprisal as fixed effects. we do not include interaction  effects between the surprisal and the other fixed effects. table   gives an overview of the variables used  in each model.  for each lmer model with surprisal  we calculate the log likelihood ratio with its corresponding  baseline model indicating the decrease in model deviance due to adding the surprisal measures. the  more the surprisal values decrease the model deviance  the better these values predict the human reading  data. we call this log likelihood ratio the goodness of fit between the surprisal and the data. if the  surprisal values actually predict effects contrary to what we expect  we show this by reversing the sign of  the goodness of fit so that negative values indicate the lmer models where high surprisal predicts lower  gaze duration  reading time  or n    size. the lmer analysis results in a set of     goodness of fit  measures which are used in the second stage of the analysis.   . .     generalised additive modelling    as said before  it is well known that surprisal values derived from better lms are a better fit to human  reading data  monsalve et al.        frank et al.        goodkind and bicknell       . we use generalised additive modelling  gam  to assess whether the lms differ in their ability to explain the human  reading data at equal language modelling capability  that is  because of their architectural differences and  not due being a better lm. we used the r package mgcv by wood       . we take the log likelihood  ratios obtained in the previous analysis step as a measure of how well each lm explains the human reading data. we use each lm s average log probability  i.e.  negative average surprisal  over the datapoints  used in the lmer analysis as a measure of the lm s language modelling capability. separate gams     were fitted for each of the three datasets. we used the lm type  single layer gru  two layer gru etc.   as an unordered factor so that separate smooths are fit for each lm type. furthermore  we add training  repetition  i.e. the random training order and embedding initialisation  as a random smooth effect.        .     results  lm quality and the effects on goodness of fit    figure   shows the goodness of fit values resulting from the linear mixed effects models and the smooths  fitted by the gams. as in aurnhammer and frank         we see that for lower lm quality the surprisal  values actually predict effects contrary to what we expect  especially in the gaze duration and the n     size  as indicated by negative goodness of fit . this effect occurs in both our gru lms and our transformer lms  showing this is not particular to rnns. overall we see the expected relationship where  higher lm quality generally results in higher goodness of fit. the transformer models notably have a  higher minimum lm quality than the grus. the models do seem to reach similar levels of lm quality at the end of training. the average log probability of the best lm  two layer transformer  is only   .   higher than worst lm  two layer gru . the lm quality increases monotonically during training  meaning the clusters seen in the scatter plots correspond to the points during training where the network  parameters were stored.   .     gam comparisons    figure   shows plots of the estimated differences between the gam curves in figure  . we indicate  where the     confidence interval of the estimated differences does not include zero. the two layer  gru does not seem to improve over the single layer gru. it outperforms the single layer gru only in  the early stages of training on the eeg et data with the single layer gru outperforming it on the spr  data. the two layer gru also reaches a lower maximum lm quality on all datasets. for the transformers we see the opposite  with the two layer transformer outperforming the single layer transformer on  the n    data at the end of training and never being outperformed by its shallower counterpart. the  two layer transformer reaches a higher maximum lm quality on all datasets.  here we only compare the best model of each type  i.e.  the single layer gru and the two layer  transformer  . we see that while the gru outperforms the transformer in the early stages of training     k   k sentences  on the n    data  the transformer clearly outperforms the gru at the end of  training on both the spr and n    data. note that even these stretches of language model quality at the  end of training where the transformer outperforms the gru seem small  they represent roughly     of  the training cycle   st and  nd epoch  for the eeg data and roughly       m sentences   st and  nd  epoch  for the spr data. on the gaze duration data  there are some performance differences with the  transformers and grus outperforming each other at different points during training but there are no  differences in the later stages of training.   .     alternative lm architectures    the results prompted us to investigate further and train two more lm types. the transformer benefited  from the additional complexity of increased depth as shown by the difference on the n    data and  by reaching overall higher lm quality. we trained a four layer transformer model to see if this trend  continues. the gru did not seem to benefit from additional depth and instead we trained a singlelayer gru that was followed by a transformer attention operation in between the gru output and the  linear classification layer. the resulting smooths and plots of the estimated differences can be found in  appendix b. the addition of attention to the gru did not improve its performance. the attention gru  outperforms the gru on the spr data corresponding to the end of the first epoch but the difference is  gone after the second epoch. the four layer transformer does not seem to improve the performance of  the transformer more than the addition of a second layer did. the four layer model outperforms the     the comparisons between the two layer gru and the transformers and the single layer gru and single layer transformer  can be found in appendix a.     figure    the top row shows the results of the linear mixed effects regression analysis grouped by lm  type. these scatter plots show the resulting goodness of fit values plotted against the average surprisal  over the included test data. the bottom row shows the smooths resulting from the gams fitted on the  goodness of fit data  with their     confidence intervals.    figure    estimated differences in goodness of fit score. the markings on the x axis and the vertical  lines indicate intervals where zero is not within the     confidence interval. each curve represents  a comparison between two models  with an estimated difference above zero meaning the first model  performed better and vice versa for differences below zero.     two layer model early on in the n    data and near the end of training in the spr data but this difference  disappears at after the second epoch.   .     shorter and longer sentences in spr    while the eeg and et datasets used the same test stimuli  the spr experiment included sentences that  were not in the eeg et data. the spr data contains a subset of sentences longer  in terms of characters   than those seen in the eeg et data. we repeated the analysis of the single  and two layer grus and  transformers but only on those sentences from the spr data that also occurred in the eeg et data.  on this data we surprisingly find the exact opposite of our previous results  where the two layer gru  outperforms both transformers at the end of training. when we test on only those sentences that were not  included in the eeg et experiments  i.e. the longer sentences   we see that the transformers outperform  the grus as they did on the complete spr dataset. the plots for these results can be found in appendix  c.         discussion    we trained several language models based on transformer and gru architectures in order to investigate  if there is any difference in how well these neural networks are able to model human reading data. we  compared the architectures at equal lm quality and found that in general the transformers seem to  outperform the grus. previous work had shown there are no significant differences between different  rnn types despite differences in their gating mechanisms  aurnhammer and frank       . it seems that  the transformer s attention based computation allow it to better capture the human self paced reading  time data and to a lesser extent the eeg data.  somewhat surprisingly  adding more depth to the gru models did not seem to improve performance  and even hurt it in the case of the reading time data  despite previous research showing that increasing  layer depth in rnns allows them to capture more complex patterns in linguistic data  giulianelli et al.         abnar et al.       . the transformers did show improvement when adding a second layer and  improving slightly further on the spr data for the even deeper   layer model. this could be explained by  the fact that our single layer transformer cannot make use of implicit order information in the sequence   and hence we add explicit order information to the input embeddings following vaswani et al.       .  when adding more layers to our transformer  the subsequent layers operate no longer on raw input  embeddings but on contextualised hidden states allowing the model to utilise implicit information in the  order of the input. further research could investigate how powerful this implicit order information is   and whether multi layer transformer lms no longer require the additional explicit order information.  it is notable that the transformer outperformed the gru on the two datasets which consist of a reading  task where sentences were presented to the subjects word by word  spr and eeg . there is neurophysiological evidence that natural reading  whole sentences  places different demands on the reader than  reading in a word by word setting  leading to different encoding and reading strategies  metzner et al.        . metzner et al. speculate that a word by word setting places greater demand on the reader s working memory  leading to faster retrieval of previously processed material. this seems to be supported by  our results  the transformer has direct access to previous inputs and hidden states is better at explaining the rt and n    data from the word by word reading experiments. however  when we split the  spr data by sentences that were also present in the et eeg data  the results seem to suggest that the  transformers  advantage is mainly due to performing better on longer sentences. this question could be  resolved with new data gathered in experiments where the same set of stimuli is used for the spr and  eeg experiment.  in conclusion  we investigated how the recently introduced transformer architecture holds up as a  model of human sentence processing compared to the gru. our transformer lms are better at explaining the eeg and spr data even though the transformer s attention operation contradicts the widely held  idea that human sentence processing involves recurrent and immediate processing with lossy retrieval of  previous input.     