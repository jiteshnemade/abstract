introduction    many recent advancements in natural language  processing have come from the introduction of  recurrent neural networks  rnn   elman         goldberg       . one class of rnns  the long  short term memory rnn  lstm   hochreiter  and schmidhuber        has been able to achieve  impressive results on a suite of nlp tasks  including machine translation  language modeling  and  syntactic parsing  sutskever et al.        vinyals  et al.        jozefowicz et al.       . but the nature of the representations learned by these models is not properly understood. as these models  are being deployed with increasing frequency  this  poses both engineering  accountability  and theoretical problems.  one promising line of research aims to crack  open these  black boxes  by investigating how  lstm language models perform on specially controlled sentences designed to draw out behavior    that indicates representation of a syntactic dependency. using this method  linzen et al.        and  gulordava et al.        demonstrated that these  models are able to successfully learn the number  agreement dependency between a subject and its  verb  even when there are intervening elements   and mccoy et al.        found that rnns learn  the hierarchical rules of english auxiliary inversion. in this paper  we broaden and deepen this line  of inquiry by examining what lstms learn about  an unexplored syntactic relationship  the filler gap  dependency. the filler gap dependency is novel   insofar as learning it requires the network to generalize about the absence of material.  for our purposes  filler gap dependency refers  to a relationship between a filler  which is a whcomplementizer such as  what  or  who   and a  gap  which is an empty syntactic position licensed  by the filler. in example   a   the filler is  what   and the gap appears after  devoured   indicated  with underscores. if the filler were not present  the  gap would be ungrammatical  as in   b .      a. i know what the lion devoured at sunrise.  b. i know that the lion devoured at sunrise.  there is also a semantic relationship between the  filler and the gap  in the sense that  what  is semantically the direct object of  devoured . in this  work  we study the behavior of language models   and so we treat the filler gap dependency purely  as a licensing relationship.  elman        found that simple distributed  models have some success predicting post verbal  gaps in sentences containing object extracted relative clauses. however  correct representation  of filler gap dependencies and the constraints  on them has proven challenging even in handengineered symbolic models. furthermore  they  are subject to numerous complex island constraints  ross       . because of their complex      ity and ubiquity  these dependencies have figured prominently in arguments that natural language would be unlearnable by children without  a great deal of innate knowledge  phillips          cf. pearl and sprouse        ellefson and christiansen         the remainder of the paper is structured as follows. section   presents our methods in more  detail. section   gives evidence that lstm language models represent the basic filler gap dependency in multiple syntactic positions despite intervening material. section   investigates whether  lstm language models are sensitive to various  constraints  wh islands  adjunct islands  complex  np islands  and subject islands. we find that the  language models are sensitive to some but not all  of these constraints. section   concludes.    the probability is calculated from the rnn s softmax activation. the logarithm is taken in base     so that surprisal is measured in bits.  the degree of surprisal for a word or sentence  tells us the extent to which that word or sentence  is unexpected under the language model s probability distribution. it is known to correlate directly  with human sentence processing difficulty  hale         levy        smith and levy       . in this  paper  we look for cases where the surprisal associated with an an unusual construction such as a  gap is ameliorated by the presence of a licensor   such as a wh word. if the models learn that syntactic gaps require licensing  then sentences with  licensors should exhibit lower surprisal than minimally different pairs that lack a proper licensor.         we test whether the lstm language models have  learned filler gap dependencies by looking for a   x  interaction between the presence of a gap and  the presence of a wh licensor. this interaction indicates the extent to which a wh licensor reduces  the surprisal associated with a gap  so we call  it the wh licensing interaction. in studying constraints on filler gap dependencies  we look for  interactions between the wh licensing interaction  and other factors  for example  whether the whlicensing interaction decreases when a gap is in a  syntactic island position as opposed to a syntactically licit position  section   .  we use experimental items where the gap is located in an obligatory argument position  e.g. in  subject position or as the direct object of a transitive verb  as judged by the authors. the phrase  with the gap is embedded inside a complement  clause. we chose this paradigm over bare whquestions because it eliminates do support and  tense manipulation of the main verb  resulting in  higher similarity across conditions. each item appears in four conditions  reflecting a       experimental design manipulating presence of a whlicensor and presence of a gap. for example        a. i know that the lion devoured a gazelle at  sunrise.  no wh licensor  no gap   b. i know what the lion devoured a gazelle at  sunrise.  wh licensor  no gap   c.  i know that the lion devoured at sunrise.   no wh licensor  gap      .     methods  language models    we study the behavior of two pre existing lstms  trained on a language modeling objective over english text. our first model is presented in jozefowicz et al.        under the name big lstm cnn  inputs  we call it the google model. it was trained  on the one billion word benchmark  chelba  et al.        and has two hidden layers with       units each. it uses the output of a character level  convolutional neural network  cnn  as input to  the lstm. this model has the best published perplexity for english text. our second model is the  one presented in the supplementary materials of  gulordava et al.         which we call the gulordava model. trained on    million tokens of english wikipedia  it has two hidden layers of      units each. our goal in using these models is to  provide two samples of the state of the art. as a  baseline  we also study an n gram model trained  on the one billion word benchmark  a   gram  model with modified kneser ney interpolation   fit by kenlm with default parameters   heafield  et al.       .   .     dependent variable  surprisal    we investigate rnn behavior primarily by studying the surprisal values that an rnn assigns to  words and sentences. surprisal is log inverse probability   s xi       log  p xi  hi       where xi is the current word or character  hi   is  the rnn s hidden state before consuming xi   and     .     experimental design      we indicate the gap position with underscores for expos     itory purposes  but these underscores were not included in  experimental items.     d. i know what the lion devoured   wh licensor  gap     at sunrise.    we measure surprisal in two places  at the word  immediately following a  filled  gap and summed  over the whole region from the gap to the end  of the embedded clause. we look at immediateword surprisal because a gap s licitness should  have local effects on network expectation. we look  at whole region surprisal because the presence of  a filler also changes expectations about overall  well formedness of the sentence a global phenomenon. until the final punctuation is reached  in   b  there are potential gap containing continuations that render the sentence syntactically licit   e.g.  with .  . therefore  we might expect no  large spike in surprisal at any one point  but small  increases in surprisal when the network encounters filled argument structure roles and at the end  of the sentence. measuring summed surprisal captures these distributed  global effects.  if the network is learning the licensing relationship between fillers and gaps then two things  should be true  first  if a wh licensor sets up  a global expectation for the presence of a gap   then in sentences containing a wh licensor but no  gap we expect higher surprisal in syntactic positions where a gap is likely to occur resulting  in higher summed surprisal. that is  s   b      s   a   should be a large positive number. second  the presence of a gap in the absence of a whlicensor should also result in higher surprisal than  when the wh licensor is present  that is s   d      s   c   should be a large negative number. given  the four sentences in      the full wh licensing  interaction is   s  b    s  a      s  d    s  c    this represents how well the network learns both  parts of the licensing relationship. a positive whlicensing interaction means the model represents  a filler gap dependency between the wh word and  the gap site  a licensing interaction indistinguishable from zero indicates no such dependency. for  the purposes of brevity  we will give examples that  mirror item   d   above  but items of type   a      c  were also constructed in order to calculate the  full licensing interaction.  following standard practice in psycholinguistics  we derive the statistical significance of the  interaction from a mixed effects linear regression  model predicting surprisal given sum coded conditions  baayen et al.       . we include random  intercepts by item  random slopes are not neces     sary because we do not have repeated observations  within items and conditions  barr et al.       . in  our figures  error bars represent     confidence  intervals of the contrasts between conditions  computed by subtracting out the by item means before  calculating the intervals as advocated in masson  and loftus       .    although our method can indicate whether  there is a link between fillers and gaps  the relationship between language model probability and  grammaticality is complex  lau et al.        and  interpreting our patterns in terms of grammaticality judgments would require auxiliary assumptions  that we don t pursue here. to be clear  our goal  is to investigate whether rnns model the probabilistic dependencies between fillers and gaps at  all  not whether the outputs of such models can be  used to classify sentences as  grammatical  or not.         representation of filler gap  dependencies    the filler gap dependency has three basic characteristics. first  the relationship is flexible  whphrases can license gaps in diverse syntactic positions. second  the relationship is robust to intervening material  syntactic position  not linear  distance  determines grammaticality. third  the relationship is one to one  except in certain special  cases  one wh phrase licenses one gap. in this section  we demonstrate that the rnns have learned  these three properties of filler gap dependencies  by comparing their performance to a simple ngram baseline model.   .     flexibility of wh licensing    if the rnn has learned the flexibility of the filler   gap dependency  then we predict to find a whlicensing interaction when the gap appears in subject  object  and indirect object positions       a. i know who showed the presentation to  the visitors yesterday.  subj   b. i know what the businessman showed to  the visitors yesterday.  obj   c. i know who the businessman showed the  presentation to yesterday.  pp   to test the flexibility of the model s filler gap dependency representation  we created    test items  containing either an obligatorily ditransitive verb     our    studies were preregistered on aspredicted.org   to see the preregistrations go to aspredicted.org x.pdf  where x    md ax  hd df  mp dv  uu b   rj sk .     or a transitive verb with an obligatorily argumenttaking preposition  as in    . the obligatoriness of  verb and preposition transitivity was judged by the  authors. to control for the infrequent wh licensor   verb bigram when the gap is in subject position   in all cases the embedded clause was separated  from the wh phrase by either an adverbial  e.g.   despite protocol   or by words introducing a secondary embedded clause  e.g.  my brother said  .  for each item  we created three variants  subj  obj   and pp  corresponding to the items in example    .  the top row of figure   demonstrates how the  wh licensing interaction was calculated for this  experiment. the two panels at left show the main  effect of wh licensing  with surprisal in post gap  material shown in  a  and summed whole clause  surprisal in  b . the red bars indicate the effect of a  wh licensor on surprisal in the non gapped condition  or s  b  s  a   to use the example from  . .  the blue bars show the effect of a wh licensor on  surprisal in the gapped conditions  or s  d  s  c    to use the same example. the difference between  the red bars and the blue bars in each condition is  the licensing interaction  which is shown directly  in  c  and  d . not pictured are results from the  n gram baseline model  which yielded exactly    licensing interaction in all positions.  the bottom row of figure   shows a region byregion visualization of wh licensing interaction.  region by region behavior is consistent across  conditions  the licensing interaction spikes in the  immediate post gap material and returns to near  zero levels for the rest of the sentence. the height  of the licensing  spike  in each condition is equivalent to the size of the wh licensing interaction  in  c   and the difference between the bars in   a . meanwhile  the area under the  wh licensing  curve  is equivalent to the summed wh licensing  interaction shown in  d  and the difference between the bars in  b . all of these wh licensing interactions are significant  p    .    in all cases .  this experiment was designed to test whether licensing interaction exists in multiple syntactic positions  which we turn to now. in the post gap material  there is no significant difference in licensing  interaction between conditions. but when we sum  wh licensing interaction across the entire embedded clause model behavior does diverge. for the  gulordava model  there is no significant difference  between the three variants. for the google model  there is a significant reduction in licensing effect    between the subj and obj variants  p    .    and  the subj and pp variants  p    .    . the stronger  licensing effects for subject gaps indicates that the  networks have a stronger expectation for gaps in  this position. this matches human online processing results  in so far as gap expectation may be  one reason why subject extracted clauses are easier to process than other clauses  king and just        . overall  these experiments provide strong  evidence that both models are learning the filler   gap dependency. furthermore  both rnn models  are learning the flexibility of the dependency  as  they exhibit similar wh licensing effects for all  three argument roles tested.   .     robustness of wh licensing to  intervening material    all syntactic dependencies are robust to intervening material. in      the dependency is determined  by the syntactic relationship between the complementizer  what  and the position of the gap  modifying the subject doesn t change the relationship   and thus has no effect on filler gap licensing       a. i know what your friend gave to sam during the picnic yesterday.  b. i know what your new friend from the south  of france who only just arrived last week  gave to sam during the picnic yesterday.  having shown previously that rnns have expectations for filler gap dependencies  in this section we ask how well they are able to maintain  those expectations over intervening material. we  designed    sentences  like those in      with an  obligatorily transitive verb and either an indirect  object or a pp modifier. for each sentence we  produced four variants  a short modified version  with     extra intervening words between the whlicensor and the gap site  a medium version with      additional words and a long version  with     additional words. in all cases the extra material modified the subject of the embedded clause.  for each length gradation we produced two further variants  one in which the direct object was  extracted  obj  as in      and one variant in which  the indirect object or prepositional object was extracted  goal  where  sam  is in     . for each  variant  we measured the wh licensing interaction  in the post gap material and across the embedded  clause. treating the number of intervening words  as a continuous variable  we calculated the correlation between the length of the intervener and the      b                gap  no gap  gap      subj    obj    pp    subj    obj    pp    gulordava    whole clause    google    google    gulordava    gulordava          gap        no gap  gap                        subj    obj    pp    subj    obj    pp                               subj    obj    pp    gap location    subj    obj    pp    subj    obj    gap location    google    pp    subj    obj    pp    gap location    gulordava         gap position               subj         obj         pp    eo  s   .     un  c    h    s  e    th    th    e    pr  e                    af  te  rl    gu  es  t    tio         to               nt  a         sh  ow  ed    h         pr  ot  oc  ol  th  e  c  eo    at   w  th         de  sp  ite    no  w  ik    h         eo  s   .     s  e    th                                 un  c    to            gu  es  t         tio         nt  a  se    sh  ow  ed    pr  e  e  th    h  at   w            de  sp  ite    th    pr  ot  oc  ol  th  e  c  eo                 n            af  te  rl              n                       se         ik    wh licensing interaction    gap location    google     d   post gap material    wh licensing interaction    gulordava     c   wh main effect  whole clause    wh licensing interaction    google    surprisal with wh complimentizer vs. surprisal with that complimentizer    wh main effect  post gap material    no  w    surprisal with wh complimentizer vs. surprisal with that complimentizer     a     figure    wh licensing by syntactic position. charts  a  and  b  show the effect of wh licensors on surprisal   c  and   d  show the wh licensing interaction by syntactic position. the difference between the non gapped and gapped  conditions  red and blue bars  in  a  and  b  correspond to the total licensing interaction  or the height of the bars  in  c  and  d . the bottom chart displays wh licensing interaction summed across all words within each region.    strength of the wh licensing interaction. optimally  we would find zero correlation  a negative correlation indicates that the strength of the interaction  decays with increasing intervening words.  results of this study can be seen in figure  .  first  as a baseline  across the eight experiments  shown below  the average number of positive licensing interaction measurements was   .  . the  vast majority of the time  the presence of both a  filler and a gap reduced surprisal superadditively   producing a positive licensing interaction. moving  on to the effect of intervener length itself  for the  google model  intervener length was not a significant predictor of wh licensing interaction in any  of the conditions. for the gulordava model  intervener length was not a significant predictor of  wh licensing interaction size when measurements  were taken across the entire embedded clause. but  length did correlate with wh licensing interaction  size when measured in the post gap material for  the object position       .      p    .      and  goal position       .      p    .     . these extremely small effect sizes  combined with the otherwise mixed results from both models  indicate    that interveners do not consistently attenuate the  size of the licensing interaction.  while inconsistent with the formal linguistic literature on filler gap dependencies  the negative  values of all but one of the correlations are consistent with known effects in human sentence processing  where increasing distance between fillers  and gaps usually causes processing slowdown   grodner and gibson        bartek et al.       .  in the n gram baseline  all licensing effects are exactly zero  indicating the n gram model has no representation of the filler gap dependency.   .     multiple gaps    except for a few special cases  such as with acrossthe board  atb  movement and parasitic gaps  a  one to one relationship must be maintained between the wh phrase and the gap it licenses. the  presence of two gaps in   c  violates this one toone relationship  accounting for its relative badness compared to   a  and   b .      a. i know what the lion devoured at sunrise.  b. i know what devoured a mouse at sunrise.  c.  i know what devoured at sunrise.     obj position  post gap material     .    .                                                               google                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         google                                                 goal position  whole clause                                                                                                       gulordava                               length of modifier in words              gulordava                                                                                                                                                                                                                                                                                                                                                                                                                                     no gap          gap    no gap    gap    no gap    presence of upstream gap         gap    no gap    gap    presence of upstream gap                          length of modifier in words    figure    wh licensing interaction as a function of intervener length. zero is marked with a red line.    to test whether rnns have learned this one toone feature of wh licensing  we created    items  all with gaps in object position like those in       with two variants  one without a subject gap like    a   no subj gap  and one with a subject gap  as  in   c   subj gap . we took special care to use only  obligatorily transitive verbs. half of the test items  contained  what  and half  who  as wh licensors.  we measured the wh licensing interaction for the  two rnn models and the n gram model  in both  the post gap pp and across the embedded phrase.  figure   shows the results of this experiment.  first  the relatively high bars in the grammatical no subject gap condition is another example  of the rnn learning the filler gap dependency   the n gram baseline  not shown  exhibits no whlicensing interaction under this condition. for the  two lstms  the presence of an upstream gap increases surprisal in the target region  resulting in  a significantly lower licensing effect across the  board  p    .    in all conditions . meanwhile   the presence of a gap in the baseline condition results in no significant change in wh licensing interaction. overall these experiments demonstrate  that the lstms have learned the last of the three  main filler gap dependency characteristics  and   for the typical object position expect wh phrases  to be paired with only one gap.         gulordava         length of modifier in words              google                                   licensing interaction    licensing interaction    google                                 entire clause  gulordava         obj position  whole clause       google              length of modifier in words          post gap material    gulordava               wh licensing interaction               .     goal position  post gap material    gulordava         wh licensing interaction         licensing interaction    licensing interaction    google   .     syntactic islands    even though the filler gap dependency is flexible  and potentially unbounded  it is not entirely unconstrained. ross        identified five syntactic  positions in which gaps are illicit  dubbing them  syntactic islands. it remains an open question  whether these  island constraints  are true gram     figure    wh licensing interaction as a function of  double gapping  singly gapped sentences are shown  in red  doubly gapped sentences in blue. prepositional  phrases following the gap constitute post gap material.    matical constraints  or whether they are effects of  processing difficulty or discourse structural factors  ambridge and goldberg        hofmeister  and sag        sprouse and hornstein       .  in the following experiments  we examine  whether rnn language models have learned constraints on filler gap dependencies by comparing  the wh licensing interaction in non islands to that  within islands. the strongest evidence for an island constraint would be if the wh licensing interaction goes to zero for a gap in island position  implying that  in the distribution over strings  implied by the network  the appearance of a whlicensor is totally unrelated to the appearance of a  gap in the island position. more generally  we can  look for a weakened wh licensing interaction for  island vs. non island positions  which would mean  that the network believes a relationship between  the wh licensor and the island gap is less likely.  a positive but nonzero wh licensing interaction  would be in line with human acceptability judgments  which do not always categorically rule out  gaps in island positions  ambridge and goldberg          and with human online processing experiments  which have shown that gap expectation is  attenuated during processing of areas where gaps  cannot occur licitly  but does not always disappear entirely  stowe        traxler and pickering         phillips       . therefore  in this section we  take a significant reduction in the island relative to  the non island case to constitute evidence that the  model has  learned  the constraint.   .     wh island constraint    a gap cannot appear inside doubly nested  clauses headed by wh complementizers. this phe          a.    b.    c.    null comp  i know what alex said your friend devoured at  the party.  extraction from the object position of an embedded  clause with a null complementizer. no island violations.  that comp  i know what alex said that your friend devoured  at the party.  extraction from an embedded clause headed with  the complementizer  that.  no island violations.  wh comp   i know what alex said whether your friend devoured at the party.  extraction from an embedded clause headed with  the complementizer  whether.  whc violation.    to test whether our lstm language models have  learned this constraint  we constructed    items  following the conditions in    . we measured the  wh licensing interactions at the sentence final pp   as well as across the entire embedded clause for  both conditions.  figure   shows the wh licensing interaction  for both lstms  with non island conditions in  red and green and island conditions in blue. in  all conditions  extraction out of a wh island resulted in a significantly lower licensing interaction than extraction out of a null headed embedded clause  p    .   . for the google model  extraction out of an island resulted in significantly  lower wh licensing interaction than extraction out  of a that headed embedded clause  p    .       and while the gulordava model showed similar  behavior  none of the reductions were significant   p    .    for the post gap material and p    .     for the whole clause measurement . in all cases  there was no significant difference between extraction out of the two non island conditions  except  for in the gulordava model whole clause condition  where licensing interaction for the that comp  condition was significantly lower than the nullcomp condition  p    .    . these results indicate that the google model has learned the whisland constraint insofar as it has relatively similar expectations for extraction from null headed    post gap material    entire clause    google    google    gulordava    licensing interaction  whole clause                         .      .      .     p    p    om    p  m    m    at     c    co  h   w    th    p    co  l     om   c  at    th    nu  l    p    p    m    m    co    co    l     h     nu  l    m  co  h   w    presence of wh complementizer    w    p    p    m    om    co     c    l     at    nu  l    th    p    p  co     c    h     at    w    th    m    om    m  co  l     p     .   p    licensing interaction  post gap material    gulordava      .          nu  l    nomenon is called the wh island constraint   whc .     gives three sentences that demonstrate  this phenomenon. as these three sentence variants will serve as the basis for our experiment  we give each variant a condition name  on the  top  and a brief description below. we will use  this three row expository technique name  example  description for each of the island conditions tested in this section and use condition names  to label graphs and figures.    presence of wh complementizer    figure    effect of embedded clause complementizer  on wh licensing interaction. post gap material effect is  in the left panel  whole clause effect on the right panel.    and that headed clauses  which differ from from  its expectations about wh headed clauses. the gulordava model has learned wh islands  but gradiently  treating that headed embedded clauses as a  semi island condition.   .     adjunct island constraint    gaps cannot be licensed in an adjunct clause  as  demonstrated by the relative unacceptability of    b  and   c   compared to   a . we will refer to  this constraint as the adjunct constraint  ac .      a.    b.    c.    object  i know what the librarian in the dark blue  glasses placed on the wrong shelf.  material is extracted from the object position of the  embedded verb. no island violations.  adjunct back   i know what the patron got mad after the librarian placed on the wrong shelf.  material is moved from the object position of an  embedded sentential adjunct. ac violation.  adjunct front   i know what  after the librarian placed on the  wrong shelf  the patron got mad.  material is moved from an embedded sentential adjunct that has been fronted to before the main verb  of the embedded clause. ac violation.    to test whether rnns were sensitive to the ac  we devised    items following the variants in    .  filler material was added to the object condition  to control for sentence length across variants. we  used three different prepositions to construct temporal adjuncts   while    after  and  before . we  measured the wh licensing interaction in the postgap pp and across the entire embedded clause.  figure   shows the wh licensing interaction for  both models. for the google model there is a significant  p    .     reduction in wh licensing interaction between the object condition and the two  adjunct conditions when measurement is taken in  the post gap material. the difference in licensing     post gap material    entire clause    google    obj position  whole clause    google    gulordava    gulordava    google         subject position  whole clause    gulordava    google                   licensing interaction     .           licensing interaction    licensing interaction    licensing interaction        .     gulordava      .      .      .      .      .                  location of extraction domain    figure    effect of extraction site on wh licensing interaction for adjunct islands. post gap material effect is  in the left panel  whole clause effect on the right panel.    is also significant when measurements are taken  across the embedded clause  p    .   for the object adj front difference and p    .   for the object adj back difference . the gulordava model  shows similar results. in the post gap material   there is a significant difference when wh licensing  interaction is measured in the post gap material   p    .   for the object adj front difference  p     .   for the object adj back difference . results  are also significant when the whole embedded  clause is measured  p    .   for both differences .  to sum up  in all cases  the placement of a gap  within an adjunct results in a significantly lower  licensing interaction. this difference in licensing  interaction suggests that the models have learned  the ac inasmuch as they have attenuated expectations for wh licensing within sentential adjuncts.   .         a.    b.    object  i know what the family bought last year.  extraction of embedded clause object.  that rc obj   i know who the family bought the painting that  depicted last year.  extraction from  that  headed relative clause modifying embedded object. cnpc violation.    bj    bj    su    su    ep       pr    t    ub  j   s    c     rc     r    h     at    w    th    bj    ec  su  bj    su    su  c     ep       pr    t  ec    ub  j   s    rc     r    h     at    w    th    su  bj    j    j    ob    bj    ob    ep       pr    c      o  h     rc  th    at     r    j    ct  ob    je  w    j    ob    ob  c     ep       pr    ct    bj    je     o     r  at    w    location of extraction domain    location of extraction domain    figure    effect of extraction site location in complex  np islands on wh licensing interaction  measurement  taken across the whole embedded clause. object position is at left  subject position at right.    c.    d.    e.    f.    g.    complex np and subject islands    the complex np constraint  cnpc  holds that  a gap cannot be hosted in a sentential clause dominated by a noun phrase with a lexical head noun.  this constraint accounts for the unacceptability of    b     c     f  and   g  below. the cnpc does  not apply to other np modifiers  such as pps  unless the modified np occurs in subject position   huang       . this ban  called the subject constraint  sc   accounts for the unacceptability of    h  compared to   d .    th    h     rc    ob    location of extraction domain    bj    nt    ju  nc    tf    ro    n    ck    tio    ba  ct  ju  n    ad    ad    nt  ro    tp  os  i    tf  ju  nc    ec  o  bj    n    ck  ba  ct    ju  n    ad    ad    o  bj    ec    tp  os  i    tio    nt    ju  nc    tf    ro    n    ck  ad    ad    ju  n    ct    ba    tio    nt  ro    tp  os  i    tf  ju  nc    ec  o  bj    ba  ct  ju  n    ad    ad    o  bj    ec    tp  os  i    tio    n    ck     .     h.    wh rc obj   i know who the family bought the painting  which depicted last year.  extraction from  wh  headed relative clause modifying embedded object. cnpc violation  prep obj  i know who the family bought the painting by  last year.  extraction from pp attached to embedded object.  subject  i know what fetched a high price at auction.  extraction of embedded clause subject.  that rc subj   i know who the painting that depicted  fetched a high price at auction.  extraction from  that  headed relative clause modifying embedded subject. cnpc violation  wh rc subj   i know who the painting which depicted  fetched a high price at auction.  extraction from  wh  headed relative clause modifying embedded subject. cnpc violation.  prep subj   i know who the painting by fetched a high  price at auction.  extraction from pp attached to embedded subject.  sc violation.    to test whether rnns were sensitive to the cnpc  and sc  we constructed    items for the variants shown in      which resulted in   conditions.  for prep obj and prep subj special care was taken  to use prepositions that unambiguously attach to  the object and subject np  respectively. as post  gap material varied between variants  only wholeclause wh licensing interaction measurement is  given for this experiment.  results for object variants can be seen in the  left panel of figure    and results for the subject variants on the right. in all cases the comparatively large licensing interaction in non island  conditions  object and subject  shrinks when the  extracted material occurs inside a complex np      the middle bars in each chart . for the google  model the difference is significant for both cnp  islands when extraction occurs in object position   p    .    . for subject position  the difference is  significant when the rc is headed by a wh word   wh rc subj   p    .     but there is no significant  difference when the rc is headed by  that   as in  wh that subj. for the gulordava model  both differences are significant in subject  p    .    and  object position  p    .   . of the eight comparisons in   between cnpc islands and their nonisland counterparts  seven show significant reduction in wh licensing interaction. these differences  indicate that both lstms do not generally expect  extraction to occur from within complex nps.  however  the lstms demonstrate divergent licensing behavior when extraction occurs from out  of a prepositional phrase. if the models were learning the sc  we would expect no significant difference between object and prep obj  but a islandlike reduction in licensing interaction between the  subject and prep subj conditions. however  for the  google model there is no significant difference  in licensing interaction in any condition  and for  the gulordava model the difference is significant   p    .    in all cases. these results demonstrate  that neither model has learned the subject constraint  categorizing pps as either licit extraction  domains in all positions  the google model  or  treating them like islands  the gulordava model .         conclusion    we have provided evidence that state of the art  lstm language models have learned to represent filler gap dependencies and some of the constraints on them. these results capture the bidirectional nature of the dependency  due to the  fact that our measure wh licensing interaction   measures both the salutary effect of a gap given the  presence of an upstream filler  as well as the salutary effect of a filler given a gap. we found strong  licensing effects in both subject  object and indirect object locations  as well as an expectation that  the filler gap relationship was one to one and relatively unaffected by grammatically irrelevant interveners. the models also learned constraints on  the dependency  insofar as licensing effect shrank  when gaps were located in wh islands  adjunct  islands and most complex np islands  although  the subject constraint was not clearly learned and  some trace licensing interaction remained.    while the google model was trained on ten  times more data  contained ten times as many  hidden units and uses character cnn embeddings  its performance was not qualitatively more  human like than the gulordava model. both models failed to correctly generalize island constraints  in two conditions  the google model failed to  learn that headed complex np islands  the gulordava model to learn wh islands  and both failed to  learn subject islands. these results indicate that   beyond a certain point increased model size and  training regimen give diminishing returns.  in other recent work  chowdhury and zamparelli        tested the ability of neural networks  to separate grammatical from ungrammatical extractions using similar metrics to ours  finding that  their neural networks do not represent the unboundedness of filler gap dependencies nor certain strong island constraints. we believe the difference between our results and theirs is due to  experimental design  they choose to measure the  probability of the question mark punctuation as a  proxy for the rnns gap expectation  and use sentence schemata instead of hand engineered experimental items. while chowdhury and zamparelli         conclude that the networks are not learning island like constraints  but rather displaying  sensitivity to syntactic complexity plus order  we  demonstrate island like effects where both the island and the non island item are equally complex   in e.g. wh islands . note also that our work is focused on finding evidence that networks represent  the probabilistic contingencies implied by island  constraints  without attempting to directly model  grammaticality judgments.  our work shows these dependencies and their  constraints can be learned to some extent by a  generic sequence model with no obvious inductive  bias for hierarchical structures. this is evidence  against the idea that such an inductive bias is necessary for language learning  although the amount  of data these models are trained on is much larger  than the typical input to a child learner.    acknowledgements  all experimental materials and scripts are available at https     osf.io zpfxm . egw would like to acknowledge support from the mind brain behavior graduate student grant   as well as emmanuel dupoux and the cognitive machine  learning group at the ens. rpl gratefully acknowledges  support to his laboratory from elemental cognition and from  the mit ibm watson ai lab. this work was supported by a  gpu grant from the nvidia corporation.     