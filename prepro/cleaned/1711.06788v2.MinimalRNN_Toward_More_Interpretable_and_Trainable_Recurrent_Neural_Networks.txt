introduction    recurrent neural networks have been widely applied in modeling sequence data in various domains   such as language modeling             translation      speech recognition     and recommendation  systems        . among them  long short term memory networks  lstm      and gated recurrent  units  gru      are the most prominent model architectures. despite their impressive performance   the intertwine and recurrent nature of update rules used by these networks has prevented us from  gaining thorough understanding of their strengths and limitations    .  recent work on chaos free networks  cfn       inspected these popular networks from a dynamical  system viewpoint and pointed out that existing rnns  including vanilla rnns  lstm and grus   intrinsically embody irregular and unpredictable dynamics. even without interference from input   external  data  the forward trajectories of states in these networks attract to very different points with  a small perturbation of the initial state. take grus as an example  it updates its states over time as  follows   ht   ut    ht          ut      tanh wh  rt    ht       wx xt   bh             where ut and rt are the update and reset gates respectively. the authors identified the multiplication  wh  rt ht     in the second part of the update  i.e.  mixing of different dimensions in the hidden  state  as the cause of the chaotic behavior. to address that  the authors proposed cfn  which updates  its hidden states as  ht   ut    tanh ht       it    tanh wx xt   bx  .           here ut and it are the update and input gates. by ruling out the mixing effect between the different  dimensions in the hidden state  the network presents a much more predictable dynamic. the simpler  network achieves comparable performance as the more dynamically complex lstms or grus for  various language modeling tasks.  inspired by the success of cfn  we propose another recurrent neural network architecture named  minimal recurrent neural networks  minimalrnn   which adopts minimum number of operations  interpretable ml symposium    st conference on neural information processing systems  nips        long  beach  ca  usa.     within rnn without sacrificing performance. simplicity not only brings efficiency  but also interpretability and trainability. there have been evidences that favorable learning dynamic in deep  feed forward networks arises from input output jacobians whose singular values are o            .  we empirically study the input output jacobians in the scope of recurrent neural networks and show  that minimalrnn is more trainable than existing models. it is able to propagate information back to  steps further back in history  that is  capturing longer term dependency.         method    figure   illustrates the new model architecture named minimalrnn. it trades the complexity of the  recurrent neural network with having a small network outside to embed the inputs and take minimal  operations within the recurrent part of the model.  at each step t  the model first maps its input xt to a  latent space through  zt     xt         here can be any highly flexible functions such  as neural networks. in our experiment  we take       as a fully connected layer with tanh activation. that  is    xt     tanh wx xt   bz  .  given the latent representation zt of the input  minimalrnn then updates its states simply as   ht   ut    ht          ut      zt           where ut     uh ht     uz zt   bu   is the update  figure    model architecture of minimalgate.  rnn.  latent representation. the dynamic of minimalrnn prescribed by these updates is fairly straight forward. first  the encoder      defines the latent  space. the recurrent part of the model is then confined to move within this latent space. at each  step t  minimalrnn takes its previous state ht   and the encoded input zt   then simply outputs  a weighted average of both depending on the gate ut . that is  dimension i of the rnn state hit is  activated by input zti and relax toward zero without any new input from that dimension. the rate of  relaxation is determined by the gate uit . it gets reactivated once it sees zti again.  comparing with lstm  gru or cfn  minimalrnn resorts to a much simpler update inside the  recurrent neural network. it retains the gating mechanism  which is known to be critical to preserve  long range dependencies in rnns. however  only one gate remains. the update rule bears some  similarity to that of cfn  in that both forbid the mixing between different dimensions of the state.  trainability. recurrent neural networks are notoriously hard to train due to gradient explosion and  vanishing        . several recent works              study information propagation in deep networks  and suggest that well conditioned input output jacobians leads to desirable learning dynamic in  deep neural networks. in particular  if every singular value of the input output jacobians remains  close to   during learning  then any error vector will preserve its norm back propagating through the  network. as a result  the gradient will neither explode nor vanishing. thanks to the simple update  rule employed in minimalrnn  we can easily write out the input output jacobian  i.e.  derivatives of  rnn state ht w.r.t. input xt k as follows         y   ht   h  i    ht k  zt k             xt k   hi    zt k  xt k  t k i t    where     hi   hi           dui   d hi    zi      ui    ui   uh    here du denotes a diagonal matrix with vector u as the diagonal entries. assuming the weight matrix  uh is unitary  that is  the singular values of uh are all    then we can easily see that the maximum   hi  t k  singular value of  h  is bounded by  . similarly for  h   zt k .  i          figure    map    evaluated on the test sets progressed over   m learning steps.  in comparison  the jacobian of gru is much more complex    hi  wh  dri   dhi   ri    ri   rh          dui   d hi    z i   ui    ui   uh   d   ui      z    i     hi    here z i   tanh wh  rt hi       wx xi   bh   and rh is the weight matrix used in the reset gate  of gru. the jacobian has the additional multiplication term between wh and rh in each layer   which we hypothesize will result in grus more prone to exploding or vanishing gradient.         experiments    we demonstrate the efficacy of our method on a recommendation task of production scale. the goal  is to recommend to users items of interest given user s historical interactions with items in the system.  dataset. the dataset contains hundreds of millions of user records. each one is a sequence of  itemid   pageid  time  tuples  recording the context under which a recommended item consumed by an user.  we consider user activities up to several months and truncate the sequence to a maximum length of     . the item vocabulary contains   million most popular items of the last    hours.  setup. our production system uses a gru as the  core mechanism to capture the evolving of user interest through time. the model takes a sequence of user  actions on items  and aims to predict the next item  the user is going to consume. we replace the gru  component with various recurrent neural networks architectures  such as vanilla rnn  cfn and minimalrnn  and compare their performance to the production system. the main performance metric that we  monitor offline is the mean average precision   .  performance. figure   plots the map    of the  recommender system with different recurrent neural  networks over   m learning steps. all the weights in  the recurrent neural nets are initialized to be unitary.  as our data is refreshed daily  we were able to compare these methods over multiple datasets. figure    left and middle show two runs. in both cases  vanilla  rnn failed during early stage of the learning due to  gradient explosion. the other three models perform  rather similar  reaching map    of  .  . since the  update in minimalrnn is much simpler  it takes less  time to train. learning finished in    hours comparing with cfn which takes    hours and    hours for  gru  as shown in the right column of figure  .    figure     top . weight matrix wx that transform input to latent space   bottom . weight  matrix uh that computes the update gate according to previous hidden state ht   .    latent representation. in this experiment  we attempt to look inside the hidden states learned by minimalrnns. our intuition is that the stricter updates in minimalrnn forces its states to reside in  some latent space defined by the input encoder     . each dimension of the state space focuses on  some factor of the input  but not the others.        vanilla    gru    cfn    minimal   a  k       b  k       c  k       figure    histograms of the singular values of input output jacobians  initial point  with weight matrices random initialized to be unitary.     d  k      ht   xt  k    for k                at    the first row of figure   plots the weight matrix wx  that is used to transform the input to the latent  space. each row is one input dimension and each column is one dimension in the latent space. entry   i  j  indicates the activation of input feature i on the latent dimension j. the input are grouped by  blocks  the first block is the item embedding  and second block is the page embedding  etc.. we can  see that most of the latent dimensions are used to capture the item embedding  while remaining ones  capture the other contexts  such as the page on which the item is displayed  the software the user used  to access the item  and time information. the bottom row of figure   plots the weight matrix uh  that is used to compute the update gate. each entry  i  j  indicates the activation of previous state  hjt   on the forget gate entry uit . it shows several interesting properties. first  we observe strong  activations on the diagonal. that is  the rate of forgetting depends mostly on the previous state from  the same dimension  hit     uit . second  we can observe several dimensions  columns  having  strong activations across all rows. in other words  these dimensions impact the rate of forgetting  for almost all the dimensions in the hidden states  and these dimensions mostly corresponds to the  dimensions that are capturing the context information as shown in the top of the figure.  trainability. in these experiments  we take the input output jacobians computed from different  recurrent neural networks at initial point and during learning to understand the trainability of these  rnn structures. figure   plots the histogram of the singular values of the jacobian matrix over  various k at initial point. all the weights in the networks are initialized to be unitary. when k       we are looking at the derivatives of the rnn hidden state w.r.t. the current input  while k       depicts the derivatives w.r.t. input that is    step back. we can see that the singular values of the  input output jacobians in vanilla rnn quickly vanishes towards zero as k increases. the singular  values of the input output jacobians for the grus starts to stretch in some directions  and shrink in  others when k reaches     which we hypothesize is due to the additional multiplication as shown in  equation    . the input output jacobians of cfn and minimalrnn on the other hand are relatively  well conditioned even for k     . especially for minimalrnn  the singular values stay normally  distributed as k increases  and neither stretching nor shrinking in any directions.  as pointed out in       a good initialization does not necessarily guarantee trainability. figure    plots the distribution of the singular values of the jacobian matrices throughout the whole learning  process. as learning of vanilla rnn failed quite early  we ignore it in the comparison. we can see  that the singular values of the jacobian matrix in gru grows rapidly in some iterations  suggesting  the back propagation error could be stretching over those directions. the singular values of the  jacobians in cfn are shrinking mostly toward   as learning goes on. in comparison  the jacobians of         a  gru     b  cfn     c  minimalrnn    figure    distribution of singular values of input output jacobian  x ht t k at k       first row  and  k       second row  during   m learning steps. each line on the chart represents a percentile in the  distribution over the data  for example  the top line shows how the maximum value has changed over  time  and the line in the middle shows how the median has changed. reading from top to bottom  the  lines have the following meaning   maximum                                    minimum .    minimalrnn are relatively well conditioned throughout the learning. we can observe similar trends  for different values of k  k     . these results suggest that minimalrnn could be able to capture  input far back in the history  i.e.  longer range dependencies.         future work    it remains to be seen if this extremely simple recurrent neural network architecture is able to carry  over to a wide range of tasks besides the one presented here. our most performant model for this task  so far only uses one fully connected layers in     . it will be interesting to find data of more complex  input patterns that will require us to increase the capacity in the input encoder  . we would like to  build upon recent success of understanding information propagation in deep networks using random  matrix theory      to further study learning dynamics in these recurrent neural networks.    