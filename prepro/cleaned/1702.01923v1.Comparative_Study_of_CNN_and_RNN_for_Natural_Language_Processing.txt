introduction    natural language processing  nlp  has benefited  greatly from the resurgence of deep neural networks  dnns   due to their high performance  with less need of engineered features. there are  two main dnn architectures  convolutional neural network  cnn   lecun et al.        and recurrent neural network  rnn   elman       . gating mechanisms have been developed to alleviate  some limitations of the basic rnn  resulting in  two prevailing rnn types  long short term memory  lstm   hochreiter and schmidhuber         and gated recurrent unit  gru   cho et al.       .  generally speaking  cnns are hierarchical and  rnns sequential architectures. how should we  choose between them for processing language   based on the characterization  hierarchical  cnn   vs. sequential  rnn    it is tempting to choose a  cnn for classification tasks like sentiment classification since sentiment is usually determined by  some key phrases  and to choose rnns for a se     quence modeling task like language modeling as  it requires flexible modeling of context dependencies. but current nlp literature does not support such a clear conclusion. for example  rnns  perform well on document level sentiment classification  tang et al.         and dauphin et al.         recently showed that gated cnns outperform lstms on language modeling tasks  even  though lstms had long been seen as better suited.  in summary  there is no consensus on dnn selection for any particular nlp problem.  this work compares cnns  grus and lstms  systematically on a broad array of nlp tasks  sentiment relation classification  textual entailment   answer selection  question relation matching in  freebase  freebase path query answering and partof speech tagging.  our experiments support two key findings.  i   cnns and rnns provide complementary information for text classification tasks. which architecture performs better depends on how important it is to semantically understand the whole sequence.  ii  learning rate changes performance  relatively smoothly  while changes to hidden size  and batch size result in large fluctuations.         related work    to our knowledge  there has been no systematic  comparison of cnn and rnn on a large array of  nlp tasks.  vu et al.        investigate cnn and basic  rnn  i.e.  no gating mechanisms  for relation  classification. they report higher performance  of cnn than rnn and give evidence that cnn  and rnn provide complementary information   while the rnn computes a weighted combination of all words in the sentence  the cnn extracts the most informative ngrams for the relation and only considers their resulting activations.      a  cnn     b  gru     c  lstm    figure    three typical dnn architectures  both wen et al.        and adel and schu tze         support cnn over gru lstm for classification of long sentences. in addition  yin et al.         achieve better performance of attentionbased cnn than attention based lstm for answer selection. dauphin et al.        further argue  that a fine tuned gated cnn can also model longcontext dependency  getting new state of the art in  language modeling above all rnn competitors  in contrast  arkhipenko et al.        compare  word vec  mikolov et al.         cnn  gru and  lstm in sentiment analysis of russian tweets   and find gru outperforms lstm and cnn.  in empirical evaluations  chung et al.         and jozefowicz et al.        found there is no clear  winner between gru and lstm. in many tasks   they yield comparable performance and tuning hyperparameters like layer size is often more important than picking the ideal architecture.         models    this section gives a brief introduction of cnn   gru and lstm.   .     convolutional neural network  cnn     input layer sequence x contains n entries.  each entry is represented by a d dimensional  dense vector  thus the input x is represented as a  feature map of dimensionality d   n. figure   a   shows the input layer as the lower rectangle with  multiple columns.  convolution layer is used for representation  learning from sliding w grams. for an input se     quence with n entries  x    x    . . .   xn   let vector ci   rwd be the concatenated embeddings  of w entries xi w     . . .   xi where w is the filter width and     i   s   w. embeddings  for xi   i     or i   n  are zero padded. we  then generate the representation pi   rd for  the w gram xi w     . . .   xi using the convolution  weights w   rd wd    pi   tanh w   ci   b            where bias b   rd .  maxpooling all w gram representations pi   i           s   w      are used to generate the  representation of input sequence x by maxpooling  xj   max p  j   p  j            j              d .   .     gated recurrent unit  gru     gru  as shown in figure   b   models text x as  follows   z     xt uz   ht   wz    r           r    r     xt u   ht   w    s         s    st   tanh xt u    ht     r w             ht        z    st   z   ht             xt   rd represents the token in x at position t   ht   rh is the hidden state at t  supposed to encode the history x            xt . z and r are two gates.  all u   rd h  w   rh h are parameters.      .     long short time memory  lstm     lstm is denoted in figure   c . it models the  word sequence x as follows   it     xt ui   ht   wi   bi    f    f    o    o    ft     xt u   ht   w   bf    ot     xt u   ht   w   bo    q    q    qt   tanh xt u   ht   w   bq                            pt   ft   pt     it   qt            ht   ot   tanh pt              lstm has three gates  input gate it   forget gate  ft and output gate ot . all gates are generated by  a sigmoid function over the ensemble of input xt  and the preceding hidden state ht   . in order to  generate the hidden state at current step t  it first  generates a temporary result qt by a tanh nonlinearity over the ensemble of input xt and the preceding hidden state ht     then combines this temporary result qt with history pt   by input gate it  and forget gate ft respectively to get an updated  history pt   finally uses output gate ot over this updated history pt to get the final hidden state ht .        .     experiments  tasks    sentiment classification  sentic  on stanford  sentiment treebank  sst   socher et al.       .  this dataset predicts the sentiment  positive or  negative  of movie reviews. we use the given split  of      train      dev and      test sentences.  as in  kalchbrenner et al.        le and mikolov          we treat labeled phrases that occur as subparts of training sentences as independent training  instances. measure  accuracy.  relation classification  rc  on semeval       task    hendrickx et al.       . it consists of sentences which have been manually labeled with     relations     directed relations and other         sentences in train and      in test. as there is  no dev set  we use      training examples as dev   similar to vu et al.       . measure  f .  textual entailment  te  on stanford natural language inference  snli   bowman et al.        . snli contains premise hypothesis pairs   labeled with a relation  entailment  contradiction   neutral . after removing unlabeled pairs  we end  up having         pairs for train        for dev  and       for test. measure  accuracy.  answer selection  as  on wikiqa  yang  et al.         an open domain question answer    dataset. we use the subtask that assumes that there  is at least one correct answer for a question. the  corresponding dataset consists of        questioncandidate pairs in train        in dev and        in test where we adopt the standard setup of only  considering questions with correct answers in test.  the task is to choose the correct answer s  from  some candidates for a question. measures  map  and mrr.  question relation match  qrm . we utilize webqsp  yih et al.        dataset to create  a large scale relation detection task  benefitting  from the availability of labeled semantic parses  of questions. for each question  we  i  select the  topic entity from the parse   ii  select all the relations relation chains  length      connecting to  the topic entity  and  iii  set the relations relationchains in the labeled parse as positive and all the  others as negative. following yih et al.        and  xu et al.         we formulate this task as a sequence matching problem. ranking loss is used  for training. measure  accuracy.  path query answering  pqa  on the path  query dataset released by guu et al.       . it  contains kb paths like eh   r    r            rt   et   where  head entity eh and relation sequence r    r            rt  are encoded to predict the tail entity et . there are                           paths in train dev test   respectively. measure  hit   .  part of speech tagging on wsj. we use the  setup of  blitzer et al.        petrov and mcdonald         sections      are train  section    is  dev and section    is test. measure  accuracy.  we organize above tasks in four categories.  i   textc. text classification  including sentic and  rc.  ii  semmatch including te  as and qrm.   iii  seqorder. sequence order  i.e.  pqa.  iv   contextdep. context dependency  i.e.  pos tagging. by investigating these four categories  we  aim to discover some basic principles involved in  utilizing cnns   rnns.   .     experimental setup    to fairly study the encoding capability of different basic dnns  our experiments have the following design.  i  always train from scratch  no extra knowledge  e.g.  no pretrained word embeddings.  ii  always train using a basic setup without  complex tricks such as batch normalization.  iii   search for optimal hyperparameters for each task  and each model separately  so that all results are     cnn  gru  lstm  textc  cnn  rc  f    gru  lstm  cnn  te  acc   gru  lstm  cnn  semmatch as  map   mrr   gru  lstm  cnn  qrm  acc   gru  lstm  cnn  seqorder  pqa  hit      gru  lstm  cnn  gru  lstm  contextdep pos tagging  acc   bi gru  bi lstm  sentic  acc     performance    .      .      .      .      .      .      .      .      .       .     .        .     .        .     .       .      .      .      .      .      .      .      .      .      .      .      lr   .    .    .    .     .     .    .    .    .    .     .    .    .      .    .    .     .    .    .    .    .    .    .     hidden                                                                                                         batch sentlen filter size                                                                                                                                                                                                                                                         margin                              .    .    .    .     .     .     .    .    .                    table    best results or cnn  gru and lstm in nlp tasks  gold cnn gru examples  t  f  t it  s a movie   and an album   you wo n t want to miss  f  t  f these are names to remember   in order to avoid them in the future  in the second half of the film   frei  s control loosens in direct proportion to the  f  f  t  amount of screen time he gives nachtwey for self analysis  t  t  f the result is mesmerizing   filled with menace and squalor  table    cnn vs gru in sentiment classification  based on optimal hyperparameters.  iv  investigate the basic architecture and utilization of each  model  cnn consists of a convolution layer and  a max pooling layer  gru and lstm model the  input from left to right and always use the last hidden state as the final representation of the input.  an exception is for pos tagging  we also report  bi directional rnns as this can make sure each  word s representation can encode the word s context of both sides  like the cnn does.  hyperparameters are tuned on dev  hidden size   minibatch size  learning rate  maximal sentence  length  filter size  for cnn only  and margin in  ranking loss in as  qrm and pqa tasks.     .     results   analysis    table   shows experimental results for all tasks  and models and corresponding hyperparameters.  for textc  gru performs best on sentic and  comparably with cnn in rc. for semmatch   cnn performs best on as and qrm while gru   and also lstm  outperforms cnn on te. for  seqorder  pqa   both gru and lstm outperform cnn. for contextdep  pos tagging   cnn  outperforms one directional rnns  but lags behind bi directional rnns.  the results for seqorder and contextdep are  as expected  rnns are well suited to encode order information  for pqa  and long range context  dependency  for pos tagging . but for the other      .      .    train  dev  test     .      .    .    .    .     acc    proportion     .       .       .     cnn  gru  gru cnn     .    .       .    .      .         .                                                                                                                                    sentence length ranges    sentence lengths    figure    distributions of sentence lengths  left  and accuracies of different length ranges  right .   .       .    cnn  gru  lstm     .     .     cnn  gru  lstm     .       .    .     .     .       .      acc    acc    acc     .    .      .       .     .       .     .      .     .    .    cnn  gru  lstm     .    .        .        .       .       .      .      .    .     .           .                                                                           learning rate                            hidden size                                   batch size     .       .       .      cnn  gru  lstm     .       .       .     .      .      .    .      mrr    mrr    mrr     .     .       .     .       .       .       .     .      .       .   cnn  gru  lstm     .     .        .        .       .       .      .      .          cnn  gru  lstm     .      .                                    learning rate                      hidden size                                               .                                                                           batch size    figure    accuracy for sentiment classification  top  and mrr for wikiqa  bottom  as a function of  three hyperparameters  learning rate  left   hidden size  center  and batch size  right .  two categories  textc and semmatch  some unexpected observations appear. cnns are considered good at extracting local and position invariant  features and therefore should perform well on  textc  but in our experiments they are outperformed by rnns  especially in sentic. how can  this be explained  rnns can encode the structuredependent semantics of the whole input  but how  likely is this helpful for textc tasks that mostly  depend on a few local regions  to investigate the  unexpected observations  we do some error analysis on sentic.  qualitative analysis table   shows examples            in which cnn predicts correctly while  gru predicts falsely or vice versa. we find that  gru is better when sentiment is determined by  the entire sentence or a long range semantic de     pendency   rather than some local key phrases    is involved. example     contains the phrases   won t  and  miss  that usually appear with negative sentiment  but the whole sentence describes a  positive sentiment  thus  an architecture like gru  is needed that handles long sequences correctly.  on the other hand  modeling the whole sentence  sometimes is a burden   neglecting the key parts.  the gru encodes the entire word sequence of the  long example      making it hard for the negative  keyphrase  loosens  to play a main role in the final  representation. the first part of example     seems  positive while the second part seems negative. as  gru chooses the last hidden state to represent the  sentence  this might result in the wrong prediction.  studying acc vs sentence length can also support this. figure    left  shows sentence lengths in  sst are mostly short in train while close to nor      mal distribution around    in dev and test. figure    right  depicts the accuracies w.r.t length  ranges. we found that gru and cnn are comparable when lengths are small  e.g.       then gru  gets increasing advantage over cnn when meet  longer sentences. error analysis shows that long  sentences in sst mostly consist of clauses of inverse semantic such as  this version is not classic  like its predecessor  but its pleasures are still plentiful . this kind of clause often include a local  strong indicator for one sentiment polarity  like  is  not  above  but the successful classification relies  on the comprehension of the whole clause.  hence  which dnn type performs better in text  classification task depends on how often the comprehension of global long range semantics is required.  this can also explain the phenomenon in semmatch   gru lstm surpass cnn in te while  cnn dominates in as  as textual entailment relies on the comprehension of the whole sentence   bowman et al.         question answer in as instead can be effectively identified by key phrase  matching  yin et al.       .  sensitivity to hyperparameters we next check  how stable the performance of cnn and gru are  when hyperparameter values are varied. figure    shows the performance of cnn  gru and lstm  for different learning rates  hidden sizes and batch  sizes. all models are relativly smooth with respect  to learning rate changes. in contrast  variation in  hidden size and batch size cause large oscillations.  nevertheless  we still can observe that cnn curve  is mostly below the curves of gru and lstm in  sentic task  contrarily located at the higher place  in as task.         conclusions    this work compared the three most widely used  dnns   cnn  gru and lstm   in representative sample of nlp tasks. we found that rnns  perform well and robust in a broad range of tasks  except when the task is essentially a keyphrase  recognition task as in some sentiment detection  and question answer matching settings. in addition  hidden size and batch size can make dnn  performance vary dramatically. this suggests that  optimization of these two parameters is crucial to  good performance of both cnns and rnns.    