word order preferences?
richard futrell1 and roger p. levy2
1 department
2 department

of language science, uc irvine, rfutrell@uci.edu
of brain and cognitive sciences, mit, rplevy@mit.edu

arxiv:1811.01866v1 [cs.cl] 5 nov 2018

abstract
rnn language models have achieved state-ofthe-art results on various tasks, but what exactly they are representing about syntax is as
yet unclear. here we investigate whether rnn
language models learn humanlike word order
preferences in syntactic alternations. we collect language model surprisal scores for controlled sentence stimuli exhibiting major syntactic alternations in english: heavy np shift,
particle shift, the dative alternation, and the
genitive alternation. we show that rnn language models reproduce human preferences
in these alternations based on np length, animacy, and definiteness. we collect human
acceptability ratings for our stimuli, in the
first acceptability judgment experiment directly manipulating the predictors of syntactic alternations. we show that the rnnsâ€™ performance is similar to the human acceptability
ratings and is not matched by an n-gram baseline model. our results show that rnns learn
the abstract features of weight, animacy, and
definiteness which underlie soft constraints on
syntactic alternations.

