
the process of designing neural architectures requires expert knowledge and
extensive trial and error. while automated architecture search may simplify these
requirements, the recurrent neural network (rnn) architectures generated by
existing methods are limited in both flexibility and components. we propose a
domain-specific language (dsl) for use in automated architecture search which
can produce novel rnns of arbitrary depth and width. the dsl is flexible
enough to define standard architectures such as the gated recurrent unit and
long short term memory and allows the 