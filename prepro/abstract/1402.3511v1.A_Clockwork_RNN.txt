
sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. recurrent neural networks (rnns) have the ability,
in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections.
however, in practice they are difficult to train successfully when the long-term memory is required.
this paper introduces a simple, yet powerful modification to the standard rnn architecture, the
clockwork rnn (cw-rnn), in which the hidden
layer is partitioned into separate modules, each
processing inputs at its own temporal granularity,
making computations only at its prescribed clock
rate. rather than making the standard rnn models more complex, cw-rnn reduces the number
of rnn parameters, improves the performance
significantly in the tasks tested, and speeds up the
network evaluation. the network is demonstrated
in preliminary experiments involving two tasks:
audio signal generation and timit spoken word
classification, where it outperforms both rnn
and lstm networks.

1. 