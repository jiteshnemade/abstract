we investigate online nonlinear regression with
continually running recurrent neural network networks (rnns),
i.e., rnn-based online learning. for rnn-based online learning,
we introduce an efficient first-order training algorithm that
theoretically guarantees to converge to the optimum network
parameters. our algorithm is truly online such that it does not
make any assumption on the learning environment to guarantee convergence. through numerical simulations, we verify
our theoretical results and illustrate significant performance
improvements achieved by our algorithm with respect to the
state-of-the-art rnn training methods.
index termsâ€”online learning, neural network training, recurrent neural networks, sequential learning, regression, online
gradient descent.

i. 