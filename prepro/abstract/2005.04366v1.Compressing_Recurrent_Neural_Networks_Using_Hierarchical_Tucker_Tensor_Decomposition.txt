
recurrent neural networks (rnns) have been
widely used in sequence analysis and modeling.
however, when processing high-dimensional data,
rnns typically require very large model sizes,
thereby bringing a series of deployment challenges.
although the state-of-the-art tensor decomposition
approaches can provide good model compression
performance, these existing methods are still suffering some inherent limitations, such as restricted
representation capability and insufficient model
complexity reduction. to overcome these limitations, in this paper we propose to develop compact
rnn models using hierarchical tucker (ht) decomposition. ht decomposition brings strong hierarchical structure to the decomposed rnn models, which is very useful and important for enhancing the representation capability. meanwhile, ht
decomposition provides higher storage and computational cost reduction than the existing tensor
decomposition approaches for rnn compression.
our experimental results show that, compared with
the state-of-the-art compressed rnn models, such
as tt-lstm, tr-lstm and bt-lstm, our proposed ht-based lstm (ht-lstm), consistently
achieves simultaneous and significant increases in
both compression ratio and test accuracy on different datasets.

1

