

arxiv:1706.03542v1 [cs.cl] 12 jun 2017

recent work has explored the syntactic
abilities of rnns using the subject-verb
agreement task, which diagnoses sensitivity to sentence structure. rnns performed
this task well in common cases, but faltered in complex sentences (linzen et al.,
2016). we test whether these errors are
due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus. we trained a single rnn to perform both the agreement
task and an additional task, either ccg supertagging or language modeling. multitask training led to significantly lower error rates, in particular on complex sentences, suggesting that rnns have the
ability to evolve more sophisticated syntactic representations than shown before.
we also show that easily available agreement training data can improve performance on other syntactic tasks, in particular when only a limited amount of training data is available for those tasks. the
multi-task paradigm can also be leveraged
to inject grammatical knowledge into language models.

1

