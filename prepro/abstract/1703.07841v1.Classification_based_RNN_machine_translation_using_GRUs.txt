
we report the results of our classification-based machine translation model, built upon the framework
of a recurrent neural network using gated recurrent units. unlike other rnn models that attempt to
maximize the overall conditional log probability of sentences against sentences, our model focuses
a classification approach of estimating the conditional probability of the next word given the input
sequence. this simpler approach using grus was hoped to be comparable with more complicated
rnn models, but achievements in this implementation were modest and there remains a lot of room
for improving this classification approach.

1

