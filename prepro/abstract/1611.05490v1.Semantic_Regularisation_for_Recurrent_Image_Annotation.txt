

ference of producing a complete natural language sentence
description conditioned on the image content, rather than a
simple unordered set of labels. for both problems an effective model needs to fulfil two closely-related tasks well:
predicting a set of visual concept labels and modelling interlabel correlations. for label-correlation modelling, structured learning strategies are typically employed, which in
the case of multi-label classification helps to better distinguish visually ambiguous concepts as well as suppress false
predictions (e.g., modelling the car-sky-sea correlation can
rectify false prediction of sea in place of sky when a car is
present). for image captioning, structured learning is even
more critical to generate an ordered list of words that encode a valid as well as relevant sentence.

the “cnn-rnn” design pattern is increasingly widely
applied in a variety of image annotation tasks including
multi-label classification and captioning. existing models
use the weakly semantic cnn hidden layer or its transform
as the image embedding that provides the interface between
the cnn and rnn. this leaves the rnn overstretched with
two jobs: predicting the visual concepts and modelling their
correlations for generating structured annotation output.
importantly this makes the end-to-end training of the cnn
and rnn slow and ineffective due to the difficulty of back
propagating gradients through the rnn to train the cnn.
we propose a simple modification to the design pattern that
makes learning more effective and efficient. specifically, we
propose to use a semantically regularised embedding layer
as the interface between the cnn and rnn. regularising
the interface can partially or completely decouple the learning problems, allowing each to be more effectively trained
and jointly training much more efficient. extensive experiments show that state-of-the art performance is achieved on
multi-label classification as well as image captioning.

recently, the convolutional neural network – recurrent
neural network (cnn-rnn) encoder-decoder design pattern has become popular to address the structured label prediction task in both multi-label classification [14, 31] and
image captioning [29, 30, 33, 35]. a cnn is used to encode the image into a fixed length vector, which is then
fed into an rnn that either decodes it into a list of tags
(multi-label) or sequence of words composing a sentence
(captioning). with this encoder-decoder architecture, the
cnn and rnn can be trained end-to-end, inputting an image and outputting an ordered list of labels. existing work
differs slightly in how the cnn and rnn models are interfaced (see figs. 1(a)-(c)). however, they share a key characteristic: the image embedding that provides the cnn-rnn
interface is the final feature layer of the cnn [14, 22, 31]
(e.g. the fc7 layer of alexnet [18] or the final pooling layer
of googlenet [27]) or its linear transform [29, 30].

1. 