
neural networks have a remarkable capacity for
contextual processing—using recent or nearby inputs to modify processing of current input. for
example, in natural language, contextual processing is necessary to correctly interpret negation
(e.g. phrases such as “not bad”). however, our
ability to understand how networks process context is limited. here, we propose general methods for reverse engineering recurrent neural networks (rnns) to identify and elucidate contextual
processing. we apply these methods to understand rnns trained on sentiment classification.
this analysis reveals inputs that induce contextual
effects, quantifies the strength and timescale of
these effects, and identifies sets of these inputs
with similar properties. additionally, we analyze
contextual effects related to differential processing of the beginning and end of documents. using
the insights learned from the rnns we improve
baseline bag-of-words models with simple extensions that incorporate contextual modification,
recovering greater than 90% of the rnn’s performance increase over the baseline. this work
yields a new understanding of how rnns process
contextual information, and provides tools that
should provide similar insight more broadly.

1. 