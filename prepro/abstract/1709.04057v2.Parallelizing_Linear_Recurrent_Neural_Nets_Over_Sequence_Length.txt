
recurrent neural networks (rnns) are widely used to model sequential data but
their non-linear dependencies between sequence elements prevent parallelizing
training over sequence length. we show the training of rnns with only linear
sequential dependencies can be parallelized over the sequence length using the
parallel scan algorithm, leading to rapid training on long sequences even with small
minibatch size. we develop a parallel linear recurrence cuda kernel and show
that it can be applied to immediately speed up training and inference of several state
of the art rnn architectures by up to 9x. we abstract recent work on linear rnns
into a new framework of linear surrogate rnns and develop a linear surrogate
model for the long short-term memory unit, the gilr-lstm, that utilizes parallel
linear recurrence. we extend sequence learning to new extremely long sequence
regimes that were previously out of reach by successfully training a gilr-lstm
on a synthetic sequence classification task with a one million timestep dependency.

1

