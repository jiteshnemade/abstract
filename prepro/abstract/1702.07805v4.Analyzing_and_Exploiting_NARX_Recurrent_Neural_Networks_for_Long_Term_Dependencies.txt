
recurrent neural networks (rnns) have achieved state-of-the-art performance on
many diverse tasks, from machine translation to surgical activity recognition, yet
training rnns to capture long-term dependencies remains difficult. to date, the
vast majority of successful rnn architectures alleviate this problem using nearlyadditive connections between states, as introduced by long short-term memory
(lstm). we take an orthogonal approach and introduce mist rnns, a narx
rnn architecture that allows direct connections from the very distant past. we
show that mist rnns 1) exhibit superior vanishing-gradient properties in comparison to lstm and previously-proposed narx rnns; 2) are far more efficient
than previously-proposed narx rnn architectures, requiring even fewer computations than lstm; and 3) improve performance substantially over lstm and
clockwork rnns on tasks requiring very long-term dependencies.

1

