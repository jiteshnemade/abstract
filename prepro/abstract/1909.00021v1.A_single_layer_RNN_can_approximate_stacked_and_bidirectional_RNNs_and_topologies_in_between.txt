
to enhance the expressiveness and representational capacity of recurrent neural
networks (rnn), a large body of work has emerged exploring stacked architectures with additional topological modifications like shortcut connections or bidirectionality. however, choosing the best network for a particular problem requires
a combinatorial search over architectures and their hyperparameters. in this work,
we show that a single-layer rnn can perfectly mimic an arbitrarily deep stacked
rnn under specific constraints on its weight matrix and a delay between input and
output. this obviates the need to manually select hyperparameters like the number
of layers. additionally, we show that weakening weight constraints while keeping the delay gives rise to partial acausality in the single-layer rnn, much like a
bidirectional network. synthetic experiments confirm that the delayed rnn can
mimic bidirectional networks in perfectly solving some acausal tasks, outperforming them in others. finally, we show that in a challenging language processing
task, the delayed rnn performs within 0.3% of the accuracy of the bidirectional
network while reducing computational costs.

1 